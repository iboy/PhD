<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="441">
            <Title>Notes from Videos</Title>
            <Text>Contexts as/of Resistance

Check Out:

taxonomies of gesture

the weave

GOOGLE: anthropotechnic entities - Sylvia Navel (?)

--
SOURCE: http://www.dtic.mil/dticasd/docs/Human_Factors_Definitions.pdf

Bernotat, R. K., &amp; Gartner, K.-P. (1972)
Anthropotechnics and Human Engineering:

"'Anthropotechnics'...means and goals are similar to those of the American 'Human Engineering'...Anthropotechnics is the scientific discipline dealing with the interrelationship between man and machine and is aimed at the optimum of this functional unit in terms of efficiency, reliability, and cost effectiveness through the adaptation of the machine to man's capabilities and requirements." (p. 5)
"...try to adapt the human operator to the machine. This discipline which is oriented towards the "human factor", is largely a field activity of medical and psychological experts..." (p. 5)
"...adaptation of the machine to given human characteristics, i.e. 'Human Engineering' in a more restricted sense...Work in the field of Human Engineering calls for wide interdisciplinary knowledge covering a multitude of subjects such as special vehicle technology, psychology, physiology, control theory, information theory, experimental physics, electronics, measuring techniques, and so on." (p. 5)

--

http://www.sciy.org/2010/02/07/unweaving-the-program-stiegler-and-the-hegemony-of-technics-by-andres-vaccari/

"Unweaving the Program: Stiegler and the Hegemony of Technics" By Andrés Vaccari
http://www.transformationsjournal.org/journal/issue_17/article_08.shtml



http://www.ctheory.net/articles.aspx?id=496
Intelligence and Representability
Louis Armand

1000 Days of Theory: td024
Date Published: 11/16/2005
www.ctheory.net/articles.aspx?id=496
Arthur and Marilouise Kroker, Editors

EXCERPT: "Dependency upon the faculty of recognition is one way in which cybernetics can be seen to avail itself of a certain humanistic-theological movement, but at the same time it points to a very real problem as to how otherwise to conceive of intelligence as such, if intelligence is not simply to be regarded as a uniquely human attribute that must, in the absence of a deity, be projected, via the techné of computability, into the universe at large. Such a project would be enough if it were simply a question of immortalising the human legacy by way of machines: imparting sense to base matter, and thereby redeeming a world or a universe we now realise to be godless. To this extent, artificial intelligence may be regarded as a form of creationist revisionism, that puts man -- and by virtue of man, man's god -- "back" into the cosmic framework. Intelligence thus perceived functions abstractly as a type of Cartesian Artifex Maximus (the brain of god?) -- but like all gods, it exists only insofar as one believes that it does.


The export of human intelligence is not the same thing as arriving at a generalised understanding of what intelligence is, or of being able to develop, let alone recognise, what could properly be called "artificial" intelligence. And here lies the basic dilemma: is it possible to recognise, and consequently affect, an intelligence remotely different -- or even moderately different -- from human intelligence?

Considering that even the least intuitive models of organic intelligence describe mind and consciousness in terms of schematised mechanisms, on the one hand, or a complex of neuro-biological and environmental/experiential relations, on the other -- in short, a synthesis of the brain's and the entire central nervous system's activities -- then it is hardly surprising that intelligence thus conceived should be anthropocentric or anthropo-technic. Nor is it surprising that any psychological or physiological rationale for testing intelligence should proceed upon a human conception: the theoretical and practical sciences are after all human activities. We might tentatively describe the necessary tenets of such a project as Aristotelean, in that they are formally anthropomorphic and thereby "computable" in rationalistic terms. This distinction can be usefully clarified if we consider the relation of human intelligence to radically non-anthropomorphic types of organic "intelligence" (as a prelude to discussion of any "artificial" intelligence) -- such as the organisation of the nervous system in invertebrates, like those of the phylum Echinodermata, specifically the common starfish or Asteroidea."</Text>
        </Document>
        <Document ID="330">
            <Title>Journal Titles / Associations TO CHECK</Title>
            <Text>Digital Games Research Association
http://www.digra.org/


Performing Arts Resources - Volumes, 1974-Current
Theatre Library Association
http://tla-online.org/events/symposiumone/index.html



Performance Research
</Text>
        </Document>
        <Document ID="442">
            <Title>Proposal</Title>
            <Text>##  Thesis Title
Expressivity and the Digital Puppet: Mechanical, Digital and Virtual Objects in Games, Art and Performance


![Selected images from the project][Fig:selectionofprojectimages]

[Fig:selectionofprojectimages]: Selection of Project Images "Fig:selectionofprojectimages"



## Aim of the Investigation
The current study explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with *users* (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

In this context 'innovative' means both emerging, new, technology or established technologies that are being re-defined by their communities of use and are finding new applications within the performing arts, particularly puppetry performance.

I aim to explore the related contexts of digital puppetry, real-time animation, mimetic and non-mimetic kinetic objects, automata, 'cybernetic sculpture', performance systems and the technological interfaces to such phenomena. 

I aim to create evaluate and create puppet/object theatre performances/installations that use original software and hardware systems that are designed to explore 'performance expressivity', with reference to relevant historical, art, entertainment and technological precedents.

I wish to theorise and form a taxonomy of 'expressivity' in relationship to digital domains and puppetry. By 'expressivity', I refer to different domains of action including: voice, face, body, hands and gesture.

## Scope and Focus: finding patterns in an interdisciplinary study

The scope may first appear broad and the research focus wide. However, the researcher agrees with the statement:

"a cross-disciplinary approach yields information about patterning that is not visible from any single discipline." (Dorosh, 2008, 14, my emphasis)

Initially the surveyed areas and practice are diverse. The research progresses with the solid intent to articulate the patterns between (and the inter-relatedness of) cultural forms, and control systems, across computer media and performance art disciplines. In forming a taxonomy of 'expressivity' in relationship to digital control-systems and puppetry, a broad range of phenomena will need to be studied.

Focus is to be achieved through the practical work and case studies detailed in the 'Proposed Plan of Research', below.

## Details of your proposed research in lay terms
Many innovations in contemporary computing and the way we interact with machines have applications beyond the domains for which they are designed.

Computer vision for gesture recognition, touch surfaces (like the Apple's iPhone, Microsoft's Surface and TUIO systems), wireless control devices, accelerometers and game control devices, like data gloves, can be used beyond their originally designed purpose. Increasingly these technologies and the means to programme them are finding their way into the hands of non-specialists and are crossing boundaries of practice. The present study works in an interdisciplinary way between human-computer interaction/interface design, computer programming and the performance practice of puppetry, evaluating the reciprocal opportunity for innovative practice.

In my practice and research, I aim to create low-cost hardware and software that allows a skilled and unskilled puppeteers to control dynamic physical objects (like a robot) or a virtual object (like a game character) in ways that calibrate and test new interfaces for their expressive potential. Coming from a background in puppetry, where a special approach to gesture, movement and mechanism applies, I wish to find a playful fusion between emerging technology and performance traditions.

The established puppetry traditions, as categorised by control mechanisms and form: rods, shadows and glove, (one may add the direct manipulation of objects and the related area of toys and automata), have fascinating parallels with moving objects in the virtual worlds of interactive art and games. The thesis will clearly establish and test the boundaries of these parallel forms.


## Proposed plan of work, including its relationship to previous work
## Focus and Scope  
The proposed study is interdisciplinary and seeks to establish patterns between diverse cultural forms and technological practices. The focus for the study is created by analysing specific case studies in practice that involve tactile and gestural interaction with interfaces for expression and improvisation.

The current work has a practical, experimental and media archeological approach that seeks to explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

Although the area seems broad (for example, puppetry is a large subject within performance studies, has numerous world traditions and a long history), I find focus through particular practical case studies where I design and test interfaces for expression and improvisation centered around gestural interaction, touch, optical and physical capture of motion.

Puppetry has distinct practices in various world traditions, but in computer space can be defined as the expressive use of interfaces to control objects. It is necessary, for the thesis, to offer an extended definition of 'object', which becomes interesting in computational contexts as does 'expressive behavior' and 'affect'.

In exploration of practices akin to puppetry, the research draws on instances of performance, games and installation art practice in wider cultural practice and the practical explorations of the author. This focus is multi-disciplinary. 

The case studies are focused in the following ways: 

(1) Physical interfaces to on-screen (or virtual) performing and expressive objects and 
(2) Physical control of physical performing and expressive objects. I foresee multiple interfaces and approaches to objects will be designed, tested and used and may be shared and adapted between case studies.

## Research Questions
Through practical, theoretical and historical research I wish to explore and answer the following research questions:

Primary Question: What is 'expressivity' in the context of computer controlled and user controlled physical and virtual objects?

## Three main overarching questions
(1) How do new and emerging technologies facilitate innovative techniques of design and control of puppet-like objects?

(2) What are the most effective designs for interactive tools to create and sustain experiences of expressive play?

(3) How do the domains of traditional puppetry and emerging interactive technologies relate?

## Sub-Questions: Definitions and Detail
The following questions further refine and dimensionalise the key research questions.

What defines a digital puppet?
What defines a virtual puppet?
Can we describe the distinctions between virtual and physical kinetic expressive systems?
     - What is the difference between an automata and a puppet in the context of virtual 
       creatures, animatronics (performance robotics) and artificial-life?

How do current performance technologies facilitate expression?

What is 'expressivity' in the context of computer controlled objects?

What happens to 'expressive acts' when mediated through interactive technology? Eg.

- Gestural
- Vocal
- Whole Body (Viscerality and the wider sensorium of the body etc.)
- Facial
- Kinetics, movement and touch

What are the qualitative distinctions between live and captured movement?
How can we use movement capture as an analytic tool within puppetry studies?
How can we use movement capture as a tool for play within puppet performance?
What are the most effective methodologies for studying interdisciplinary puppetry practice?
To what extent is it useful to compare diverse virtual and physical kinetic forms as forms of digital puppetry?

## New Interfaces for Expression
What are the most effective interactive interfaces to capture the expressive acts of puppeteers?

How do we best describe the 'expressive potential' of custom interfaces for puppetry?

What are the perceptions of traditional puppeteers towards new technology and new interfaces for expression?

In the [Hello World](#methodsandmethodologies), we see a load of hot air!
 
## Methods and Methodologies – A Media Archeology of Kinetic Behavioural Sculpture [Methods and Methodologies]
The writing seeks to historicise in and around the spaces occupied by digital (or virtual) puppets, the avatar, automata, the robot, artificial life, and the animatronic. All these phenomena can be grouped, as by Reas (1996), in the realm of 'kinetic behavioural sculpture':

"A behavioral kinetic object is a dynamic system, meaning it changes with time. This system is composed of a source of energy, inputs, outputs, and a control architecture which converts the information from the inputs into information which stimulates the outputs. These elements sum to form the complete object, but other elements may be added to provide mass or form." (Reas 1996, 22)

An emerging methodological approach called media archeology, will be used to trace how traditional puppetry forms, often described by their mode of interaction/media, (rods, shadows, strings and gloves) - map into current paradigms of interaction with virtual worlds, digital objects, games, and automated animation; and associated areas of digitally enhanced dolls, toys and automata. Media Archeology is a field that reflects on today's technologies by linking them to the socio-technical histories out of which they emerged.

"There is a gang of artists, theoreticians, and artist-theoreticians who have a very strong affinity (moreover, one that links them to a figure such as Artaud): they burn and burn up in the endeavour to push out as far as possible the limits of what language and machines, as the primary instances of structure and order for the last few centuries, are able to express and in doing so to actually reveal these limits" (Siegfried Zielinski, 2009, my emphasis)

Ethnographic Methods - Thick Description of Performance and User Testing
My experience in ethnographic methodology also assists to bring the study within a particular social, cultural way of seeing.  How to systematically observe and perform thick description (Geertz, 1977) of contexts such as software design and study is relatively unique, but common in anthropology when interpreting performance culture and forms, such as puppetry.

Media archeology, in the way documents and artefacts are studied has an affinity with historical ethnography. The moment of study is past as well as present. I will apply ethnographic methods of theme analysis – observation, thick description, coding and dimensionalising -  to support a broader semiotic approach to the cultures and histories of digital puppetry.

"The concept of culture I espouse…is essentially a semiotic one. Believing with Max Weber, that a man [sic] is an animal suspended in webs of significance he himself has spun. I take culture to be those webs, and the analysis of it to be therefore not an experimental science in search of law but an interpretive one in search of meaning." (Geertz, 1977, 5)

Sherry Turkle (2005) has developed an approach to understand how we think and express through evocative objects. I argue digital puppets are a special class of evocative object. In addition to Turkle, I will consider multiple approaches to theorising the object in cultural practice: again this is drawn from a variety of domains (e.g. John Dewey's expressive object in Art as Experience (Dewey, 2005) to Baudrillard's complex system of objects (Baudrillard, 1988). I have started this work in relation to automata and talking toys (see the published works).

Puppetry, Kinetic Sculpture and Gestural Interaction
All styles of puppet manipulation rely on gestural interaction. The study considers the performative/expressive potential of numerous computer interaction systems that utilise tactility and touch, whole-body, face, hand interactions. In a cultural study of expressive automata and toys, the study considers the role of the sonic in puppetry: the rhythmic gesture, sound and voice.

I wish to explore: How do new and emerging technologies facilitate innovative techniques of design and control over puppet-like objects and create experiences of expressive play?

The human body in movement and the computational capture of such movement to make meaning through expressive acts mediate by evocative objects – is another way to state the primary exploration of the thesis.

"The world of objects and needs would thus be a world of general hysteria. Just as the organs and functions of a body in hysterical conversion become a gigantic paradigm which the symptom replaces and refers to, in consumption objects become a vast paradigm designating another language through which something else speaks," (Baudrillard:1988, 10-29)

Plan of Literature Review, Research, Practical Work and Writing
Below (FIG: GANTT), I supply a static image of a dynamic Gantt chart detailing an approximate time-plan of how the practical work relates to the evaluation of literary research material and writing activity.

The GANNT chart is a snapshot produced by an interactive project management tool, OmniPlan, and is dynamic. It changes as research, practice and writing activities develop. A current, zoomable, snapshot can be found here:

&lt;!--\url{http://www.daisyrust.com/phd/dissertation_timeplan_current_ian_grant.png}--> 

## Establishing Cultural Patterns through Practice
I will produce and evaluate: 

(i) Performance, Artwork and Kinetic Behavioural Sculpture; 
(ii) Original software, software art and computer based creative production techniques; 
(iii) Innovative control systems and interfaces that will be applicable hopefully beyond the domain of performance; 
(iv) Papers, videos, websites and published outcomes that will contribute to the thesis, documentation and other elements of the research.

As a minimum, I plan to make and test: 
(i) Several expressive, physical objects; 
(ii) Several virtual objects, that are controlled by
(iii) Several innovative control systems, in prototype, including touch surfaces, optical motion/expression/gesture capture, digital input devices.

It will be ideal if the systems are tested in performance or in installation contexts. Standard HCI user testing methods, including focus groups, cognitive walkthroughs, Goals, Operators, Methods, and Selection Rules (GOMS) analysis, will be employed, evaluated and discussed where relevant.

Relationship to Previous Work: Comparative Case Studies of Practice
The thesis will analyse case study material, including software and performances by the author, other artists, puppeteers, animators and sculptors. Case studies include:

* Theo Jansen's Walker and his Staadcreatures: Covering ideas of Virtual Creatures, Object Orientedness, Automata and Computer Simulated Physics; Cybernetic Sculpture; See (Jansen 2007)


Figure 1: Theo Jansen Walker As Physical, Virtual and Textual (Code) Object
* Surfaces and Shadows: Synchronous and Asynchronous Techniques: 
	- Lotte Reiniger, Stop motion animator
* Phil Worthington "Shadow Monsters". Animation and Computer Vision: AR installations;

Figure 2: Phil Worthington "Shadow Monsters" 2005
* Golan Levin: New media artist. (Reface [Portrait Sequencer] 2007), Snout (2008) Double-Taker (Snout) (2008) Opto-Isolator (2007) and other projects: face and expression recognition; automata and robotics; video hybrids;

  
Figure 3: Golan Levin - physical and virtual projects (2006-2009)
* Embodied Interactions: finger, hand, whole-body, gestural, eye and facial interactions;

Examples of how these areas map with my own work can be see in Ian Grant "Video Hybrids, explorations in digital puppetry", presented as a performance piece, software and documentary write up (see sample chapter).

## Dissertation Writing Practice
The thesis type-setting is automated using a LATEX template (created by the author using University of Sussex standards / guidelines) and a BIBTEX database. 

## Registration Document References

Baudrillard, Jean and Mark.Poster. Selected writings (of) Jean Baudrillard. Stanford University Press, Stanford (Calif.), 1988.

Dewey, John. Art as Experience. Perigee Trade, 2005.

Dorosh, D. Patterning: The Informatics of Art and Fashion. PhD Thesis. UEL. Awarded in June 2008.

Geertz, Clifford.The Interpretation of Cultures. Basic Books, NY. 1977.

Jansen, Theo. The Great Pretender. 010 Uitgeverij, 2007.

Levin, Golan. Projects. &lt;!--\url{http://www.flong.com/projects/}--> Date modified: 2009. Date Accessed: 1st June 2009.

Reas, Casey. Behavioral kinetic sculpture. Master's thesis, MIT, 1996.

Turkle, Sherry. Computer Games As Evocative Objects: From Projective Screens. To Relational Artifacts  in Raessens, Joost and Jeffrey Goldstein "Handbook of Computer Game Studies" MIT Press, London, 2005.

Zielinski, Siegfried. Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. MIT Press, 2008.

Zielinski, Siegfried. Media Archaeology. &lt;!--\url{http://www.ctheory.net/articles.aspx?id=42#text%201}--> Date modified: 7th Nov, 1996. Date Accessed: 1st June 2009.

## Example Writing
For an extended abstract and sample writing, please see the sample chapters supplied for annual review AY08-09, AY09-10. The annual review documents and work in progress statements are available here:

Link: &lt;!--\url{http://www.daisyrust.com/phd/}-->


## Current Research Bibliography
The current bibliographic database (BIBTEX) contains 180 references already considered in the literature review. For brevity, I only include references included in several important categories. An asterisk (*) indicates critical and key texts. There may be some repeated entries where items are cross-referenced. I (mostly) exclude instructional texts in computer technologies (e.g. software and programming text-books, e.g. iPhone programming, OpenGL Red Book, OpenCV, etc.). 



### Puppetry, theory, history, technology, digital puppetry; 
(1)	Bacon, M. No Strings Attached: Inside Story of Jim Henson's Creature Shop. Virgin Books, 1997.  
(2)	Baird, B. The Art of the Puppet. With illustrations. Ridge Press, Inc., 1965.  
(3)	Bar-Lev, A., Bruckstein, A. M., and Elber, G. Virtual marionettes: a system and paradigm for real-time 3d animation3d animation. The Visual Computer 21 (2005), 488–501.  
(4)	Baran, I., and Popović, J. Automatic rigging and animation of 3d characters. Proceedings of the 2007 SIGGRAPH conference 26 (2007).  
(5)	Barnes, C., Jacobs, D., Sanders, J., Goldman, D., Rusinkiewicz, S., Finkelstein, A., and Agrawala, M. Video puppetry: a performative interface for cutout animation. SIGGRAPH Asia '08: SIGGRAPH Asia 2008 papers (Dec 2008).  
(6)	Baudelaire, C. The philosophy of toys. In Essays on Dolls, I. Parry, Ed. Syrens, London, 1994.  
(7) * 	Bell, J. Strings, Hands, Shadows: A Modern Puppet History, illustrated edition ed. Wayne State University Press, 2000.  
(8) * 	Bell, J., Ed. Puppets, Masks, and Performing Objects. MIT Press, Boston, 2001.  
(9)	Bicat, T. Puppets and Performing Objects: A Practical Guide. The Crowood Press Ltd, 2007.  
(10)	Blumenthal, E. Puppetry and Puppets: An Illustrated World Survey. Thames &amp; Hudson, 2005.  
(11)* Collective author. PUCK no. 9 :Images virtuelles Thematic review. Institut international de la Marionnette, CHARLEVILLE-MEZIERES, 1996.  
(12)* Conner, S. Dumbstruck: A Cultural History of Ventriloquism. Oxford University Press, New York, 2000.  
(13)	Currell, D. Puppets and Puppet Theatre. The Crowood Press Ltd, 1999.  
(14)	Hall, V. Mike (the talking head). &lt;!--\url{http://mambo.ucsc.edu/psl/mike.html}-->, Date Created: nodate. Date Accessed: 01/02/2007.  
(15)	Jurkowski, H. Aspects of Puppet Theatre. Puppet Centre Trust, 1988.  
(16)	Jurkowski, H., and (Editor), P. F. A History of European Puppetry from Its Origins to the End of the 19th Century: Volume 1. Edwin Mellen Press Ltd, 1996.  
(17)	Jurkowski, H., and (Editor), P. F. A History of European Puppetry: The Twentieth Century. Volume 2. Edwin Mellen Press Ltd, 1998.  
(18)* Kaplin, S. A puppet tree: A model for the field of puppet theatre. TDR (1988-) 43 (Oct 1999), 28–35. Puppets, Masks, and Performing Objects.  
(19)	Kaplin, S. A puppet tree - a model for the field of puppet theatre. In Puppets, Masks, and Performing Objects, J. Bell, Ed. MIT Press, Boston, 2001, pp. 18–25.  
(20)* Knep, B., Hayes, C., Sayre, R., and Williams, T. Dinosaur input device. CHI '95: Proceedings of the SIGCHI conference on Human factors in computing systems (May 1995).  
(21)* Lecoq, J. The Moving Body: Le Corps Poetique, revised edition ed. Methuen Drama, 2002.  
(22)	Mack, J. Masks: The Art of Expression, new edition ed. British Museum Press, 1996.  
(23)	Magnenat-Thalmann, N. The making of a film with synthetic actors. Leonardo. Supplemental Issue 1 (Jan 1988), 55–62. Electronic Art.  
(24)	Mazalek, A., and Nitsche, M. Tangible interfaces for real-time 3d virtual environments. ACE '07: Proceedings of the international conference on Advances in computer entertainment technology (Jun 2007).  
(25)* Meschke, M., and Sörenson, M. In Search of Aesthetics for the Puppet Theatre. Indira Gandhi National Centre for the Arts, New Delhi, 1992.  
(26)	Ninomiya, D., Miyazaki, K., and Nakatsu, R. Networked virtual marionette theater. In Technologies for E-Learning and Digital Entertainment. Springer Berlin / Heidelberg, 2008.  
(27)	Obraztsov, S. My Profession. Fredonia Books, 2001.  
(28)	Parry, I. Essays on Dolls. Syrens, London, 1994.  
(29)	Rilke, R. M. Dolls: On the wax dolls of lotte pritzel. In Essays on Dolls, I. Parry, Ed. Syrens, London, 1994.  
(30)	Sch&lt;!--\"{o}-->newolf, H. Play with Light and Shadow: Art and Techniques of Shadow Theatre. Littlehampton Book Services Ltd, 1969.  
(31)* Segal, H. B. Pinocchio's Progeny : Puppets, Marionettes, Automatons and Robots in Modernist and Avant-Garde Drama. Johns Hopkins University Press, Baltimore, 1995. Normal Loan 791.53 SEG.  
(32)	Shi, X., Zhou, K., Tong, Y., Desbrun, M., Bao, H., and Guo, B. Mesh puppetry: cascading optimization of mesh deformation with inverse kinematics. International Conference on Computer Graphics and Interactive Techniques (2007).  
(33)	Silk, D. William the Wonder-Kid: Plays, Puppet Plays, and Theater Writings. Sheep Meadow Press, U.S., 1996.  
(34)	Tillis, S. Toward an Aesthetics of the Puppet: Puppetry as a theatrical art. Greenwood, New York, 1992. Normal Loan 791.53 TIL.  
(35)* Tillis, S. The art of puppetry in the age of media production. In Puppets, Masks, and Performing Objects, J. Bell, Ed. MIT Press, Boston, 2001, pp. 172–183.  

### 3D Graphics, Programming, Virtual and Augmented Reality
(1)*  Fiala, S. C. M. Augmented Reality: A Practical Guide: The Complete Guide to Understanding and Using Augmented Reality Technology. Pragmatic Bookshelf, 2008.  
(2)	Gobbetti, E., Balaguer, J., and Thalmann, D. Vb2: an Architecture for Interaction in Synthetic Worlds. Proceedings of the 6th annual ACM symposium on User interface software and technology (1993), 167–178.  
(3)	Govindaraju, V., Djeu, P., Sankaralingam, K., Vernon, M., and Mark, W. Toward a Multicore Architecture for Real-Time Ray-Tracing. MICRO '08: Proceedings of the 2008 41st IEEE/ACM International Symposium on Microarchitecture (Nov 2008).  
(4)	Junker, G. Pro OGRE 3D Programming (Expert's Voice in Open Source). APress,US, 2006.  
(5)* Mazalek, A., and Nitsche, M. Tangible interfaces for real-time 3d virtual environments. ACE '07: Proceedings of the international conference on Advances in computer entertainment technology (Jun 2007).  
(6)	Salti, S., Schreer, O., and Stefano, L. Real-time 3d arm pose estimation from monocular video for enhanced hci. VNBA '08: Proceeding of the 1st ACM workshop on Vision networks for behavior analysis (Oct 2008).  
(7)*	Zhai, S. Human Performance in Six Degree of Freedom Input Control. PhD thesis, University of Toronto, 1995.

### Augmented Reality Techniques
(1)*	Billinghurst, M., Kato, H., and Poupyrev, I. Tangible augmented reality. SIGGRAPH Asia '08: SIGGRAPH ASIA 2008 courses (Dec 2008).  
(2)	McQuiggan, S., Rowe, J., and Lester, J. The effects of empathetic virtual characters on presence in narrative-centered learning environments. CHI 2008, April 5–10 Papers (2008).  
(3)	Mistry, P., Maes, P., and Chang, L. Wuw - wear ur world: a wearable gestural interface. CHI EA '09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).  
(4)	Molyneaux, D., and Gellersen, H. Projected interfaces: enabling serendipitous interaction with smart tangible objects. TEI '09: Proceedings of the 3rd International Conference on Tangible and Embedded Interaction (Feb 2009).  
(5)*	Wang, R., and Popović, J. Real-Time Hand-Tracking with a Color Glove. SIGGRAPH '09: SIGGRAPH 2009 papers (July 2009).  

### Digital Art, Objects, Virtuality and Performance
(1)* Blundell, B. G. An Introduction to Computer Graphics and Creative 3-D Environments. Springer, 2008.  
(2)*	Candlin, F., and Guins, R. The Object Reader. Routledge, 2008.  
(3)	Candy, L., and Edmonds, E. Explorations in Art and Technology: Intersections and Correspondence. Springer, 2002.  
(4)*	Dixon, S. Digital Performance, annotated edition ed. The MIT Press, 2007.  
(5)	Ede, S. Art and Science. I B Tauris &amp; Co Ltd, 2008.  
(6)	Gabor, D. Technological civilisation and man's future. In Cybernetics, art and ideas, J. Reichardt, Ed. Studio Vista, London, 1971, pp. 18–24.  
(7)	Grau, O. Virtual Art: From Illusion to Immersion (Leonardo Book S.), rev sub ed. The MIT Press, 2003.  
(8)*	Grau, O. MediaArtHistories. The MIT Press, 2007.  
(9)*	Jansen, T. Theo Jansen: The Great Pretender. 010 Uitgeverij, 2007.  
(10)	Mealing, S. Computers and Art, 2nd revised edition ed. Chicago University Press, 2008.  
(11)	Reichardt, J., Ed. Cybernetics, art and ideas. Studio Vista, London, 1971.

### Computer Vision Techiques for Digital Puppetry
(1)*	Barnes, C., Jacobs, D., Sanders, J., Goldman, D., Rusinkiewicz, S., Finkelstein, A., and Agrawala, M. Video puppetry: a performative interface for cutout animation. SIGGRAPH Asia '08: SIGGRAPH Asia 2008 papers (Dec 2008).  
(2)	Cabral, M., Morimoto, C., and Zuffo, M. On the Usability of Gesture Interfaces in Virtual Reality environments. CLIHC '05: Proceedings of the 2005 Latin American conference on Human-computer interaction (Oct 2005).  
(3)	Datcu, D., and Rothkrantz, L. Facial Expression Recognition in Still Pictures and Videos Using Active Appearance Models: a Comparison Approach. Proceedings of the 2007 international conference on Computer systems and technologies (2007).  
(4)	Salti, S., Schreer, O., and Stefano, L. Real-time 3d arm pose estimation from monocular video for enhanced hci. VNBA '08: Proceeding of the 1st ACM workshop on Vision networks for behavior analysis (Oct 2008).  
(5)*	Vlasic, D., Baran, I., Matusik, W., and Popović, J. Articulated mesh animation from multi-view silhouettes. SIGGRAPH '08: SIGGRAPH 2008 papers (Aug 2008).  


### Tangible Interaction and Embedded Control Technologies
(1)*	Barnes, C., Jacobs, D., Sanders, J., Goldman, D., Rusinkiewicz, S., Finkelstein, A., and Agrawala, M. Video puppetry: a performative interface for cutout animation. SIGGRAPH Asia '08: SIGGRAPH Asia 2008 papers (Dec 2008).  
(2)	Billinghurst, M., Kato, H., and Poupyrev, I. Tangible augmented reality. SIGGRAPH Asia '08: SIGGRAPH ASIA 2008 courses (Dec 2008).  
(3)	Fishkin, K. A Taxonomy for and Analysis of Tangible Interfaces. Personal and Ubiquitous Computing 8 (2004), 347–358.  
(4)*	Guo, C., Young, J., and Sharlin, E. Touch and Toys: New Techniques for Interaction With a Remote Group of Robots. CHI '09: Proceedings of the 27th international conference on Human factors in computing systems (Apr 2009).  
(5)	Mazalek, A., and Nitsche, M. Tangible interfaces for real-time 3d virtual environments. ACE '07: Proceedings of the international conference on Advances in computer entertainment technology (Jun 2007).  
(6)	Mistry, P., Maes, P., and Chang, L. Wuw - wear ur world: a wearable gestural interface. CHI EA '09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).  
(7)	Molyneaux, D., and Gellersen, H. Projected interfaces: enabling serendipitous interaction with smart tangible objects. TEI '09: Proceedings of the 3rd International Conference on Tangible and Embedded Interaction (Feb 2009).  
(8)	Vertegaal, R., and Poupyrev, I. Eek!  a mouse!  organic user interfaces: tangible, transitive materials and programmable reality. CHI EA '09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).  
(9)	Weller, M., Do, E., and Gross, M. Posey: instrumenting a poseable hub and strut construction toy. TEI '08: Proceedings of the 2nd international conference on Tangible and embedded interaction (Feb 2008).  
(10)	Weller, M., Gross, M., and Do, E. Tangible sketching in 3d with posey. CHI EA '09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).  
(11)	Zaman, B., Abeele, V., Markopoulos, P., and Marshall, P. Tangibles for children,: the challenges. CHI EA '09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).  

### Tangible Interaction and Embedded Control Technologies (interactive toys)
(1)	Newman, M. Interactive Barney: Good or evil?  Conferees worry about where computerized 'character' toys are going next. &lt;!--\url{http://www.post-gazette.com/businessnews/19990521barney1.asp}-->, Date Modified: 21/05/1999. Date Accessed: 01/03/2007.  
(2)	Parent, A. Read Reviews of Hasbro Aloha Stitch Doll 3570 at eOpinions. &lt;!--\url{http://www.epinions.com/content_163285929604?linkin_id=8003929}-->, Date Created: 28/11/2004. Date Accessed: 01/02/2007.  
(3)	Shenk, D. Behold the Toys of Tomorrow (The Atlantic Online - Digital Culture). &lt;!--\url{http://davidshenk.com/webimages/atlantic1.htm}-->, Date Created: 07/01/1999. Date Accessed: 01/02/2007.  
(4)	Strommen, E. F. When the Interface is a Talking Dinosaur: Learning Across Media with ActiMates Barney. &lt;!--\url{http://www.playfulefforts.com/archives/papers/CHI-1998.pdf}-->, Online PDF of published work. Date Written: 1998. Date Accessed: 01/03/2007.  
(5)	Strommen, E. F. Learning from Television With Interactive Toy Characters As Viewing Companions. &lt;!--\url{http://www.playfulefforts.com/archives/papers/SRCD-1999.pdf}-->, Online PDF of published work. Date Written: 1999. Date Accessed: 01/03/2007.  
(6)*	Strommen, E. F. Interactive Toy Characters as Interfaces For Children. &lt;!--\url{http://www.playfulefforts.com/archives/papers/IA-2000.pdf}-->, Online PDF of published work. Date Written: 2000. Date Accessed: 01/03/2007.  
(7)	Strommen, E. F. Play?  Learning?  Both...or neither? &lt;!--\url{http://www.playfulefforts.com/archives/papers/AERA-2004.pdf}-->, Online PDF of unpublished work. Date Written: 2004. Date Accessed: 01/03/2007.  


### Methods (Cultural Studies, Performance Studies and Theatre Anthropology)
(1)*	Candlin, F., and Guins, R. The Object Reader. Routledge, 2008.  
(2)*	Eugenio Barba, N. S. A Dictionary of Theatre Anthropology: The Secret Art of the Performer. Routledge, 1991.  
(3)*	Turkle, S. Evocative Objects: Things We Think with. The MIT Press, 2007.  
(4)	Zielinski, S. Media Archaeology, November 1996.  
(5)*	Zielinski, S. Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. MIT Press, 2008.  



## Summary Of The Elements Of The Investigation That Are Novel, Original Or Creative And That May Constitute Production Of Original Knowledge Or An Original Interpretation Of Existing Knowledge
- A deepening of the material and thought and cultural philosophy surrounding puppetry studies and technology;

- Original Performance, Artwork and Artefacts;

- Original Software products and creative production techniques;

-  New hardware control systems and interfaces for expressive play that will be applicable hopefully beyond the domain of performance and digital puppetry;

- Papers and published outcomes that will contribute to the thesis, documentation and other elements of the PhD.</Text>
        </Document>
        <Document ID="331">
            <Title>Untitled</Title>
        </Document>
        <Document ID="220">
            <Title>figure_001_composite</Title>
        </Document>
        <Document ID="443">
            <Title>Article Notes</Title>
        </Document>
        <Document ID="332">
            <Title>Spatharis - Karaghiozi</Title>
            <Text>Notes on Karaghiozi Posters

Karaghiozi in Albania: 1940
The epic of the Albanian campaign was of decisive importance in modern Greek history.
During that campaign, the Greeks demonstrated their faith in liberty and the ideals of their race - the ideals on which they had been nurtured for generation after generation. Greece decided to fight in October 1940, when Italy declared war on it, and despite the enormous difficulties was victorious over the forces of autocracy and Fascism. The message the Greeks sent out at that time was one of Liberty, Democracy and the right of all mankind, values which are deeply rooted in the soul of every Greek - and, of course, of Karaghiozi. </Text>
        </Document>
        <Document ID="221">
            <Title>figure_010_minnie_hell</Title>
        </Document>
        <Document ID="110">
            <Title>6.3.3 Custom Software</Title>
            <Text>4.3.3 Custom Software
</Text>
        </Document>
        <Document ID="444">
            <Title>Untitled</Title>
            <Text>CFP: MULTIPLE SENSORIAL MEDIA ADVANCES AND APPLICATIONS: NEW DEVELOPMENTS IN MULSEMEDIA
Author: Ian Grant
Role/Affiliation: 
Acting Deputy Head of School, Art and Design. Faculty of the Arts. Thames Valley University. Ealing, UK. W5 5DX.
Email: &lt;ian.grant@tvu.ac.uk>
Proposal Title: Multiple Sensory Media and Expressive Play
---start---
What are the most effective designs for multiple sensorial media when creating and sustaining experiences of expressive play?
Computer vision for gesture recognition, touch surfaces (like the Apple's iPhone, Microsoft's Surface and TUIO systems), wireless control devices, accelerometers and game control devices, like data gloves, can be used beyond their originally designed purpose. Increasingly these technologies and the means to programme them are finding their way into the hands of non-specialists and are crossing boundaries of practice. The present chapter works in an interdisciplinary way between human-computer interaction/interface design, computer programming and the performance practice of puppetry, evaluating the reciprocal opportunity for innovative multi-modal practice.
The established puppetry traditions, as categorised by control mechanisms and form: rods, shadows and glove, (one may add the direct manipulation of objects and the related area of toys and automata), have fascinating parallels with moving objects in the virtual worlds of interactive art and games. The chapter will clearly establish and test the boundaries of these parallel forms.
An emerging methodological approach called media archeology, will be used to trace how traditional puppetry forms, often described by their mode of interaction/media, (rods, shadows, strings and gloves) - map into the emerging paradigm of multiple sensory interaction and multi-modal practice.
Puppetry and mulsemedia are broad areas. The chapter will find focus through particular case studies where artists, scientists and designers create computer forms for expression and improvisation centred around gestural interaction, touch, and the optical and physical capture of motion.
Puppetry has distinct practices in various world traditions, but in computer space can be defined as 'the expressive use of interfaces to control objects'. It is necessary, for the chapter, to offer an extended definition of 'object', 'expressive behaviour' and 'computational affect', which become interesting in the context of mulsemedia.

(count 300)
---end---
Keywords

Multi-modal interaction
Mulsemedia and virtual reality
Tactile/haptic interaction
Mulsemedia and Play
Mulsemedia and Expression
Mulsemedia and Performance



Ian Grant, 20 December 2009
</Text>
        </Document>
        <Document ID="333">
            <Title>Shadows_Screenshot_iPad_Physics_Touch_Animation_001</Title>
        </Document>
        <Document ID="222">
            <Title>figure_009_cab_poster</Title>
        </Document>
        <Document ID="111">
            <Title>Screen</Title>
            <Text>4.4 Screen
</Text>
        </Document>
        <Document ID="112">
            <Title>6.4.1 Multiscreen and 3D Volumetric Displays</Title>
            <Text>4.4.1 Multiscreen and 3D Volumetric Displays
</Text>
        </Document>
        <Document ID="445">
            <Title>New Folder</Title>
        </Document>
        <Document ID="334">
            <Title>Shadows_Screenshot_iPad_Physics_Touch_Animation_002</Title>
        </Document>
        <Document ID="223">
            <Title>figure_012_3D_Scene_02</Title>
        </Document>
        <Document ID="190">
            <Title>Untitled</Title>
        </Document>
        <Document ID="113">
            <Title>Other Control Technology</Title>
            <Text>Other Control Technology
</Text>
        </Document>
        <Document ID="446">
            <Title>Mulsemedia</Title>
        </Document>
        <Document ID="335">
            <Title>Shadows_Screenshot_iPad_Physics_Touch_Animation_003</Title>
        </Document>
        <Document ID="224">
            <Title>figure_011_3D_Scene_01</Title>
        </Document>
        <Document ID="80">
            <Title>Background</Title>
            <Text>Background</Text>
        </Document>
        <Document ID="191">
            <Title>Glossary</Title>
            <Text>\printglossary</Text>
        </Document>
        <Document ID="114">
            <Title>Touch Surfaces</Title>
            <Text>Touch Surfaces
</Text>
        </Document>
        <Document ID="447">
            <Title>Untitled</Title>
            <Text>
As promised I am writing to support you with the direction of content for the Mulsemedia book. We will probably group your chapter with another that looks at use of mulsemedia for entertainment. 

The fact that your work is not VR, and is not the perceived normal type of entertainment solution is good. 

Your provision of interactive theatre is good as it implies that these technologies can be applied more widely accepted in the entertainment domain. 

Interaction is a key area of interest for me, as it allows anyone to get involved  - especially interesting when combined with multi-touch technology, such as the I-pad. 

The concepts of how mulsemedia is used, 
how perception is managed, 
techniques and need for interaction, 

background on the technologies involved and / or details of the prototype involved would be interesting. 

I would personally be interested in informal responses from uses (but that is because of my research background), and potential areas where you think mulsemedia supports the arts.

Moreover, you may focus of how mulsemedia increases 
immersion and / or 
dissemination and / or accessbiliy and supports older entertainment forms (such as shadows puppets) onto new devices, etc.



I hope this gives you some interesting headings !? If not please get in touch. I will leave you to think about this, but please keep me in the loop of how it is going. We have informed the IGI formatting team that there are two chapters coming (so you are not the only one), however the sooner we get a good draft the better. Your word limit is 10,000, so you are welcome to bring in a supporting author if this will help you complete something in time. 

Something by end of next week is a must!

Stephen Gulliver</Text>
        </Document>
        <Document ID="336">
            <Title>Quotations and Notes - Baudrillard</Title>
            <Synopsis>[Baudrillard (2005 (1996))][#Baudrillard:2005vn]</Synopsis>
            <Text>
: "Each of our practical objects is related to one or more structural elements, but at the same time they are all in perpetual flight from technical structure towards their secondary meanings, from the technological system to the cultural system." [p.6][#Baudrillard:2005vn]

the industrial to the craft object

the useless object, the gadget (French trans. lays stress on gadget as novelty item with no function or use value

the personalisation of objects

perceptual materiality
automatism - perfection and making automatic:

: "When [an object] becomes automatic, its function is fulfilled." [p.118][#Baudrillard:2005vn]
: "Because the automated object 'works by itself', its resemblance to the autonomous human being is unmistakable... . We are in the presence of a new anthropomorphism. ... it is no longer his [sic.] gestures, his energy, his needs and the image of his body that man projects into automated objects, but instead the autonomy of his consciousness, his power of control, his own individual nature, his personhood." [p.120][#Baudrillard:2005vn]

obsessive manipulation and contemplation [p.122][#Baudrillard:2005vn]
: "Like all obsessions, this particular variety [of aesthetic object] has its poetic side, as manifested to a greater or lesser degree in Pacabia's machines, in Tinguely's mechanical constructions, in the simple clockwork of a discarded watch, or in any object whose original use we simply cannot remember but whose mechanism still arouses a sort of delighted fascination in us. Something that serves no purpose whatsoever may in this sense still serve us" [p.122-123][#Baudrillard:2005vn]

an aesthetic approach -- subjectively functional (obsessional) omits function and becomes concerned with an exaltation of 'pure mechanism'.
: "No sooner does an object lose its concrete practical aspect than it is transferred to the realm of mental practices. In short, behind every real object is a dream object." [p.126][#Baudrillard:2005vn]
: "Modes of the imaginary follow modes of technological evolution, and it is therefore to be expected that the next mode of technical efficiency will give rise to a new imaginary mode. At present its traits are difficult to discern, in the wake of the animistic and energetic modes, we shall need to turn our attention to the structures of a cybernetic imaginary mode... ." [p.127][#Baudrillard:2005vn]
: "The current fashion for `happenings' has brought the great science-fiction event of the `suicide' or murder of the object a little closer to home. The happening involves an orgiastic destruction and debasement of objects, a veritable hecatomb whereby our whole satiated culture revels in its own degradation and death." [p.132][#Baudrillard:2005vn]

To digress: the first video of an iPad bought and instantly destroyed found it's way onto YouTube - [find DATE].

Discuss GLITCH, the emotion / responsive reactions of playing with objects also under the control of the simulation of physics. BROKEN, CRUELTY, CONTROL.
The psychodynamics of controlling expressive objects. 
Autonomy - out of control...
 

</Text>
        </Document>
        <Document ID="81">
            <Title>Definition: Digital Puppetry</Title>
            <Text>Definition: Digital Puppetry</Text>
        </Document>
        <Document ID="225">
            <Title>figure_wii_controlled_avatar_001</Title>
        </Document>
        <Document ID="192">
            <Title>Abstract - Old</Title>
            <Text>The current study explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with *users* (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

In order to define *expressivity* in the context of computer mediated performance, I wish to explore the gestural and kinetic properties of digital puppetry, also known as performance animation.

The thesis will:

- Define The Expressive;
 + As a Quality Of Digital Performance;
 + As an Outcome Of Interpretation / Reception of the Kinetic Qualities of Real-time Animation;
 + As a Quality Of Design;

Then I seek to practically:
- Explore and Evaluate Digital Puppetry Contexts;
- Create Custom Software to Facilitate Real-time Animation and Performance;
 
In exploration of practices akin to puppetry, the research draws on instances of performance, games and installation art practice in wider cultural practice and the practical explorations of the author. This focus is multi-disciplinary. 

The writing seeks to historicise in and around the spaces occupied by digital (or virtual) puppets, the avatar, automata, the robot, artificial life, and animatronic the in the realm of the 'kinetic behavioural sculpture' [][#Reas:1996p1313]

A methodological approach, resonant with a *media archeology*, will trace how traditional puppetry forms, often described by their mode of interaction/media, (rods, shadows, strings and gloves) -- map into current paradigms of interaction with virtual worlds, digital objects and automated animation; and associated areas of technologically enhanced dolls, toys and automata

Sherry Turkle has developed an approach to understand how we think and express through *evocative objects*. I argue digital puppets are a special class of evocative object. In addition to Turkle, I will consider multiple approaches to the theorisation of the object in technological cultural practice: again this is drawn from a variety of domains (e.g. John Dewey's *expressive objects* in *Art as Experience* [][#Dewey:2005jy] to Baudrillard's complex *system of objects* [][#Baudrillard:1988la].

All styles of puppet manipulation rely on gestural interaction. The study considers the performative/expressive potential of numerous computer interaction systems (including computer vision techniques) that utilise tactility and touch, whole-body, face, hand interactions. In a cultural study of expressive automata and toys, the study considers the role of the sonic in puppetry: the rhythmic gesture, sound and voice.

The thesis will answer: How do new and emerging technologies facilitate innovative techniques of design and control over puppet-like objects and create experiences of expressive play?

The human body in movement, and the computational capture of such movement to make meaning through expressive acts mediate by evocative objects, is a major exploration of the thesis.

"The world of objects and needs would thus be a world of general hysteria. Just as the organs and functions of a body in hysterical conversion become a gigantic paradigm which the symptom replaces and refers to, in consumption objects become a vast paradigm designating another language through which something else speaks," [p.10-29][#Baudrillard:1988la]

The thesis will analyse case study material, including software and performances by the author, other artists, animators and sculptors:

- Theo Jannsen's Walker and his Staadcreatures: Covering ideas of Virtual Creatures, The Simulation of the Mechanical Object, Object Orientedness, Automata and Physics-Based Animation (PBA);
- Surfaces and Shadows: Synchronous and Asynchronous Techniques: Lotte Reiniger, Animation and Computer Vision: Augmented Reality installations (Worthington 2007, Levin 2000-2010); New media installations and computer vision techniques established by Myron Krueger (1991);
- Embodied Interactions and Cybernetic Games: finger, hand, whole-body, gestural, eye and facial interactions; Digital Puppetry as a Metaphor;
</Text>
        </Document>
        <Document ID="82">
            <Title>Definition: Behavioural Kinetic Sculpture</Title>
            <Text>Behavioural Kinetic Sculpture</Text>
        </Document>
        <Document ID="115">
            <Title>Surfaces</Title>
            <Text>Surfaces
</Text>
        </Document>
        <Document ID="448">
            <Title>Meta-Data</Title>
            <Text>latex input:mmd-dissertation-header  
Title: Expressivity and the Digital Puppet: Mechanical, Digital and Virtual Objects in Games, Art and Performance (Research Proposal and Plan)
Author: Ian John Grant
Description: A DISSERTATION  
SUBMITTED IN PARTIAL FULFILMENT OF THE REQUIREMENTS  
FOR THE DEGREE OF  
DOCTOR OF PHILOSOPHY  
IN CREATIVE MEDIA PRACTICE
Department: Presented to the School of Media, Film and Music.  
of the University of Sussex (United Kingdom)
Revision: 004
Base Header Level: 2  
latex mode: memoir  
Keywords: Puppetry, Technology, Performance, Interaction, Games, Real-Time Graphics   
CSS: http://fletcherpenney.net/css/document.css  
copyright: 2011 Ian John Grant.  
This work is licensed under a Creative Commons License. http://creativecommons.org/licenses/by-nc-sa/3.0/
mybibliostyle: agsm
BibTex: ../../Bibliography/Master
latex input: mmd-natbib-plain
latex input: mmd-dissertation-begin-doc  
latex footer: mmd-dissertation-footer  
</Text>
        </Document>
        <Document ID="337">
            <Title>Quotations and Notes - Kozel</Title>
            <Text>On violence:

On her disembodied and immaterial experiences performing in Paul Sermon's `Telematic Dreaming', Susan Kozel writes:

: "Someone elbowed me hard in the stomach and I doubled over, wondering why since I didn't actually feel it. ... The famous claim with virtual technology is that the body is obsolete, replaced by an infinitely enhanced electronic construct. If this is so, then why did nastiness or violence enacted on my image hurt? How could the body be irrelevant yet still exert a basic visceral control over my movement?" [p.97][#Kozel:2007qf]
the figurative, the cruel, the accidental contortion

Earlier tests of interactions with antro digital puppetry like interactions with 

![fig:Expressive_Box2D_RagDoll_Empathy_all][]
[fig:Expressive_Box2D_RagDoll_Empathy_all]: Expressive_Box2D_RagDoll_Empathy_all "Expressive Box2D RagDoll Empathy all" 

![fig:Expressive_BulletPhysics_Creature_Empathy_all][]
[fig:Expressive_BulletPhysics_Creature_Empathy_all]: Expressive_BulletPhysics_Creature_Empathy_all "Expressive BulletPhysics Creature Empathy all" width="200px"

![fig:Expressive_Box2D_Creature_Empathy_001_all][]
[fig:Expressive_Box2D_Creature_Empathy_001_all]: Expressive_Box2D_Creature_Empathy_001_all "Expressive Box2D Creature Empathy 001 all" 

Terminology: performance animation

On motion capture - critique of the term. Mentions key thinkers / practitioners Brad DeGraf and Emre Yilmaz
SPONTANEITY / IMPRO DEFINITION
: "Performance animation is a new kind of jazz. Also known as digital puppetry or motion capture, it brings characters to life, i.e. `animates' them, through real-time control of three-dimensional computer renderings, enabled by fast graphics computers, live motion sampling and smart software. It combines the qualities of puppetry, live action, stop motion animation, game intelligence and other forms into an entirely new medium." [p.34][#deGraf:1999ys]

On input devices and the presence of the silhouette in performance training:
: "What input devices one needs, what kind of performer one needs, the performer-to-character mapping and direction, all crucially affect the results. This is where a lot of our expertise and experience lies. While this is rather different from the set of skills required to be a good keyframe animator, it is similar in that it also requires sensitivity to the character, its personality, and what kinds of movements will look good on the design. For instance, trying to get the performance to read in silhouette has been a puppetry training method even longer than it's been a rule of thumb for animation." [p.35][#deGraf:1999ys]
procedural animation
: "It's becoming fairly common among animators to use expressions and procedural animation to take care of a lot of the work. This is a particularly rich area of exploration. For instance, in animating a dinosaur, it's possible to write expressions that open its claw-foot as it's about to land on a surface, and to close the claw-foot again as it's raised. Then the animator only has to take care of foot positions; the toes are computed automatically. Similarly, writing expressions to control a number of low-level features from one high-level attribute makes for much richer characters than could otherwise be practically controlled. The extremes to which this can be taken, and the particular utility of these methods in performance animation, are not so well known. When you're trying to perform everything live, the more motion you can derive, or get `for free', the better." [p.35][#deGraf:1999ys]
Automatic blinking, breathing hand gestures, locomotion, reflex, intention -- rich expressive qualities.

Kozel: segues from discussing puppetry, the digital form and performance animation to discussing the classic essay on marionette theatre by Henrich Von Kleist.
Location of the soul at the centre of gravity;
pure flow of movement a physical phenomenon 

anthropomorphism and anthropocentrism (e.g. human centric data / motion capture).
the order of object
the order of subject
double-belongingness
: "Animism ... if we define it as the attribution of life, consciousness, or spirit to the nonliving, it takes us beyond the familiar contours of the human form." [p.224][#Kozel:2007qf] 
: "Puppetry is the sister art of performance animation, akin to alchemical practice, with a long history of transforming wood, cloth, and string into human, animal, or fantastical beings." [p.221][#Kozel:2007qf]

Kozel issues an interesting warning:
: "If performance animation becomes, in some narrow sense, performance animism, one would hope that we are not just mapping a narrow notion of humanity onto our digital creations. Strategic and creative animism can help avoid the dual grasps of anthropocentrism and technocentrism pervading our Western industrial societies." [p.TODO: FIND PAGE][#Kozel:2007qf]
dynamic simulation
: "Real puppets often incorporate a lot of `secondary motion' into the design. Long fur or hair that drags behind a motion, or arms that dangle and swing, can add life to a puppet. Even a puppet that's only going to have one hand controlling it, and thus not a lot of direct control, can get a lot of `free' motion from physics this way. We can use similar tricks with our digital puppets. Max Rodentae is a peppy little rodent character we built for the Virtual Ed Sullivan Show on UPN (1998). His performance is supplemented with a number of dynamic simulations - that is, simulating the physics of masses, springs, gravity and other forces. His tail, ears and belly are all controlled this way. It gives him a lot of nice secondary motion, and adds a sense of weight and believability. The result is a floppy, fun character whose whole body is expressive, and who doesn't betray the human inside." [p.37][#deGraf:1999ys]

Check out: "Real Gestures, Virtual Environments" Sally Jane Norman. Institut International de la Marionette, Charleville-Mézières, France and ZKM.


</Text>
        </Document>
        <Document ID="226">
            <Title>figure_video_movie_timebase_patch_demo_001</Title>
        </Document>
        <Document ID="83">
            <Title>Control and Interactivity</Title>
            <Text>Control and Interactivity
</Text>
        </Document>
        <Document ID="193">
            <Title>Whole Body Interactions</Title>
            <Text>[Zhan et al. (2006)][#Zhan:2006p274]

: "This is a quotation"</Text>
        </Document>
        <Document ID="116">
            <Title>Digital Scenography</Title>
            <Text>Digital Scenography
</Text>
        </Document>
        <Document ID="84">
            <Title>Chapters Overview</Title>
            <Text>Overview of Chapters</Text>
        </Document>
        <Document ID="449">
            <Title>MMD - Latex Snippet for timestamp</Title>
            <Text>\color{red}DRAFT: {\currenttime}. {\today}</Text>
        </Document>
        <Document ID="338">
            <Title>Shadows_Sleeping-beauty3_compagnie_akselere_phillippe_moulin</Title>
        </Document>
        <Document ID="227">
            <Title>figure_quartz_composer_GLSL_chromakey</Title>
        </Document>
        <Document ID="85">
            <Title>Critical Theory and Philosophical Method</Title>
            <Text>Critical Theory and Philosophical Method</Text>
        </Document>
        <Document ID="194">
            <Title>Acronyms</Title>
        </Document>
        <Document ID="117">
            <Title>Software Design</Title>
            <Text>Software Design
</Text>
        </Document>
        <Document ID="86">
            <Title>Puppetry and Performance Theory</Title>
            <Text>2.4 Puppetry and Performance Theory</Text>
        </Document>
        <Document ID="339">
            <Title>Additional Notes</Title>
            <Text>media development for cultural learning

the recontextualisation of artifacts in museums

a call "New kinds of creativity synergising old and new media"

: "Approaches like these, which link 'exotic' cultural resources with the scholarly and industrial pursuits of technologised modernity, should be encouraged by the proponents of new cultural policy, and tied into media development projects. These original visions of ancient cultures through new technologies appear all the more valuable in that they creatively challenge positions adopted by certain institutions which consider themselves the sole and rightful interpreters of 'True Culture'. Yet one of the strengths of digital data is precisely its aptitude for integration into an infinite variety of works, conveying multiple interpretations and visions." [Norman (1998)][#Norman:1998ly]

Project to articulate problems across a number of domains relating to motions capture and collaborative problem solving...

reworking motion capture.
[Norman et al. (2010)][#Norman:2010ve]
complexity of vision and touch (Merleau-Ponty

: "Videoplace, Krueger's interactive platform, marks a further and, in some sense, ultimate, stage in the restoration of autonomy to the responsive environment, understood as encompassing ... the embodied visitor. Videoplace works by capturing an image of the visitor's movement, only in this case the image presents the outline of the visitor's body processed (and distorted in various ways) by the computer." [p.35][#Hansen:2006qf]
a technical and aesthetic accomplishment -- action-response synchronicity

: "On account of the synchronicity [of the movement of the video image] with the movement of the [visitor's] body it is no longer a question of distinguishing between the activity of the system and the activity of the visitor. The computer system's role as interaction partner fades into the background, and it now makes itself available as an instrument for the visitor to use." \citep{Hansen:2006qf}

[Dinkla cited in][p.36]</Text>
        </Document>
        <Document ID="228">
            <Title>figure_image_flow_003</Title>
        </Document>
        <Document ID="87">
            <Title>Kinetic Art Theory</Title>
            <Text>Kinetic Art Theory
</Text>
        </Document>
        <Document ID="195">
            <Title>Talking Toys and Digital Puppets: Expressivity and Voice</Title>
        </Document>
        <Document ID="118">
            <Title>Avatars and Controllable Characters</Title>
            <Text>Avatars and Controllable Characters
</Text>
        </Document>
        <Document ID="229">
            <Title>figure_image_flow_002</Title>
        </Document>
        <Document ID="88">
            <Title>Animation Theory</Title>
            <Text>2.4 Animation Theory
</Text>
        </Document>
        <Document ID="89">
            <Title>Computers in Art and Performance</Title>
            <Text>Computers in Art and Performance
</Text>
        </Document>
        <Document ID="196">
            <Title>Untitled</Title>
        </Document>
        <Document ID="119">
            <Title>Conclusion</Title>
            <Text>5.11 Conclusion</Text>
        </Document>
        <Document ID="197">
            <Title>Introduction</Title>
            <Text>:	"Do dolls have souls? All children talk to their toys, says Charles Baudelaire, and *the overriding desire of most children is to get at and see the soul of their toys*. Rainer Maria Rilke would agree, with the difference that his child expects the doll to answer back and is disgusted when it doesn't." [Parry (1994), v][#Parry:1994vn]

When artificial objects speak, the presence of breath is intimated and an illusion of life should be more or less guaranteed. However, from our everyday experience of talking technology, we experience alienation, self-conciousness, strangeness, a sense of artificiality or spookiness (*uncanny valley* or *frankenstein complex* like feelings) or an acute sense of dysfunctionality. 

: "In so far as a sound is recognised as a voice, rather than a sound, it is assumed to be coming from a person or conscious agency." [][#Conner:2000uq]

I will refer to key moments in the history of mechanical and digital talking toys with special reference to the emergence of talking virtual *creatures*, not only as surrogate pets, but as kinds of digital puppets. (the full extent of what I mean by a digital puppet is stated elsewhere). I will draw on insights from the field of performance studies, particularly: puppetry and digital puppetry, and relate emerging ideas to the human computer interaction of talking digital toys and play objects. I intend to touch on how the current technologies of computer based speech synthesis and recognition are being applied in the creation of talking toys and games. 

I am interested in what happens to the perception we hold about objects when they *talk* and are seen to be talking, either through human agency or through embedded technology, like voice chips and facial animatronics. For example: I will discuss the relationship between non-animate talking dolls and different kinds of animated puppets like the *mouth* puppet and *character* toys. Other examples of digital talking toys range from objects like talking greetings cards, Texas Instruments *Speak and Spell* to humanoid or anthropomorphised talking toys like the MP3 playing storytelling bears, like Teddy Ruxpin or the iTeddy. 
Through the research (in a more long term view), I seek to clarify the qualities that lead to distinctions between the animate and inanimate speaking object, including the following patterns: the anthropomorphic (human form and qualities in the non-human), zoomorphic (human form and qualities to the animal) and the anthropopathic (the attribution of human emotion into non-human form normally God).
What seductive, alluring pleasure, education and magic is lost when our play-things talk back at us? Or, conversely, what is gained?[^ch_cc_gainedfootnote]

: "[The doll] remained silent, not because it felt superior, but silent because this was the established form of evasion and because it was made of useless and absolutely unresponsive material. It was silent, and the idea did not even occur to it that this silence must confer considerable importance on it in a world where destiny and indeed God himself have become famous mainly by not speaking to us." [(Rilke 1913/1914 in Parry (1994), v)][#Parry:1994vn] 

[^ch_cc_gainedfootnote]: See: http://www.romanceher.com/talkingteddybear.htm - "A digital message player inside the bear lets you record a 10 second message in your own voice. Each time the bear is hugged, the message plays back. This cute teddy bear is twelve inches tall. The white teddy bear is holding a bouquet of roses and the hidden pocket in the back which holds the recorder can also hold a note, ring, small gift etc. Batteries are included with it. Out of Stock  Indefinitely."

</Text>
        </Document>
        <Document ID="198">
            <Title>Key Questions</Title>
            <Text>**About trends:**

What are the trends in the interaction design of *innovative* toys that embed digital speech technologies? 
How is digitised speech used in contemporary toys? 
How are talking toys used to create narrative based experiences?
What happens when talking toys automatically move or have facial animation?

**About play:**

What happens to imaginative play and art when *things* speak? 
Do talking digital toys enhance *imaginative* play, story-making and children's role play? 
What happens to improvisational play when toys both speak and move and are, in effect, programmed automatons?

**About technology:**

What are the emerging models of HCI in speech activated and speaking objects? 
How have embedded speech recognition and learning systems been used to devise *dialogical* digital toys? 
What is the current state of voice activation and command recognition in toys and how is it being used to stimulate or constrain imaginative play? 

In order to discuss these questions effectively and include historical and contemporary references to key toys, companies, critical voices, theory, technology, design trends and other ideas, I propose ten emerging ideas in [][tenemergingideasinvolvingtalkingtoysandtechnology] as starting points for discussion.</Text>
        </Document>
        <Document ID="199">
            <Title>5.3.1 Scope</Title>
            <Text>In researching this chapter, I have uncovered a quantity of talking technology issues and experiments, both historical and contemporary, that will be discussed in following papers. These include the role of talking technology and objects in education; patents and animatronics in the techno-entertainment industries, linguistic and semantic modelling in a-life simulations and screen-based games, experiments in analogue and digital voice synthesis in computer games and chat-bots and the like, and, now, talking dildos. I acknowledge emerging trends like the use of embedded audio recording devices to include player voices in games and tangible objects (in "Nintendogs" and "Talking Teddy" from "romanceher.com"). This paper is very much an early exploration that, hopefully, will establish and discuss useful questions around the complex of multi-disciplinary issues involved: these areas include developmental and social psychology, learning and play theory, computer science and AI, HCI and design, and creative writing. 

For reference purposes, here is a list of the talking toys and objects I've considered for this paper [they will form an slideshow backdrop to my reading, what they say and the technology (the voice chips and interactive flow):

Hasbro's "T.J. Bearytales™", "Talking Furby™", Talking "Aloha Stich™", "FurReal™" (they are as the blurb states "for real") 
Mattel's "Teen Talk Barbie™" 
Backpack Toys' "Teddy Ruxpin™" 
Thinkway Toys' "Interactive buddies" (their slogan "I'm a thinking toy®", e.g. "the Buzz LightYear room guard" 
Microsoft's Actimates™, particularly "Barney the Dinosaur" complete with a William-Gibson-like body-port for downloading TV signals. 
Crying baby simulators (used in UK health trusts to dissuade teenage pregnancy). "Baby Think it Over" in the USA (Stanford University). 
Various Greetings Cards with embedded speech (some recordable). 
Mattel's "Diva Petz™" with Voice Signal's Speaker-Independent Speech Recognition Technology. 
"Talking Nano™" (non-representational 'Tamagotchi') 
"Yo, Dude™", from DSI Toys, Inc. 
"Rock Buddies™", from MGA Entertainment. 
"Amazing Amanda", "P.O.D.Z.™", from Playmates Toys Inc. 
"Hey, Man TM" from Wow Wee 
romanceher.com's "Romeo" Talking Teddy 
Dr Allison Druin's "P.E.T.S". 
Justine Cassell's "StoryMat™" from the MIT.</Text>
        </Document>
        <Document ID="450">
            <Title>Latest Bibliography Import</Title>
            <Text>André, E., Rist, T. &amp; Baldes, S., 2002. From Simulated Dialogues to Interactive Performances. Proceedings of the 9th ECCAI-ACAI/EASSS 2001, AEMAS 2001, HoloMAS 2001 on Multi-Agent-Systems and Applications II-Selected Revised Papers.
Beguin, P. &amp; Rabardel, P., 2000. Designing for instrument-mediated activity. Scandinavian Journal of Information Systems.
Bendle, M., 2002. Teleportation, cyborgs and the posthuman ideology. Social Semiotics, 12(1), pp.45-62.
Cameron, D. &amp; Carroll, J., 2009. Encoding liveness: Performance and real-time rendering in machinima. Breaking New Ground: Innovation in Games, Play, Practice and Theory: Proceedings of DiGRA 2009.
de Preester, H. &amp; Knockaert, V., 2005. Body image and body schema: interdisciplinary perspectives on the body‎. p.343.
Hare, J., Karam, M. &amp; Lewis, P., 2005. iGesture: A Platform for Investigating Multimodal, Multimedia Gesture-based Interactions.
Ishii, H. et al., 1998. ambientROOM: integrating ambient media with architectural space. CHI 98 conference ….
Karam, M., A framework for research and design of gesture-based human-computer interactions. PhD thesis.
Karam, M., 2005. A taxonomy of gestures in human computer interactions.
Karam, M. et al., 2006. Ambient Gestures. Technical Report ECSTR-IAM06-001, Intelligence, Agents, Multimedia Group. University of Southampton. (Unpublished).
Klemmer, SR, 2004. Tangible user interface input: tools and techniques. PhD Thesis.
Klemmer, SR &amp; Landay, J., 2009. Toolkit support for integrating physical and digital interactions. Human-Computer Interaction, 24(3), pp.315-366.
Klemmer, Scott, Hartmann, B. &amp; Takayama, L., 2006. How bodies matter: five themes for interaction design. DIS '06: Proceedings of the 6th conference on Designing Interactive systems.
Levin, G., 2000. Painterly interfaces for audiovisual performance‎. PhD Thesis MIT, p.149.
Norman, S. et al., 2009. AMUC: Associated Motion capture User Categories. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 367(1898), p.2771.
Orio, N., Schnell, N. &amp; Wanderley, M., 2001. Input devices for musical expression: borrowing tools from HCI. Proceedings of the 2001 conference on New interfaces for musical expression, p.4.
Rabardel, P. &amp; Waern, Y., 2003. From artefact to instrument. Interacting with computers.
van den Hoogen, W., IJsselsteijn, W. &amp; de Kort, Y., 2009. Effects of Sensory Immersion on Behavioural Indicators of Player Experience: Movement Synchrony and Controller Pressure. Breaking New Ground: Innovation in Games, Play, Practice and Theory: Proceedings of DiGRA 2009.
Verillon, P. &amp; Rabardel, P., 1995. Cognition and artifacts: a contribution to the study of though in relation to instrumented activity. European journal of psychology of education.
Vérillon, P., 2000. Revisiting Piaget and Vigotsky: In search of a learning model for technology education. the Journal of Technology studies, 26(1), pp.3-10.</Text>
        </Document>
        <Document ID="340">
            <Title>Notes on Krueger VideoPlace and Levin</Title>
            <Text>Interactive Art / Shadowgraphs

FROM YOU TUBE: 

: "The 'Interstitial Fragment Processor" (2007: Golan Levin) is an interactive artwork which allows visitors to explore the audiovisual potentials of negative spaces. The installation collects and drops the contoured shapes formed within and between the bodies of its participants. Elastic red and blue animated objects plummet toward the gallery floor, producing audiovisual improvisations on vertical descent and collision. Available from bitforms gallery, NYC." 

![fig:Shadows_Levin_Interstitial_Space_001][]
[fig:Shadows_Levin_Interstitial_Space_001]: Shadows_Levin_Interstitial_Space_001 "Levin's Silhouette Projects 001. Source: [TBC]." width=400px 
DESC FROM YOU TUBE

: "Messa di Voce (2003: Golan Levin, Zachary Lieberman, Jaap Blonk, and Joan La Barbara) augments the speech, shouts and songs produced by a pair of virtuoso vocalists with real-time interactive visualizations. The project touches on themes of abstract communication, synaesthetic relationships, cartoon language, and writing and scoring systems, within the context of a sophisticated, playful, and virtuosic audiovisual narrative. Custom software transforms every vocal nuance into correspondingly complex, subtly differentiated and highly expressive graphics. Messa di Voce lies at an intersection of human and technological performance extremes, melding the unpredictable spontaneity and extended vocal techniques of human improvisers with the latest in computer vision and speech analysis technologies. Utterly wordless, yet profoundly verbal, Messa di Voce is designed to provoke questions about the meaning and effects of speech sounds, speech acts, and the immersive environment of language." 

![fig:Shadows_Levin_messo di voce_001][]
[fig:Shadows_Levin_messo di voce_001]: Shadows_Levin_messo di voce_001 "Levin's Silhouette Projects 002. Source: [TBC]." width=400px 
![fig:Shadows_Levin_messo di voce_002][]
[fig:Shadows_Levin_messo di voce_002]: Shadows_Levin_messo di voce_002 "Levin's Silhouette Projects 003. Source: [TBC]." width=400px 
: "Imagine becoming your own shadow and to being able to interact with other peoples shadows in a simulated space -- to touch them and be touched. Kreugers aim is to create us such a shadow world where people can relate to each other in a way quite impossible in the physical realm." [anonymous narrator speaking on VideoPlace in Myron Krueger clip]

![fig:Shadows_kreuger_videospace_002][]
[fig:Shadows_kreuger_videospace_002]: Shadows_kreuger_videospace_002 "Kreuger's VideoSpace 002. Source: [TBC]." width=400px 

: "It is as if evolution has prepared us for seeing ourselves on television screens combined with computer images but all, one of the main attractions is the juxtaposition of large and small. So that two people are now interacting and, to some extent discover what the possibilities are and what is suggested emotionally by scale." [Myron Krueger in you tube clip]

![fig:Shadows_kreuger_videospace_001][]
[fig:Shadows_kreuger_videospace_001]: Shadows_kreuger_videospace_001 "Kreuger's VideoSpace 001. Source: [TBC]." width=400px 
: "Each participant's video image is digitised  and is fed to a series of specialist processors that analyses the resulting silhouettes. These processors analyse each image in isolation (e.g. posture, rate of movement) and with respect to graphic objects  and live images on the screen. ... When the participant's actions are understood by the specialised processors, they are reported to the executive processor that decides what the responses should be. Depending on the participant's behaviour, it can move an object, change that objects colour, move the participant's image, or make a sound " [p.44-45][#Krueger:1991uq]

Krueger et. at. [MEMO list others] designed nearly fifty interactions (or compositions), each playing with the interaction of silhouettes of differing scales, with objects and interventions from other (often distant) spaces. At once telematic and performative, the system became a technological and aesthetic base-line for future computer vision and art installation based systems.


CHECK OUT: Miwa Matreyek, CalArts Experimental Animation Program
http://www.semihemisphere.com/
</Text>
        </Document>
        <Document ID="451">
            <Title>Daily</Title>
            <Text>Daily</Text>
        </Document>
        <Document ID="341">
            <Title>Videos</Title>
        </Document>
        <Document ID="230">
            <Title>figure_004_interactive_eyes</Title>
        </Document>
        <Document ID="452">
            <Title>TO DO</Title>
        </Document>
        <Document ID="30">
            <Title>Research Objectives and Original Contribution</Title>
            <Text>Research Objectives

Define the problem of a complex and hybrid domain with focus and clarity;

To produce widely applicable and innovative technologies, with scope beyond the original intended use and contexts;

To find materials and techniques that are low cost and re-configure widely accessibly components;

To apply an analysis of expressivity meaningfully to creative digital and technical contexts.

Original Contribution to Interdisciplinary Field

I intend to deliver:

- Performance, Artwork and Artefacts;
- Software product and creative production techniques;
- Innovative control systems and interfaces that will be applicable hopefully beyond the domain of performance;
- Papers and published outcomes that will contribute to the thesis, documentation and other elements of the PhD.
</Text>
        </Document>
        <Document ID="31">
            <Title>Personal Experience and Interests</Title>
            <Text>Personal Experience and Interests

Teaching experience: Five years at Brunel University, UK, teaching modern drama studies; Six years teaching digital art at TVU, UK, with teaching and research experience across a number of areas (please see my CV for fuller detail): History of Computer Art; Virtual and Augmented Reality; Social Media; Creative Social Networks; Narrative and Games; New Media and Cinematics; Real-Time 3D Graphics and Processing; Applied Visual Effects; Digital Puppetry and Animation; Live and Digital Performance; Sculpture, Form and Light in Virtual and Physical Performance; Applied Media Technologies; Futurology, Internet and Digital Culture; e-Learning and Virtual Learning Environments; Critical Theory.

I am seeking a doctoral experience that will value the diversity of my interests while providing a focus that respects inter-disciplinarity and hybrid practice.

Related Publications and Performance: Recent published papers (AISB UK), performances (DAW'07 Zurich, Shunt, London) and presentations have explored dimensions that I wish to include in the PhD studies - voice, performing objects, expressivity, musical performance and digital (video-based) puppetry and game controllers in performance. Further details in my CV.
I am currently working on a book contracted by Pragmatic Programmers (US) "Apple's Quartz Composer - a pragmatic guide" - surrounding technical methodology (real time data rich, graphical environments and visual programming) relevant to the doctoral research. It could become a contribution to the doctoral research.

Technologist: I create software, do application design and coding (mac specialist), I'm an open source enthusiast (hardware and software), with a desire to learn more about programming real time 3D environments (e.g. ogre3D) and openGL programming - an area I have been studying informally for a while.
</Text>
        </Document>
        <Document ID="342">
            <Title>Video - Jeff Han - Touch Shadow Puppets</Title>
        </Document>
        <Document ID="231">
            <Title>figure_image_flow_001</Title>
        </Document>
        <Document ID="120">
            <Title>Case Study in Practice and Response</Title>
            <Text>This is a Case Study in Practice and Response.</Text>
        </Document>
        <Document ID="453">
            <Title>Weekly</Title>
            <Text>Weekly</Text>
        </Document>
        <Document ID="32">
            <Title>Academic Literature and Indicative Bibliography</Title>
            <Text>Academic Literature and Indicative Bibliography [not consistently formatted nor ordered]

The select bibliography ballooned the word-count - so I have placed the longer copy online 

Link: http://ellington.tvu.ac.uk/research/proposal/bib.html 

C. Baudelaire. The philosophy of toys. In I. Parry, editor, Essays on Dolls. Syrens, London, 1994.

J. Bell, editor. Puppets, Masks, and Performing Objects. MIT Press, Boston, 2001.

R. A. Brooks. Robot: The Future of Flesh and Machine. Penguin, London, 2002.

H. B. Segal. Pinocchio’s Progeny : Puppets, Marionettes, Automatons and Robots in Modernist and Avant-Garde Drama. Johns Hopkins University Press, 
Baltimore, 1995.

S. Kaplin. A puppet tree - a model for the field of puppet theatre. In J. Bell, editor, Puppets, Masks, and Performing Objects, pages 18–25. MIT Press, Boston, 2001.

M. Fuller. (2003) Behind the Blip: Essays on the Culture of Software. Tandem.

M. Fuller (ed.) (2008) Software Studies: A Lexicon (Leonardo Book) MIT Press.

Cawood, Stephen and Mark Fiala. (2008) Augmented Reality: A Practical Guide: The Complete Guide to Understanding and Using Augmented Reality Technology. Pragmatic.

S. Tillis. Toward an Aesthetics of the Puppet: Puppetry as a theatrical art. Greenwood, New York, 1992. Normal Loan 791.53 TIL.

S. Tillis. The art of puppetry in the age of media production. In J. Bell, editor, Puppets, Masks, and Performing Objects, pages 172–183. MIT Press, Boston, 2001.

Weizenbaum, Joseph. (1993) Computer Power and Human Reason (Penguin Science). Penguin.

Poepel, Cornelius. (2005) On Interface Expressivity: A Player-Based Study. Available: http://hct.ece.ubc.ca/nime/2005/proc/nime2005_228.pdf. Viewed: 28/4/2008.

Pelachaud, Catherine. http://webperso.iut.univ-paris8.fr/~pelachaud/ e.g.
J.-C. Martin, R. Niewiadomski, L. Devillers, S. Buisine, C. Pelachaud,  Multimodal complex emotions: Gesture expressivity and blended facial expressions, International Journal of Humanoid Robotics, Special Edition "Achieving Human-Like Qualities in Interactive Virtual and Physical Humanoids", 2006

E. Bevacqua and C. Pelachaud, Speaking With Emotions: Available: http://linc.iut.univ-paris8.fr/greta/papers/aisb04-bevacqua.pdf. Viewed: 28/4/2008

Dobson, Kelly,  Brian Whitman and Daniel P.W. Ellis. (2005) Learning Auditory Models of Machine Voices. Available: http://web.media.mit.edu/~monster/dobson_whitman_ellis_machinevoices.pdf. Viewed: 24/4/2008.
Juul, Jesper. (2005) Half-Real: Video Games between Real Rules and Fictional Worlds. MIT Press.</Text>
        </Document>
        <Document ID="33">
            <Title>Original Contribution</Title>
        </Document>
        <Document ID="343">
            <Title>Video-Levin 2006 ars electronica</Title>
        </Document>
        <Document ID="232">
            <Title>figure_015_total_QC_to_wii_patch_horiz</Title>
        </Document>
        <Document ID="121">
            <Title>Chapter 4: Expressivity</Title>
        </Document>
        <Document ID="34">
            <Title>Methodology, Methods, Tools and Approaches</Title>
            <Text>Methodology, Methods, Tools and Approaches

My methodological philosophies will involve: 

- Interpretative work on relevant performance history, artefacts and technologies.
- Broad ethnographic strategies in defining case studies of relevant work, including:

	Performance Documentation - A/V and photographic;
	Observation and Thick Description;
	Deep Conversation / Interviews - with artists, developers, workshop participants and audiences;
	Computer assisted analysis, dimensionalising and visualisation of collected research material;
	Coding and Commenting as Creative Research;

- Establishing a 'software studies' approach when elaborating on the cultural contexts of software design and creative practice. Software studies is an emerging academic field with key proponents including Matthew Fuller (Goldsmiths) and Lev Manovich (UCSD). Software Studies:

"...proposes that software can be seen as an object of study and an area of practice for art and design theory and the humanities, for cultural studies and science and technology studies and for an emerging reflexive strand of computer science. Secondly, it takes the form of a series of short ‘studies’: creative and critical texts on particular algorithms, logical structures and digital objects. Such objects can be drawn from any layer of computational culture – from the front, to the back end. Software is often a blind spot in the theorisation and study of computational and networked digital media. It is the very grounds and ‘stuff’ of media design. In a sense, all intellectual work is now ‘software study’, in that software provides its media and its context, but there are very few places where the specific nature, the materiality, of software is studied except as a matter of engineering. (Fuller, 2006. http://pzwart.wdka.hro.nl/mdr/Seminars2/softstudworkshop)
</Text>
        </Document>
        <Document ID="454">
            <Title>Two Weekly</Title>
            <Text>Two Weekly

- Sync Bibliography</Text>
        </Document>
        <Document ID="35">
            <Title>Accessibility of Research Material and Expertise</Title>
            <Text>Accessibility of Research Material and Expertise

SmartLab has a track record of interest and expertise in areas surrounding digital puppetry, performance technology, virtual and augmented realities and storytelling in performance.

- The VIP project;
- Staff/supervisors interests incorporate the work;
- At a recent PhD symposium, I saw presentations indicating other PhD students are pursuing complementary projects - so there is a community of interest;
- My proposed research engages expertise across a number of the publicised SmartLab research groupings: Performance Technologies, Mobile Platform Games, Accessible Technologies &amp; Personal/Community Fabrication, Immersive Play, Multimodal Interfaces, Haptics &amp; Robotics, Assistive Technology, SMARTart &amp; gaming, Interactive Screen and Digital Narratives.
</Text>
        </Document>
        <Document ID="344">
            <Title>Video-Levin 2007 Interstitial Fragment Processor</Title>
        </Document>
        <Document ID="233">
            <Title>figure_015_total_QC_to_wii_patch</Title>
        </Document>
        <Document ID="122">
            <Title>Introduction</Title>
            <Text>4.1 Introduction
</Text>
        </Document>
        <Document ID="455">
            <Title>Monthly</Title>
            <Text>Monthly</Text>
        </Document>
        <Document ID="36">
            <Title>Timescale</Title>
            <Text>Timescale

I seek to register part time but seek to fulfil the doctoral requirements as quickly as is allowed.

I wish to contribute to SmartLab culture and collaborate with projects across the institute, including bartering skills and project work for fees if possible and desirable.

Should the application be accepted, I will use project management tools OmniPlan &lt;http://www.omnigroup.com/applications/omniplan/> to produce a Gantt Chart - of research activity against time. The project scope and objectives need to be focused, clarified and agreed with a supervisor before this can happen. For example: the early stage process should identify a performance, an object, a control system, some software that will practically explore the doctoral focus. These agreed outputs can then be set against available time. 

I have a body of work completed that will contribute to the proposed study in a worthwhile way.
</Text>
        </Document>
        <Document ID="37">
            <Title>Harvard Reference Format for URLs</Title>
            <Text>Harvard Reference Format for URLs

Ian Grant. (2001). Digital Puppetry is Here To Stay. Available: http://daisyrust.com/article/. Last accessed 12 May 2007.

[Author] (date). [Title]. Available: [URL]. Last accessed [Date].
</Text>
        </Document>
        <Document ID="345">
            <Title>Video-Levin et al Messa di Voce 2003</Title>
        </Document>
        <Document ID="234">
            <Title>figure_014_piano_with_avatar</Title>
        </Document>
        <Document ID="123">
            <Title>2.4.2 Psychoanalytic Theory</Title>
            <Text>Psychoanalytic Theory
</Text>
        </Document>
        <Document ID="456">
            <Title>Inter Library Loans</Title>
            <Text>Inter Library Loans

---------------------------------------------
Flow, gesture, and spaces in free jazz : towards a theory of collaboration
	G Mazzola; Paul B Cherlin; Tetrade.
Find more information about:    
ISBN:	9783540921943 354092194X
OCLC Number:	301547272
Description:	xiii, 141 p. : ill., music ; 25 cm. + 1 sound disc (digital ; 4 3/4 in.)
Contents:	pt.1. Getting off ground. What is free jazz? (The social, and political, and musical origins of the movement; A provisional positive characterization) --
Jazz in transition (Archie Shepp in Donaueschingen; John Coltrane's A love supreme; Cecil Taylor and Buell Neidlinger; Bill Evans : gestural dialogs). pt. 2. The landscape of free jazz. Out of this world (Sun Ra : an extraterrestrial romantic; Coltrane's Om; Mythologies of the Art Ensemble of Chicago) --
The art of collaboration (A short overview of the classical ontological landscape of music; The oniontological [sic] extension to the fourth dimension; What is the art of collaboration?). pt. 3. Collaborative spaces in free jazz. Which collaboratories? (Ornette Coleman's melodic spaces in Free jazz; John Coltrane's harmonic spaces in Ascension) --
The innards of time (Cecil Taylor : unit structures; Trance spaces : Archie Shepp's The magic of ju-ju; Dervish dances : Albert Ayler's Love cry). pt. 4. Gestural creativity. From philosophy to thought experiments. (Philosophy, performance, music theory; The French approach to gestures; Châtelet's gestural thought experiments) --
Geometry of gestures (Gestures are diagrams of curves; Definition of gestures and hypergestures; Hypergestures, cognitive science, and Cavaillès) --
The Escher theorem and gestural creativity (The Escher theorem; Group creativity and categories of hypergestures; Rebecca Lazier's Vanish : Lawvere, Escher, Schoenberg; Musical poetology). pt. 5. What group flow generates. What is flow? (Mihaly Csikszentmihalyi's flow concept; Keith Sawyer's group flow; Miles Davis' Bitches brew; Gestures in Geisser's and Mazzola's Chronotomy; What does group flow produce?) --
The symbolic axis of distributed identity (Groups from gestures; The Fourier ballet ; Passion; Archie Shepp's Coral rock). pt. 6. Epilogue. From pre- to postproduction : the infinite listening --
Global strategies for free jazz --
The future of free jazz. Compact disc: Liquid bridges / by Tetrade. Colors of dawn (16:20) --
Liquid bridges (18:52) --
Trance figuration (28:34) / all compositions by Heinz Geisser and Guerino Mazzola, SUISA.


Where: The British Library, British National Bibliography 
BNB
Wetherby, West Yorkshire, LS23 7BQ United Kingdom

Part IV Gestural Creativity
From Philosophy to Thought Experiments
	Philosophy, Performance, Music Theory
	The French Approach to Gestures
	Châtelet's Gestural Thought Experiments
Geometry of Gestures
	Gestures are Diagrams of Curves
	Definitions of Gestures and Hypergestures
	HyperGestures, Cognitive Science and Cavaillès
The Escher Theorem and Gestural Creativity
	The Escher Theorem
	Group Creativity and Categories of Hypergestures
	Rebecca...
	Musical Poetology
	
What Group Flow Generates
	What is flow?
		Mihaly Csikszentmihalyi's Flow Concept
		...
	The Symbolic Axis of Distributed Identity
------------------------------------------------------

Where: Royal Holloway
	Martin, John 
Title	 The Mysore manual : a path of actor’s creativity /
Imprint	 London : Pan Project, Goldsmiths’ College , 1990
Description	 117 p. , ill
Subject - L.C.	 Acting.
Add.Entry	 University of London Goldsmiths’ College
Location	Founders : Main (Book) 792.028 MAR
System Number	 000219611</Text>
        </Document>
        <Document ID="38">
            <Title>ian_grant_paper_final</Title>
            <Text>Digital Puppetry and Talking Toys Ten emerging theses involving talking toys and technology
Ian Grant1
Abstract.	Digital artist and lecturer Ian Grant will outline develop- ing trends and scenarios where talking and listening to the speech of humanoid and non humanoid objects, toys, robots is a part of play and other imaginative work. Working from an expertise in pup- petry, automata and the emerging new field of ’digital puppetry’, the author will take concepts from many disciplines, embedded com- puting, voice synthesis, performance studies and educational drama (metaxis, role-play, absorption, projection, performativity) and apply them to the world of interactive toys and ’will see what happens’.
To organise the discussion, the author discusses ten emerging ideas in relation to speaking puppets, digital technology and talking toys.
1 INTRODUCTION
”Do dolls have souls? All children talk to their toys, says Charles Baudelaire, and ’the overriding desire of most children is to get at and ’see the soul’ of their toys’. Rainer Maria Rilke would agree, with the difference that his child expects the doll to answer back and is disgusted when it doesn’t.” (Parry, 1994, v) [9]
When artificial objects speak, the presence of breath is intimated and an illusion of life should be more or less guaranteed. However, from our everyday experience of talking technology, we experience alienation, self-conciousness, strangeness, a sense of artificiality or spookiness (’uncanny valley’ like feelings) or an acute sense of dys- functionality.
”In so far as a sound is recognised as a voice, rather than a sound, it is assumed to be coming from a person or conscious agency.” (Con- ner 2000:24) [4]
I will refer to key moments in the history of mechanical and digital talking toys with special reference to the emergence of talking virtual ’creatures’, not only as surrogate pets, but as kinds of digital puppets. I will draw on insights from the field of performance studies, particu- larly: puppetry and digital puppetry, and relate emerging ideas to the human computer interaction of talking digital toys and play objects. I intend to explore how the current technologies of computer based speech synthesis and recognition are being applied in the creation of talking toys and games.
I am interested in what happens to the perception we hold about objects when they ’talk’ and are seen to be talking, either through human agency or through embedded technology, like voice chips and facial animatronics. For example: I will discuss the relationship between non-animate talking dolls and different kinds of animated puppets like the ’mouth’ puppet and ’character’ toys. Other exam- ples of digital talking toys range from objects like talking greetings cards, Texas Instruments ’Speak and Spell’ to humanoid or anthropo- morphised talking toys like the MP3 playing storytelling bears, like
1 Thames Valley Univeristy, London, UK., email: ian.grant@tvu.ac.uk
Teddy Ruxpin or the iTeddy. What seductive, alluring pleasure and magic is lost when our play-things talk back at us? Or gained 2?
”[The doll] remained silent, not because it felt superior, but silent because this was the established form of evasion and because it was made of useless and absolutely unresponsive material. It was silent, and the idea did not even occur to it that this silence must confer con- siderable importance on it in a world where destiny and indeed God himself have become famous mainly by not speaking to us.” (Rilke 1913/1914 in Parry 1994,33) [11]
1.1	Key Questions
Some key questions and issues:
• Whathappenstoimaginativeplayandartwhen’things’speak? • What anthropomorphic role does voice, or en-voicing, have when
we create virtual creatures or digital pets? • What are the emerging models of HCI in speech activated and
speaking objects? • How have embedded speech recognition and learning systems
been used to devise ’dialogical’ digital toys? • Howisdigitisedspeechusedincontemporarytoys? • Howaretalkingtoysusedtocreatenarrativebasedexperiences? • What is the current state of voice activation and command recog-
nition in toys and how is it being used to stimulate or constrain
imaginative play? • What happens when talking toys automatically move or have fa-
cial animation? • Do talking digital toys enhance ’imaginative’ play, story-making
and children’s role play? What happens to improvisational play when toys both speak and move and are, in effect, programmed automatons?
• What are the trends in the interaction design of ’innovative’ toys that embed digital speech technologies?
In order to discuss these questions effectively and include his- torical and contemporary references to key toys, companies, critical voices, theory, technology, design trends and other ideas, I propose ten emerging theses in section 2 as starting points for discussion.
2 See: http://www.romanceher.com/talkingteddybear.htm - ”A digital mes- sage player inside the bear lets you record a 10 second message in your own voice. Each time the bear is hugged, the message plays back. This cute teddy bear is 12 inches tall. The white teddy bear is holding a bouquet of roses and the hidden pocket in the back which holds the recorder can also hold a note, ring, small gift etc. Batteries are included with it. Out of Stock Indefinitely.”
1.1.1	SCOPE
In researching this paper, I have uncovered a quantity of talking tech- nology issues and experiments, both historical and contemporary, that cannot be fully discussed or included here due to space. These in- clude the role of talking technology in deaf and special needs educa- tion, patents and animatronics in the techno-entertainment complex, linguistic and semantic modelling in a-life simulations and screen- based games, experiments in analogue and digital voice synthesis in computer games and chat-bots and the like. Emerging trends like the use of embedded audio recording devices to included player voices in games and tangible objects (in ”Nintendogs” and ”Talking Teddy” from ”romanceher.com”) This paper is very much an early explo- ration that, hopefully, will establish and discuss useful questions around the complex of multi-disciplinary issues involved: these ar- eas include developmental and social psychology, learning and play theory, computer science and AI, HCI and design, and creative writ- ing.
For reference purposes, here is a list of the talking toys and objects I’ve considered for this paper:
• Hasbro’s	”T.J.	BearytalesTM ”,	”Talking	FurbyTM ”,	Talking	’	Aloha StichTM ’,	”FurRealTM ”	(they	are	as	the	blurb	states	”for	real”)
• Mattel’s	”Teen	Talk	BarbieTM ” • Backpack	Toys’	”Teddy	RuxpinTM ” • Thinkway Toys’ ”Interactive buddies” (their slogan ”I’m a think-
ing toy ⃝R ”, e.g. ”the Buzz LightYear room guard” • Microsoft’s	ActimatesTM ,	particularly	”Barney	the	Dinosaur”. • Crying baby simulators (used in UK health trusts to dissuade
teenage pregnancy). ”Baby Think it Over” in the USA (Stanford
University). • Various Greetings Cards with embedded speech (some record-
able). • Mattel’s ”Diva PetzTM” with Voice Signal’s Speaker-Independent
Speech Recognition Technology. • ”Talking NanoTM” (non-representational ’Tamagotchi’) • ”Yo, DudeTM”, from DSI Toys, Inc. • ”Rock	BuddiesTM ”,	from	MGA	Entertainment. • ””Amazing Amanda”, ”P.O.D.Z.TM”, from Playmates Toys Inc. • ”Hey,	ManTM ,	from	Wow	Wee • romanceher.com’s ”Romeo” Talking Teddy • Dr Allison Druin’s ”P.E.T.S”. • Justine Cassell’s ”StoryMat”.
2	TEN EMERGING THESES INVOLVING TALKING TOYS AND TECHNOLOGY
1. Embedding talking technology in toys is more about control than play or exploratory learning.
2. Talking toys are first technological experiments, second, play- things.
3. Talking toys are monologic rather than dialogic. 4. Talking character-based toys, that are dependent on other media,
are derivative and closed narrative systems. 5. Talking Toys represent an adult intervention into child-play. 6. What talking toys say is more important than how they say it. 7. Talking toys are of greater value when they are programmable and
configurable by children. 8. Animated facial mechanisms attempt to re-embody disembodied
voice and, in turn, over-concretise and limit imaginative play. 9. Talkingtoysareextraordinarysimulatorsofintelligenceandpres-
ence.
10. Talking toys, traditional and digital puppets and animated media forms are more inter-connected than we may first think.
2.1
Embedding talking technology in toys is more about control than play or exploratory learning
Puppetry, the emerging forms of digital puppetry and puppet like talking toys are all about ’control’ in two important senses. There’s the good old fashioned sense that such toys are ’cybernetic’ systems where there is a feedback loop with the ’movement - action’ chain and in the sense that such toys embed pedagogical rules and struc- tures.
”The thing about playing is always the precariousness of the inter- play of personal psychic reality and control of actual objects” (Win- nicot 1971 cited in Cassell and Ryokai, 1, 2001 [2])
Certain talking toys, the interactive series of ’Actimates’ from Mi- crosoft, have been criticised for the empty way they encode, like passive vessels, content from other media channels. ’Barney the Di- nosaur’, for example is controlled by signals from PCs, TV broad- casts and video tape. The dolls mouth syncs and sings with the repre- sentation of Barney on screen. ”Most commercial applications in the domain of tangible personal technologies for children are variants on dolls, with increasingly sophisticated repertoires of behaviours. Microsoft Actimates’ ”Barney” and Mattel’s ”Talk with Me Barbie” have embedded quite sophisticated technology into familiar stuffed animals and dolls. These toys, however, deliver adult-scripted con- tent with thin layers of personalization, and do not engage children in their own fantasy play. In both cases the toy is the speaker and the child is firmly in the position of listener.” (Cassell and Ryokai, 2001) [2]
Microsofts learning toy theorist and actimate guru, Erik Strommen positions Barney as a mate, a learning pal, a friend.
But another aspect of ’control’, is the propensity of talking toys to lie:
An extended extract from an interactive toy conference review, ”Interactive Barney: Good or evil?” : ”When I hear Barney say, ’You’re my special friend’ – that’s a disingenuous statement,” said Allen Cypher, a founder of Stagecast Software, which designs chil- dren’s programs. ”It’s a fraudulent claim. It deceives kids into be- lieving that Barney has some emotional attachment to them, and that’s not true.” Other panelists worried about Barney’s ”author- itarian tone,” or that he discouraged imaginative play. And some said that, while Barney himself was basically harmless, he may be a harbinger of worse to come: an interactive Cartman from ”South Park,” perhaps, spewing expletives and insulting his owner”3
And one member of the audience asked if a child could take Barney apart and ”reprogram him to say, ’Please slap me.’ ” ”These products are designed to prevent that,” Strommen said.’ [7]
When discussing ’control’ it is important to note that it is not meant in a purely sinister, ideologically manipulative, way. Play, and particularly play where children animate and give voice to objects that surround them, is about children asserting control and (dis)order over facets of their environment: ”One essential aspect of childrens’ spontaneous storytelling play is that it is child-driven. And this is im- portant since children feel a sense of achievement and empowerment when they know that they can create and control the content of their play objects. So, if technology is to encourage childrens’ creativity and, in particular, play a role in childrens’ storytelling play, it must not dampen that child-driven aspect of their play.” [2]
3 See Hasbro’s ’Aloha Stitch Doll’ for an example of such a moody toy.
2.2	Talking toys are first technological experiments, second, play-things.
The history of talking toys and automata is clearly a story of technical innovation and development for the purposes of celebration, enter- tainment and play. According to Jasia Reichardt talking statues have been known since 2500 BC: ”Some incorporated concealed speak- ing trumpets through which someone hidden could address a gath- ering. The idea was that gods communicateed through the statues which represented them” (Reichardt, 1978, 9)[10] Of interest here, Jacques de Vaucanson created a number of mechanical automata in- cluding a ’flute player and defecating duck’ (circa 1737-1738). On the ’flute player’: ” This automaton ’breathed’. Even though the art of mechanics was sophisticated enough by then to make the machine perform many other movements, and even though Vaucanson un- veiled the fact that this breath was created by bellows, the very act of breathing, seen in an inanimate figure, continued to cause a stir well into the following century.” (Wood, 2002, 21-22) [19]
The first talking doll was patented by Johann Nepomuk Maelzel in 1824. According to Gaby Wood ”He designed a pair of bellows that, when attached to a tube, a widening oral cavity and a set of valves, could say ’papa’ or ’maman’”. (Wood 2002, 118-119) [19]
Thomas Edison’s ’Talking Doll’ (1891) - conceived as an adver- tisement for his sound recording device - embedded a miniaturised phonograph mechanism that played wax cylinder recordings of nurs- ery rhymes, prayers and stories: ”[The phonograph] began by speak- ing the words of a child, and it was not long before a child was in- vented to give it shape, or to give it life. So the capturing and re- production of speech were accompanied by a casing for it in human form” (Wood, 2002, 18) [19]
The context around Edison’s toy development has shaped the in- dustrialised processes surrounding technical innovation and toys ever since. There is little perceived difference between Edison grafting a mechanical phonograph into a toy and iTeddy’s implanted mp3 / mp4 playback device. Yet the former was an exercise in creating perfect representational forms of human (female) life, and the other a toy to placate media hungry children.
”Edison’s colleague, W.K.L. Dickson, wrote that it was ’per- haps the daintiest and most suggestive of all the multiform uses to which the phonograph has been put.’ He described ’roseate lips’ which would ’lisp out the oft-conned syllables of nursery rhymes, pipe the familiar of Mother Goose’s ballads, and give forth the coo- ing and wailing sounds of baby life Under such auspices into what enchanted realm will our ordinary toys be transformed.” (Dickson cited in Wood, 2002, 114) [19]
Duncan Bannatyne, on a recent broadcast of BBC TV’s venture capital reality show 4, said of iTeddy: ”I’m so sad. Reading bedtime stories is a father’s [sic] job. I don’t want to be replaced by a teddy bear.”
Talking toys that emerge from University research labs and uni- versity start ups are philosophically worlds apart from corporate toys from the likes of Microsoft, Disney franchises and the enormous toy companies like Hasbro and Mattel. The work of Justine Cassell at MIT with ”StoryMat”, Dr Alison Druin with the ”PETS” projects 5 (from the Human Computer Interaction Lab at the University of Maryland) are distinct in pedagogy and interactive strategy from most commercially available toys. The toys have a clear philosophy of use as ’learning technologies’ rather than simply embedding the
4 See http://www.bbc.co.uk/dragonsden/ 5 P.E.T.S. - ”Personal Electronic Teller of Stories” robotic pets that support
children in the storytelling process
latest speech recognition and synthesis chips in order to maximise rich play or to aim for ’realism’, or to service a franchise.
It should be noted that sponsorship relationships exist between the toy companies and innovative research groups in universities. An extended quotation from David Shenk’s article Behold the Toys of Tomorrow illustrates the connections between technological innova- tion, the toy corporations and the University researcher. It also con- nects:
”The computerisation of toys also dovetails nicely with the am- bitions of computer evangelists, those whose life’s mission it is to deliver the power of computation into every aspect of every person’s life. Nicholas Negroponte, the director of MIT’s famously innova- tive Media Laboratory (the Vatican of techno-evangelism), noted last year in his Wired column that toys are the ”fastest evolving vehicles on the infobahn,” meaning that because of their astonishing turnover rate (each year, 75 percent of the toys on shelves are newly designed), they’re the only class of objects that can truly keep up with the rapid pace of hardware and software innovation. That, combined with the tantalizing prospect of winning young, impressionable children over to the virtues of computers, has catapulted toy technology into high- priority status for the Media Lab. While researchers there have been exploring the issue for decades, they substantially upped the ante earlier this year with the formation of an industry-research consor- tium called ”Toys of Tomorrow.” A dozen or so companies, including Mattel, Tomy, Intel, and Bandai (makers of the infamous Tamagotchi ”virtual pets”), have signed up, committing to at least three years of the $250,000 annual sponsorship fee. In return for the funding (a modest R&amp;D investment for any sizable company), sponsors get first crack at the new technology and ideas – a head start that seems bound ultimately to be worth many times that sum.
The Media Lab is a Willy Wonka factory for technophiles, where the only limitations are in the creators’ imaginations. Intoxicated by the MIT fumes, one thinks: How could this not be a boon to society?” (Shenk, 1999) [12]
It should be noted that talking toys and animated toys are often adult orientated, rather than for children. This may be because such devices express the extraordinary fascination with what contempo- rary technologies can do. Jacques de Vaucanson’s ’defecating duck’ was not a toy - but a remarkable exploration of what clockwork and air power could do. Likewise, Edison’s talking doll:
”One can only conclude that the [Edison’s] dolls were not for chil- dren, and adults like [Albert Hopkins (Editor of ’scientific american’ c1890)] were not alone in picking up on their aggressive horror. Formanek-Brunell quotes a survey taken at the time Edison’s dolls were manufactured, in which a four year old girl, fusing the animate with the inanimate in a way that recalls Vaucanson’s duck, said she didn’t like talking dolls, because ’the fixings in the stomach are not good for digestion’.” (Wood, 2002, 118) [19]
2.3	Talking toys are monologic rather than dialogic
Talking toys are rarely conversational agents, and interaction is heav- ily pre-determined. Randomising responses is one strategy that pro- vides an illusion of knowing, active conversation. Such illusions are broken when the pattern or repetition is noticed.
Arguably, talking toys fix patterns and structure play and are, in nature, didactic and instructional. However, structured or program- matic experiences are crucial to learning, play and language develop- ment. I am not simply dismissing such toys dogmatically from some valorised over-emphasis on the value of ’free play’. Lev Vygotsky:
”Let us turn now to the role of play and its influence on a child’s
development. I think it is enormous. I think that play with an imag- inary situation is something essentially new, impossible for a child under three; it is a novel form of behavior in which the child is liber- ated from situational constraints through his activity in an imaginary situation.” (Vygotsky, 1933) [18]
I would argue that more ’situational constraints’ are imposed by over-structured, adult led, media influenced play, than child centred play. It may be a useful moment to introduce the term ’metaxis’ used in education drama contexts for describing the ’dualness’ of percep- tion during role-play and ’as-if’ contexts. Metaxis has been defined as ”the state of belonging completely and simultaneously to two dif- ferent autonomous worlds” (Boal 1995, 43) [1]. This definition has interesting resonance when considering the virtual realities created by digital talking toys.
2.4	Talking character-based toys, that are dependent on other media, are derivative and closed narrative systems
Often talking toys become extensions of pre-existing media that project ideas outwards, from toy to child, rather than being empty vessels that facilitate projection from child to toy. This is particularly acute in any toy that represents known characters from other media productions, the huge so-called ’character toy’ market.
What difference does it make if the imaginary topic of make- believe style play with talking toys is sourced from existing media, rather created from within than the child herself?
Justine Cassell creates story environments and interactive objects to research the quality of technology assisted spontaneous play and story creation. She [2] carefully documents and quantifies the gener- ative, creative effects of certain (I would call ’dialogical’) interactive technologies. Using quantitative and visualisation methods, she care- fully annotates the original spontaneous vocal contributions offered by children while playing with interactive toys. She also transcribes and qualitatively analyses the text of stories children create using her ’environment’.
Viewing Companions” [16], and in later work 6, builds an extended theory of how talking interactive toys can act as ’scaffolding’ for learning interactions, act as ’buddies’ and simulated co-learners. In more recent work, Strommen clearly delineates between interactive toys as surrogates for adult interventions, toys as establishing shared contexts for extended social interactions (i.e. such toys need parental supervision and interaction) and pure play without any pedagogical intent: ”Whos in charge? If the children are the ones setting things up not just physically but conceptually, if they are showing each other what to do, collaborating, its play. If the children are being told what to do, led, directed, or tested, its not play” (Strommen, 2004, ) The more interesting counter-examples of toys not obviously promoting language acquisition are speaking toys that babble and create their own non-human languages that parody child language. Such toys do not induct nor reinforce the adult designed language structures of nursery rhymes, alphabet led rote learning and traditional language learning games Such toys and characters, eg. the Norns in the Crea- tures series ’language’ [3] and the talking Furbies native language ’Furbish’, communicate through prosody and gesture and are not limited by the need to process real language structures. Bizarrely (and wonderfully - in terms of creativity and useless play) such toys have lead to the players acquiring and learning nonsense languages.
On developing ”Nornish”:
”We decided to look for a way of converting anything the Norns said into sounds, in such a way that a) the words sounded like speech, and b) a word would sound the same every time it was spoken, and c) different words should have different pronunciations. Just to make life difficult for myself, I also added d) similar words should sound similar. The first step was to record some speech. Luckily, one of the artists working on the game had something of a gift for mak- ing bizarre noises, so we gave him a script full of gibberish and recorded him babbling away. This was then chopped up into indi- vidual syllables, and electronically treated to give male and female voices. Having been presented with a large collection of syllables, I went through them and decided whether they sounded like the start of a sentence, the middle of a sentence or the end of a sentence. Hav- ing established these groups, I let ”nature” do the tricky bit for me. I came up with a way of using a random set of numbers to convert any group of three letters into one of these syllables. I then let these random numbers ”breed” until I had a vocabulary that fitted all my requirements - all groups of letters had a corresponding sound, simi- lar groups sounded similar, and I could recognise the starts and ends of sentences. Norns have a very small vocabulary, so in principle, it should be possible to learn to understand ”Nornish” - I confess I’ve never had the patience, though I’d love to hear if anyone has.” (Peter Chilvers, no-date) [3] People have learnt ”Nornish”.
2.5
Talking Toys represent an adult intervention into child-play
First, adults buy toys and design toys, and their associated pedagogy, for children. Children of a certain age exert pressure and express de- sires for certain toys, stoked by the marketing messages of the larger toy companies. Most talking toys enshrine messages and pedagogy from adults to children and on occasion seek to replace or act as sur- rogates for social parental contact: A selling point of a recent the UK designed toy, iTeddy, a strange ’Tellie-Tubbies’ and iPod hybrid, a bear with a media player embedded in it’s stomach, was the ’com- forting’ effect the toy had to placate children during the absence of a peer, buddie, parent or supervisor. The surrogate suckling / child rear- ing function of childrens media and TV are transferred into the toy itself. Like many toys of this ilk, iTeddy, refreshes its onboard con- tent of nursery rhymes and stories using networked connectivity to a custom web-site. Interactive toy guru, Erik Strommen is having a fas- cinating career that takes in companies as diverse as ”The Childrens Television Workshop” creators of educational puppet-fest ”Sesame Street”, several game companies and Microsoft, where he worked on and promoted the Actimate series of interactive toys. In his 1999 paper Learning from Television With Interactive Toy Characters As
2.6
What talking toys say is more important than how they say it
Talking toys enshrine a pedagogy that has, in effect, remained un- changed since Edison’s ’talking doll’ of the late 19th Century’. The pedagogy is built on cautionary tales and stories, wrote learning of nursery songs and instruction led play.
Elsewhere in this paper, I have mentioned Microsoft’s Actimate ’Barney the Dinosaur’ being accused of lying - mainly because he doesn’t have a consciousness yet talks freely of love. The ideological
6 When the Interface is a Talking Dinosaur: Learning Across Media with Ac- tiMates Barney (1998) [13] Learning from Television With Interactive Toy Characters As Viewing Companions (1999) [14] Interactive Toy Characters as Interfaces For Children (2000) [15]
function of what talking dolls say is critical when evaluating talking toys.
The New York based Barbie Liberation Organisation (BLO), a group of feminist hactivists formed circa December 1993, undertook a remarkable operation to swop the voice boxes of 300 Barbie and G.I. Joe dolls.
”When Barbie speaks, little girls listen, which is why controversy erupted in 1992 when Teen Talk Barbie exclaimed, ”I love shop- ping,” ”Meet me at the mall,” and ”Math class is tough.” This last phrase struck an especially sour note, given the under-representation of women in the sciences” (Dery 1994) [5]
”The Simpsons” episode Lisa vs. Malibu Stacey is a wonderful parody of the ’Teen Talk Barbie’ controversy. The episode explores issues of what dolls are programmed to say and activist / feminist responses to such things. 7. There is even a subtle reference to the underground work of the Barbie Liberation Organisation when the girl doll ”Malibu Stacey” is heard to say ”My Spidey-sense is tin- gling. Anyone call for a webslinger?”.
Although the technologies of embedded speech are fascinating, whether using the latest embedded microprocessers for speech recog- nition and synthesis processors 8, bellows and air, miniature phono- graphs, it seems that whenever toys talk the meaning of the iteration outweighs the possibilities inherent in technical act of production.
2.7	Talking toys are of greater value when they are programmable and configurable by children
2.8	Animated facial mechanisms attempt to re-embody dis-embodied voice and, in turn, over-concretise and limit imaginative play
”Phenomenologically, there is a close relationship between the voice and the face; both the voice and the face are parts of us that are turned outwards and by which the world knows us, but which we can our- selves only see or hear partially. They signify intimacy and vulner- ability. We are our faces and we are our voices . ” (Conner, 2000, 401) [4]
It is a trend in recent talking toys, to have complex animated faces. This is in part due to a desire to a create an illusion of an ’embodied’ voice. Disembodied voices tend towards the ’uncanny’.
In the history of puppet theatre, there is a pronounced difference between forms that attempt to locate the voice ’within’ an object by rhythmically mapping gesture and movement to voice and articulated face parts, and those forms that rely on the play of light on static sculpted forms of faces to suggest expression and facial motion. The misbegotten primary aim of ’more’ articulation is ’more’ realism - greater verisimilitude in the imitation of living forms.
This is echoed in the aims of makers Hasbro and Voice Signals recent technology expressed in a press release. The seek to offer a more interactive, ’richer’ play experience through speech activation and recognition:
”We are extremely pleased that Hasbro has selected Voice Sig- nal’s MicroREC speech recognition software for this product [Aloha Stitch],’ commented Stewart Sims, Voice Signal’s executive vice pres- ident of marketing. ’Hasbro has a well-deserved reputation for creat- ing fun, innovative, quality products, and we are delighted that they have chosen Voice Signal to supply the speech recognition software
7 First broadcast: in ”The Simpsons” Season 5, September 30, 1993 May 19, 1994
8 See the wonderfully named E.L.V.I.S. ”the Embedded Large Vocabulary Interface System platform” from VoiceSignals Technologies. [17]
that increases the interactivity of their toys and brings ’Aloha Stitch’ to life.” [17]
In Hasbro’s ’Aloah Stich’, the Voice Signal voice chip creates a ’bi-polar’ toy, that varies its responses to a set number questions ac- cording to one of two moods. I quote an online review of the toy by a parent at length as it is hard to access information about the perfor- mance and interactive sequences of most of the toys under consider- ation in the present paper:
”So just what kind of smack does Stitch talk? Here’s a rundown of cues and replies
You say - ”What’s your badness level” He says - ”Mostly Good Today” or a sheepish, ”I’m having a pretty good day” when he’s nice and Naughty! Blarrrtghll! when he’s rotten.
You say, ”Are you hungry?” He says, ”Coconut cake and coffee, please.” when he’s nice and ”Not anymore, I ate your dinner” when he’s rotten.
You say, ”Sing a Song.” He sings Aloha Oe when he’s happy and burps it when he’s rotten (way too funny).
You say, ”I know you can talk.” He says, ”Okay Okay” when he’s nice and ”Doggies cannot talk” or ”Bark! Bark!” when he’s rotten. You say, ”Got to Sleep” He says, ”Very Sleepy. . Snore” when he’s
nice and barks, ”No, make me a sandwich!” when he’s rotten. You say, ”Where are you?” He says, ”I’m with my family” or a very sad, ”I’m lost” when he’s nice and ”Stinky Planet Earth”
(which comes out yarth) when he’s rotten. You say, ”Will you play?” He replies, ”Surf’s Up! Cowabunga!”
or ”I can’t I have nothing to wear.” when he’s happy and ”I’m busy get lost!” or ”I’m busy you go away, okay?” when’s he’s rotten.”
(anon, 2004) [8]
Inanimate children’s toys, which the child enlivens with move- ment and either unspoken or spoken voice are, to me, more enduring playful simulations - as the mutability, the changeability, the fluidity of roles are emphasised through imaginative projections on a static face, rather than the repetitive ’fixed movements’ of most automated articulated movement.
In an article on ’Mike the Talking Head’ (an extraordinary head mounted facial performance capture system and CG digital puppet documented circa 1988), Valarie Hall comments: ”When it all comes together, the quest for realism in character animation is but a test for the animator to find out how good he/she is at using the tools they have at their disposal.” (Hall, nodate) [6]
Any aesthetic assessment about ’realism’ in talking toys and how it effects play needs to balance manufactures promotional excitements with an assessment of the whole interactive system.
3 CONCLUSION
By means of conclusion, I will simply state the two remaining theses and leave them hanging for further cogitation. Also, this study has uncovered a topic richer and more varied than I had initially specu- lated and it is a pregnant ground for future research.
3.1	Talking toys are extraordinary simulators of intelligence and presence
3.2	Talking toys, traditional and digital puppets and animated media forms are more inter-connected than we may first think
ACKNOWLEDGEMENTS
I would like to thank the referees for their comments which helped improve this paper and the symposium conveners for their patient support.
REFERENCES
[1]	Augusto Boal, The Rainbow of Desire: the Boal Method of Theatre and, Routledge, London, 1995.
[2] Justine Cassell and K Ryokai, Making Space for Voice: Tech- nologies to Support Children’s Fantasy and Storytelling, http://web.media.mit.edu/	kimiko/publications/PersonalTech.pdf, Last Modified: 2001. Date Accessed: 01/03/2007.
[3]	Peter Chilvers, Creature Labs - Norn Babblings, http://www.gamewaredevelopment.co.uk/creatures more.php,	Date Created: nodate. Date Accessed: 01/02/2007.
[4]	Steven Conner, Dumbstruck: A Cultural History of Ventriloquism, Ox- ford University Press, New York, 2000.
[5] Mark Dery, Hacking Barbie’s Voice Box: ’Vengeance is Mine!’, http://www.levity.com/markdery/barbie.html. New Media magazine, ”Technoculture” column., Date Created: 05/1994. Date Accessed: 1/2/2007.
[6] Valarie	Hall,	Mike	(the	talking	head), http://mambo.ucsc.edu/psl/mike.html, Date Created: nodate. Date Accessed: 01/02/2007.
[7] Michael Newman, Interactive Barney: Good or evil? Conferees worry about where computerized ’character’ toys are going next, http://www.post-gazette.com/businessnews/19990521barney1.asp, Date Modified: 21/05/1999. Date Accessed: 01/03/2007.
[8] Anonymous	Parent,	Read	Reviews	of	Has-
[18]	Lev Vygotsky, Play and its role in the Mental Development of the Child, http://www.marxists.org/archive/vygotsky/works/1933/play.htm, First Published: 1933. Date Created: 2002 . Date Accessed: 01/02/2007.
[19]	Gaby Wood, Living dolls : a magical history of the quest for mechanical life, Faber, London, 2002.
eOpinions, http://www.epinions.com/content 163285929604?linkin id=8003929,
bro	Aloha	Stitch	Doll	3570	at
Date Created: 28/11/2004. Date Accessed: 01/02/2007. [9]	Idris Parry, Essays on Dolls, Syrens, London, 1994.
[10]	Jasia Reichardt, Robots : fact, fiction and prediction, Thames and Hud- son, London, 1978. (by) Jasia Reichardt. ill(some col), plans, ports ; 28cm (Pbk).
[11]	Rainer Maria Rilke, ‘Dolls: On the wax dolls of lotte pritzel.’, in Essays on Dolls, ed., Idris Parry, Syrens, London, (1994).
[12]	David Shenk, Behold the Toys of Tomorrow (The Atlantic Online - Dig- ital Culture), http://davidshenk.com/webimages/atlantic1.htm, Date Created: 07/01/1999. Date Accessed: 01/02/2007.
[13] Erik F. Strommen, When the Interface is a Talking Di- nosaur: Learning Across Media with ActiMates Barney, http://www.playfulefforts.com/archives/papers/CHI-1998.pdf,	On- line PDF of published work. Date Written: 1998. Date Accessed: 01/03/2007.
[14] Erik F. Strommen, Learning from Television With In- teractive	Toy	Characters	As	Viewing	Companions, http://www.playfulefforts.com/archives/papers/SRCD-1999.pdf, Online PDF of published work. Date Written: 1999. Date Accessed: 01/03/2007.
[15]	Erik F. Strommen, Interactive Toy Characters as Interfaces For Chil- dren, http://www.playfulefforts.com/archives/papers/IA-2000.pdf, On- line PDF of published work. Date Written: 2000. Date Accessed: 01/03/2007.
[16] Erik F. Strommen, Play? Learning? Both...or neither?, http://www.playfulefforts.com/archives/papers/AERA-2004.pdf, Online PDF of unpublished work. Date Written: 2004. Date Accessed: 01/03/2007.
[17] VoiceSignals Technology, VoiceSignals Technology Press Release, http://www.voicesignal.com/news/press/release 02 19 02.html,	Date Created: 19/02/2002. Date Accessed: 1/2/2007.</Text>
        </Document>
        <Document ID="346">
            <Title>Video-myron krueger video place 88 doc</Title>
        </Document>
        <Document ID="39">
            <Title>Keywords</Title>
            <Text>Keywords

Digital Puppetry;
Performance;
Performance Technology;
Virtual Reality;
Augmented Reality;
Interface Design for Real Time Applications;
Gestural and Haptic Interfaces;
3D Programming;
3-D Graphics;
Computer Vision;
</Text>
        </Document>
        <Document ID="235">
            <Title>figure_013_3D_Scene_03</Title>
        </Document>
        <Document ID="124">
            <Title>2.4.3 Puppets as Evocative Objects</Title>
            <Text>Puppets as Evocative Objects</Text>
        </Document>
        <Document ID="457">
            <Title>Selection of Project Images</Title>
        </Document>
        <Document ID="347">
            <Title>Video-myron krueger video place doc excerpt</Title>
        </Document>
        <Document ID="236">
            <Title>figure_wii_remote_patch</Title>
        </Document>
        <Document ID="125">
            <Title>Virtual</Title>
            <Text>3.2 Virtual
</Text>
        </Document>
        <Document ID="348">
            <Title>worthington_shadow_monsters</Title>
        </Document>
        <Document ID="237">
            <Title>figure_wii_eye_composite_surreal_001</Title>
        </Document>
        <Document ID="126">
            <Title>Physical</Title>
            <Text>3.3 Physical 
</Text>
        </Document>
        <Document ID="459">
            <Title>Websites to Check</Title>
            <Text>Improv: Interactive Improvisational Animation and Music

Eric Singer, Athomas Goldberg, Ken Perlin, Clilly Castiglia, Sabrina Liao

Media Research Laboratory

New York University


http://ericsinger.com/ISEA96.html

Cynthia Breazeal</Text>
        </Document>
        <Document ID="349">
            <Title>Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_001</Title>
        </Document>
        <Document ID="238">
            <Title>figure_wii_controlled_avatar_004</Title>
        </Document>
        <Document ID="127">
            <Title>Video</Title>
            <Text>3.4 Video
</Text>
        </Document>
        <Document ID="239">
            <Title>figure_wii_controlled_avatar_003</Title>
        </Document>
        <Document ID="128">
            <Title>Hybrid</Title>
            <Text>3.5 Hybrid

</Text>
        </Document>
        <Document ID="129">
            <Title>Communication</Title>
            <Text>4.2 Communication
</Text>
        </Document>
        <Document ID="90">
            <Title>New Media Theory: Media Archeologies</Title>
            <Text>New Media Theory: Media Archeologies
</Text>
        </Document>
        <Document ID="91">
            <Title>Theories of Interaction</Title>
            <Text>2.7 Interactive Theory
</Text>
        </Document>
        <Document ID="92">
            <Title>Theories of Play and Learning</Title>
            <Text>2.8 Game Theory
</Text>
        </Document>
        <Document ID="93">
            <Title>Instructional Texts</Title>
            <Text>Instructional Texts
</Text>
        </Document>
        <Document ID="460">
            <Title>Presentation - Sussex</Title>
        </Document>
        <Document ID="94">
            <Title>Methods and Methodology</Title>
            <Text>Methods and Methodology
</Text>
        </Document>
        <Document ID="95">
            <Title>Conclusion: Hybrid Practice</Title>
            <Text>Conclusion: Hybrid Practice</Text>
        </Document>
        <Document ID="461">
            <Title>Suseex Update - 2011 June</Title>
            <Text>Defining the Digital Puppet: PhD Practice Update 

I wish to propose my definition of the digital puppet and present two current projects:
 (1) Shadows and Surfaces: Designing the ShadowEngine for Multitouch Interaction
(2) Soft Bodies: Exploring Malleable Digital Objects

]
My recent work, Surfaces and Shadows, explores the interface between traditional shadow puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis as a whole will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of different modes of interactive digital puppetry. For this purpose, I am creating a new performance animation system and software. 

'Shadows and Surfaces' presents the design and testing of one first stage prototype application: the 'ShadowEngine' running on an multitouch device. As a practical exploration, the 'Shadows and Surfaces' project presents an exegesis of the software design process for a prototype application called the 'ShadowEngine', that makes multi-touchable digital shadow theatre possible using physics based real-time animation and I indicate some preliminary design level insights and present potential approaches to project evaluation and testing. The animatable characters and objects are curiously expressive, and the analysis begins to refine the hermeneutic issues involved when operating and viewing complex multi-jointed characters in a physics-based 3D (pseudo 2D) environment.

In the current project, 'Soft Bodies', I am exploring (i) different interaction approaches, new configurations of analogue puppet control methods and (ii) the expressive potential of simulated deformable objects using different mass/spring models. There are analogies to traditional types of puppet and object theatre, but challenges are presented to our emerging definition of the 'digital puppet'.

The main aim is to a perform a hybrid performance and design a digital performance system to construct real-time collaborative character animation, additionally, with scenographic control. The current projects are presented as research and development and proof-of-concept(s) towards this end.
</Text>
        </Document>
        <Document ID="350">
            <Title>Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_002</Title>
        </Document>
        <Document ID="96">
            <Title>Glossary</Title>
            <Text>\printglossary</Text>
        </Document>
        <Document ID="97">
            <Title>Performer Testing</Title>
            <Text>7.3 Performer Testing
</Text>
        </Document>
        <Document ID="240">
            <Title>figure_wii_controlled_avatar_002</Title>
        </Document>
        <Document ID="462">
            <Title>Current Leads / Notes</Title>
            <Text>DR MELISSA TRIMINGHAM undertook a research project creating a performance (Manna from Heaven) with Shadowland Theatre, Ward’s Island, Toronto in May/June 2005 with Research Committee funding. Toronto has a thriving puppet theatre scene led by such groups as Shadowland Theatre and Puppetmongers. While in Toronto, Dr Trimingham attended an informal Puppet Forum at Puppetmongers workshop, alongside many leading Canadian puppeteers including Ronnie Burkett.
In June 2005, after the short but intensive preparatory period, Manna from Heaven was taken down to New York as part of the Great Small Works Seventh International Toy Theater Festival at St Ann’s Warehouse. Great Small Works are one of the leading puppet companies in New York and have just been awarded the Jim Henson Award for services to Puppetry. Members include John Bell who edited Puppets, Masks and Performing Objects for the MIT Press (2001). This festival included performances by puppet theatre companies from Mexico, Palestine and Holland as well as groups from America. The Festival included a temporary Toy Theater Museum, with over 100 intricate small scale exhibits, curated and designed by Alessandra Nichols. (more details soon)
The Research committee funded a day long puppet/robotics workshop in April 2005 in preparation for a presentation by Dr Melissa Trimingham at the PARIP (Practice as Research in Performance) International Conference at Leeds University, Bretton Hall campus, in June/July 2005. The committee also funded Dr Trimingham’s attendance at that conference.
The workshop was led by Dr Melissa Trimingham working with Dr Gordon Ramsay, of Loughborough University (currently at Nottingham University), professional puppeteer Yvonne Stone and freelance engineer Paul Snow.
The April workshop tested the application of pneumatic control mechanisms developed by Paul Snow, following a previous robotics workshop, in December 2003 at Leeds University working with ShadowRobot Company. (more details soon)

http://www.kent.ac.uk/arts/research/researchcommittee.html
--


http://www.edrants.com/the-early-films-of-jim-henson/

--
I would say also make sure to check out the Prague Linguistic School
theories of semiotics of puppet theater.  Matejka and Titunik's
"Semiotics of Art: The Prague School Contributions" has a number of good
articles about the function of puppets.  Both Jurkowski and Tillis are
indebted to that work.


j bell
PUPCRIT
--

http://www.nyu.edu/classes/bkg/issues/bell-objects.htm

--
http://institute.emerson.edu/wip/johnbell/samplechapter.pdf


--

http://artsci.wustl.edu/~perfhist/spr_07_essays/natural_dance.html


--

http://culture.wnyc.org/articles/talk-me/2011/apr/29/behind-warhorse-puppeteers-new-school/

Behind 'War Horse': The Puppeteers at The New School
Friday, April 29, 2011
By Sarah Montague

		# Enlarge  Basil Jones and Adrian Kohler of the Handspring Puppet Company with friends. (Sarah Montague/WNYC)  
		Talk To Me More
One of the most powerful aspects of “War Horse,” which opened at Lincoln Center on April 14, is, of course, the astonishing puppets. Minutes into this riveting tale of a boy and his horse against the background of World War I (see our feature here), the audience has completely invested the “horses” with life.
This is just what the co-founders of the Handspring Puppet Company, Basil Jones and Adrian Kohler, who developed the production with Great Britain’s National Theatre, intended.
However, at a lecture given in The New School’s Tishman Auditorium the night before the opening--the event was co-sponsored by The Vera List Center for Art and Politics and the Sheila C. Johnson Design Center--puppeteers pulled back the curtain during a lively panel discussion and demonstration.
Joined by South African-born poet Yvette Christianse and Obie Award-winning puppeteer Dan Hurlin, Kohler and Jones talked about the origins of their company in apartheid South Africa; the use of puppetry as vehicle of advocacy; its place in the theatrical tradition; different schools of puppetry; and the form’s emotional and psychological impact. 
Steered by the delicate-voiced Christianse, the quartet also explored some of puppetry’s central paradoxes: that we see (but choose not to see) the puppeteers; and that like a poet refining herself out of a poem (Christianse’s image) the operators must be willing to disappear for the puppet to live.
Since the material and immaterial go hand in hand in puppetry, Jones and Kohler showed some video clips (excised in our audio above) of '“War Horse' in rehearsal,” and demonstrated the origins of their legendary horse puppets in some earlier old friends—a hyena demon from their production of “Faustus in Africa,” and Lisa, a chimpanzee from their piece “The Chimp Project.” 
Bon Mots:
Adrian Kohler on submission to the puppet: "There is a form of…subjection.  You have to put your whole being into this emotional prosthesis."
Basil Jones on movement as thought: "We’re moving towards a slightly radical and cheeky proposition—that movement is thought. We’re trying to say that thought and the body are not two separate things."
Adrian Kohler on the impact of “War Horse”: "That loss [of nearly a million horses in World War I] that affected us all is somehow reflected in our love of the beauty of a real horse: we no longer live with them, and that’s part of the tragedy of the story."
Dan Hurlin on what we see: "Because of the distance between the puppets and us, they are actually better mirrors of who we are."
Yvette Christianse on art and puppetry: "The true aim of the artist is to disappear."
		The Secret Language of Puppets: 'War Horse' Comes to Lincoln Center
--

http://www.nyu.edu/classes/bkg/issues/bell-objects.htm
More John Bell

--
BODY SPACE IMAGE, NOTES TOWARDS IMPROVISATION AND PERFORMANCE.
http://www.dancebooks.co.uk/body-space-image-notes-towards-improvisation-and-performance-p-204.html</Text>
        </Document>
        <Document ID="351">
            <Title>Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_003</Title>
        </Document>
        <Document ID="98">
            <Title>Collaborative Acts</Title>
            <Text>7.2 Collaborative Acts</Text>
        </Document>
        <Document ID="99">
            <Title>2.4.1 Audience Studies, Semiotic and Reception Theory</Title>
            <Text>Audience Studies and Reception Theory
</Text>
        </Document>
        <Document ID="241">
            <Title>figure_003_real_time_garbage_matte</Title>
        </Document>
        <Document ID="130">
            <Title>The Kinetic Behavioural Object</Title>
            <Text>4.3 The Kinetic Behavioural Object 
</Text>
        </Document>
        <Document ID="463">
            <Title>Workshop Design</Title>
            <Text>Test and Evaluate strategies by designing workshop structures. ...</Text>
        </Document>
        <Document ID="352">
            <Title>Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_004</Title>
        </Document>
        <Document ID="242">
            <Title>Introduction and Aims</Title>
            <Text>*Digital Puppetry* is a hybrid art form that presently includes a broad range of work and creative practices:

* live *real-time* animation and performance capture processes

* rendered computer generated animation process, referring to animation performance systems, like Pixar Studio's *Marionette* software

* automata and kinetic art practice, involving servo/microprocessor controlled automata, e.g. the art of Ken Feingold[^ChQCVidKenFeingold]

* live multimedia performance 

* user-controlled characters in animations or *machinima*, often made with Adobe's *Flash*, game engines or Linden Lab's *Second Life*.

[^ChQCVidKenFeingold]: Ken Feingold [http://www.kenfeingold.com/](http://www.kenfeingold.com/)

When comparing digital puppetry to tradition forms, discussion often involves (i) the distance between the performer and the physical or virtual object and (ii) how the performer is embodied within the physical or virtual object. In the current work, I compose a wirelessly controllable on-screen object using video-graphic elements of a performer (their mouth and eyes) that are reconfigured in real-time into a new image. The same performer can then control the orientation and expressive motion of the speaking image/avatar. I am interested in making virtual objects speak and ask: how does a visual image of a speaking mouth embed the performer in the puppet and create the perception of an expressive living character?

The performance piece, *Of Minnie the Moocher and Me*, had a central visual idea that combined a real-time chroma-keyed mouth and interactive pre-recorded video of eyes. Through movement and reconfiguration, I believed these abstracted facial elements could form a coherent expressive system for a digital puppet (see [][#fig:figure_001_composite]). Add to this real-time vocalisation and synchronised speech, gestural object control using a Nintendo *Wii Remote*[^ChQCWiiRemote], MIDI controlled sequencing of scenography and special effects using foot-switches and a control deck, and I may have a new -- low cost -- approach to digital puppet performance and capture.

[^ChQCWiiRemote]: Wii-Remote Controller: [http://wii.nintendo.com/controller.jsp](http://wii.nintendo.com/controller.jsp)

![fig:figure_001_composite][]
[fig:figure_001_composite]: figure_001_composite "Composite of real-time mouth, recorded eyes and digital scenography" width=418px height=314px

For visual reference, I turned to American popular culture and a number of out of copyright (public domain) animation forms. I have always had a love of the Fleischer brothers' cartoons of the 1920s and 1930s. Mainly famous for creating *Betty Boop*, *Popeye* and the *Out of the Ink Pot* clown, the Fleischers  are less well known for creating *rotoscoping* -- a technique of painting inks on cells over a mechanism projecting film -- basically tracing film in order to create animation. They established the technique by tracing the performance of one of my favourite jazz singers,  Cab Calloway. In *Snow White* (1933)[^chQCSnowWhite], preceding Walt Disney's version of *Snow White* by a number of years, Calloway sang *Saint James Infirmary* -- a mourning/blues song -- and created an enduring stylistic affinity between early animation and jazz.

[^chQCSnowWhite]: The Fleischer Studios *Snow-White* (1933) See:\\ [http://www.imdb.com/title/tt0024578/](http://www.imdb.com/title/tt0024578/)

![fig:figure_002_cab_roto][]
[fig:figure_002_cab_roto]: figure_002_cab_roto "Rotoscoping in the Fleischer brothers' `Snow White' (1933). Image generated using a `Quartz Composer' composition by Ian Grant" width=418px height=314px

[][#fig:figure_002_cab_roto] depicts the opening and mid-point sequences from the Fleischers' `Snow White' (1933) and illustrates the live-footage moments and the final animated *rotoscoped* sequence.

It is with the same spirit of technological innovation that I aimed to explore similar material with contemporary technology. I aimed to create a real time performance system where I create and control a video based digital character that sings the song, *Minnie the Moocher* by Cab Calloway. I aimed to create the following:

* A performance control system using Apple's *Quartz Composer*[^ChQCQC], a general purpose MIDI controller and a wireless game controller, the Nintendo *Wii-Remote*.

[^ChQCQC]: Apple's *Quartz Composer:*\\ [http://developer.apple.com/graphicsimaging/quartz/quartzcomposer.html](http://developer.apple.com/graphicsimaging/quartz/quartzcomposer.html)

* A video processing/projection system with instant chroma-keying of a blue faced performer.

* Some nifty realtime effects, transitions and visual compositions, i.e. elegant digital scenography -- illustrated in [][#fig:figure_007_full_title] -- [][#fig:figure_013_3D_Scene_03] below.

* To work towards a fuller expanded performance telling the extended saga of *Minnie the Moocher*.



</Text>
        </Document>
        <Document ID="131">
            <Title>Psychological Processes of Projection</Title>
            <Text>4.4 Psychological Processes of Projection
</Text>
        </Document>
        <Document ID="464">
            <Title>More Leads</Title>
            <Text>Google: a typology of puppets
</Text>
        </Document>
        <Document ID="353">
            <Title>Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface</Title>
        </Document>
        <Document ID="243">
            <Title>MMD - Testing Images</Title>
            <Text>The snippet below works:

------------------------------------------------
Here is a link to an image --- [Nautilus
Star](#nautilusstar) and [Nice Hello](#nicepic). If you  have a local copy of the  image, you can include
the image in a pdf. The square bracket turns into text in latex and the (#xxx) turns into an auto-ref. The xxx is the lowercase, no space 'label' that gets generated from the image tag. To get the image included: add it to Scrivener, then insert a scriver link to the image in the 

![This is a **bolded** caption][Nautilus Star]
[Nautilus Star]: Nautilus_Star "Nautilus Star" width="3in" height="2.4in"

![This is my figure caption][Nice Pic]
[Nice Pic]: figure_014_piano_with_avatar "Not sure what this is for maybe some xhtml alt or title text not used in the latex" 

------------------------------------------------

In the LATEX - to make a framebox, you can wrap the included image: \framebox{ \includeimage … }


The follow doesn't seem to work:
--xx--
![fig:figure_001_composite][]
[fig:figure_001_composite]: figure_001_composite "Composite of real-time mouth, recorded eyes and digital scenography" width=418px height=314px

Testing a link to the image: [][#fig:figure_001_composite]


Make sure there are no underscore characters in the label name... I got errors.


Image Width = whole column = 418pt


I've been using 400pt


798 = 418
599 = 314

(factor) = 0.52380952</Text>
        </Document>
        <Document ID="132">
            <Title>Vocal</Title>
            <Text>4.4 Vocal 
</Text>
        </Document>
        <Document ID="465">
            <Title>Methodology</Title>
            <Text>Methodology



For Geert Lovink, "media archaeology is first and foremost a methodology, a hermeneutic reading of the 'new' against the grain of the past, rather than a telling of the histories of technologies from past to present" (2003, 11). 


For Erkki Huhtamo, it is "the study of the cyclically recurring elements and motives underlying and guiding the development of media culture," and "the 'excavation' of the ways in which these discursive traditions and formulations have been 'imprinted' on specific media machines and systems in different historical contexts, contributing to their identity in terms of socially and ideologically specific webs of signification" (1996). ( My personal favorite (the theorist who inspired me most while working on my thesis) is Siegfried Zielinski, who commands that the media (an)archaeologist "not seek the old in the new, but find something new in the old" (2006, 3). I don't see Kittler mentioned often, but Chun cites him as an early practicioner, along with Foucault, whose notion of archaeology has been influential (if somewhat inverted)."</Text>
        </Document>
        <Document ID="354">
            <Title>Shadows_dragon003</Title>
        </Document>
        <Document ID="244">
            <Title>New Folder</Title>
        </Document>
        <Document ID="133">
            <Title>Facial</Title>
            <Text>4.5 Facial
</Text>
        </Document>
        <Document ID="466">
            <Title>book to check</Title>
            <Text>Bernadette Wegenstein - Getting Under the Skin: Body and Media Theory</Text>
        </Document>
        <Document ID="355">
            <Title>Shadows_kreuger_videospace_001</Title>
        </Document>
        <Document ID="245">
            <Title>Multi-Modal Practice</Title>
        </Document>
        <Document ID="134">
            <Title>4.6.1 Masks as Expressive Objects</Title>
        </Document>
        <Document ID="467">
            <Title>early notes</Title>
            <Text>Interesting References



Casey Reas. Behavioral Kinetic Sculpture (Masters Thesis, MIT 2001)

Diagram after Frank Popper

&lt;http://acg.media.mit.edu/people/creas/thesis/>


ARTWORK
Tony Oursler. Glimmer, 1999
&lt;http://tonyoursler.com/>
&lt;http://tonyoursler.com/tonyourslerv2/main.html>


Eyes (1996)

Talking Photo (1996)

The Influence Machine (2002)


---

Eugenio Barba

Eugenio Barba. (1995) The Paper Canoe, Routledge, London.


Eugenio Barba (in collaboration with Nicola Savarese) The Secret Art of the Performer,  (1991)


Pre-expressive principles:

"In an organised performance the performers physical and vocal presence is modelled according to principles which are different from those of daily life" (Barba, 1995, p9)

There is an intercultural 'common pre-expressive state'.

Principles of the  Pre-Expressive Body / Performer
pre-expressive technique

(1) Balance in Action;
(2) The Dance of Oppositions;
(3) The Virtue of Omission - simplification of daily techniques (the extra-daily)


Eugenio Barba
-------------
The performer's different techniques can be conscious and codified or else unconscious but implicit in the use and repetition of a scenic practice. Transcultural analysis shows that it is possible to distinguish recurring principles in these techniques. The recurring principles, when applied to certain physiological factors - weight, balance, the position of the spinal column, the direction of the eyes in space - produce physical, pre-expressive tensions. These new tensions generate a different energy quality, they render the body theatrically "decided", "alive", "believable" and manifest the performer's "presence", or scenic bios, attracting the spectator's attention before any form of message is transmitted. This, of course, is a matter of a logical, and not chronological before.
The pre-expressive layer constitutes the elementary level of organisation in theatre. The various levels of organisation are for the spectator and in the performance, inseparable and indistinguishable. They can only be separated by means of abstraction, in a situation of analytical research or during the technical work of composition done by the performer. The capacity to focus on the pre-expressive level makes possible the expansion of knowledge with immediate consequences both in the practical, professional, as well as in the historical and critical fields of work.
Knowledge of the pre-expressive principles which govern the scenic bios can make it possible for one to learn to learn.

Source:
&lt;http://www.odinteatret.dk/ista/anthropology.htm>

---

http://www.jstor.org/pss/1146380

Mask Face and Machine Face
Mikhail Yampolsky and Larry Joseph
TDR (1988-), Vol. 38, No. 3 (Autumn, 1994), pp. 60-74   (article consists of 15 pages)
Published by: The MIT Press


---

Jules Verne - notes on the manuscript of Around the World in 80 Days - the Face isn't the only expressive organ
&lt;http://home.netvigator.com/~wbutcher/books/awed%20crit%20mat.htm>


The Digital Marionette

---

Presence and Pre-Expressivity,Part II
by Ralph Yarrow (Editor)
Publisher: Taylor &amp; Francis, Inc.
Pub. Date: March 1998
ISBN-13: 9789057021756
64pp
Series: Contemporary Theatre Review Series





---


"Mechanical information is the most basic of all types of information, 
relating to such qualities as mass, force, inertia, consistency and friction. 
These are qualities shared by all objects living and inanimate, including 
ourselves, and in the detection and interpretation of which we are very 
efficient, but for the communication or expression of which we have no 
adequate method except, perhaps, poetry. We can, for instance, record or 
transmit by radio the sound that a bird makes and we can photograph or 
televise a bird's appearance but we cannot record or transmit what it feels 
like to hold a bird in one's hands, except by making verbal comparisons."


The Relevance of Manipulation to the Process of Perception  
Edward Ihnatowicz 
Published in The Institute of Mathematics and its Applications, May 1977, 
pp 133-135 

---



Transdisciplinary Digital Art: Sound, Vision and the New Screen. 2008. R. Adams, S. Gibson, and S. Müller Arisona (eds.). Communications in Computer and Information Science (CCIS), Volume 7, Springer Berlin Heidelberg.


&lt;http://www.springer.com/computer/information+systems/book/978-3-540-79485-1>

--


live real-time animation and performance capture processes 

rendered computer generated animation process, referring to animation per- 
formance systems, like Pixar Studio’s Marionette software 

automata and kinetic art practice, involving servo/microprocessor controlled 
automata and animatronics

live multimedia performance 

user-controlled characters in animations or machinima, often made with 
real-time game engines or Linden Lab’s Second Life. 

---

Reconfiguring Human-Robot Relations
Suchman, L.
Robot and Human Interactive Communication, 2006. ROMAN 2006. The 15th IEEE International Symposium on
Volume , Issue , 6-8 Sept. 2006 Page(s):652 - 654
Digital Object Identifier   10.1109/ROMAN.2006.314474

Summary:
This paper explores cultural imaginaries in projects dedicated to the design of human-like machines. Working with discussions of mimesis as developed by anthropologists Michael Taussig, M (1993) and Gell, A (1998), the author looks at some exemplary realizations of 'socially intelligent' robots, proposing an approach aimed at demystifying and reenchanting such encounters. This alternative is developed through a close analysis of a project at the intersection of computing and new media art, performance artist Stelare's prosthetic head. Drawing on recent discussions within cultural anthropology, science and technology studies, and feminist theory, the author offers some suggestions for how we might differently conceptualize relations between humans and computational machines


---

---

The Machine. 
by Hulten, K.G. Pontus.
Publisher Information:
No Publisher Information Available 
Hulten, K.G. Pontus. The Machine as seen at the end of the Mechanical Age. Museum of Modern Art, NY 1968, 1st edition. Sq. 4to, orig. pictorial aluminum binding, 218, (2) pp, profusely illustrated. Remarkable embossed and stamped metal binding with a color image of the Museum (designed by Anders Osterlin.) A striking MOMA exhibit which provided a wide variety of artistic commentary of the technical age. Slight bend on lower front cover, o/w fine.


Book Id: 7542

Price: $145.00


---


</Text>
        </Document>
        <Document ID="356">
            <Title>Shadows_kreuger_videospace_002</Title>
        </Document>
        <Document ID="246">
            <Title>Concept Description -- Real Time Art</Title>
            <Text>In one way, the piece stands alone as a performance, in another it is an exploration of the software and technologies that enable digital storytelling.

Apple's *Quartz Composer* is indebted to experimental analogue video of the 1970s and 1980s -- where physical patch based video synthesisers and multiplexers were used to process real-time analogue video and do things like blue-screening, chroma-keying, vision-mixing and other visual effects. Such technology today is emulated in software and inspires digital creatives from visualists and VJs [Faulkner (2006)][#Faulkner:2006qy] to multimedia performers like Robert LePage (see *The Far Side of the Moon*[^ChQCLepage]) and Laurie Anderson (see her recent work as NASA's artist in residence[^CHQCAnderson]). The influence of early video processing has resonated through and been reanimated by  recent advances in computer graphics processing unit (GPU) capabilities. 

[^ChQCLepage]: Robert Lepage: [http://lacaserne.net/index2.php/theatre/the_far_side_of_the_moon/](http://lacaserne.net/index2.php/theatre/the_far_side_of_the_moon/)

[^CHQCAnderson]: Laurie Anderson: [http://www.laurieanderson.com/](http://www.laurieanderson.com/)

Visual programming software applications like *Quartz Composer*, Cycling 74's *max/msp*[^ChQCMax] and Troikatronix's *Isadora*[^ChQCIsodora] (named after dancer Isadora Duncan) have brought real-time 3D graphics and video processing into the hands of the wider performing arts community.

[^ChQCMax]: Cycling 74's *max/msp*: [http://www.cycling74.com/](http://www.cycling74.com/)

[^ChQCIsodora]: Troikatronix's *Isadora*: [http://www.troikatronix.com/isadora.html](http://www.troikatronix.com/isadora.html)

To place my explorations in a wider context -- one could call it an exploration in *augmented reality*. From Wikipedia: 

: "Augmented reality (AR) is a field of computer research which deals with the combination of real world and computer generated data (in real time). At present, most AR research is concerned with the use of live video imagery which is digitally processed and augmented by the addition of computer generated graphics." [Wikipedia Community (2009)][#Community:2009er]

The concept of real-time is crucial in the context of performance. There are huge semiotic differences between the live and recorded arts. I am exploring the dialectic between the improvisational and the composed (in other words *planned* or *sequenced*). Real-time computer media will help open up more improvisation space for the media artist. I think this is expressed through a varying number of channels: e.g. performance systems, improvisation while coding, while editing video or music, through playing with new styles of UI or interaction systems. Computer power and the ethos of cinematic interfaces drive us towards very playful systems. See Jeff Han's recent work [(Han, 2009)][#Han:fj] on haptic interfaces and multi-touch surfaces -- particularly the shadow puppet-like drawing with forward kinematics -- for a brilliant demonstration of an improvisational control system[^ChQCHan]

[^ChQCHan]: Multitouch Interaction Research: [http://cs.nyu.edu/~jhan/ftirtouch/index.html](http://cs.nyu.edu/~jhan/ftirtouch/index.html) and *Unveiling the genius of multi-touch interface design' [http://www.ted.com/index.php/talks/view/id/65](http://www.ted.com/index.php/talks/view/id/65)

Apple's *Quartz Composer* allows the use of MIDI control signals and other input data in a straightforward way. I have designed my digital puppet and scenographic space to allow elements of playful interaction that do not involve a conventional screen-based user interface, but are sequenced using wireless game controllers and MIDI decks. The movement of my digital character is tied to the real-time gestural possibilities of a consumer level haptic device.

Computer media, historically tied up with *un-dynamic* or stored media, is tending towards responsive, data-aware and dynamic systems. To clarify (and expand): 

: "The impossibility of technologically processing data in real time is the possibility of art... As long as processing in real time was not available, data always had to be stored intermediately somewhere -- on skin, wax, clay, stone, papyrus, linen, paper, wood, or on the cerebral cortex -- in order to be transmitted or otherwise processed. ..." [Sieger cited Gere (2006), 26][#Gere:2006lr]. 

Gere extends this nicely: "Art exists only within a certain *economy* of time produced by the materiality and temporality of culture's means of inscription, storage and exchange." [Gere (2006), 26][#Gere:2006lr]. *Quartz Composer* is one such environment that is tending towards complex, interactive real-time media production.</Text>
        </Document>
        <Document ID="135">
            <Title>Gestural</Title>
            <Text>4.6 Gestural
</Text>
        </Document>
        <Document ID="468">
            <Title>PhD Latest Search Terms</Title>
            <Text>Whole-Hand Input / Interaction

Interactive Shadows

Vision-based hand pose estimation

Dinasaur Input Device



Gesture Based Command Sets
</Text>
        </Document>
        <Document ID="357">
            <Title>Shadows_Levin_Interstitial_Space_001</Title>
        </Document>
        <Document ID="40">
            <Title>Bib (Published)</Title>
            <Text>Academic Literature and Indicative Bibliography [not consistently formatted nor ordered]

The select bibliography ballooned the word-count - so I have placed a fuller copy online 

Link: http://ellington.tvu.ac.uk/research/proposal/bib.html 

C. Baudelaire. The philosophy of toys. In I. Parry, editor, Essays on Dolls. Syrens, London, 1994.

J. Bell, editor. Puppets, Masks, and Performing Objects. MIT Press, Boston, 2001.

R. A. Brooks. Robot: The Future of Flesh and Machine. Penguin, London, 2002.

H. B. Segal. Pinocchio’s Progeny : Puppets, Marionettes, Automatons and Robots in Modernist and Avant-Garde Drama. Johns Hopkins University Press, 
Baltimore, 1995.

S. Kaplin. A puppet tree - a model for the field of puppet theatre. In J. Bell, editor, Puppets, Masks, and Performing Objects, pages 18–25. MIT Press, Boston, 2001.

M. Fuller. (2003) Behind the Blip: Essays on the Culture of Software. Tandem.

M. Fuller (ed.) (2008) Software Studies: A Lexicon (Leonardo Book) MIT Press.

Cawood, Stephen and Mark Fiala. (2008) Augmented Reality: A Practical Guide: The Complete Guide to Understanding and Using Augmented Reality Technology. Pragmatic.

Bimber, Oliver and Ramesh Raskar. (2005) Spatial Augmented Reality: A Modern Approach to Augmented Reality. Peters.

J. Cassell and K. Ryokai. Making Space for Voice: Technologies to Support Children’s Fantasy and Storytelling. http://web.media.mit.edu/ kimiko/publications/PersonalTech.pdf, Last Modified: 2001. Date Accessed: 01/03/2007.

S. Tillis. Toward an Aesthetics of the Puppet: Puppetry as a theatrical art. Greenwood, New York, 1992. Normal Loan 791.53 TIL.

S. Tillis. The art of puppetry in the age of media production. In J. Bell, editor, Puppets, Masks, and Performing Objects, pages 172–183. MIT Press, Boston, 2001.

G. Wood. Living dolls : a magical history of the quest for mechanical life. Faber, London, 2002.

P. Chilvers. Creature Labs - Norn Babblings. http://www.gamewaredevelopment.co.uk/creatures_more.php, Date Created: nodate. Date Accessed: 01/02/2007.

S. Conner. Dumbstruck: A Cultural History of Ventriloquism. Oxford University Press, New York, 2000.

M. Dery. Hacking Barbie’s Voice Box: ’Vengeance is Mine! ’. http://www.levity.com/markdery/barbie.html. New Media magazine, "Technoculture" column., Date Created: 05/1994. Date Accessed: 1/2/2007.

I. Parry. Essays on Dolls. Syrens, London, 1994.

Goldblatt, David. (2005). Art and Ventriloquism: Critical Voices in Art, Theory and Culture (Critical Voices in Art, Theory &amp; Culture). Routledge.

J. Reichardt. Cybernetics, art and ideas. Edited by Jasia Reichardt. London: Studio Vista, 1971. Essays by various authors.

J. Reichardt. Robots : fact, fiction and prediction. Thames and Hudson, London, 1978. (by) Jasia Reichardt. ill(some col), plans, ports ; 28cm (Pbk).

J. Reichardt. (1971). The Computer in Art. Studio Vista.

R. M. Rilke. Dolls: On the wax dolls of lotte pritzel. In I. Parry, editor, Essays on Dolls. Syrens, London, 1994.

K. Salen and E. Zimmerman. Rules of play : game design fundamentals. MIT, London, 2004. Katie Salen and Eric Zimmerman. ill. ; 24 cm.

V. Hall. Mike (the talking head). http://mambo.ucsc.edu/psl/mike.html, Date Created: nodate. Date Accessed: 01/02/2007.

B. Laurel. Computers as theatre. Addison-Wesley Pub. Co, Reading, Mass. ; Wokingham, 1993. GBA201776 bnb 2672 Brenda Laurel. ill. (some col.) ; 24 cm. "Now featuring Post-virtual reality"–Cover. Includes bibliographical references (p. 215-222) and index.

M. Newman. Interactive Barney: Good or evil?  Conferees worry about where computerized ’character’ toys are going next. http://www.post-gazette.com/businessnews/19990521barney1.asp, Date Modified: 21/05/1999. Date Accessed: 01/03/2007.

A. Parent. Read Reviews of Hasbro Aloha Stitch Doll 3570 at eOpinions. http://www.epinions.com/content_163285929604? linkin_id=8003929, Date Created: 28/11/2004. Date Accessed: 01/02/2007.

D. Shenk. Behold the Toys of Tomorrow (The Atlantic Online - Digital Culture). http://davidshenk.com/webimages/atlantic1.htm, Date Created: 07/01/1999. Date 
Accessed: 01/02/2007.

E. F. Strommen. When the Interface is a Talking Dinosaur: Learning Across Media with ActiMates Barney. http://www.playfulefforts.com/archives/papers/CHI-1998.pdf, Online PDF of published work. Date Written: 1998. Date Accessed: 01/03/2007.

E. F. Strommen. Learning from Television With Interactive Toy Characters As Viewing Companions. http://www.playfulefforts.com/archives/papers/SRCD-1999.pdf, Online PDF of published work. Date Written: 1999. Date Accessed: 01/03/2007.

E. F. Strommen. Interactive Toy Characters as Interfaces For Children. http://www.playfulefforts.com/archives/papers/IA-2000.pdf, Online PDF of published work. Date Written: 2000. Date Accessed: 01/03/2007.

VoiceSignal Technologies Announces Breakthrough Speech Interface For Mobile Phones and Handheld Computers. http://www.voicesignal.com/news/press/release_02_19_02.html, Last Modified: 19/02/2002. Date Accessed: 01/03/2007.

VoiceSignals Technology Press Release. http://www.voicesignal.com/news/press/release_02_19_02.html, Date Created: 19/02/2002. Date Accessed: 1/2/2007.

L. Vygotsky. Play and its role in the Mental Development of the Child. http://www.marxists.org/archive/vygotsky/works/1933/play.htm, First Published: 1933. Date 
Created: 2002 . Date Accessed: 01/02/2007.

Wikipedia Community. "Augmented Reality.” Retrieved 10/3/2007, from http://en.wikipedia.org/wiki/Augmented reality (2007). 

Han, Jeff. "Multi-Touch Interaction Research.” Retrieved 1/1/2008, from http://cs.nyu.edu/∼jhan/ftirtouch/index.html. (2006) 

Schechner, R. (1988) Performance Theory. Francis and Taylor.

Barba, E and N. Savarese. (1991) Dictionary of Theatre Anthropology: The Secret Art of the Performer. Routledge.

Weizenbaum, Joseph. (1993) Computer Power and Human Reason (Penguin Science). Penguin.

Poepel, Cornelius. (2005) On Interface Expressivity: A Player-Based Study. Available: http://hct.ece.ubc.ca/nime/2005/proc/nime2005_228.pdf. Viewed: 28/4/2008.

Pelachaud, Catherine. http://webperso.iut.univ-paris8.fr/~pelachaud/ e.g.
J.-C. Martin, R. Niewiadomski, L. Devillers, S. Buisine, C. Pelachaud,  Multimodal complex emotions: Gesture expressivity and blended facial expressions, International Journal of Humanoid Robotics, Special Edition "Achieving Human-Like Qualities in Interactive Virtual and Physical Humanoids", 2006

E. Bevacqua and C. Pelachaud, Speaking With Emotions: Available: http://linc.iut.univ-paris8.fr/greta/papers/aisb04-bevacqua.pdf. Viewed: 28/4/2008

Dobson, Kelly,  Brian Whitman and Daniel P.W. Ellis. (2005) Learning Auditory Models of Machine Voices. Available: http://web.media.mit.edu/~monster/dobson_whitman_ellis_machinevoices.pdf. Viewed: 24/4/2008.
Juul, Jesper. (2005) Half-Real: Video Games between Real Rules and Fictional Worlds. MIT Press.

Self Authored Publications / Presentations on Digital Puppetry Practice and Related Techniques

(2008) Grant, Ian. Chapter: Experiments in Digital Puppetry: Video Hybrids in Apple’s Quartz Composer. In Transdisciplinary Digital Art: Sound, Vision and the New Screen, Communications in Computer and Communication Science. Edited by Randy Adams, Steve Gibson, Stefan Muller Arisona. Springer. 

(forthcoming, July 2008) The Cinematic Interface in Public Space. A research paper accepted by EVA LONDON. Conference: Electronic Visualisation and the Arts.

(forthcoming) Grant, Ian. Apple’s Quartz Composer, a pragmatic guide (provisional title). A book for the US publisher ‘The Pragmatic Programmers’. www.pragprog.com 

(February 2007) Grant, Ian. Talking Toys and Digital Puppetry. Society for Artificial Intelligence and the Simulation of Behaviour (AISB) ’07 at Newcastle University, Newcastle Upon Tyne, 2-5 April 2007. Conference: Artificial and Ambient Intelligence. Symposium on Virtual Companions and Digital Pets. "The Reign of Catz &amp; Dogz? The Role of Virtual Pets in a Computerised Society". Paper published in proceedings.

(Jan 2001) Finding the Wooden Voice in Puppetry Into Performance: A Users Guide. London: Theatre Museum, Central School of Speech and Drama and the Puppet Centre Trust. </Text>
        </Document>
        <Document ID="247">
            <Title>figure_wii_controlled_avatar_001</Title>
        </Document>
        <Document ID="136">
            <Title>Spatial / Architectural</Title>
            <Text>4.7 Spatial / Architectural</Text>
        </Document>
        <Document ID="41">
            <Title>Development</Title>
        </Document>
        <Document ID="469">
            <Title>some categories for coding</Title>
            <Text>thing/entity
kind
part
property
material
process
operation
patient
product
by-product
agent
space
time</Text>
        </Document>
        <Document ID="358">
            <Title>Shadows_Levin_messo di voce_001</Title>
        </Document>
        <Document ID="42">
            <Title>Seminar Notes - Claire</Title>
            <Text>Domain Analysis...
-----


working title 

a study of expressivity in relation to animated objects (virtual and physical).

--
scene settting
setting the context via an historical overview plotting the transformation of the idea of the puppet over time, with special focus on digital interfaces and contemporary computer revolutions in interaction design.


taxonomy of the puppet...
----
how do the different groups, or classifications, of the puppet resonate with emerging digital interfaces and physical computing. The computer detection of edge as shadow, the joystick as rod, motion capture suit as embodiment or MASK.
ANALOGIES

Analogues...

interdisciplinary moments...



mediation...



intermediary...


via a series of case studies
experimental design 
and the creation of artefacts to be 


a central question
--------------------
what are the experiences of interfacing with animated objects from the perspectives of performer and audience?

----
a subsidiary question
---------------------
In what ways is it meaningful to think about the concept of a 'digital puppet'?
- richness of concept of thought to the contemporary artefact - 'CLAIRE'
- what is missing is the humane... the human element... the expressive, the playful, the ludic...
- the playful
- bringing expressivity to 'communication technology'

- puppets (in therapeutic and education contexts) are used as communication tools - a media
communicating 'through' - medium...

Is expressivity, expressive behaviour, simply information.
How far is expressivity a CONFIGURATION...

How far does expressive potential come from human agency?
How far is the expressive of an object determined by its calibration? X

---
EG.
Compare Sack Boy with Improbable Theatre's Impromptu newspaper and sellotape construction...

SECOND LIFE: Experiments with Expressivity through animation... real-time non real-time... 
In second life an animation can be prepared and uploaded.. then performed..



---

--

puppets as technology
--
automata
cybernetics - performance and control...
human / puppet interface... performance through manipulation of a control system...


---
interdisciplinary...
---


made up word: artefaces 


---
capturing motion
-- mouse = 2 axis of postition in space
-- strings = multiple


-------
Why am I interested in this?
-------
Marvel.
Wonder.
Magic.
Wow.

Interaction and paralells - 
direct -> remote 
embodied to the disembodied
---

Finding AWE in objects that defy their own material constraints...


Objects empowered with qualities and actions they do not obviously possess. ...

Perception of motion or movement.
Agency.
Giving objects the illusion of life. 

Embodies aspects of our imaginations: PROJECTION of SELF INTO EXTERNAL OBJECTS.
Anthropmorphism
---

---
Aesthetic / Poetic
---
The more simple a form the more expressive it's power

---

Another Subsidiary Question
---
How useful are established typologies of the puppet in understanding related cross-media forms that involve interactive control over virtual (or physical) objects?
---

FOCUS - PLUMB LINE
---

Aesthetic pleasure, emotional and psychological power of animated objects
fundamental role of play and playfulness






</Text>
        </Document>
        <Document ID="248">
            <Title>Development</Title>
            <Text>The project can be split into three broad phases of production: (i) Understanding *Quartz Composer* -- see [][#understandingquartz composer], (ii) Custom Software Production -- [][#customsoftwareproduction] and (iii) Content Creation in [][#visualdocumentationwithcommentary].

</Text>
        </Document>
        <Document ID="137">
            <Title>Introduction</Title>
            <Text>The current study explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

I aim to explore the related contexts of digital puppetry, real-time animation, mimetic and non-mimetic kinetic objects, automata, 'cybernetic sculpture', performance systems and the technological interfaces to such phenomena. 

I aim to create, evaluate and create puppet/object theatre performances/installations that use original software and hardware systems that are designed to explore 'performance expressivity', flow and play, with reference to relevant historical, art, entertainment and technological precedents.
</Text>
        </Document>
        <Document ID="359">
            <Title>Shadows_Levin_messo di voce_002</Title>
        </Document>
        <Document ID="43">
            <Title>Seminar Notes - Claire-1</Title>
        </Document>
        <Document ID="44">
            <Title>PhD Proposal</Title>
        </Document>
        <Document ID="249">
            <Title>7.4.1 Understanding Quartz Composer</Title>
            <Text>*Quartz Composer* is a technology that exploits the graphics processing unit (GPU) of the host Apple Mac, running Mac OS 10.4 (Tiger). Major enhancements have been made to *Quartz Composer* in Mac OS 10.5 (Leopard). Some key points:

* you create *compositions* in a simple to use visual programming environment where you graph connections between objects (*patches*). There are around two hundred built in patches. *Leopard* developments provide an API for programmers to created custom functionality by writing their own patches;

* it works in a 3D context as defined by *Open-GL* and supports *Open-GL Shader Language (GLSL)*, enabling full and multi screen real-time video, image processing and 3D;

* it uses Apple's *Quicktime* technology for media importing and the playback of compositions;

* it works with data aggregation standards defined by *RSS* and a logic/control language defined by *javascript*/*ECMAScript*;

* MIDI, OSC[^ChQCOSC] and audio input processing allowing sophisticated network communication and composition control by external sources and sensors;

[^ChQCOSC]: Open Sound Control Introduction: [http://opensoundcontrol.org/introduction-osc](http://opensoundcontrol.org/introduction-osc)

* *Xcode*, Apple's free developers environment[^ChQCXcode], allows *Quartz Composer* compositions to be embedded in native applications for more complex control over rendering, user interface design and interoperability with other technologies.

[^ChQCXcode]: Xcode: [http://developer.apple.com/tools/xcode/][http://developer.apple.com/tools/xcode/]

*Quartz Composer* is an opportunity for interactive motion graphics artists to explore *resolution independent* imaging and the sophisticated graphics technology of the latest *Mac OS*. *Quartz* is Apple's brand name for a suite of technologies known internally at Apple as *Core Graphics*. It is based on a PDF graphics/imaging model and extends from still image and text into 3D graphics with Open-GL and real-time video processing with a collection of techniques and APIs called *Core Video*.

In addition to real-time interactions and integration with the image processing libraries Apple calls *Core Image*[^ChQCCoreImage], *Quartz Composer*, according to Apple, can **make data-driven visual effects, and even perform live performance animations." (Apple Computer, 2007a).

[^ChQCCoreImage]: *Core Image*: Resolution-independent 2D graphics based on PDF technologies and 3D graphics based on hardware accelerated *Open-GL*.

*Quartz Composer* is a visual programming environment, where patches are plumbed together with *noodles* that represent data flow through a sequence of connected objects. For the non-programmer, it is a joy to use -- providing rapid prototyping of complex programmatic ideas. For example, applying live video textures to interactive forms in real-time 3D space is a five second operation -- needing no lines of textual code.

![fig:figure_video_movie_timebase_patch_demo_001][]
[fig:figure_video_movie_timebase_patch_demo_001]: figure_video_movie_timebase_patch_demo_001 "Live video capture is blended with a movie in Quartz Composer" width=418px height=314px

[][#fig:figure_video_movie_timebase_patch_demo_001] shows each kind of basic patch in *Quartz Composer* -- *providers* (blue), *processors* (green) and *consumers* (pink) [Apple Computer, 2009a][#apple_quartz_-2]. It also demonstrates how some patches have a *time-base*. One imported Quicktime movie plays according to the frames per second of the composition, the other accepts numerical data on the input port called *Patch Time*. Such changing numerical data could, for example, come from the normalised ($-$1 to 1) X co-ordinates of the mouse -- effectively scrubbing the playback of the video. [][#fig:figure_004_interactive_eyes] illustrates the scrubbing of a short movie that contains eyes moving left--centre--right--centre.

![fig:figure_004_interactive_eyes][]
[fig:figure_004_interactive_eyes]: figure_004_interactive_eyes "Controlling the time-base of pre-recorded video in Quartz Composer -- Interactive Video Eyes" width=418px height=314px

In an earlier iteration, *Quartz Composer*, as *PixelShox Studio*[^ChQCPixelShox] had a vibrant life favoured by VJs and live-motion graphics artists. *PixelShox Studio* was written by Pierre-Olivier Latour, circa 2003. Pierre-Olivier Latour now works for Apple and *Quartz Composer* is a small, but significant, part of a wider development architecture. *Quartz Composer* compositions integrate nicely into the *Cocoa* programming environment, utilising *bindings* to make it easy to write applications that connect GUI controls to visual compositions. Also *Quartz Composer* compositions can be turned into *Image Units* and made available as video processing plug-ins for compatible applications, like Avid's *Xpress Pro*, Apple's *Final Cut Pro* and *Motion*[^ChQCNoiseInd]

[^ChQCPixelShox]: *PixelShox Studio*: [http://www.pol-online.net/pixelshox_technology/index.php](http://www.pol-online.net/pixelshox_technology/index.php)

[^ChQCNoiseInd]: Noise Industries' *FXFactory*: [http://www.noiseindustries.com/](http://www.noiseindustries.com/).

An early vision for *PixelShox* was for artistic use "professionally in any project where interactive visuals are required: V-Jaying (sic), multimedia installations, presentations, [or] concerts." [Latour (2003)][#Latour:2003ee]. *Quartz Composer* is significantly pruned of functionality when compared with *PixelShox*. Although it was never completed, it is likely that the past functionality of *PixelShox* may be a rough roadmap of where *Quartz Composer* is heading. One repeated feature request is support for importing arbitrary 3D objects.

</Text>
        </Document>
        <Document ID="138">
            <Title>Theories into Practice</Title>
        </Document>
        <Document ID="45">
            <Title>Handbook Selection</Title>
            <Text>
5.4  Visual Map of the PhD: Its Knowledge 
Base and Its Boundaries 
We suggest that you ﬁll in this ‘map’ at each supervision. 
Sometimes the boundaries shift as your research evolves. 
You must always be able to place yourself clearly within this 
kind of a schematic. 
Name of candidate: 
Title of thesis: 

Central big circle:  Field of Knowledge = 
Cognate Fields: 
Cognate Fields: 
Cognate Fields: </Text>
        </Document>
        <Document ID="46">
            <Title>handbook04</Title>
            <Text>SMARTlab PhD Student Handbook
February 2009
THE SMARTLAB DIGITAL MEDIA INSTITUTE	1
0.
Table of Contents 1. Introduction to SMARTlab
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
1.1 1.2. 1.3
What is SMARTlab? The Ethos What is a PhD? What is a Practice-Based PhD (with SMARTlab)?
2. Admissions, Enrolment and Registration at UEL (Processed in chronological order)
2.1	PhD Timeline 2.2	Admissions 2.3	Enrolment 2.4	MRes – Research Skills Training Modules 2.5	Progress and Review Procedures
2.6	Registration &amp; Research Ethics Approval 2.7	Transfer from MPhil to PhD 2.8	Writing Up 2.9	Mock Vivas &amp; Vivas
3. Regulations, Roles and Responsibilities
3.1	You and Your Supervisors – Roles and Responsibilities 3.2	Presentation Guidelines for SMARTlab students 3.3	UEL Rules &amp; Guidelines for Formatting and Submitting your Thesis
4. SMARTlab Attendance &amp; Ettiquette Requirements
4.1	Engagement, Participation, Respect &amp; Constructive Criticism/Critique 4.2	Etiquette At Seminars 4.3	Between Seminars
5. SMARTlab Methods: Practising for Success
5.1	YOUR research – Plumbline 5.2	Basic Research Formula / Abstract 5.3	Hints on Preparing for the Mock Viva 5.4	Visual Map of the PhD: its Knowledge Base and its Boundaries 5.5	SMARTlab Research Clusters 5.6	SMARTdoc Online Resources/Online Supervisions &amp; Seminars 5.7	Useful External Links 5.8	Library Resources at UEL, and in the UK 5.9	Useful Publications for Doctoral Students 5.10	Useful Publications for doctoral students:
Appendix
I.	FAQ Student Fees II.	FAQ Accommodation III.	Who to Contact About What – UEL staff list IV.	Successfully Completed PhDs Supervised by SMARTlab Faculty V.	Lizbeth’s Rules of the SMARTlab PhD
THE SMARTLAB DIGITAL MEDIA INSTITUTE
2
1.
Chapter One:
1. Introduction to SMARTlab
1.1 What is SMARTlab? The Ethos
Welcome to SMARTlab! From our purpose-built interactive performance studios at the Docklands campus, SMARTlab fosters collaborations and high-level projects and publications with faculty throughout UEL and nationally/internationally. SMARTlab engages with a broad spectrum of research areas which operate across three research ‘clusters’: Performance &amp; Technology; E-Learning &amp; Virtual Worlds; and Accessible Technology &amp; Social Inclusion and Outreach. The team and our method are intrinsically trans-disciplinary. Artists, audiences, technologists and scholars collaborate in the design of new bespoke tools, and assess their potential through socially engaged research models.
Founded by Professor Lizbeth Goodman in 1992 while at the BBC Open University, where research underway was informing the new field of interactive learning models for students at a distance, our research team and our methods have evolved through four institutions (via the University of Surrey and Central Saint Martins, UAL), before arriving at our base in the London Docklands. SMARTlab was brought to its new dedicated space at UEL in November 2005 in order to foster high-level cross-disciplinary research: a major priority development area for UEL as a whole.
Over the years, we have found that many scholars engaged in practical research (whether in the arts, technology, or social and educational innovation) have encountered difficulties in placing their work in relation to the academy: in finding appropriate ways to 'measure' artistic practice in 'research exercises', in identifying appropriately flexible and experimental forms for innovative research processes and outcomes, and also in competing for academic funding.
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
THE SMARTLAB DIGITAL MEDIA INSTITUTE
3
SMARTlab’s core research strategy is to bring together performance specialists with artists, scholars, computer scientists and policy makers to share a commitment to creative technology in writing and public dissemination of findings in more scholarly forms. A few of the team are post-doctoral faculty who completed their PhDs with us in recent years. We promote a collaborative research and writing/publication model, resulting in numerous jointly authored papers by our team and our collaborators in the science and technology fields as well. Thus we aim to make a real contribution to the performing arts and to show the impact of performance on other fields, both within and beyond HE.
The SMARTlab Digital Media Institute supports a highly selective group of PhD researchers. This group works together live and online, with contributors from all around the world, to co-create and debate the nature of 'practice- based research'. Candidates are encouraged to work together on joint experiments, to share work in progress for group feedback, and to meet regularly with experts joining debates online and offering feedback to the cohort.
We meet 'live' three times a year (in February, July, and October annually) for intensive retreat seminars here at our MAGIC Playroom and associated studios. These seminars focus on research methods and transdisciplinary critical practices, group critique, feedback and the relationship between practice and theory. These seminars are mandatory for all.
We also meet monthly in live and online sessions wherein students and faculty from around the world share work in progress, lectures and debates on topics of relevance to the groups and sub-groups or 'research clusters'. We provide our own customised online learning space for these sessions, which links to UEL's main online support space as a front end and university administration portal, and to a
1.
Chapter One:
1. Introduction to SMARTlab
range of bespoke online learning tools in Drupal, Second Life, multiple ivisit spaces etc) for more intensive shared learning sessions. Students and faculty are encouraged to add to the range of multimedia learning space options available to the group, as part of our Open Source/Creative Commons lab ethos.
Most students are also required to attend additional sessions (whether live or online) on research methods, depending on levels of prior academic and practical experience. The Institute’s core team has worked together for some fifteen years, graduating over 30 practice-based PhDs in that time, while also achieving major international status.
1.2 What is a PhD?
A PhD is the highest degree that can be awarded. The full title is Philosophy Doctorate (sometimes known as Doctor of Philosophy, or (DPhil). Many different methods of attaining a PhD exist in many universities. What all have in common is the basic understanding that this degree marks out the very highest level of original thought and academic achievement.
The PhD, however defined, MUST by definition be a scholarly, academic work that contributes to the field of knowledge by making a philosophical or high-level intellectual contribution.
For this reason, at least some part of a PhD must normally be written down in text. It is highly unusual, though not totally impossible, to attain the degree entirely by practical work, for instance by means of mathematical equations or symbolic logical proofs. A few PhDs have been submitted internationally in musical notation or score format.
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
THE SMARTLAB DIGITAL MEDIA INSTITUTE
4
The scholarly community will continue to wrangle with the exact definition of the PhD for years to come. For now, it is easiest to understand that at base it must be: “An original contribution to the field of knowledge of publishable quality”.
Keywords - A PHD MUST BE:
Original: unique and also written/created by the student. When research is conducted collaboratively, students must demonstrate that some part of the work is theirs and theirs alone. That is the work to be defended, and for which the degree will be awarded.
Substantial: while guidelines on length of dissertations vary, the norm in the UK is no less than 30,000 words and no more than 80,000 words. What is more important than length, however, is substance: the thesis must make an important, significant contribution to knowledge: it must provide a solid base from which future research can be conducted.
Field of Knowledge: the discipline to which the thesis makes its intellectual (or philosophical) contribution. This is where the intellectual or scholarly argument is based, and so this must be clearly identified early in the research. Cognate fields (or fields that overlap with the main field) can also be identified early on, to focus the work.
Publishable: (not necessarily published or even likely to be published, but of publishable quality). When a dissertation is complete and passed, it is submitted to the British Library and the UEL Library, at which point it is ‘made public’, whether or not it is also published in book or journal format, in whole or in part. The thesis must therefore be of the highest standard in terms of presentation (format) and of ideas (content).
1.
Chapter One:
1. Introduction to SMARTlab
1.3 What is a Practice-based PhD (with SMARTlab)?
Scope and Components of a Practice-based PhD
The admission of ‘practice’ in a PhD context is premised on the notion that research questions in your field can be rigorously worked through in a range of practices (of which writing is only one). Where practice (creative practice, software development etc) forms a significant outcome of the research project, references to the ‘thesis’ are understood to denote the totality of the submission without privileging any of the submitted components.
Any prescriptive model of creativity and reflection is avoided in order to enable students to develop their own praxis. The specificity of each project, its scope and the location(s) of its examinable presentation(s) in terms of PhD submission must be established in the applications and admissions procedure (below).
Length
The average length of a practice-based PhD is 30- 40,000 words if incorporating a major practical component, or 50-65,000 words (maximum) if a written text analyzing a non-examined set of case studies. Note: Appendices DO NOT count in word totals; references and Bibliographies DO COUNT.
The average lengths for PhDs in general are 30,000 words minimum (with a major practical submission), 50-60,000 words on average (without a major practical components), and 80,000 words maximum in all cases.
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
THE SMARTLAB DIGITAL MEDIA INSTITUTE
5
Balance Between Written and Practical Components of the Submission
The balance between written and practical outcomes will be determined by individual students with their supervisors under the auspices of regulatory frameworks of UEL. Normally, a written submission of at least 30,000 words will constitute roughly 50% of the project. While the weighing on practical work and written work can be negotiated project per project, the 30,000 word minimum for the written work is a given at UEL.
It is up to the student, in discussion with the Director of Studies and the supervisory team, to determine the balance of written and practical work to be submitted, the length and format of the submission and the range of work to be examined in the final Viva Voce exam. It is possible to be examined only on the written submission, or on both the writing and the practice in some agreed balance, to be negotiated with your supervisory team.
The Practical Component - Guidelines
The practical component must demonstrate a high level of skill in the manipulation of the materials of production and involve a research inquiry.
Practice should be accepted as methodological process of research inquiry and a mode of dissemination of research in its own right.
The written outcome will contextualise the project and should therefore include a retrospective analysis of the process and outcomes, reflecting on chosen research methodologies and production processes and the relation between them.
A practice-based PhD undertaken through the SMARTlab programme is first and foremost your
1.
Chapter One:
1. Introduction to SMARTlab
work and as such, when completed, it must make an original and substantial contribution to the field of knowledge, of publishable quality, and must be referenced and therefore checkable by future scholars, artists, and researchers. This means that each and every claim, phrase, sentence and footnote must be totally correct as printed and must lead directly to the exact page in print or online (in archived format) of the material you cite.
Your PhD must be a thoroughly researched and documented high-level argument that demonstrates respect for previous scholarship. If you paraphrase or quote secondary sources such as reviews of articles about key ideas/books, you must also have taken the time and trouble to have gone a step further towards the original publication discussed, and must give direct reference to that primary source and demonstrate an understanding of the original idea in its original context as well as in its secondary context (and be aware that both reviewers of the text of your thesis and examiners in a Viva are perfectly entitled to grill you on the primary sources of ideas discussed, even if you only cite secondary sources!). Similarly, if you refer to critical ideas taken from translations into English of original articles written in other languages, you must make every effort to understand whether the translation you are using is noted as a good translation, and must cite the original context of publication as well as the (modern) translation.
Any practical work submitted must be presented in finished form with a substantial, original and professionally formatted presentation, with due credit noted to any artistic collaborators in the process and ‘products’ of that work.
A PhD is YOUR project
You are responsible for maintaining progress, reading the guidelines in your handbook and re-reading them
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
THE SMARTLAB DIGITAL MEDIA INSTITUTE
6
annually in case they change, and for keeping up-to-date with all the information and forms on the web:
http://www.uel.ac.uk
Your thesis is the construction of an original, important and well-documented argument that you can prove and reference, and upon which other scholars will be able to build in future.
Who does the work? - YOU Who gets the degree? - YOU
There are no courses at doctoral level in the UK system but students (home and overseas) are required to attend seminars three times a year (in October, February and July) and to spend an average of six weeks in the UK each year – negotiable in discussion with your Director of Studies and the College Research Office.
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
2.
Chapter Two.
2. Admissions, Enrolment and Registration at UEL
(Processed in chronological order)
2.1. Timeline
The timeline below ones an overview of the administrative processes involved in your PhD from beginning to end.
2.2. Admission
Below is some crucial information about the practical requirements and University systems for processing your PhD. You can download all forms and guidelines discussed here from the UEL graduate School document library: (http://www.uel.ac.uk/ gradschool/resources/doclibrary/index.html)
APPLICATION
DEADLINES 3 TIMES A YEAR; ALLOW APPROX 2 MONTHS FOR PROCESSING
ENROLMENT
REGISTRATION
FULL TIME: WITHIN 6 MONTHS AFTER ENROLMENT PART TIME: WITHIN 12 MONTHS AFTER ENROLMENT MPHIL STUDENT STATUS
You need:
• • •
• • • •
Complete application form 2 academic references (sent by email directly to Anna Sophia Schenk from the referee) CV Copies of previous degree certificates 1000-word research proposal If you are an overseas student, a scanned copy of your passport and/or visa, and a complete Financial Assessment form
You need:
To pay your fees on your start date each academic year, in order to enrol and stay enrolled. You are not officially a PhD student until you have paid your fees. Once enrolled, you are committed to attending seminar sessions 3 times a year as well as actively engaging in mandatory monthly on-line seminars, and you must upload supervision reports after each session.
You need:
•
• • •
•
To have completed the 2 MRes modules To be up to date on fees To have attended seminars regularly To fill in the registration form with the guidance of your supervisors including a detailed research plan;
To have completed a ‘mock transfer’ with your supervisors.
You receive:
A letter from SMARTlab informing you that you have been accepted into the programme followed by a formal offer letter from the UEL Graduate School. You need to accept the offer letter in order to receive instructions on how to enrol. If you do not respond to the letter in writing within one month, the offer is retracted and/or you will have to pay the university a late fee if you decide to enrol at a later date. It is therefore very important that you enrol immediately upon receiving this letter.
You receive:
A student ID which enables you to access UEL and UK library resources; SMARTlab supervisory team; tuition at the PhD seminar weeks in Oct, Feb and July each year; online resources through the SMARTlab PhD student site.
You receive:
A letter sent out by Graduate School confirming your registration details
The same support and resources as an enrolled student with the standing of MPhil candidate.
THE SMARTLAB DIGITAL MEDIA INSTITUTE	7
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
2.
Chapter Two.
2. Admissions, Enrolment and Registration at UEL
(Processed in chronological order)
The first point of contact for practical enquiries about your PhD is Anna Sophia Schenk (a.s.schenk@uel.ac.uk), PhD administrator at SMARTlab. It is best to send your inquiries direct to Anna Sophia and she will then liaise with other University departments regarding administrative processes/enquiries.
The SMARTlab Digital Media Institute is an independent research institute with links to other UEL Schools. For the administration and regulation of the Practice-based PhD course, SMARTlab processes all its applications, enrolments, registrations
TRANSFER TO PHD
APPLICATION: AFTER 9-15 MONTHS OF FULL TIME STUDY (OR PART-TIME EQUIVALENT) DOCTORAL STUDENT STATUS
SUBMISSION
ALLOWED AFTER TOTAL OF: FULL TIME: 33-60 MONTHS PART TIME: 45-72 MONTHS
VIVA
TO BE ARRANGED APPROX 6 WEEKS AFTER SUBMISSION
2-4 HOURS
• • •
• •
You need:
To be up to date on fees To have attended seminars regularly To have written 2 full chapters towards your thesis (roughly 10,000 Words). To present your research progress and plan of intended further work to a transfer committee
You need:
To have completed your practice-based project and analysed the findings in a written thesis of no less than 30,000 words. Your thesis must be signed off by your supervisory team before you submit.
You need:
To be intimately familiar with every line of your thesis and every reference in your bibliography; and to be able to speak confidently about them. You should prepare with a ‘mock viva’.
You receive:
A letter sent out by Graduate School confirming your transfer details
The same support and resources as an enrolled student, but now with the standing of Doctoral candidate rather than MPhil candidate.
You receive:
A date for your defence by oral examination.
You receive:
A 2-4 hour oral examination on your work carried out by the examiners and chaired by a senior UEL academic. If the examiners recommend that you pass, then you get your Doctorate!
THE SMARTLAB DIGITAL MEDIA INSTITUTE	8
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
2.
Chapter Two.
2. Admissions, Enrolment and Registration at UEL
(Processed in chronological order)
and finally submission through the School of Computing, Information Technology and Engineering (CITE) at UEL.
2.3. Enrolment
Once you have accepted your official offer letter and received the enrolment paperwork from the UEL Graduate School, you have one month in which to pay your fees and enrol online. Late payment fees may be incurred if you go past this deadline. Enrolment will entitle you to use university facilities and will give you access to professional consultation (i.e. supervision).
Enrolment consists of two things: paying your fees, and filling out the online enrolment form. Enrolment will only be complete and your place in the programme secured when you have carried out both of these tasks.
The simplest way to enrol and pay your fees is through UEL Direct, the University’s online student service.
How to log in to UEL Direct:
•	Log on to http://www.uel.ac.uk •	Click on UEL Direct Login (bottom of left-hand
menu on home page) •	Enter your user name, which is the 7-digit student
number found on your offer letter, preceded by a
‘u’ (e.g. u0799999) •	Enter your network password (your initial password
is your date of birth in the format dd-mmm-yy (i.e. enter 29-feb-80 if your date of birth is 29 February 1980)
•	Go to your “To do list” •	In your Intray, under “Action”, click on “Please
Sign-up with UEL” •	Follow the on-screen instructions •	For assistance email networkadmin@uel.ac.uk
You can also pay in person with UEL Credit Control at the Docklands Campus, and it is possible for students to set up a payment plan and spread the payments over the year by Direct Debit. Overseas students must pay 50% of their
fees at enrolment, and thereafter the fees can be paid in installments throughout the rest of the year.
Once you have enrolled, you must go to the Student Services at the Docklands Campus to get your student card and ID, which allows access to buildings and facilities such as the library. In order to get your ID issued, you will be required to provide proof of your identification which can be either a full passport or two of the following: a full or provisional driving license showing current address, an International Driving Licence, a cheque book or credit / debit card (one only) or an original birth certificate in English.
Enrolment is an annual process and must be completed at the beginning of each academic year for which the student is in attendance. It is important that you check your UEL Direct email account regularly, for information from UEL regarding procedural requirements.
2.4.	MRes – Research Skills Training Modules
Unless you have already completed a Masters of Research (whether with UEL or another university), or in exceptional circumstances, if you have received prior written excusal from the modules based on extensive professional/ scholarly experience, you will be required to undertake two short modules on Research Skills and Methods.
As part of your PhD at SMARTlab you must take the two core modules in research skills during your first year (or part time equivalent). This course is run either through the Graduate School for London-based students who can attend on campus, or via the School of Distance and E-Learning (UEL Connect). See this link for further details: http:// www.uel.ac.uk/uelconnect/distance_learning/ module_info/pg_research_modules.htm
THE SMARTLAB DIGITAL MEDIA INSTITUTE
9
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
2.
Chapter Two.
2. Admissions, Enrolment and Registration at UEL
(Processed in chronological order)
The modules are: GSM008 - Research Preparation and Planning (RPP) GSM007 - Understanding Research Processes and
Contexts (URPC)
The modules run once per year starting in September and February respectively. You must contact Irene Smith at the School of Distance and E-Learning directly to enrol on these courses (i.smith@uel.ac.uk). Failure to complete both modules in your first year could lead to your failure to proceed to registration. It is YOUR responsibility to initiate and follow through on these modules without the intervention of the SMARTlab team, and to report on your progress on the modules at each supervision with your SMARTlab team.
NB: SMARTlab students do not have to pay fees for these courses as the cost is included in PhD fees. Please be sure to state this in your correspondence with the School of Distance and E-Learning when you contact them to enrol.
2.5.	Progress and Review Procedures
Supervisory Reports
All students are required to complete a Supervision report after each supervision, and to upload that to their space on the SMARTlab PhD Student Site’s server, where supervisory teams can gain easy access and can keep efficient archives of work in progress.
Each report should include discussion of the main topics addressed in a session, the suggested action items and further reading/writing/practical work assigned.
It is the student’s responsibility to ensure that satisfactory progress is maintained to enable the successful and timely completion of their research degree programme. A research student’s progress will be regularly reviewed both informally between the student and members of the supervisory team, where general matters are discussed, and formally through the annual review process.
The main purpose of progress and review is to provide support for the student towards the successful completion within the appropriate timescale for their degree. Additionally the annual review process provides an opportunity for students to feedback on the student experience.
Meetings between the research student and the supervisory team take place at least three times a year, with continued online contact, in order to monitor progress, provide feedback and reflect on personal developments. Details of discussions and all such meetings should be recorded.
Once a year, each student must also attend a Review, where one external academic from UEL (who is not a member of the supervisory team) joins the student and supervisor in a thorough progress report and quality review exercise. The results of this review goes to RDSC for approval, to note the decisions concerning each student’s continuation of study.
Annual review
The progression of research students will be formally reviewed and monitored by the School Research Degrees sub-committee who will provide an annual report to Academic Board Research Committee on progress.
The purpose of the annual review is to provide both the student and supervisory team an opportunity to critically reflect on progress in their research degree programme in the preceding year.
Additionally, the annual review enables School Research Degree sub-committees to monitor the work being undertaken by their students, to ensure that the training needs of students and supervisors are being met and to identify and resolve any difficulties that the student is experiencing.
The annual review procedure contains the following core elements:
THE SMARTLAB DIGITAL MEDIA INSTITUTE
10
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
2.
Chapter Two.
2. Admissions, Enrolment and Registration at UEL
(Processed in chronological order)
•	All MPhil and PhD students must have an annual review;
•	The annual review process will include the student preparing a written progress report and self evaluation of their research skills against the joint statement by the Research Councils/AHRB on skills training requirements for research students;
•	An annual review panel will meet to discuss the progress report and make a recommendation to the School Research Degree sub-committee. This panel will be constituted by research active members of staff within the School independent of the student’s supervisory team. The student and supervisory team will attend the annual review meeting;
•	Progress reports will be considered by the relevant School Research Degree sub-committee and a timetable for action and follow-up action completed;
•	Each School Research Degrees sub-committee will make an annual report to the PGR Review Committee on the progression of research degree students, to include confirmation that process has been completed for all students and any action agreed where progress is not satisfactory;
•	A personal development plan should be agreed as part of the process;
•	The progress report will include a schedule for the completion of the thesis agreed by the student and supervisor;
•	Continuation on the research degree programme will be conditional on the satisfactory outcome of the annual review.
Details of the process of annual review, the constitution of review panels and the possible outcomes can be found on the Graduate School webpage.
2.6.	Registration &amp; Research Ethics Approval
Registration
Having enrolled as a student you must then register for a particular class of research degree. The process of registration should be completed within six months of enrolment for full-time students and within twelve months for part-time students. During this period you will refine your research plans and bibliography as well as working towards the completion of the two MRes modules.
Registration is the formal process to confirm your place on a specific programme of research. The process involves completing a detailed report about your work, and is the point at which your research plans, supervisory team and timescale for completing your PhD (or MPhil) are officially confirmed.
All research proposals for registration should be able to demonstrate that:
THE SMARTLAB DIGITAL MEDIA INSTITUTE
11
•
•
•
The research project has clear aims and objectives, and methodologies for achieving these aims; The student has or can acquire the knowledge and skills to successfully complete the project within the appropriate timescale;
The proposed supervisory team has the skills, knowledge and experience necessary to successfully supervise the project;
•	The research environment is suitable and that
sufficient resources are available.
Once you have completed all the documents in liaison with your supervisory team, the forms must be submitted to SMARTlab in both electronic and signed hard copies. The necessary supporting documents will then be gathered in-
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
2.
Chapter Two.
2. Admissions, Enrolment and Registration at UEL
(Processed in chronological order)
house, and your form will be sent for consideration by the CITE Research Degrees Sub-Committee, which meets roughly every two months and where SMARTlab students are represented by Professor Lizbeth Goodman and Dr Leslie Hill.
If the committee requires that changes or improvements be made to your registration form, it will be returned with notes for amendment and you will be asked to resubmit at the next committee. If your registration is approved, it will be passed for final approval to the Graduate School Postgraduate Review sub-committee. Once passed by the Graduate School, you will be sent a letter confirming your registration details. This will include a statement of the appropriate programme of research (PhD via MPhil), mode of study (part-time/full-time), and notification of your completion deadline (‘Registration Period’).
The periods of registration are as follows: PhD (via transfer from MPhil registration)
Research Ethics Approval
As part of the Registration process, it is normal that a student will complete the process of seeking Research Ethics Approval.
Any research that involves human participants must be reviewed and approved by the Graduate School Research Ethics Committee. Any research plan involving children and/or vulnerable adults as subjects or where children and
vulnerable adults are present will require Criminal Records Bureau clearance.
If your research plans require ethics approval, you will need to submit an application to the Research Ethics Committee before registration. All applications must be signed by the student and the supervisor and submitted in electronic and hard copy to SMARTlab for signature by the Head of School. . Further Guidance can be found at: http://www.uel.ac.uk/gradschool/research/ethics.htm.
2.7. Transfer from MPhil to PhD
An important process that takes place after enrolment and registration is the process of seeking committee approval to transfer officially from MPhil to PhD status.
According to UEL Guidelines this usually happens after nine to fifteen months’ fulltime study (or part-time equivalent) and the submission to your supervisory team of roughly two chapters or 10,000 words of written work. Yet, in order to ensure your full commitment to the course and to supply you with the most suitable supervision for your research, SMARTlab will organise for you a ‘mock transfer’ before your formal registration, followed by the confirmation of your place on your specific programme of research and your ‘official’ transfer to PhD.
Transfer from MPhil to PhD involves completing a short form supported by a 3,000 to 6,000 word progress report, which must incorporate a brief review and discussion of the work already covered, and details of further plans and continued contribution to knowledge.
At SMARTlab, we also require a set of two draft chapters to be written and submitted along with the Progress
Min
Max
Full-time
33 months
60 months
Part-time
45 months
72 months
THE SMARTLAB DIGITAL MEDIA INSTITUTE
12
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
2.
Chapter Two.
2. Admissions, Enrolment and Registration at UEL
(Processed in chronological order)
Report, your current thesis abstract, a table of contents and a timeline for plans to complete the PhD.
Once the form and all these supporting documents are completed in liaison with your supervisory team, you need to hand them in electronic and signed hard copy to SMARTlab. A Transfer Review Committee will be called to consider your case, attended by a member of CITE and two members from SMARTlab (but not from your supervisory team). One of your supervisors may also be present to minute the meeting, but will not take part in the discussion. The decision of the Transfer Review Committee will then be passed through the CITE Research Degrees sub- committee and confirmed by the Graduate School.
The point of this process is to ensure that you, the student, understand the level of commitment and work required to complete a PhD successfully, and that both you and a panel of experts agree that you are capable of doing so, within a reasonable amount of time.
If for any reason either you or your supervisors and examiners are uncertain about your ability to complete the PhD, this is the point at which the degree of MPhil can be considered as an alternative exit degree. Your team will normally recommend that you be registered for either the PhD or the MPhil, based on the work you submit at this stage.
2.8. Writing Up
Once you finished actively pursuing your research, you may transfer to ‘write up’ status to reduce the level of fees payable. This must be agreed with your supervisory team and normally occurs when the final shape of the thesis is clear and time is needed to complete the documentation.
The application for write up can only be made once sufficient time has been spent in ‘full enrolment mode’ to meet statutory requirements for completion and is tenable for 12 months, with the possibility of an extension for a further twelve months only.
2.9. Mock Vivas &amp; Vivas
A viva voce examination is the final form in which your completed PhD (or MPhil) will be examined. This is an extended interview at which you will be supported by one member of your supervisory team, and will be examined by two external senior academic experts, one from UEL and one from another university.
As practice for the actual viva, SMARTlab includes a ‘mini-viva’ as part of the MPhil upgrade process, and also offers ‘mock viva’s or rehearsals before final vivas. At these ‘mock vivas’, members of our own faculty play the roles of the examiners, asking the hardest questions we can think of: not in order to trick you but in order to best prepare you for the real thing. We are always the toughest ‘mock examiners’ so that you will sail through when the real moment of truth arrives!
Further detailed hints on preparing for both the mock and ‘real’ vivas appear below.
THE SMARTLAB DIGITAL MEDIA INSTITUTE
13
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
3.
Chapter Three.
3. Regulations, Roles and Responsibilities
3.1 You &amp; Your Supervisors – Roles and Responsibilities
Once you are enrolled, you will be assigned a supervisory team composed of one Director of Studies as the main contact and one or two other supervisors. Normally SMARTlab students have two supervisors on staff who they meet with regularly and sometimes a third supervisor who periodically acts as advisor. The role of the supervisory team is for them to collectively provide academic and pastoral support and guidance to the student.
You will meet with your supervisory team three times a year during PhD seminar weeks, as well as staying in touch with them via email and the SMARTdoc site. A positive working relationship between the student and supervisory team is central to the successful progression of the research degree programme and the student must take responsibility for their conduct and regular contact with the supervisory team. In addition the student is responsible for;
•	Accepting ultimate responsibility for their own enrolment, continued learning and research activity and to ensure their continued candidacy for the degree;
•	Acting as a responsible member of UEL’s academic community;
•	Ensuring that they are familiar with the relevant aspects of our health and safety framework and our academic rules and regulations;
•	Ensuring that they are familiar with the code of good practice on research and our research misconduct policy, and ensuring that the research is ethical;
•	Making appropriate use of any teaching and learning facilities and training opportunities made available by Schools, the Graduate School and external providers;
•	Discussing with their supervisors their skills training needs and agreeing a regular schedule of meetings;
•	Providing adequate explanation of any failure to attend meetings or meet other commitments;
•	Maintaining the progress of the work in accordance with the stages agreed with their supervisor including, in particular, the presentation of written material as required in sufficient time to allow for comments and feedback;
•	Ensuring any extenuating circumstances that might require amendments to their registered research degree programme is brought to the attention of the supervisory team, the School Research Degrees sub-committee and the Graduate School;
•	Communicating their research findings to our internal and the external academic community;
•	To ensure contact details are up-to-date and correct.
3.2 Presentation Guidelines for SMARTlab students
SMARTlab PhD students are all required by UEL to submit 5,000 words of written work per academic year as well as attending fifteen days of seminars on campus. Students are also invited to present work three times a year at PhD seminar weeks. The presentation slots can be used to present written work to fulfil the 5,000 words per year requirement, and we encourage this as it is a good way for students to get feedback from the wider group. Each student works at their own pace, but here are some flexible guidelines on what might be good goalposts as you progress through the programme. This will all be discussed and agreed in more detail with your supervisors.
Year 1
1st presentation – Previous Work &amp; PhD Proposal
THE SMARTLAB DIGITAL MEDIA INSTITUTE
14
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
3.
Chapter Three.
3. Regulations, Roles and Responsibilities
A short (20 min) presentation on your previous work, i.e. the ‘practice’ you will be drawing from and your project proposal. This is the session where your peer group will first get to know in detail about you and your work.
2nd presentation – Abstract
A short (20 min) presentation – read your abstract to the group (or hand out copies to the group if you like) and leave time for a discussion. Your abstract is the key to your PhD, so writing a good abstract is crucial to framing your research project and using your time efficiently.
3rd presentation – Literature Review and Table of Contents
A short (20 min) presentation – present your annotated bibliography to the group, demonstrating what scholarly works you have been drawing on and where you agree or disagree with other scholars or practitioners in your field.
Year 2
1st presentation – 3,000-5,000 word essay
This essay is the beginning of chapter writing. You need not to write an entire chapter, just a good essay that can then be used as part of the body of one of your chapters.
2nd presentation – 3,000-5,000 word essay
Present a revised Table of Contents &amp; Abstract as well as an essay. The essay can be the continuation of the first one you submit in order to get a rough draft of an entire chapter, or you may prefer to write an essay that will form part of a new chapter.
3rd presentation – practice/theory
Present your practice-based work in the format you are planning to submit it in the thesis and give a talk/paper that gives an academic analysis of the work.
Year 3
1st presentation – 3,000-5,000 word essay
An academic essay the contents of which can be used in part or in full in one of your chapters and/or a conference paper.
2nd presentation – 3,000-5,000 word essay
Present a revised Table of Contents &amp; Abstract as well as an academic essay whose contents can be used in part or in full in one of your chapters and/or a conference paper.
3rd presentation – practice/theory
Present your practice based work in the format you are planning to submit it in the thesis and give a talk/paper that gives an academic analysis of the work.
From Year 3 onwards:
Present chapters as you complete them and show practice-based work formatted for the thesis.
The FINAL presentation
In the final presentation you will be working more towards conveying the overall impression of your thesis. Although a Viva is structured as an interview, you need to be able to explain what your thesis is about, what it contributes to the field of learning, and any specifics that you feel are important. At last, it is time to put it all together and show people what you have achieved! It is also important to remember that although your interviewees will have read your thesis, they won’t have had the benefit of hearing three years of presentations about it. This presentation should therefore summarise your work and present its major findings. It is also an important point at which fellow students and staff can ask questions that may occur as a
THE SMARTLAB DIGITAL MEDIA INSTITUTE
15
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
3.
Chapter Three.
3. Regulations, Roles and Responsibilities
result of your presentation or that they might feel will be part of the viva, to clear up any points where there is a lack of clarity, and to give you confidence for the final push!
3.3 UEL Rules &amp; Guidelines for Formatting your Thesis
Presentation of the Thesis: MPhil/PhD/ Professional Doctorate
University Regulations part nine, “Research Degrees”, and in particular section nineteen, “Presentation of the Thesis for MPhil, Professional Doctorate and PhD”, makes reference to the presentation of the thesis, which is also informed by International Standard Organisation ISO 7144:1986, copies of which are available from our Library. This note is not intended to replace either document, but it distils them into guidance on the necessary presentation sufficient for nearly all theses.
Length of Theses:
The text of the thesis should not normally exceed the following word length:
•	Practice-based MPhil – 20,000-40,000 •	Professional Doctorate – 60,000 •	Practice –based PhD 30,000– 80,000 •	Footnotes and references: If the footnotes and
references are discursive, they will be included into the word limit (unless they are simply referential).
Presentation
•	Paper: you should use good quality white A4 paper of 80-100 gsm weight and black print or type (80 gsm paper is standard copier/ printing paper). Colour should only be used as necessary for illustrations, graphs and diagrams. Remember that, in order to be copied for deposit in the British Library, black-and-white copying is likely to be used, so ideally you should use illustrations that work in black and white/greyscale.
•	Margins: 40mm on the left hand side and 20mm on the right hand side, top and bottoms margins. The wider margin on the left hand side is to allow for the binding. The paper size should be set to A4.
•	Unless necessary, you should use a portrait format.
•	Typing: on one side of the paper only. Double or one and a half spacing must be used in typescript, except for embedded quotations or footnotes, for which single line spacing may be used.
•	Font: the preferred font is Times New Roman or Ariel, in 12pt font size.
•	Page numbering: all pages should be numbered consecutively throughout the thesis, including preliminaries and appendices. Page numbers should be located centrally at the bottom of each page, approx 10mm above the edge. The preliminary sections should be numbered in lower case Roman numerals (starting at i), and the text of the thesis itself in Arabic numerals (starting at 1).
•
The thesis should be presented in English, unless you have special permission to present in another language.
Layout
THE SMARTLAB DIGITAL MEDIA INSTITUTE
16
Material should be arranged in the following sequence:
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
3.
Chapter Three.
3. Regulations, Roles and Responsibilities
Preliminaries: •	Title page – (see specimen title page) •	Abstract (no more than 300 words) •	Declaration •	Table of contents, including subsections •	List of tables, figures, illustrations etc. (if any) •	List of accompanying material (if any) •	Definitions (if any, being a list of definitions of any
terms specific to the work) •	Abbreviations (if any) •	Acknowledgements •	Dedication (optional)
Main thesis text: •	Introduction and main text, divided into chapters,
sections and subsections •	References and bibliography •	Appendices (if any) •	Glossary (if any) •	Index (if any)
Binding
Initial Temporary binding •	For the first part of the examination, the thesis
must be submitted in a temporary binding. •	This needs to be of a fixed type so pages cannot be
removed or replaced, usually as ‘soft’ or ‘perfect’
binding with either thin card or transparent covers. •	In the first instance, three copies of the temporary
bound thesis are to be submitted to the Graduate School, (two copies for each Examiner and one copy for the Chair of Examiners). You would normally have an identically bound copy for your own use at the viva.
Final Permanent binding
•	On notification that the examiners are satisfied with the thesis (which may follow any required changes and re-submission in temporary binding if required), candidates are requested to submit one permanent and sewn bound copy of the thesis (also known as library binding).
•	The binding shall be of a fixed type so that pages cannot be removed or replaced. The front and rear boards shall have sufficient rigidity to support the weight of the work when standing upright.
•	The outside front board shall bear the full title of the work in at least 24pt type capitals, gold lettering. The initials and name of the candidate, the qualification and year of submission shall also be shown on the front board (see example below). The same information (excluding the title of the work) shall be similarly shown on the spine of the work, reading down the spine.
•	Where more than one volume is submitted, each volume shall be appropriately numbered on the spine and front board.
A useful contact for binding near UEL is: Avalon Associates
John and Helen Spelman 01245 468706 email: Avalon.assoc@virgin.net
(They also do printing and delivery services).
THE SMARTLAB DIGITAL MEDIA INSTITUTE
17
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
4.
Chapter Four.
4. SMARTlab Attendance &amp; Etiquette Requirements
4.1	Engagement, Participation, Respect &amp; Constructive Criticism/Critique
The basic set of guidelines for engagement with SMARTlab are based on a principle of trust. We offer our space, our time, our attention to each other and to all the students, and ask everyone to take an equal, responsible role in maintaining that circle of trust.
Thus, our only ‘rules’ in the lab group arise from the need to show respect for all members of the group, to avoid speaking while others are speaking (or trying to speak), to avoid use of technologies while we are present with one another in shared space.
The aim is to offer a positive, safe atmosphere where ideas, even if in early stages of thought, can be expressed freely and can receive constructive criticism and reflective group critique.
Beyond the academic aims of the PhD programme, we seek to recognize the individual talents of each student, and to foster an engaged and responsive community of practitioner-scholars whose joint efforts and achievements add up to more than the sum of their parts.
The aim of SMARTlab is positive transformation – of the world, and of the scholarly academic structures that provide our scaffolds for understanding as we seek to make these social transformations.
The SMARTlab ‘circle of trust’ is therefore offered as a real physical space in our labs, as an online safe space for creative exchange, and also as a framework for understanding the need for community engagement within the group of students and faculty. Between these spaces, and between seminars, we continue to offer a new form of ‘safe space’ for scholarly endeavour that will, we hope, empower all of you who study with us now, and will then leave a legacy of trust as well as academic achievements, upon which future students and scholars may build.
4.2 At Seminars
Attendance at Seminars
There is no option of skipping seminars (in full or in part). Only in very extreme circumstances with written agreement in advance, or if you are in the final stages of writing up, can you be excused from attending. If you are enrolled and you do not turn up, your 'enrolment' clock has to turn back and you have to stay in the programme longer to achieve the right to complete. Please all bear that in mind - we meet three times a year rather than weekly so missing one seminar is the equivalent of missing a third of a year of study, and it's not easy, indeed not really possible, to fully catch up and keep up the flow of your studies. The learning model is based on equal and full group participation, so missing a seminar disadvantages your colleagues too!
This is a rule set in stone, but for your own good, so please heed!
•	Students need to arrive on time and stay the whole day.
•	Student attendance will be recorded in days rather than in weeks, so students need 15 days per year to stay on track in terms of UEL attendance requirements for remote students, otherwise your graduation may be delayed.
•	Daily attendance registers will be kept for am and pm sessions (so if someone leaves early or arrives a few hours late they don’t get credit for the whole day)
•	We will keep a register for each student for the week and input it into the database
THE SMARTLAB DIGITAL MEDIA INSTITUTE
18
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
4.
Chapter Four.
4. SMARTlab Attendance &amp; Etiquette Requirements
Etiquette at Seminars
•	Students with media for presentations need to arrive early to sort their AV connections instead of doing this during presentation time.
•	No use of laptops (or desktops in lab) during presentations unless you are the speaker (applies for student &amp; guest presentations alike).
•	In or out policy on guest lectures in the lab – the acoustics in the MAGIC lab don’t allow for talking in the kitchen while a presentation is ongoing, so people who aren’t attending the presentations need to find a space other than the kitchen to hang out in.
•	We discourage eating during presentations (student or guest) unless the slot is billed as ‘wine &amp; nibbles’ or some such. We will aim to have little breaks in the day plus a full hour lunch break each day so that students can be focused and attentive during presentations, not cracking into their lunch, brewing a cuppa etc...
•	We need to clear the lab out and lock up right at the end of events, so no late night parties or sleeping in the lab.
4.3 Between Seminars
The following methods for ensuring constant progress between seminars are in place and must be followed:
Supervision reports (as above):
These are completed by the student immediately after each supervision and must be uploaded to the shared site regularly.
Work in Progress
Work in progress must be uploaded to the shared site. Between seminars all students are expected to update their supervisory teams and to send work in progress (whether written or practical) at least six weeks before each seminar, for feedback to shape the work between meeting times.
Active participation in at least two online seminars per term is also required.
Please feel free to contact your supervisors during this time with breakthroughs, problems or for advice in research matters. Please do however remember that like yourselves, many of us do not work in the University full time or live elsewhere, and that we are busy thinking clever thoughts very much as we hope you are! Although meeting can be impractical, e-mail, Skype and the main site are all useful ways of getting hold of us and making any further arrangements.
Try to use good time management techniques over this period in order to accommodate your work. We run regular workshops on ways to do this in the seminars as we appreciate that you all have very different lives. The key point is however, don’t leave it until the last moment! Some Theatre Game Exercises that we use as part of the core SMARTlab methodology:
THE SMARTLAB DIGITAL MEDIA INSTITUTE
19
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
5.
Chapter Five.
5. SMARTlab Methods: Practising for Success
Some Theatre Game Exercises that we use as part of the core SMARTlab methodology:
5.1 Your Research - Plumbline
The Plumbline
Lizbeth firmly believes that theatre games can help us all understand our own place in relation to our ideas, mindmaps and ways of interacting with others, whether socially or intellectually. The following suggestions have evolved over fifteen years of PhD supervision and are used in all our seminars and workshops.
What is the Plumbline for your work? The plumbline is the one hook, line and sinker that keeps you motivated, that tugs at your head and heartstrings and pulls you into the research and the energized practice that enlivens your PhD studies, connects you to the creative spaces of research methodologies, and also to your colleagues.
The term comes from theatre training, where the actors and directors seek the one line in a play that holds the weight of all the others. The gesture to symbolize this idea is that of casting out a fishing line, weighted so that the string will reach down through the water and hold steady, while fish nibble from around the edges. The weight holds the process still, while allowing flexibility and ripples around the edges.
We spend a good deal of time in seminars helping each other to find the plumbline of each student’s research. Between seminars, we return to the idea of the plumbline regularly, in discussion of the personal engagement with the PhD research that inevitably forms and shapes each student’s working process. When we return to seminars, students and faculty explore plumblines again and discuss
shifts in meaning, focus and depth of research as part of the process of updating each other on progress.
In practical terms, as you write, the plumbline is the line that cannot be edited out, or removed: the thought or ethos in your life that connects to the part of the PhD you care most about: that you will not let go! Remember it as you rethink the role of the PhD in your life.
5.2 Basic Research Formula / Abstract
Basic Research formula
Here is the rubric we ask you all to fill in each time you attend a seminar. If you can do this more and more clearly and succinctly each time, then the odds are that you are making good progress in your research.
I am investigating . . . (an area of study/ a concern); so that I can understand how . . . (a knowledge outcome);
in order to be able to identify . . . (a broader generalization outcome).
This is important because . . . (contextualize and frame the scope of the problem/issue).
I hope this work will be helpful to future scholars in the field of . . . (name your field of knowledge).
5.3 Hints on Preparing for the Mock Viva
In your final Viva you will be expected to be able to answer some questions very briefly and clearly, and then be able to provide detailed evidence if asked. The questions are the same ones your supervisory team will ask you all through your time at SMARTlab.
THE SMARTLAB DIGITAL MEDIA INSTITUTE
20
What is your thesis?
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
5.
Chapter Five.
5. SMARTlab Methods: Practising for Success
What is the methodology you have selected to achieve the aims of your thesis? (And why? Give concrete examples.) What is your field of study? Where are the borders between the disciplines in which you place your work (and are therefore willing to be grilled on in exam) and those disciplines that lie beyond your scope (and which you are entitled not to be grilled on)?
What is your substantial and original contribution to the field of study? How will future scholars (not artists, not audiences, not collaborators, but people you don’t already know from the international field of scholarship) be able to build upon this original contribution you are making and to use your work as a platform to take the field further?
Why do you want a PhD? (i.e. where will it take you professionally that you couldn’t get without the degree?) What is the difference between your larger life’s work and the argument you make in your thesis?
Passing or failing the viva can depend on being able to answer these questions clearly, and the more concisely you answer them up front, in print, in your thesis, in particular in your abstract, your introduction and your conclusion the more readily your examiners will be able to engage in deeper conversation with you about content.
Role Play
Another theatre game, this one on perspective: imagine yourself to be the reader of your own PhD – the person who has to examine it. Then give yourself an easy job. Read the set of university forms that accompany a viva and put yourself in the place of the administrators who must process them (so fill them out clearly for those who know nothing about your work), and then read the final Examiners Report form that the Viva exam team must complete before and at your Viva. What the examiners write on that form determines whether you pass or fail. Make life easy for them by making sure that your thesis itself and your
performance in the viva provides them with brief, clear positive things to say in each box they have to fill in!
5.4 Visual Map of the PhD: Its Knowledge Base and Its Boundaries
We suggest that you fill in this ‘map’ at each supervision. Sometimes the boundaries shift as your research evolves. You must always be able to place yourself clearly within this kind of a schematic.
Name of candidate: Title of thesis:
␣␣␣␣␣␣␣␣␣␣␣␣␣
Interdisciplinary Knowledge Base
␣␣␣␣␣␣␣␣␣␣␣␣␣
Interdisciplinary Knowledge Base
␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣␣
Interdisciplinary Knowledge Base
␣␣␣␣␣␣␣␣␣␣␣␣␣
Central big circle - Field of Knowledge = ________________ Other circles - Cognate Fields of X, Y &amp; Z
THE SMARTLAB DIGITAL MEDIA INSTITUTE
21
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
5.
Chapter Five.
5. SMARTlab Methods: Practising for Success
Overlapping bits between circles - mark in grey and label: areas of cognate fields the candidate is responsible to know
Task
Fill in the circles to reflect your field of Knowledge, Cognate Fields and crossover areas that you are responsible to know inside out. Name the key authors in each of these areas (and add detailed notes on your intellectual position in relation to the works of each key author, in your Literature Review).
Then under the chart, fill in the blanks as concisely as possible. Draft and redraft until you have a clear summary of your thesis argument and its original contribution to knowledge.
Thesis
I argue that . . .
The thesis is practice-based, meaning that the original practical work created (an artefact, computer code, choreography, a novel, etc.) has provided the major case study for the academic argument of the thesis project overall. It has informed the work by . . .
The practice-based part of the research involved: method X leading to result Y.
This body of practical research informed the scholarly study of the thesis in the following ways:
Resulting in a new discovery of:
The scholarly method that underpinned the research and literature review, and that informed the academic writing, was . . .
This method drew on the cognate fields of . . .
This method brought together the ideas as developed in an interdisciplinary study by . . .
Overall, then, the thesis makes an Original Contribution: This has never been done before in this way, although A, B, C, have done similar work that explores related issues of . . .
This thesis matters because . . . (e.g. it will make an impact on scholarship and/or on practice and/or on the world at large but with reference to some academic issue or debate).
It makes an original contribution to scholarship in the following ways:
I offer this thesis as an original and substantial contribution to the Field of Knowledge: And to the Cognate Fields:
I hope that future scholars will be able to build upon this work.
5.5 SMARTlab Research Clusters
Our three overlapping research clusters are each chaired by a senior member of staff, supported by faculty and post- docs. Each group includes PhD students.
Performance &amp; Technology Cluster
Keywords: performance, dance, film, media, installation, interactive arts, motion capture, telematics, live-mediated events, community arts. Co-chaired by Dr Leslie Hill and Dr Susan Kozel, with Dr Chris Hales, Dr Deveril, Prof Lizbeth Goodman, Dr Sher Doruff et al.
THE SMARTLAB DIGITAL MEDIA INSTITUTE
22
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
5.
Chapter Five.
5. SMARTlab Methods: Practising for Success
E-Learning &amp; Virtual Worlds Cluster
Keywords: serious gaming, role-play, digital narratives, new media, sonic arts, live and online worlds, interactive communities, inclusion. Chaired by Professor Lizbeth Goodman with Suzanne Stein, Dr Esther MacCallum-Stewart, Dr Celia Pearce, Dr Jacki Morie, Celine Llewellyn-Jones.
Accessible Technology &amp; Social Inclusion Outreach Cluster
Keywords: accessibility, mobile tech, wearable tech, community, SMEs, business incubation, sustainable tech, accessible tech, open source ethos, assistive tech, relevant tech, HCI – Human Computer Interfaces.
Chaired by Dr Mick Donegan with Lizbeth Goodman, Dr Brian Duffy, et al.
5.6	SMARTdocs: Resources &amp; Online Resources for your studies
SMARTlab PhD programme has its own dedicated ‘SMARTdoc’ PhD site for our students. On the site you can set up your profile and store all your documents including supervision report forms, written submissions and annual reports. You can also sign up for subscriptions to various discussion groups and types of announcements (conference calls, funding opportunities etc.). The site also features a forum where faculty and students can post information on upcoming events and opportunities, as well as an events calendar. Administration announcements are also posted via the SMARTdoc site.
The SMARTdoc site can be found at:
http://www.drupal.smartlabphd.com/
(Available until April 2009)
A new and more user-friendly version of the site will be made available in March 2009 and can be accessed at: http://smartlabphd.com/phd.
The new SMARTlab PhD website will also feature a blog for students through which they can directly communicate with their supervisors and receive feedback on their work progress. Please note that students who have been using the old site need to individually transfer all their documents from the old site to the new one.
The SMARTshell 2 learning toolkit is provided to each enrolled student, providing free access to the supervisory team and extended faculty in real time online, and to monthly seminars and symposia, as well as to group chat and gathering spaces in a number of customised virtual world spaces.
All of our bespoke tools are fully integrated with UEL’s front end University administration portal (UEL Direct) and supplement that package with our own private communication spaces including the Drupal SMARTlab PhD Student Site secure space for online seminars (for which we won a PALATINE/HEFCE award in 2005-6), and bespoke multi-site online seminar and chat spaces equipped with free webstreaming capabilities, and new 3D learning and visualisation tools including those created by our faculty and previous PhDs (Mytobii Grid, Keyworx, PORT et al), combined with access to Second Life, Olive and other learning environments. We were one of the first UK University research groups to begin to build learning spaces in Second Life back in 2002, and now operate flexibly between platforms, in collaboration with learning islands created by our PhD graduates worldwide. Students and faculty are encouraged to add to the range of multimedia learning space options available to the group, as part of our Open Source/Creative Commons lab ethos. Our team also supports users with severe disabilities by
THE SMARTLAB DIGITAL MEDIA INSTITUTE
23
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
5.
Chapter Five.
5. SMARTlab Methods: Practising for Success
providing eye-controlled access to our online tools, for those who cannot use traditional interfaces to computers.
The results speak for themselves! For more information about SMARTlab please go to:
http://www.smartlab.uk.com/
For more information about SMARTlab Faculty please go to: http://www.smartlab.uk.com/4people/coreres/index.htm Form more information on the SMARTlab Practice-based PhD go to: http://www.smartlab.uk.com/5phd/index.htm
To visit the SMARTlab PhD chatroom see:
http://www.smartlab.uk.com/chatroom/
For the SMARTlab blog go to:
http://www.smartlab.uk.com/blog/?page_id=7
5.7 Useful External Links
This list is obviously not exhaustive but it is a useful starting point. We would encourage students to use social networking as much as they can – as well as drupal, facebook, myspace and other similar sites offering useful information sharing portals as well as support networks for PhD work.
http://del.icio.us/
Del.icio.us. A social bookmarking site that allows you to tag articles and items online, and categorise them. Del.icio.us is also useful as an online search facility and for developing networks of like-minded researchers.
www.wikipedia.com
Wikipedia. For some, using Wikipedia might seem like an obvious first place to start. We encourage students to read it BUT please bear in mind the following things. Wikipedia is a wiki, so anyone can edit it. People often provide incorrect information, politicised writing or downright lies. This happens a lot more often than you might think.
Use Wikipedia as a springboard – please try not to reference it in your writing but use it to travel elsewhere, to more formal, well-written writing. Using a Wikipedia reference in your formal writing is frowned upon for the above reasons. Only use it as a last resort, or if you have some sort of agenda for doing so.
http://www.mozilla-europe.org/en/products/firefox/
Firefox (browser). Firefox is increasingly the browser of choice. It is free to install and comes with a built-in Google search bar, and it allows you to open multiple tabs at once without having to minimise/maximise. A small download is needed. Firefox is much simpler to use than Internet Explorer. Firefox will do useful things like store your del.icio.us bookmark flag on the toolbar so you can tag sites more quickly. http://www.gmail.com
Gmail is one of the increasingly versatile e-mail programs. Gmail stores emails in ladders with everything grouped according to the subject. Open one mail and you can see all the preceding mails in the same thread racked above it for quick access. It's accessible online so doesn't need downloading every time it is opened, and mails are never lost – it has almost infinite capacity for storage. http://libweb.anglia.ac.uk/referencing/harvard.htm
UEL uses the Harvard system of referencing, so all your work on the PhD will use this style (but be aware that some publishers may ask you to use a different system). The
THE SMARTLAB DIGITAL MEDIA INSTITUTE
24
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
5.
Chapter Five.
5. SMARTlab Methods: Practising for Success
Harvard system, which is increasingly used for conference and written papers, is detailed on this useful site. http://web.uvic.ca/wguide/Pages/MasterToc.html The UviC Writer's Guide is a basic but useful guide about writing papers. As with all of these sites, the requirements of SMARTlab take precedence over any conflicting advice they may give.
http://www.phdcomics.com/
This is an amusing and sometimes very truthful comic about the process of taking a PhD.
5.8 Library Resources in the UK
UEL Library and Learning Services
Our Library and Learning Services provide high quality library and information services, and academic and English language key skills learning support for all our students. There is a learning resource centre on each campus which provides a wealth of resources including books, journals, online journals, and videos/DVDs that will assist you with your research. Research students are entitled to use a range of other libraries both in the South-East region of the UK and nationally, which is particularly useful for research students requiring a wide range of reading and for those who cannot always reach the UEL campuses easily. UEL is a member of the SCONUL Research Extra (SRX) borrowing scheme which allows academic staff and research postgraduates from participating institutions to borrow books from each other’s libraries. This means that UEL academic staff and research students can reasonably expect to be able to borrow books from the majority of academic libraries in Britain and Ireland. A full list of participating institutions can be found at http://www.sconul.ac.uk/. SCONUL Research Extra cards can be obtained from the Learning Resource Centre Issue desks. As a research student you will have automatic access to UEL’s computer network
and you will have access to an account following enrolment. This gives you access to our IT facilities including word- processing and spreadsheet packages, email and internet access and a range of electronic resources. Once you have access to your account you will be able to search the Learning Resource Centre Catalogue and have access to electronic journals and databases.
UEL library
www.uel.ac.uk/lss/
British Library www.bl.uk/ www.bl.uk/catalogues/listings.html The Registration process for becoming a ‘reader’ at the British Library requires two forms of photo ID with signature and address.
5.9 Other resources UK students can access in London or online
City of London Libraries
www.cityoflondon.gov.uk/Corporation/leisure_heritage/ libraries_archives_museums_galleries/ city_london_libraries/research_services.htm http://librarycatalogue.cityoflondon.gov.uk/www-bin/ www_talis
University of London Library www.ull.ac.uk/
Questia: www.questia.com Aimed at US school children, but has a range of broader texts.
www.lancashire.gov.uk/onrl/
Has a comprehensive subject-based guide to quick reference sites
THE SMARTLAB DIGITAL MEDIA INSTITUTE
25
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
5.
Chapter Five.
5. SMARTlab Methods: Practising for Success
Arts and Philosophy database
http://www.zeroland.co.nz/
Philosophy Links
Live Art Archive
www.ahds.ac.uk/ahdscollections/docroot/liveart/ liveartsearch.jsp?string=P and
www.earlham.edu/~peters/philinks.htm	www.bris.ac.uk/theatrecollection/liveart/ liveart_archivesmain.html
Web resources related to consciousness, philosophy, and such. http://consc.net/resources.html
Learning Disabilities Online www.ldonline.org/
PALATINE: (Performing Arts Learning and Teaching Innovation Network) is the Higher Education Academy Subject Centre for Dance, Drama and Music. www.palatine.ac.uk/
Digital Resources in the Humanities www.drh.org.uk/ Digital Art Source www.digitalartsource.com/index2.shtml
Students can register for free for My Athens in the UEL library https://auth.athensams.net	www.dareonline.org/
Virtual Research Training Resource
www.v-resort.ac.uk/index.php
Online Research Help
Open Directory Project
www.dmoz.org/Arts/Digital/Resources/
Best of the Web: Digital Art botw.org/top/Arts/Digital/Resources
University of South Africa Library Digital Art Link
www.shambles.net/pages/staff/OLResearch/	www.library.unisa.edu.au/resources/subject/digitalart.asp
Online Writing Resources
http://owl.english.purdue.edu/internet/resources/ index.html
Web Annotation Information
http://en.wikipedia.org/wiki/Web_annotation
Arts and Humanities Data Service
http://ahds.ac.uk/performingarts/info/index.htm
British Library Links to Performing Arts Resources
www.bl.uk/collections/wider/perfweb.html
Digital Arts &amp; Humanities www.arts-humanities.net/
5.10 Useful Publications for doctoral
students:
There are many useful guides on the market for doctoral students that can help you plan and prepare your thesis. Depending on what research method you are using different publications will be more tailored to you. Here are two publications, which are very handy companions to the general overall approach to embarking on a large sustained research project. While they are not written specifically for practice based PhDs, most of the principles are the same. Phillips, E, Pugh, D.S. 2005, How to Write a Thesis, 4th, Open University Press, Maidenhead.
Murray, R. 2007, How to Write a Thesis, 2nd, Open University Press, Maidenhead.
THE SMARTLAB DIGITAL MEDIA INSTITUTE
26
Digital Art Resource for Education
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
a.
Appendix. I. FAQ Student Fees
All Postgraduate research programmes at UEL have the same fee, regardless of School. The fees for 2008/9 are as follows:
Your fees cover you from your start date to the same date the following year. You must remember to re-enrol and pay your fees at the beginning of each year you are at UEL.
II. Accommodation
Students are responsible for booking their own accommodation. Those requiring the best local wheelchair- accessible accommodation can ring the administrative team at SMARTlab for the most current suggestions, though the Premiere Travel Inn (listed below) suits most needs.
Here are some links which may be useful.
Hotels nearest to UEL Docklands campus
•	Custom House at Excel http://www.customhouse- hotel.co.uk/ (they seem to have good deals on room shares)
•	Premier Travel Inn (Prince Regent) http:// www.premierinn.com
•	Ramada (Prince Regent) http:// www.ramadadocklands.co.uk/
Recommended Central London Hotels
•	MIC Hotel and Conference Centre 81-103 Euston Street (near Euston Station) Tel:020 7380 0001 http://www.micentre.com/
•
Ace Hotel, 16-22 Gunterstone Road, West Kensington Tel:0207 602 6600
You can find a variety of hotels and check prices and availability at: http://www.tripadvisor.co.uk/
Accommodation on campus
During term time (late September-June) accommodation on campus is only available to the full-time students. Please contact residential services if this is your case: http://www.uel.ac.uk/residential/oncampus.htm
Sometimes rooms are available on campus during the summer months for part-time or non-resident students. These places go on a first come first serve basis and they fill up quickly, so if you would like to stay on campus for the July seminars, please be sure to arrange this well in advance. The rooms are normally charged at the conference rate of approx. £35 per person per night. For booking and information please contact Anne Barrit from Residential Services at the Docklands Campus on +44 208 223 2897.
There are also off campus options available through the UEL site. Follow this link for details: http:// www.uel.ac.uk/residential/offcampus.htm
III. Who to Contact About What – UEL Staff List
Please contact Anna Sophia Schenk (a.s.schenk@uel.ac.uk) for all initial practical/administrative enquiries about your PhD. She can then refer you to the best person within UEL and help negotiate the quickest path through UEL departments.
IV. Successfully Completed SMARTlab PhDs
Dr Fatina Amran-Zerrifii, Dr Anna Birch, Dr Eleanor Bowen, Dr Ryya Bread, Dr Deveril, Dr Jane de Gay, Dr Daria Dorosh, Dr Sher Dorruf, Dr Mary Flanagan, Dr Jools Gilson-Ellis, Dr Said Graioud, Dr Christopher Hales, Dr Petra Kueppers, Dr Vic Merriman, Dr Vesna Milanovic, Dr Mourad Mknisi, Dr Jackie Morie, Dr Gayil Nalls, Dr Anne Nigten, Dr Helen Paris, Dr Celia Pearce, Dr Jane Prendergast, Dr Claire Tomlinson, Dr Fioba Wilkie, Dr Axel Vogelsang
Full-time UK/EU students
£3,860
Full-time International students
£8,930
Part-time UK/EU students
£1,940
Part-time International students
£4,460
THE SMARTLAB DIGITAL MEDIA INSTITUTE
27
SMARTLAB STUDENT HANDBOOK	FEBRUARY 2009
a.
Appendix. V. Lizbeth’s Rules of the SMARTlab PhD
We respect each and every one of you as smart, capable, creative students. We expect you to respect each other equally, and to respect all of the faculty and SMARTlab team members, who all give more of their time than is required in order to ensure that you have the best possible student experience.
When we gather at supervisions (live or online) or at seminars, we give of our time and total attention to all of you, for the good of your scholarly growth and future careers. We expect you to respect that gift of time and to return it, with attention, to all of us and to each other!
In recent months, too much faculty time and administrative time has been wasted on chasing for information and assignments. This will not be allowed to continue. We all have much better things to do, including all of you!
We therefore have to put forward some new rules, which have always been implicit and now must be made very explicit.
1) All students must attend all three seminars per year IN FULL, without asking for late arrival or early departure. Failure to attend will result in mandatory extension of the programme of study. In other words, until you have attended the correct number of seminar days you will not be permitted to graduate, even if your thesis is complete. Additional terms must be paid for. So failure to attend seminar days costs you time and money. No exceptions: this is a UK government rule and we are audited!
2) For the same reason, you MUST attend each of the monthly online seminars. Attendance involves active participation: not just logging on and lurking in the background!
3) Preparation and posting of all assignments MUST be done in a timely and professional manner. Supervision reports are required after each and every session, and must be posted to the Drupal site. Draft chapters and assignments must also be done on time and posted to the site. You will not be reminded to do this after your first term of study, but if you fail to do this consistently you will receive a few gentle
warnings, and then will be strongly encouraged to leave the PhD programme.
4) If you need for any reason to stop your studies for a period of time, it is YOUR responsibility to contact your Director of Studies and Anna Sophia (a.s.schenk@uel.ac.uk) in good time so that we can help you to do the paperwork to request intermission from the programme, in good time and without undue stress. . .
THE SMARTLAB DIGITAL MEDIA INSTITUTE
28</Text>
        </Document>
        <Document ID="139">
            <Title>Digital Puppetry</Title>
        </Document>
        <Document ID="47">
            <Title>Administration</Title>
        </Document>
        <Document ID="48">
            <Title>Thesis Structure</Title>
            <Text>
Preliminaries: 
• Title page – (see specimen title page) 
• Abstract (no more than 300 words) 
• Declaration 
• Table of contents, including subsections 
• List of tables, ﬁgures, illustrations etc. (if any) 
• List of accompanying material (if any) 
• Deﬁnitions (if any, being a list of deﬁnitions of any terms speciﬁc to the work) 
• Abbreviations (if any) 
• Acknowledgements 
• Dedication (optional) 

Main thesis text: 

• Introduction and main text, divided into chapters, 
sections and subsections 

• References and bibliography 
• Appendices (if any) 
• Glossary (if any) 
• Index (if any) 
</Text>
        </Document>
        <Document ID="49">
            <Title>Glossary</Title>
            <Text>Performance Animation Controller

...</Text>
        </Document>
        <Document ID="360">
            <Title>Annotated Images</Title>
            <Text>![fig:Shadows_dragon001][]
[fig:Shadows_dragon001]: Shadows_dragon001 "Digital Rendering of Dragon Puppet from the Greek Karaghiozis Shadow Tradition" width=400px

In the following figure, [][#fig:Shadows_dragon001] we can see some visual characteristics of the 'pre-renderered' non-realtime generated image. Taking over four minutes to generate a frame (granted on an old laptop), USE photon mapping, 'radiosity' or global illumination (where the colour characteristics of bounced or 'secondary rays' are simulated - showing washes of colour in shadows), , sub-surface scattering, surface properties affecting translucency

RIDICULOUS (but fun) approach. I set up a simulated scene where a powerful virtual light shines through a puppet shaped mesh, with translucency enabled, where it's shadow is virtually projected onto a semi translucent screen object. I positioned the scene camera on the reverse side and see if we can see the 'shadow' colours through the layers of objects.

REMEMBER PROJECT - to create responsive real-time virtual objects people can play with
![fig:Shadows_Screenshot_iPad_Physics_Touch_Animation_001][]
[fig:Shadows_Screenshot_iPad_Physics_Touch_Animation_001]: Shadows_Screenshot_iPad_Physics_Touch_Animation_001 "iPad Physics ShadowEngine Test 001: Karaghiozis Leaping" width=400px

In the following, [][#fig:Shadows_Screenshot_iPad_Physics_Touch_Animation_001], we see an object under the control of multi-touch...
![fig:Shadows_Screenshot_iPad_Physics_Touch_Animation_002][]
[fig:Shadows_Screenshot_iPad_Physics_Touch_Animation_002]: Shadows_Screenshot_iPad_Physics_Touch_Animation_003 "iPad Physics ShadowEngine Test 002: Complex Multi-Jointed Figure (Horse) Leaping. Source: Ian Grant's ShadowEngine v001" width=400px 

![fig:Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_001][]
[fig:Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_001]: Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_001 "Jeff Han's Silhouette Projects 001. Source: [TBC]." width=400px 

![fig:Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_002][]
[fig:Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_002]: Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_002  "Jeff Han's Silhouette Projects 002. Source: [TBC]." width=400px 

![fig:Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_003][]
[fig:Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_003]: Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_003 "Jeff Han's Silhouette Projects 003. Source: [TBC]." width=400px 
![fig:Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_004][]
[fig:Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_004]: Shadows_Jeff_Han_Shadow_Object_Drawn_on_Touch_Surface_004 "Jeff Han's Silhouette Projects 004. Source: [TBC]." width=400px 
</Text>
        </Document>
        <Document ID="361">
            <Title>Chapter Notes Kozel</Title>
            <Text>
Concepts from Susan Kozel:
modes of improvisation
kinaesthetically diverse - physically articulate (or expressive)

techne / technique
An anecdote: John Roberts, a UK based marionettist / puppeter, uses a twenty string chinese marionette

the expression engine [CONTACT MEDIA MOLECULE] [PROPOSE MERL PROJECT USING LITTLE BIG PLANET]

What is a 'poetics of responsivity'?
responsivity rather than interactivity...  -- in my case, the physical simulation puts 
in place a chain of action and response.. (CONTACT IMPRO) 
[Concept] hyper-reflectivity
To use some terminology from the extremely useful and perceptive work "Closer" by Susan Kozel [(2007)][#Kozel:2007qf]

When a puppeteer performs, there is an acute sense of disembodied 'hyper-reflectivity'.
The focus and control of operation has during the moments of operation
body-segmentation
In early television puppetry (as devised by Jim Henson and others), the puppeteer often worked with puppets held aloft, while they observed the camera view of their arms, on a low level / eye level monitor.

WHAT HAPPENS TO THE PUPPETEER / PERFORMER
Diagram: of 'responsivity', drawn from Merleau-Ponty (1968 [check]), Deleuze (1992 [check]) and Schwartz (1992 [check])

:"Merleau-Ponty seems to hold sacred the form of the human body, while Deleuze disintegrates it into forces, planes, and directions." [p.185][#Kozel:2007qf]

'Deleuzian non-bodies'

In describing a digital / motion capturing glitch:

:"Another evocative moment with a malfunctioning system occurred when I stepped over the sphere (which rested on the floor) and the figure I was animating stretched and then inverted, the shift in my position having scrambled the data. Responsivity can occur while the system is malfunctioning... . " [p.188][#Kozel:2007qf]

Kozel refers to Morse frequently: Here's Morse:

: "awareness of mediation and its sensory material of expression" [online][#Morse:2003ve]

: "Whatever it may be in the larger socioeconomic and cultural sphere, artists have chosen to inflect prosaic interactivity to their own expressive ends. Metainteractive aesthetic strategies—like poetry, with its rhythms, assonances, and figures—does not merely transport us to another scene or world but is itself an experience charged by semantic and formal values of expression. Interactivity is not just an instrument or a perhaps irritating interval between clicking and getting somewhere else but an event that brings corporeal and cognitive awareness to this increasingly ubiquitous feature of the contemporary world." [online][#Morse:2003ve]
it is like playing catch. flick, artefact response and response - a game with 
gravity.

: "The limbs, which are only pendulums, then follow mechanically of their own accord, without further help. He added that this movement is very simple. When the centre of gravity is moved in a straight line, the limbs describe curves. Often shaken in a purely haphazard way, the puppet falls into a kind of rhythmic movement which resembles dance." [PAGE NUMBER][#Von-Kleist:1994dq]

: "The line the centre of gravity has to follow is indeed very simple, and in most cases, he believed, straight. When it is curved, the law of its curvature seems to be at the least of the first and at the most of the second order. Even in the latter case the line is only elliptical, a form of movement natural to the human body because of the joints, so this hardly demands any great skill from the operator. But, seen from another point of view, this line could be something very mysterious. It is nothing other than the path taken by the soul of the dancer. He doubted if this could be found unless the operator can transpose himself into the centre of gravity of the marionette. In other words, the operator dances." [PAGE NUMBER][#Von-Kleist:1994dq]

: "I said the operator's part in the business had been represented to me as something which can be done entirely without feeling - rather like turning the handle of a barrel-organ.
'Not at all', he said. 'In fact, there's a subtle relationship between the movements of his fingers and the movements of the puppets attached to them, something like the relationship between numbers and their logarithms or between asymptote and hyperbola.'" [PAGE NUMBER][#Von-Kleist:1994dq]</Text>
        </Document>
        <Document ID="250">
            <Title>7.4.2 The Future of Quartz Composer</Title>
            <Text>At the time of writing *Quartz Composer* is at version 3 -- available in Mac OS 10.5 (Leopard)[^ChQCOS]. Apple have clearly signalled they value *Quartz Composer* as a unifying application across their developer technologies. 

[^ChQCOS]: Mac OS 10.5 (Leopard): Released by Apple October 26th, 2007.

*Quartz Composer 3* provides powerful, advanced functionality for programmers not available during the time-scale of the current project -- but worth mentioning. For example, the *custom patch* feature has provided free computer vision capabilities within *Quartz Composer 3*.

Apple provide an example *custom patch* that brings the camera vision technique of *optical flow* (See [][#fig:figure_image_flow_001] and [][#fig:figure_image_flow_003]) to *Quartz Composer*. There are wide applications for digital puppetry using video based movement analysis for gestural interaction with virtual objects. In [][#fig:figure_image_flow_003], the hand and teapot are moving upwards.

![fig:figure_image_flow_001][]
[fig:figure_image_flow_001]: figure_image_flow_001 "Apple's Optical Flow plugin for `Quartz Composer 3'" width=418px height=314px

![fig:figure_image_flow_003][]
[fig:figure_image_flow_003]: figure_image_flow_003 "Video Based Gestural Interface in `Quartz Composer 3'" width=418px height=314px

Other new features in *Quartz Composer 3* include (i) custom patches -- the most requested enhancement -- to allow programmers to extend core functionality, (ii) advanced *GLSL* fragment and vertex shading environments, (iii) OSC -- support for Open Sound Control, (iv) UDP server/client patches for network multicasting of composition data, (v) audio playback -- *Quartz Composer* in Tiger (Mac OS 10.4) could not play back pre-recorded video with audio, (vi) system wide composition repository and *nested* compositions will greatly simplify my *monster patch* ([][#fig:figure_015_total_QC_to_wii_patch]). External compositions  can be *referenced* within a *mother* composition -- good for organisation and project management.

</Text>
        </Document>
        <Document ID="362">
            <Title>Shadows_dragon001</Title>
        </Document>
        <Document ID="251">
            <Title>7.4.3 Custom Software Production</Title>
            <Text>Using *Quartz Composer*, I created a monster control patch ([][#fig:figure_015_total_QC_to_wii_patch], at the end of this document), that includes a *Wii-Remote* controlled avatar patch, and also allowed MIDI controlled sequencing of the scenography, special effects and character and avatar control, The monster patch included sub-patches that performed the following tasks: (i) hooks to integrate incoming data from the Nintendo *Wii-Remote*, (ii) chroma-keying using a core image kernel (written in *Open-GL Shader Language* -- *GLSL*) and (iii) *Quartz Composer* patches for MIDI connectivity and keyboard control of screen objects and sequencing scenography.

</Text>
        </Document>
        <Document ID="140">
            <Title>Expressivity</Title>
        </Document>
        <Document ID="363">
            <Title>Expressivity, Presence and Flow</Title>
            <Text>
PROBLEMS OF WRITING 'FLOW'

Writing about motion is a conceptual problem
flow is an established concept and helps to describe immersion / involvement... the subjectivity and qualities of being absorbed (or not) in a task.

Performer expressive behaviour

CONCEPT - a model of "spontaneous to planned"

CONCEPT - the pre-expressive (performer preparedness, training, the habitualised responses of the body 

examples of practice (e.g. Jeff Han - drawing objects then animating - instantly)
preparing a library of assets </Text>
        </Document>
        <Document ID="252">
            <Title>7.4.4 Control and Visual Sequencing Software</Title>
            <Text>Using Xcode, Apple's free developers environment, software was developed that used open source libraries to connect wirelessly via bluetooth to the Nintendo *Wii-Remote*. The *Wii-Remote* is a wireless pointer device and accelerometer that can detect motion and rotation in quasi 3D space. It has a numerous buttons, including a cursor pad. Wonderfully customisable, the *Wii-Remote* would control the movement, the pitch and roll and the eye movements of an avatar within the *Open-GL* 3D space of *Quartz Composer*. I called this software *QCWii*.

*QCWii* currently has the following functionality: (i) full screen capabilities for the embedded *Quartz Composer* composition -- essential for projecting the results, (ii) a set of preferences controlling the *Wii-Remote* sensitivity, key mapping, scene sequencing and order configuration, color choices and more and (iii) Connectivity to the Nintendo *Wii-Remote* Controller ideal for virtual puppet control.

</Text>
        </Document>
        <Document ID="141">
            <Title>Interaction: Modes and Forms</Title>
        </Document>
        <Document ID="142">
            <Title>Rods, Gloves and Strings</Title>
        </Document>
        <Document ID="364">
            <Title>Myron Krueger</Title>
            <Text>Take the work of Myron J. Kreuger.

</Text>
        </Document>
        <Document ID="253">
            <Title>7.4.5 The Wii-Remote and Quartz Composer</Title>
            <Text>The *QCWii* software uses an open source software library called *Wii-Remote*, part of *Darwiin Remote* -- a project initiated by Hiroaki -- available from Sourceforge[^ChQCWii]

[^ChQCWii]: *Darwiin-Remote* at Sourceforge: [http://sourceforge.net/projects/darwiin-remote/](http://sourceforge.net/projects/darwiin-remote/).

![fig:figure_wii_remote_patch][]
[fig:figure_wii_remote_patch]: figure_wii_remote_patch "The Quartz Composer patch listing the data received from a Nintendo Wii-Remote" width=110 height=297

![fig:figure_003_real_time_garbage_matte][]
[fig:figure_003_real_time_garbage_matte]: figure_003_real_time_garbage_matte "Real-time Garbage Matte and Chroma-keying of Live Video Mouth in *Quartz Composer*" width=418px height=314px

The open source *Wii-Remote* library routes data wirelessly via bluetooth from the *Wii-Remote* controller to a *Quartz Composer* composition patch (see [][#fig:figure_wii_remote_patch]) and all of the source code discussed in this chapter is available on the accompanying DVD and website[^ChQCURL].

[^ChQCURL]: Downloads and extras available from the authors' website:\\ [http://www.daisyrust.com/](http://www.daisyrust.com/).

It is theoretically possible to connect up to four *Wii-Remote* controllers to the software, allowing collaborative control over the digital puppet elements and the scenographic environment.

</Text>
        </Document>
        <Document ID="143">
            <Title>Shadows</Title>
        </Document>
        <Document ID="365">
            <Title>MMD - Error Fixing</Title>
            <Text>Error 2 on bibtex

make sure the base tex file is in the correct relationship to the main bib.
I've not seen this error before. And it took a while to fix. Just make sure the tex file is in a subdirectory at Dissertation Builds. The presence of images creates a different subdirectory level.
</Text>
        </Document>
        <Document ID="254">
            <Title>7.4.6 Chroma-keying with GLSL in Quartz Composer</Title>
            <Text>The early design layout of the avatar was composed of a live video of a performer's chroma-keyed mouth (see [][#fig:figure_003_real_time_garbage_matte]), talking and singing in  real-time and pre-recorded video eyes, the gaze direction being controllable via the *Wii-Remote*. These video elements were composited (in real-time) and textured onto a sphere. In *Quartz Composer*, 3D objects are limited to basic primitives: spheres, cubes and teapots. 

[][#fig:figure_003_real_time_garbage_matte] illustrates how *Quartz Composer* has been used to dynamically create a *garbage matte*[^ChQCGarbageMatte] and chroma-keying to isolate non-blue elements of the actors face. There follows an annotation of the chroma-keying process in *Quartz Composer*. [][#fig:figure_quartz_composer_GLSL_chromakey] depicts the contents of a `macro patch'. In *Quartz Composer*, a macro patch contains sub-patches that normally take incoming data, here image sources[^ChQCImageSources], process that data and finally *publish* the resulting data. Making several patches into a macro is an organisational convenience and encapsulates re-usable functionality.

[^ChQCGarbageMatte]: A garbage matte is a mask that crops out unwanted elements in a video image.

[^ChQCImageSources]: An image can be a single still or each frame from a video sequence.

[][#fig:figure_quartz_composer_GLSL_chromakey] uses some annotation and layout niceties available in Mac OS 10.5 (Leopard), that help to describe the data flow: in language: the sources images *provide* dimension data to math patches that perform calculations that are, in turn, passed to an `image transform' patch to resize the background image to match the size and aspect ratio of the foreground image. The resized background images, the foreground image and two other values, a colour -- called *keyColor* -- and a number -- called *sensitivity* -- are passed into a *Core Image Filter* patch. The *Core Image Filter* executes a per-pixel routine on incoming source images written in *Open-GL Shader Language* (*GLSL*). The *GLSL* code embedded in the *Core Image Filter* patch is reproduced below ([][#code].\\

![fig:figure_quartz_composer_GLSL_chromakey][]
[fig:figure_quartz_composer_GLSL_chromakey]: figure_quartz_composer_GLSL_chromakey "Data processing in the chroma-keying sub-patch" width=418px height=139px</Text>
        </Document>
        <Document ID="144">
            <Title>Multi-Modal Practice</Title>
        </Document>
        <Document ID="366">
            <Title>Email Update</Title>
        </Document>
        <Document ID="255">
            <Title>7.4.7 Evolution and Configurations</Title>
            <Text>The images of the eyes and mouth can be arbitrarily mapped onto any object or surface. Different transformations and other properties of the images,  e.g. hue, saturation, transparency,  can be mapped onto sliders and buttons on the Behringer MIDI controller[^ChQCBehringer] and adjusted as needed during the performance.

[^ChQCBehringer]: Behringer Controller Desk: [http://www.behringer.com/BCF2000/index.cfm?lang=ENG](http://www.behringer.com/BCF2000/index.cfm?lang=ENG)

In  figures [][#fig:figure_wii_controlled_avatar_001] -- [][#fig:figure_wii_eye_composite_surreal_001] and in the visual documentation, a few different configurations can be seen. Compare figure [][#fig:figure_wii_controlled_avatar_001] with the avatar in other figures: we achieve expressivity with simple translations of the image elements controlled by the *Wii-Remote*. In figure [][#fig:figure_wii_controlled_avatar_001] the eyes are mapped onto spheres. MIDI sliders control the color saturation (here deep red) and scale of the objects. The *Wii-Remote* controls rotation, object position and zoom of the figure. In [][#fig:figure_wii_eye_composite_surreal_001], we control position, scaling, rotation and the visibility of other elements to make a spontaneous composition.

![fig:figure_wii_controlled_avatar_001][]
[fig:figure_wii_controlled_avatar_001]: figure_wii_controlled_avatar_001 "The avatar in performance" width=418px height=314px

![fig:figure_wii_eye_composite_surreal_001][]
[fig:figure_wii_eye_composite_surreal_001]: figure_wii_eye_composite_surreal_001 "Improvisational avatar control -- mouth and other eye turned off" width=418px height=314px</Text>
        </Document>
        <Document ID="145">
            <Title>Audience Response</Title>
        </Document>
        <Document ID="367">
            <Title>Documentation of Practice</Title>
            <Text>This section is written in a more informal tone and contains details that will be edited.

In terms of practice, that I am writing up in the chapter: I have the first version of my multiTouch iPad application currently called the "ShadowEngine".

It's been a very involved process, with lots of learning and experimentation to establish the most effective way to do it and make it work in a playful way:

This involved:

(i) designing or capturing images of shadow puppets and silhouettes, scenography etc 
(ii) creating 3D models of them (kind of 2D with a little cardboard like depth) - with each shadow puppet part a separate sub-object
(iii) texturing them
(iv) taking the 3D model into a game engine: creating a physics simulation of them as 'rigidbodies', applying 'joints' and 'hinges' - totally configurable joint properties that control angular and motion constraints, friction, drag, mass, spring, torque, physcial properties etc. This act of configuration is a major moment in exploring and configuring the 'expressivity' of the object (as is (i))
(v) programming the application flow, interactions and control logic. Mouse control (on the desktop version) is working lovely and it's quite good fun to play with. Multitouch control on the iPad surface is working but needs a lot of further refinement and improvement. The objects often glitch and go out of control - very expressive!

Lots of things to do and experiment with:
I need to make some 'non-figurative' objects to play with as well as the figures. And need some 'softbodies'.
I'd like to work on scenography, transitions, lighting effects and other mise en scene elements.
I need to explore context / storytelling and extra-textual things beyond the object (there is a wider context for theatrical expressivity and the puppet is just one weird semiotic component in that).
I have an idea to capture instant silhouettes of the people operating the figures and incorporate them as figures in the game world (hence why I wished for a profile image of you [leslie]) - this could be really good fun for the seminar week. If you could get me a few profiles of other smartlabbers, that could be very entertaining!

Loads of ideas have occurred to me doing the work: including a desire to approach the Little Angel, and/or the V&amp;A for their theatre / puppet collection, scan some material and turn them into 'playable' digital characters, a kind of archival / education artefact - allowing us to play and 'express' with fragile objects...
Designing and working a show is possible, but the touch based control techniques are more or less ready to test in controlled ways (e.g. have one group use monotouch (i.e. mouse control) and another multitouch and video the response and collate qualitative audience feedback)

[IMAGES]
![fig:Unity3D_ConfigurableJoint_interface][]
[fig:Unity3D_ConfigurableJoint_interface]: Unity3D_ConfigurableJoint_interface "Unity3D ConfigurableJoint interface" width=200px
![fig:Unity3D_Hinge_interface][]
[fig:Unity3D_Hinge_interface]: Unity3D_Hinge_interface "Unity3D Hinge interface" width=200px
![fig:Unity3D_Interfaced_With_Joints_Visualised][]
[fig:Unity3D_Interfaced_With_Joints_Visualised]: Unity3D_Interfaced_With_Joints_Visualised "Unity3D Interfaced With Joints Visualised" width=400px
![fig:Unity3D_RigidBody_interface][]
[fig:Unity3D_RigidBody_interface]: Unity3D_RigidBody_interface "Unity3D RigidBody interface" width=200px

 
[the app icon]
![fig:ShadowEngine][]
[fig:ShadowEngine]: ShadowEngine "ShadowEngine Icon" width=120px

[the 'app' on the iPad]
![fig:Shadow_engine001_on_the_ipad][]
[fig:Shadow_engine001_on_the_ipad]: Shadow_engine001_on_the_ipad "Shadow engine001 on the ipad" width=400px

[animating under the influence of virtual gravity]
![fig:Shadow_engine001_lotte_figure][]
[fig:Shadow_engine001_lotte_figure]: Shadow_engine001_lotte_figure "Shadow engine001 lotte figure" width=400px

[a response to my touch]
![fig:Shadow_engine001_lotte_figure_touched][]
[fig:Shadow_engine001_lotte_figure_touched]: Shadow_engine001_lotte_figure_touched "Shadow engine001 lotte figure touched" width=400px

[not the horse whisperer, the horse tickler]
![fig:Shadow_engine001_horse_prodded][]
[fig:Shadow_engine001_horse_prodded]: Shadow_engine001_horse_prodded "Shadow engine001 horse prodded" width=400px

[moving a greek karaghiozis figure [source images from Spatharis, an important Greek shadow puppeteer]
![fig:Shadow_engine001_Kara_ipad_touch_photo][]
[fig:Shadow_engine001_Kara_ipad_touch_photo]: Shadow_engine001_Kara_ipad_touch_photo "Shadow engine001 Kara ipad touch photo" width=400px
[a non-realtime render of a jointed Greek shadow creature / 
character, with colourised shadows (using involved rendering 
techniques) [source puppet from Spatharis, an prominent Greek 
shadow puppeteer] - I haven't made this one playable yet]

[visualisation of the physics simulation [and double pendulum like behaviour] of a complex
multijointed character]

[another visualisation of the physics simulation post flick - the design style of this puppet is dominated
by the long multi-jointed arm (holding the gun) - you can see it's physics produced bending, kinking and secondary motion]

</Text>
        </Document>
        <Document ID="256">
            <Title>Visual Documentation With Commentary</Title>
        </Document>
        <Document ID="146">
            <Title>Chapter 10: Conclusion</Title>
        </Document>
        <Document ID="368">
            <Title>Silhouettes in Computer Vision Techniques</Title>
            <Synopsis>shadowgraphs
augmentation
examples kreuger
examples Worthington
emerging concept outline </Synopsis>
            <Text>Silhouettes in Computer Vision Techniques

There is a curious association between contemporary techniques in computer vision and the formal aesthetic qualities of human shadowgraphs, shadow puppets and other silhouette forms.

In order to calculated shapes and forms, in a most general way, computer vision techniques need to simplify a video image of the world by reducing colours (to grey-scale or black and white), 'down-sampling' the resolution of an image, or averaging areas of an image into more generalised, lower resolution zones of changing pixels. These grides of changing greyscale values can then be compared frame by frame (or to use the jargon - 'diffed') evaluating the difference. This, effectively, is an automated process, where areas of contrast in an image are processed into images that resemble the silhouette or the shadowgraph.

In a number of projects, ShadowMonsters [DATE] by Phillip Worthington the most notably relevant to shadow play, and considered in the present chapter, the human shadowgraph is automatically augmented by computer generated eyes, teeth, hair and sounds. Semiotic elements of the silhouette especially those meaningful to the 'figurative' or 'portrait' [MEMO: Centrality of the EYES AND FACE as MARKERS OF IDENTITY] are recognised by computer vision and visually extended.

Although these formal similarities, the use of silhouettes, are possibly coincidental, the association of such computer generated images and the creation of digital shadow puppets is relatively common. When the work is taken into the performative realm, we have an interesting computationally responsive, body based discipline emerging.

Examples: in the historically significant work of Myron Kreuger (US) (VideoPlace, Critters) and in  recent examples the work of Phil Worthington (UK) (ShadowMonsters, 2003) and various projects by Golan Levin (2004-2010) [DETAILS] we see the silhouetted image: 

\begin{enumerate} 
	\item being augmented by software processing; 
	\item creating a boundless performing object; 
	\item creating an expressive incorporation of the body within the context of virtual construct; 
	\item specifically in the case of Worthington, creating an expressive environment for storytelling and play; 
	\item operating in either:
\begin{enumerate}
		\item (a) a computer simulation of real world (Newtonian?) physics or 
		\item (b) real-world physics; 
\end{enumerate}
\item the computational (or video analogue) silhouette is post processed in real-time,  
\item computer processing facilitates expressive acts of control or manipulation by the performer/ digital puppeteer.
\end{enumerate} 
Each of these points (i-vii) will be elaborated.. [TODO] ... .
[Notes]</Text>
        </Document>
        <Document ID="257">
            <Title>Artistic Insights</Title>
            <Text>Accident, improvisation and deviation from planning are important. The exercise of and desire to assert control leads to a dialectic when designing an *open*, improvisational structure for performance. The haptic wireless controller leads to a great sense of playfulness and with further development will yield significant control over numerous properties of the on-screen objects.
 
A couple of lovely moments of playful, serendipitous discovery: (i) Duplicate eye movies playing back slightly out of sync made gains in general expressivity, (ii) Silhouetting characters for copyright reasons introduced a wonderful resonance of a different puppetry tradition -- shadow theatre, particularly the amazing shadow films of Lotte Reiniger.

It was quite an accident that some of the frames from the sequence resembled the visual style of Reiniger's shadow puppet films. The *Minnie* character in performance animates quite subtly, pivoting at the waist. I wish to expand the control over these elements, with multiple *Wii-Remote* controllers, introducing a game-like collaborative control over the performance space and objects.

I wish to refine the control system and expand the visual repertoire. A key to success in performance is, I feel, the availability of a large number of scenographic elements and character objects that can be re-configured by an improvising operator on-the-fly. Rehearsal is necessary to overcome the simple linear sequence of pre-defined story-elements. The operator should be knowledgeable about the content of the system in order to be able to improvise and play. The act of performance with such a system needs time and practice in order to achieve spontaneity through control.
</Text>
        </Document>
        <Document ID="147">
            <Title>In conclusion</Title>
            <Text>10.1 Introduction
</Text>
        </Document>
        <Document ID="369">
            <Title>Performing Objects</Title>
            <Text>The silhouette is a fundamental object of interaction in contemporary HCI.
The performing object is a looser, and frequently preferred, terminology for the puppet 

Puppetry is commonly associated with a tendency to represent life and undue focus has been placed on the human, animal, leading to anthropomorphic and animistic approaches.
John Bell's definition of the performing object: 

Jurkowski's definition of the anthrocentric definition of a puppet
aniconic - the absence of the graphic representation of gods (and in some cases human figures)

the pathetic or anthropomorphic fallacy

thing theatre 

Game characters and game elements as performing objects -- article: [Westecott (2009)][#Westecott:2009fk]</Text>
        </Document>
        <Document ID="258">
            <Title>Acknowledgments</Title>
            <Text>Many thanks to the open source project *Darwiin Remote* initiated by Hiroaki for the *Wii-Remote* connection framework and Jasen Jacobsen for advice and sample code on how to *smooth* the *Wii-Remote* values for use in animation. Thanks to Sam Kass for sharing great *Quartz Composer* *GLSL* code. A big thank you to Stefan Muller Arisona and the DAW07 team in Zurich.\\


![fig:figure_015_total_QC_to_wii_patch][]
[fig:figure_015_total_QC_to_wii_patch]: figure_015_total_QC_to_wii_patch "A screen shot of the top level of the main Quartz Composer patch" width=418px height=601px
</Text>
        </Document>
        <Document ID="148">
            <Title>Summary of findings</Title>
            <Text>10.2 Summary of findings
</Text>
        </Document>
        <Document ID="259">
            <Title>stuff</Title>
            <Text>To generate the scenographic material, I considered the lyrics of *Minnie the Moocher*[^ChQCMinnieMoocher] and watched ten hours of cartoons by the Fleischer studios. Most of the animations are now in the public domain, allowing mash-ups and remixing. Particularly, I looked at the work containing Cab Calloway and featuring the character *Betty Boop*. I was scouting for figures, objects, scenes and backgrounds. The basic process was to collect the video sequence or frame-grab visuals I wished to use as inspiration for scenes and objects in the performance design.

[^ChQCMinnieMoocher]: *Minnie the Moocher* (Mills-Calloway): [http://www.heptune.com/minnieth.html](http://www.heptune.com/minnieth.html)

To give a sense of the available scenographic elements, how they were designed, used and how they are *dynamic*, there follows a scene-by-scene documentation and commentary.

[][#fig7:figure_007_full_title]: The movie title text has an animated blur and is designed to look like poor tele-cined old film. The theatre frame is from a public domain cartoon. The animated smoke effect (hard to see in the still) is generated in *Quartz Composer* and applied in real-time. The title text is dynamic and can be edited in the application preferences, again in real-time -- no re-rendering is necessary.
[][#fig:figure_009_cab_poster]: This is an example of the live video mouth and pre-recorded eyes being composited into a final animated image. Here the video puppet is a fixed scene element. The easel image can be rotated in 3D space and faded in/out using the midi controllers sliders.

[][#fig:figure_008_hot_minnie]: These are dynamic elements and can appear when-ever and where-ever the operator wishes. The iris can be used for scene transitions or to visually frame the scene providing aesthetic focus. A real-time flame effect can be saturated or de-saturated at will, allowing a careful use of colour within the largely monochrome environment. Currently the silhouette subtly moves at the hip, but is not fully articulated. The 2D shadow-graph/shadow puppet theatre look is flexible and allows simple perspective and depth effects to work.

[][#fig:figure_011_3D_Scene_01] and [][#fig:figure_013_3D_Scene_03] illustrate a multi-planer, theatrical perspective constructed from four flat layers, with transparency. This scene can be rotated using the MIDI sliders in *Open-GL* 3D space] -- some may call it --  *2-and-a-half-D*.
I was particularly looking for source scenes that contained clear receding perspective with single or multiple vanishing points. I could use these scenes to explore an idea I had to set up a multi-plane version of the frame in *Open-GL* 3D space (in *Quartz Composer*). The process involved painting mattes, masking elements and exporting transparent PNGs. I masked the foreground and background characters -- isolating them from the background, and exporting them as separate PNGs. I then re-painted the missing background on the back image, using *Photoshop* and a *Wacom* Tablet. The separate PNG (each with alpha transparency) were then layered in 3D space in *Quartz Composer*. I applied blur filters to each element where the *radius* of the blur was controllable by the MIDI faders -- allowing a fake *depth of field* effect to be created and, as I discovered with play, a *rack focus* or *pulling focus* effect. Additionally, the whole environment could be rotated in 3D space across all axis. This created nice parallax movement effects and enabled a transition between the empty ballroom and the scene populated with characters.

% Figure. Full title (with midi controlled iris) height = 4.5
![fig:figure_007_full_title][]
[fig:figure_007_full_title]: figure_007_full_title "Full title (with iris)" width=139px height=105px

% Figure. Easel with video composite. height=4.5cm
![fig:figure_009_cab_poster][]
[fig:figure_009_cab_poster]: figure_009_cab_poster "Easel with video composite" width=139px height=105px

% Figure. Minnie is hot (with iris) height=4.5cm
![fig:figure_008_hot_minnie][]
[fig:figure_008_hot_minnie]: figure_008_hot_minnie "Recorded eyes and live mouth" width=139px height=105px

% Figure. Minnie and the King of Sweden. height=4.5cm
![fig:figure_011_3D_Scene_01][]
[fig:figure_011_3D_Scene_01]: figure_011_3D_Scene_01 "Minnie and the King of Sweden" width=139px height=105px

% Figure. 3D Multi-plane and perspective in the ballroom.  height=4.5cm
![fig:figure_012_3D_Scene_02][]
[fig:figure_012_3D_Scene_02]: figure_012_3D_Scene_02 "3D multi-plane and perspective" width=139px height=105px

% Figure. Rack Focus and DOF.  height=4.5cm
![fig:figure_013_3D_Scene_03][]
[fig:figure_013_3D_Scene_03]: figure_013_3D_Scene_03 "Rack focus and depth of field" width=139px height=105px</Text>
        </Document>
        <Document ID="149">
            <Title>Sensory and Gestural Interactions</Title>
            <Text>6.2 Sensory and Gestural Interactions
</Text>
        </Document>
        <Document ID="400">
            <Title>Expressive_Box2D_RagDoll_Empathy_002</Title>
        </Document>
        <Document ID="401">
            <Title>Expressive_Box2D_RagDoll_Empathy_003</Title>
        </Document>
        <Document ID="402">
            <Title>Expressive_BulletPhysics_Creature_Empathy_001</Title>
        </Document>
        <Document ID="50">
            <Title>Glossary</Title>
        </Document>
        <Document ID="403">
            <Title>Expressive_BulletPhysics_Creature_Empathy_002</Title>
        </Document>
        <Document ID="51">
            <Title>Untitled</Title>
        </Document>
        <Document ID="52">
            <Title>Handbook</Title>
        </Document>
        <Document ID="370">
            <Title>Isodora Duncan on body as engine</Title>
            <Text>

: "Before I go out on the stage, I must place a motor in my soul. When that begins to work my legs and arms and my whole body will move independently of my will. But if I do not get time to put that motor in my soul, I cannot dance. -- Isadora Duncan" 
\citep[Isadora Duncan cited in][p.273]{Preston:2005fk}
</Text>
        </Document>
        <Document ID="404">
            <Title>Expressive_BulletPhysics_Creature_Empathy_003</Title>
        </Document>
        <Document ID="53">
            <Title>Abstract</Title>
        </Document>
        <Document ID="54">
            <Title>Acknowledgements</Title>
            <Text>A big thank you to Sher Doruff, Leslie Hill, Esther MacCallum-Stewart and all my fellow students of the SmartLab, UEL, for their close reading of, feedback and comments on this work. Without their continued help, this thesis would not take shape.

I would like to thank the participants of the symposium "Raining Catz and Dogz: Virtual Pets" at the Articial Intelligence and Simulation of Behaviour (AISB) Conference at the University of Newcastle and the organisers of Digital Arts Week and ETH, Zurich for their responses to an early, early versions of this work.

To dear Mum, Dad and Claire.</Text>
        </Document>
        <Document ID="371">
            <Title>Process: Custom Software and Middleware</Title>
        </Document>
        <Document ID="260">
            <Title>Code - ADD TO END OF CHAPTER</Title>
            <Text>\begin{scriptsize}
\noindent
{\itshape Quartz Composer Core Image Kernel -- Subset of GLSL\\}
\begin{verbatim}
	
const float pi = 3.141592654;

float distColorR(vec4 pix, vec4 color)
  {
    float diff = abs(pix.r - color.r);
    diff = diff > 3. ? 6. - diff : diff;
    return diff;
  }

vec4 RGBtoHSV(vec4 rgb)
  {
    float V = max(rgb.r, max(rgb.g, rgb.b));
    V = max(V, 0.0001); // V = 0.0 causes problems later

    float minC = min(rgb.r, min(rgb.g, rgb.b));
    float delta = V - minC;

    float S = delta / V;
    S = S > 1.0 ? 1.0 : S;

    float f = (rgb.r == minC) ? rgb.g - rgb.b : ((rgb.g == minC) ? 
      rgb.b - rgb.r : rgb.r - rgb.g);
    float i = (rgb.r == minC) ? 3.0 : 
      ((rgb.g == minC) ? 5.0 : 1.0);
    float H = i - f / (V - minC);

    return vec4(H, S, V, 1.0);
  }

kernel vec4 returnCompImage(sampler image, sampler background,
  __color keyColor, float sensitivity)
  {
    vec4 pix = sample(image, samplerCoord(image));
    vec4 backpix = sample(background, samplerCoord(background));
    vec4 color2 = unpremultiply(keyColor);
    vec4 hsv = RGBtoHSV(pix);

    float colorDist = distColorR(hsv,RGBtoHSV(color2));
    float diff = colorDist - 1./sensitivity;

    diff = hsv.b &lt; 0.05 ? 1.0 : diff;

    pix = compare(vec4(diff,diff,diff,1.0), backpix, pix);
    pix.a = 1.0;
    return pix;
  }

\end{verbatim}


(Chroma Keying Example GLSL Code adapted from Sam Kass)[^ChQCCode][^ChQCCode]: Sam Kass. Source: [http://www.samkass.com/blog/][http://www.samkass.com/blog/]

\end{scriptsize}

</Text>
        </Document>
        <Document ID="405">
            <Title>Expressive_Box2D_Creature_Empathy_001_all</Title>
        </Document>
        <Document ID="55">
            <Title>ToC</Title>
        </Document>
        <Document ID="372">
            <Title>The Cinematics of Shadow Play</Title>
            <Text>The Cinematics of Shadow Plays:

2D Puppet Forms in Digital Space

Thus, there is a need for more research in real-time generation of shadow puppet images and interactive animation of the puppets. [p.213][#Tan-Kian-Lam:2008uq]

As argued, the kinetic properties are central to the expressive potential of an object. How the object moves, is jointed, and articulated, how it is a 

As I expect to demonstrate through an experimental approach and evaluations of the performer and audiences qualitative responses to both 'rigidbody' and 'softbodies'

AXIS OF USER CONTROL - AUTOMATION

Movement, Shape

\begin{itemize}
\item Shadowgraphs and Computer Vision
\item The Human Shadowgraph and Whole Body Interactions
\item Boundary Representation / Edge Detection
\item Object as Surface Detection
\item Silhouettes as Computational Objects
\item Objects as Markers
\item Touch Techniques
\item Auto Scenography - Image Processing
\item Auto Generation of Scenic Elements
\item Hands and Augmentation
\item Translucency, Glow, Ensemble
\end{itemize}

Glow, lighting - Structured Environments
Computer Vision

\begin{itemize}
\item concepts
\item outline
\end{itemize}

Myron Kruger --- so central --- critters
Robotic Performance

 
annotated concept map

annotated contact map...

computers simulation and imitation of older cultural forms
computer simulation and augmentation of older puppet related forms...

 
-- GOALS / DATES 
JITTER - accidental movement
design -  

compound activity
Andreas Gregersen and Torben Grodal in the Video Game Reader 2 

:"Another problem with the Wii-remote--and one that we find potentially more problematic for the technology's ability to produce a robust sense of agency and ownership--has to do with both the touch systems' and the proprioceptive systems' role in action. Physical force and force dynamics are central to our understanding of the physical world and thus, to a wide extent, our engagement with the world. A basic problem with ... many game controllers is that true force feedback is impossible to implement in controllers of this kind, and ... this yields a dissociation of sensory experience ... ." [p.78][#Gregersen:2009ys]

Immersion through movement patterns.
:"Merleau-Ponty writes that the body is 'a system of possible actions'. This is a strong claim, and is seems rather obvious that even though we encounter many different action opportunities throughout our lives, our physical body does not change in many of these. As already mentioned, however, different situations change 'the experience of our embodiment'. For instance, we feel a range of situations in an almost somatosensory modality, even though the nerve endings of the somatosensory system are not being stimulated." [p.68][#Gregersen:2009ys]
:"... we feel a clear sense of both agency and ownership with tool extensions that we are thoroughly familiar with" [p.68][#Gregersen:2009ys]
:"Interactive computer art, however, can never exist only as software. The work must reach out into the world in some way to capture the human interactor's input; the interactor must either make physical contact with a physical object or make movements within an articulated region of real space. And the work must project some sort of stimulus - sound, image, kinetic movement - back into the
world for the audience to perceive." [p.117][#Saltz:1997zr]

Synthetic Realism:

:"Achieving synthetic realism means attaining two goals: the simulation of codes of traditional cinematography and the simulation of the perceptual properties of real life objects and environments." [][#Manovich:2010vn]

traditional codes of cinematography

depth of field
pov
theatrical sense of perspective
multi-planar but orthogonal and 2D - or 2.5DF

transitions, scene changes, mise-en-scene</Text>
        </Document>
        <Document ID="56">
            <Title>Tables</Title>
            <Text>Tables</Text>
        </Document>
        <Document ID="261">
            <Title>7.5.1 Content Creation</Title>
            <Text>To generate the scenographic material, I considered the lyrics of *Minnie the Moocher*[^ChQCMinnieMoocher] and watched ten hours of cartoons by the Fleischer studios. Most of the animations are now in the public domain, allowing mash-ups and remixing. Particularly, I looked at the work containing Cab Calloway and featuring the character *Betty Boop*. I was scouting for figures, objects, scenes and backgrounds. The basic process was to collect the video sequence or frame-grab visuals I wished to use as inspiration for scenes and objects in the performance design.

[^ChQCMinnieMoocher]: *Minnie the Moocher* (Mills-Calloway): [http://www.heptune.com/minnieth.html](http://www.heptune.com/minnieth.html)

To give a sense of the available scenographic elements, how they were designed, used and how they are *dynamic*, there follows a scene-by-scene documentation and commentary.

[][#fig:figure_007_full_title]: The movie title text has an animated blur and is designed to look like poor tele-cined old film. The theatre frame is from a public domain cartoon. The animated smoke effect (hard to see in the still) is generated in *Quartz Composer* and applied in real-time. The title text is dynamic and can be edited in the application preferences, again in real-time -- no re-rendering is necessary.
[][#fig:figure_009_cab_poster]: This is an example of the live video mouth and pre-recorded eyes being composited into a final animated image. Here the video puppet is a fixed scene element. The easel image can be rotated in 3D space and faded in/out using the midi controllers sliders.

[][#fig:figure_008_hot_minnie]: These are dynamic elements and can appear when-ever and where-ever the operator wishes. The iris can be used for scene transitions or to visually frame the scene providing aesthetic focus. A real-time flame effect can be saturated or de-saturated at will, allowing a careful use of colour within the largely monochrome environment. Currently the silhouette subtly moves at the hip, but is not fully articulated. The 2D shadow-graph/shadow puppet theatre look is flexible and allows simple perspective and depth effects to work.

[][#fig:figure_011_3D_Scene_01] and [][#fig:figure_013_3D_Scene_03] illustrate a multi-planer, theatrical perspective constructed from four flat layers, with transparency. This scene can be rotated using the MIDI sliders in *Open-GL* 3D space] -- some may call it --  *2-and-a-half-D*.
I was particularly looking for source scenes that contained clear receding perspective with single or multiple vanishing points. I could use these scenes to explore an idea I had to set up a multi-plane version of the frame in *Open-GL* 3D space (in *Quartz Composer*). The process involved painting mattes, masking elements and exporting transparent PNGs. I masked the foreground and background characters -- isolating them from the background, and exporting them as separate PNGs. I then re-painted the missing background on the back image, using *Photoshop* and a *Wacom* Tablet. The separate PNG (each with alpha transparency) were then layered in 3D space in *Quartz Composer*. I applied blur filters to each element where the *radius* of the blur was controllable by the MIDI faders -- allowing a fake *depth of field* effect to be created and, as I discovered with play, a *rack focus* or *pulling focus* effect. Additionally, the whole environment could be rotated in 3D space across all axis. This created nice parallax movement effects and enabled a transition between the empty ballroom and the scene populated with characters.

![fig:figure_007_full_title][]
[fig:figure_007_full_title]: figure_007_full_title "Full title (with iris)" width=209px height=157px

![fig:figure_009_cab_poster][]
[fig:figure_009_cab_poster]: figure_009_cab_poster "Easel with video composite" width=209px height=157px

![fig:figure_008_hot_minnie][]
[fig:figure_008_hot_minnie]: figure_008_hot_minnie "Recorded eyes and live mouth" width=209px height=157px

![fig:figure_011_3D_Scene_01][]
[fig:figure_011_3D_Scene_01]: figure_011_3D_Scene_01 "Minnie and the King of Sweden" width=209px height=157px

![fig:figure_012_3D_Scene_02][]
[fig:figure_012_3D_Scene_02]: figure_012_3D_Scene_02 "3D multi-plane and perspective" width=209px height=157px

![fig:figure_013_3D_Scene_03][]
[fig:figure_013_3D_Scene_03]: figure_013_3D_Scene_03 "Rack focus and depth of field" width=209px height=157px</Text>
        </Document>
        <Document ID="406">
            <Title>Expressive_Box2D_Creature_Empathy_001_all</Title>
        </Document>
        <Document ID="150">
            <Title>Gloves</Title>
            <Text>6.3 Gloves
</Text>
        </Document>
        <Document ID="57">
            <Title>Figures</Title>
            <Text>Figures</Text>
        </Document>
        <Document ID="373">
            <Title>Shadow_engine001_horse_prodded</Title>
        </Document>
        <Document ID="262">
            <Title>Conclusion</Title>
            <Text>To be completed
</Text>
        </Document>
        <Document ID="58">
            <Title>1.1 Introduction</Title>
            <Text>Chapter 1</Text>
        </Document>
        <Document ID="407">
            <Title>Expressive_Box2D_RagDoll_Empathy_all</Title>
        </Document>
        <Document ID="151">
            <Title>Strings</Title>
            <Text>6.4 Strings

MultiMarkdown has a special format for footnotes that should represent glossary terms.  This doesn't make much difference in XHTML (because there is no such thing as a glossary in XHTML), but can be used to generate a glossary within LaTeX documents.

[^glossary]: glossary: Glossary 
	A section at the end ...


</Text>
        </Document>
        <Document ID="59">
            <Title>The plumb-line</Title>
            <Text>This is Ian Grant

![Ian Grant][]

[Ian Grant]: Ian_grant_2 "Picture of Ian Grant" width=170px height=160px

This is a straight reference: Barnes and friends [Barnes et al. (2008)][#Barnes:2008p685],  write about many, many things.

[^glossaryfootnote]: glossary: normal (n)
    The actual definition belongs on a new line, and can continue on
    just as other footnotes.

This is a variation using natbib In their seminal paper, [Barnes et al. (2008); p 42][#Barnes:2008p685] argue convincingly that up is down.

Glossary entries work like footnotes.
[^glossary]: glossary: Glossary 
A section at the end. Sadly the glossary is tricky to control.

\glossary{Peanuts: }{Write and historicise the spaces around the digital puppet, the avatar, automata, the robot, artificial life, and animatronic the in the realm of the 'kinetic behavioural sculpture' (Reas 1996)}

\glossary{Aruuurah: }{Oh thats nasty. Where's the damn pull string. You dont know what you are going to find down here. Sorry, I don't string that way!} 

Including straight latex includes a reference in the glossary. But the markdown doesn't presently work.

Acronyms - can't make sense of the acronym support. Must write a \ac{SIGGRAPH} paper on it.

[Baran and Popović (2007)][#Baran:2007p353]

In [][iangrant] we see a crazy picture of the man himself. This paragraph demonstrates internal cross references. You can see information about Ian in the table, [][prototypetable]

This, I hope is a definition list:

*Apple*
:   Pomaceous fruit of plants of the genus Malus in 
    the family Rosaceae.

Orange
:   The fruit of an evergreen tree of the genus Citrus.


I hope this is a table:


|             |          Grouping           ||
First Header  | Second Header | Third Header |
 ------------ | :-----------: | -----------: |
Content       |          *Long Cell*        ||
Content       |   **Cell**    |         Cell |

New section   |     More      |         Data |
And more      |            And more          |
[Prototype table]</Text>
        </Document>
        <Document ID="374">
            <Title>Shadow_engine001_Kara_ipad_touch_photo</Title>
        </Document>
        <Document ID="263">
            <Title>Code</Title>
        </Document>
        <Document ID="408">
            <Title>Expressive_BulletPhysics_Creature_Empathy_all</Title>
        </Document>
        <Document ID="152">
            <Title>Physical Simulation</Title>
            <Text>6.5 Physical Simulation
</Text>
        </Document>
        <Document ID="375">
            <Title>Shadow_engine001_lotte_figure_touched</Title>
        </Document>
        <Document ID="264">
            <Title>Puppetry, Computers and Hybrid Practice</Title>
        </Document>
        <Document ID="409">
            <Title>Expressive Digital Shadow Puppetry</Title>
            <Text>Expressive Digital Shadow Puppetry 

Play And Production

\begin{itemize}
	\item involvement and flow
	\item dimensions of expressivity
	\item heuristics
	\item qualitative insights into post play experiences
	\item decomposition [WHAT DO I MEAN BY THAT]
	\item designing a test study
\end{itemize}

Studying the nexus of 'expressivity' betwixt and between designer, performer, audience, around object
software development and future
The current study in contemporary shadow puppetry adopts the interesting position *not* to be a about a religious or anthropological investigation of non-Western shadow performance traditions. Instead it is about the 'syncretic' act of hybridity where ancient performance forms meet the most contemporary of multimedia technologies: computer vision and interactive 'alternative realities'.

When I say 'most contemporary', I acknowledge that some of the current techniques acquiring popularity and dissemination across the internet and other media art contexts, have antecedents and a historicity that I also wish to outline.
</Text>
        </Document>
        <Document ID="153">
            <Title>Sensory Interactions</Title>
            <Text>6.6 Sensory Interactions
</Text>
        </Document>
        <Document ID="376">
            <Title>Shadow_engine001_lotte_figure</Title>
        </Document>
        <Document ID="265">
            <Title>Draft - Front Matter to Add</Title>
        </Document>
        <Document ID="154">
            <Title>Responding to Sound</Title>
            <Text>6.7 Responding to Sound
</Text>
        </Document>
        <Document ID="377">
            <Title>Shadow_engine001_on_the_ipad</Title>
        </Document>
        <Document ID="266">
            <Title>Upgrade</Title>
        </Document>
        <Document ID="155">
            <Title>6.7 Responding to Sound</Title>
        </Document>
        <Document ID="378">
            <Title>ShadowEngine</Title>
        </Document>
        <Document ID="267">
            <Title>REGRegister_Ian_Grant_11 June 2009_002_FROM ANNA_SIGNED</Title>
            <Text>

APPLICATION TO REGISTER FOR A RESEARCH DEGREE PROGRAMME
(TO BE COMPLETED BY THE PROPOSED SUPERVISORY TEAM AND THE STUDENT)

In completing this form you should refer to the relevant sections of the Research Degree Regulations (Part 9 of the UEL Manual of General Regulations) and the UEL Code of Practice for Postgraduate Research Programmes.

This form should be typewritten wherever possible. 

Confirmation of registration will be sent to the student’s and the Director of Studies’ UEL email address. 

When fully completed, this form must be submitted to the nominated individual in the School - usually the Research Administrator or Officer to the RDSC, accompanied by Form SDN for each supervisor nominated.

1. STUDENT’S DETAILS

FULL NAME
Ian Grant
UEL STUDENT NUMBER
u0751966
ENROLLED FOR  (Please Tick)
MPHIL


PROF DOC

PHD VIA MPHIL
x
PRAC DOC

PHD DIRECT

MPHIL BY PUBLICATION

PHD (EUR)

PHD BY PUBLICATION

TITLE OF PROFESSIONAL/PRACTITIONER DOCTORATE PROGRAMME (IF APPLICABLE)
SMARTlab Practice-based PhD Programme
DATE OF ENROLMENT
July 2008
CURRENT MODE OF STUDY (Please Tick)
FULL TIME

PART TIME
x
REQUESTED DATE FOR START OF REGISTRATION (Registration must occur within 6 or 12 months of the date of enrolment for FT/PT students respectively and may be backdated to that date)
July 2009
DECLARATION OF PREVIOUS REGISTRATION. If you have a previous period of registration elsewhere which you wish to transfer and to form part of your registration at UEL, please give the dates and location of this registration. Proof of this period of registration must be provided.
none
SCHOOL
SMARTlab Digital Media Institute / School of Computing, Information Technology and Engineering
NAME OF COLLABORATING ESTABLISHMENTS (IF ANY)
n/a
IF YOU ARE CONDUCTING YOUR RESEARCH OUTSIDE THE UK, PLEASE PROVIDE BRIEF DETAILS OF WHERE AND HOW YOU WILL BE SUPPORTED IN YOUR RESEARCH.
n/a
IF YOU ARE IN RECEIPT OF ANY SCHOLARSHIP, AWARD ETC IN CONNECTION WITH THE PROPOSED RESEARCH PROGRAMME, PLEASE GIVE BRIEF PARTICULARS
n/a
IF YOU HAVE UNDERTAKEN ANY TRAINING OR HAVE PRIOR EXPERIENCE THAT YOU FEEL SUPPORTS THIS FORM, PLEASE GIVE DETAILS AND DATES

Methods in Arts Research, PhD research seminars. University of Manchester, 1994-1995 I registered for a PhD 1994-1997 at Manchester University, where  
there were mandatory sessions for MPhil / PhD registrants. The sessions included: Literary Studies, Textual Analysis, Hermeneutics,  Anthropological Methods in Performance, Historiography,  Interpretative approaches, Phenomenology and other Qualitative  approaches (mainly in Educational Research but across performance  studies).

Using Computers in Qualitative Research, a series of workshops organised by SAGE and Qualitative research software company QSR (on NuDist)

IF YOU HAVE MADE ANY PUBLICATIONS THAT YOU FEEL SUPPORT THIS FORM, PLEASE GIVE REFERENCES.

Ian Grant (2008) Transdisciplinary Digital Art: Sound, Vision and the New Screen, Chapter: Experiments in Digital Puppetry: Video Hybrids in Apple’s Quartz Composer. Communications in Computer and Communication Science. Edited by Randy Adams, Steve Gibson, Stefan Muller Arisona. Springer. 342-258. 


(July 2007) Of Minnie the Moocher and Me: Explorations in Digital Puppetry. Video Hybrids in Apple’s Quartz Composer Digital Puppetry Performance Workshop and Paper (to be published in proceedings). Digital Art Weeks Festival 2007 / DAW07. ETH Zurich, Zurich, Switzerland. HYPERLINK "http://www.digitalartweeks.ethz.ch"www.digitalartweeks.ethz.ch

(February 2007) Talking Toys and Digital Puppetry. Artificial Intelligence and Simulation of Behaviour (AISB) ’07 at Newcastle University, Newcastle Upon Tyne, 2-5 April 2007.

(Jan 2001) Finding the Wooden Voice in Puppetry Into Performance: A Users Guide. London: Theatre Museum, Central School of Speech and Drama and the Puppet Centre Trust. 29-31. ISBN 09537729-42 

(June 2000) From Craft to IT: The Art of the Puppet Technology. A paper delivered at Digital Scenography, a conference held at the University of Kent, UK. 

(1996) Formation of a Research Design: Towards a Critical Ethnography of Educational Theatre: Poster Abstract (and Review by Joyce Wilkinson) in Somers,john, ed. 1996. Drama and Theatre in Education: Contemporary Research. North York, Canada: Captus Press.


2. THE PROGRAMME OF RESEARCH 

PROPOSED TITLE OF THESIS
Expressivity and the Digital Puppet:
Mechanical, Digital and Virtual Objects
in Games, Art and Performance
AIM OF THE INVESTIGATION

The current study explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

I aim to explore the related contexts of digital puppetry, real-time animation, mimetic and non-mimetic kinetic objects, automata, 'cybernetic sculpture', performance systems and the technological interfaces to such phenomena. 

I aim to create evaluate and create puppet/object theatre performances/installations that use original software and hardware systems that are designed to explore 'performance expressivity', with reference to relevant historical, art, entertainment and technological precedents.

I wish to theorise and form a taxonomy of 'expressivity' in relationship to digital domains and puppetry. By ‘expressivity’, I refer to different domains of including: voice, face, body and gesture.
DETAILS OF YOUR PROPOSED RESEARCH IN LAY TERMS

Many innovations in contemporary computing and the way we interact with machines have applications beyond the domains for which they are designed. Computer vision, touch surfaces (like the Apple’s iPhone, Microsoft’s Surface), wireless control devices, accelerometers and game control devices, like data gloves, can be used beyond their original purpose. In my practice and research, I wish to create low-cost hardware and software that allows a skilled puppeteer to control a physical object (like a robot) or a virtual object (like a game character) in ways that calibrate and test new interfaces for their expressive potential. Coming from a background in puppetry, where a special approach to gesture, movement and mechanism applies, I wish to find a new fusion between new technology and performance traditions.



PROPOSED PLAN OF WORK, INCLUDING ITS RELATIONSHIP TO PREVIOUS WORK, MAXIMUM 4,000 WORDS. Please include in your discussion a description of the research methodologies and explain why these methodologies are the most appropriate for the task. Include a list of references for all works cited.

The current work has a practical, experimental and media archeological approach that plans to explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

Puppetry has distinct overtones in various world traditions, but in computer space can be defined as the expressive use of interfaces to control objects. It is necessary, in the thesis, to offer an extended definition of ‘object’, which becomes interesting in computational contexts as does ‘expressive behavior’ and ‘affect’.

In exploration of practices akin to puppetry, the research draws on instances of performance, games and installation art practice in wider cultural practice and the practical explorations of the author. This focus is multi-disciplinary. 

Methods and Methodologies – A Media Archeology of Kinetic Behavioural Sculpture
The writing seeks to historicise in and around the spaces occupied by digital (or virtual) puppets, the avatar, automata, the robot, artificial life, and the animatronic. All these phenomena can be grouped in the realm of 'kinetic behavioural sculpture':

“A behavioral kinetic object is a dynamic system, meaning it changes with time. This system is composed of a source of energy, inputs, outputs, and a control architecture which converts the information from the inputs into information which stimulates the outputs. These elements sum to form the complete object, but other elements may be added to provide mass or form.” (Reas 1996, 22)

An emerging methodological approach called media archeology, will be used to trace how traditional puppetry forms, often described by their mode of interaction/media, (rods, shadows, strings and gloves) - map into current paradigms of interaction with virtual worlds, digital objects, games, and automated animation; and associated areas of digitally enhanced dolls, toys and automata. Media Archeology is a field that reflects on today's technologies by linking them to the socio-technical histories out of which they emerged.

“There is a gang of artists, theoreticians, and artist-theoreticians who have a very strong affinity (moreover, one that links them to a figure such as Artaud): they burn and burn up in the endeavour to push out as far as possible the limits of what language and machines, as the primary instances of structure and order for the last few centuries, are able to express and in doing so to actually reveal these limits” (Siegfried Zielinski, 2009, my emphasis)

Ethnographic Methods - Thick Description of Performance and User Testing
My experience in ethnographic methodology also assists to bring the study within a particular social, cultural way of seeing.  How to systematically observe and perform thick description (Geertz, 1977) of contexts such as software design and study is relatively unique, but common in anthropology when interpreting performance culture and forms, such as puppetry.

Media archeology, in the way documents and artefacts are studied has an affinity with historical ethnography. The moment of study is past as well as present. I will apply ethnographic methods of theme analysis – observation, thick description, coding and dimensionalising -  to support a broader semiotic approach to the cultures and histories of digital puppetry.

“The concept of culture I espouse…is essentially a semiotic one. Believing with Max Weber, that a man [sic] is an animal suspended in webs of significance he himself has spun. I take culture to be those webs, and the analysis of it to be therefore not an experimental science in search of law but an interpretive one in search of meaning.” (Geertz, 1977, 5)

Sherry Turkle (2005) has developed an approach to understand how we think and express through evocative objects. I argue digital puppets are a special class of evocative object. In addition to Turkle, I will consider multiple approaches to theorising the object in cultural practice: again this is drawn from a variety of domains (e.g. John Dewey's expressive object in Art as Experience (Dewey, 2005) to Baudrillard's complex system of objects (Baudrillard, 1988). I have started this work in relation to automata and talking toys (see the samples).

Gestural Interaction
All styles of puppet manipulation rely on gestural interaction. The study considers the performative/expressive potential of numerous computer interaction systems that utilise tactility and touch, whole-body, face, hand interactions. In a cultural study of expressive automata and toys, the study considers the role of the sonic in puppetry: the rhythmic gesture, sound and voice.

I wish to explore: How do new and emerging technologies facilitate innovative techniques of design and control over puppet-like objects and create experiences of expressive play?

The human body in movement and the computational capture of such movement to make meaning through expressive acts mediate by evocative objects – is another way to state the primary exploration of the thesis.

"The world of objects and needs would thus be a world of general hysteria. Just as the organs and functions of a body in hysterical conversion become a gigantic paradigm which the symptom replaces and refers to, in consumption objects become a vast paradigm designating another language through which something else speaks," (Baudrillard:1988, 10-29)

Plan of Practical Work
In the collected documents, I supply a Gantt chart detailing an approximate time-plan of how the practical work relates to the evaluation of literary research material and writing activity.

I will produce and evaluate: (i) Performance, Artwork and Kinetic Behavioural Sculpture; (ii) Original software, software art and computer based creative production techniques; (ii) Innovative control systems and interfaces that will be applicable hopefully beyond the domain of performance; (iv) Papers, videos, websites and published outcomes that will contribute to the thesis, documentation and other elements of the research.

As a minimum, I plan to make and test: (i) Several expressive, physical objects; (ii) Several virtual objects, that are controlled by… (iii) Several innovative control systems, in prototype, including touch surfaces, optical motion/expression/gesture capture, digital input devices.

It will be ideal if the systems are tested in performance or in installation contexts. Standard user testing methods, including focus groups, cognitive walkthroughs, Goals, Operators, Methods, and Selection Rules (GOMS) analysis, will be employed, evaluated and discussed.

Relationship to Previous Work: Comparative Case Studies of Practice
The thesis will analyse case study material, including software and performances by the author, other artists, puppeteers, animators and sculptors. Case studies include:

* Theo Jansen's Walker and his Staadcreatures: Covering ideas of Virtual Creatures, Object Orientedness, Automata and Computer Simulated Physics; See (Jansen 2007)


Figure  SEQ Figure \* ARABIC 1: Theo Jansen Walker As Physical, Virtual and Textual (Code) Object
* Surfaces and Shadows: Synchronous and Asynchronous Techniques: 
	- Lotte Reiniger, Stop motion animator
* Phil Worthington “Shadow Monsters”. Animation and Computer Vision: AR installations;

Figure  SEQ Figure \* ARABIC 2: Phil Worthington “Shadow Monsters” 2005
* Golan Levin: New media artist. (Reface [Portrait Sequencer] 2007), Snout (2008) Double-Taker (Snout) (2008) Opto-Isolator (2007) and other projects: face and expression recognition; automata and robotics; video hybrids;

  
Figure  SEQ Figure \* ARABIC 3: Golan Levin - physical and virtual projects (2006-2009)
* Embodied Interactions: finger, hand, whole-body, gestural, eye and facial interactions;

Examples of how these areas map with my own work can be see in Ian Grant “Video Hybrids, explorations in digital puppetry”, presented as a performance piece, software and documentary write up (see sample chapter).

References

Baudrillard, Jean and Mark. Poster. Selected writings (of) Jean Baudrillard. Stanford University Press, Stanford (Calif.), 1988.

Dewey, John. Art as Experience. Perigee Trade, 2005.

Geertz, Clifford.The Interpretation of Cultures. Basic Books, NY. 1977.

Jansen, Theo. The Great Pretender. 010 Uitgeverij, 2007.

Levin, Golan. Projects.  HYPERLINK "http://www.flong.com/projects/" http://www.flong.com/projects/ Date modified: 2009. Date Accessed: 1st June 2009.

Reas, Casey. Behavioral kinetic sculpture. Master’s thesis, MIT, 1996.

Turkle, Sherry. Computer Games As Evocative Objects: From Projective Screens. To Relational Artifacts  in Raessens, Joost and Jeffrey Goldstein “Handbook of Computer Game Studies” MIT Press, London, 2005.

Zielinski, Siegfried. Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. MIT Press, 2008.

Zielinski, Siegfried. Media Archaeology.  HYPERLINK "http://www.ctheory.net/articles.aspx?id=42#text%201" http://www.ctheory.net/articles.aspx?id=42#text%201 Date modified: 7th Nov, 1996. Date Accessed: 1st June 2009.

For an extended abstract, sample writing and fuller bibliography (in progress), please see the sample chapters supplied for annual review.








SUMMARY OF THE ELEMENTS OF THE INVESTIGATION THAT ARE NOVEL, ORIGINAL OR CREATIVE AND THAT MAY CONSTITUTE PRODUCTION OF ORIGINAL KNOWLEDGE OR AN ORIGINAL INTERPRETATION OF EXISTING KNOWLEDGE
- Original Performance, Artwork and Artefacts;
- Original Software products and creative production techniques;
- Innovative hardware control systems and interfaces that will be applicable hopefully beyond the domain of performance;
- Papers and published outcomes that will contribute to the thesis, documentation and other elements of the PhD.
DETAILS OF FACILITIES AVAILABLE FOR THE INVESTIGATION, INCLUDING FUINDING AND LOCATION
Self funded, private studio, work based workshop, teaching and seminar spaces at TVU
RELATIONSHIP BETWEEN WORK TO BE UNDERTAKEN IN THE COLLABORATING ESTABLISHMENT AND THAT TO BE UNDERTAKEN AT THE SPONSORING ESTABLISHMENT OR ELSEWHERE (IF RELEVANT)
n/a
A HEALTH AND SAFETY AUDIT IS REQUIRED IN RESPECT OF ALL PROPOSED LABORATORY EXPERIMENTS AND/OR FIELDWORK. 
DOES THIS INVESTIGATION REQUIRE LABORATORY EXPERIMENTS AND/OR FIELDWORK? (Please Tick)
YES

NO
audience studies and user testing – not sustained fieldwork
IF YES, A MANDATORY COPY OF THE AUDIT, SIGNED BY THE STUDENT AND THE DIRECTOR OF STUDIES, IS ATTACHED (Please Tick)
YES

NO

DOES THE PROGRAMME OF RESEARCH INVOLVE WORK THAT WOULD REQUIRE THE APPROVAL OF THE UNIVERSITY’S ETHICS COMMITTEE PRIOR TO THAT PART OF THE PROJECT COMMENCING? (Please Tick)
YES

NO
x
IF YES, HAS A FORM FOR APPROVAL ALREADY BEEN MADE TO THE UNIVERSITY’S ETHICS COMMITTEE?  (Please Tick) 
You may not proceed with this part of your research until approval has been granted.
YES

NO

WILL THE PROGRAMME OF RESEARCH LEAD TO OUTPUT(S) WHICH WILL HAVE COMMERCIAL FORM(S) AND/OR INTELLECTUAL PROPERTY OF POTENTIAL VALUE? 
If yes, you must  inform the Knowledge Transfer Office
YES
SOME ORIGINAL SOFTWARE BUT ALL IS BASED ON OPEN-SOURCE PRINCIPLES SO WILL SHARE-AND-SHARE-ALIKE (USING CREATIVE COMMONS LICENSES)
NO


3. PROPOSED SUPERVISORY TEAM
Form SDN - Nomination as a Supervisor/Director of Studies should be appended for the Director of Studies and each supervisor nominated. The RDSC will use information supplied about current and past supervisions to ensure that this proposal does not breach the maximum number of PGR students that can be supervised without consent.

NOMINATION OF DIRECTOR OF STUDIES
NAME AND TITLE
Dr Leslie Hill
SCHOOL
SmartLab Digital Media Institute / School of Computing, Information Technology and Engineering
EMAIL ADDRESS (preferably an institutional or official email address)
leslie@smartlab.uk.com
HOLDS A PHD/PROFESSIONAL DOCTORATE? (Please Tick)
YES
X
NO

NUMBER OF UK HEI RESEARCH DEGREE STUDENTS CURRENTLY SUPERVISED (excluding this one)
MPHIL

PROFESSIONAL DOCTORATE

PHD including PhD (Eur)
8
PRACTITIONER DOCTORATE

NUMBER OF UK HEI RESEARCH DEGREE STUDENTS SUCCESSFULLY SUPERVISED TO COMPLETION FOR
MPHIL

PROFESSIONAL DOCTORATE

PHD including PhD (Eur)
6
PRACTITIONER DOCTORATE


NOMINATION OF SECOND SUPERVISOR
NAME AND TITLE
Dr Esther MacCallum-Stewart
SCHOOL (and for supervisors external to UEL, their institution and full postal address)
SmartLab Digital Media Institute / School of Computing, Information Technology and Engineering
EMAIL ADDRESS (preferably an institutional or official email address)
esther@smartlab.uk.com
HOLDS A PHD/PROFESSIONAL DOCTORATE? (Please Tick)
YES
X
NO

NUMBER OF UK HEI RESEARCH DEGREE STUDENTS CURRENTLY SUPERVISED (excluding this one)
MPHIL

PROFESSIONAL DOCTORATE

PHD including PhD (Eur)
4
PRACTITIONER DOCTORATE

NUMBER OF UK HEI RESEARCH DEGREE STUDENTS SUCCESSFULLY SUPERVISED TO COMPLETION FOR
MPHIL

PROFESSIONAL DOCTORATE

PHD including PhD (Eur)

PRACTITIONER DOCTORATE


NOMINATION OF THIRD SUPERVISOR (IF APPLICABLE)
NAME AND TITLE

SCHOOL (and for supervisors external to UEL, their institution and full postal address)

EMAIL ADDRESS (preferably an institutional or official email address)

HOLDS A PHD/PROFESSIONAL DOCTORATE? (Please Tick)
YES

NO

NUMBER OF UK HEI RESEARCH DEGREE STUDENTS CURRENTLY SUPERVISED (EXCLUDING THIS ONE)
MPHIL

PROFESSIONAL DOCTORATE

PHD including PhD (Eur)

PRACTITIONER DOCTORATE

NUMBER OF UK HEI RESEARCH DEGREE STUDENTS SUCCESSFULLY SUPERVISED TO COMPLETION FOR
MPHIL

PROFESSIONAL DOCTORATE

PHD including PhD (Eur)

PRACTITIONER DOCTORATE

 

OVERALL SUPERVISORY EXPERIENCE AND ACTIVITY OF THE PROPOSED SUPERVISORY TEAM
NUMBER OF UK HEI RESEARCH DEGREE STUDENTS CURRENTLY SUPERVISED (excluding this one)
MPHIL

PROFESSIONAL DOCTORATE

PHD including PhD (Eur)
12
PRACTITIONER DOCTORATE

NUMBER OF UK HEI RESEARCH DEGREE STUDENTS PREVIOUSLY SUCCESSFULLY SUPERVISED TO COMPLETION FOR
MPHIL

PROFESSIONAL DOCTORATE

PHD including PhD (Eur)
6
PRACTITIONER DOCTORATE

IF THE COMBINED EXPERIENCE AND ACTIVITY OF THE PROPOSED SUPERVISORY TEAM DOES NOT MEETTHE REQUIREMENTS STIPULATED IN UEL’S RESEARCH DEGREE REGULATIONS, PLEASE PROVIDE A SHORT STATEMENT JUSTIFYING WHY CONSENT IS SOUGHT AND WHY THIS PARTICULAR SUPERVISORY TEAM IS MOST SUITABLE FOR THE PROGRAMME OF RESEARCH.


4. NOMINATION OF ADVISORS

NOMINATION OF FIRST ADVISOR, IF APPLICABLE
NAME AND TITLE

CURRENT POSITION, DEPARTMENT AND INSTITUTION

POSTAL ADDRESS 

EMAIL ADDRESS (preferably an institutional or official email address)

PREVIOUS POSTS HELD 

QUALIFICATIONS

NOMINATION OF SECOND ADVISOR, IF APPLICABLE
NAME AND TITLE

CURRENT POSITION, DEPARTMENT AND INSTITUTION

POSTAL ADDRESS 

EMAIL ADDRESS (preferably an institutional or official email address)

PREVIOUS POSTS HELD 

QUALIFICATIONS



5. STUDENT’S DECLARATION

I CONFIRM
THAT I WISH TO APPLY TO BE REGISTERED AS A STUDENT FOR THE POSTGRADUATE RESEARCH AWARD INDICATED AT THE HEAD OF THIS FORM. 
THAT THE PARTICULARS GIVEN IN THIS FORM ARE CORRECT.
THAT EXCEPT WITH THE SPECIFIC PERMISSION OF THE PGR REVIEW SUB-COMMITTEE, ANY WRITTEN COMPONENT OF THE PROGRAMME MUST BE SUBMITTED IN ENGLISH AND I MUST ALSO UNDERTAKE AN ORAL EXAMINATION IN ENGLISH.

STUDENT IAN GRANT
SIGNED:
DATE:  11 JUNE 2009

6. SUPERVISORY TEAM’S DECLARATION


WE CONFIRM 
THAT WE SUPPORT THIS FORM AND BELIEVE THAT THE STUDENT HAS THE POTENTIAL TO COMPLETE THE PROGRAMME OF WORK PROPOSED
THAT, AS REQUIRED BY OUR UNIVERSITY’S RESEARCH DEGREE REGULATIONS, WE ARE NOT OURSELVES CURRENTLY RECEIVING SUPERVISION ON A RESEARCH DEGREE PROGRAMME AT ANY HEI
THAT, IF APPLICABLE, WE AGREE TO THE REQUEST FOR THE BACKDATING OF REGISTRATION

WE RECOMMEND THAT THE APPLICANT BE REGISTERED FOR A RESEARCH DEGREE

DIRECTOR OF STUDIES
 SIGNED:
PRINTED:
DATE:
SECOND SUPERVISOR
SIGNED:
PRINTED:
DATE:
THIRD SUPERVISOR  (IF APPLICABLE)
SIGNED:
PRINTED:
DATE:








7. ACADEMIC REFEREES
Please nominate two referees to whom the School Research Degrees Sub-Committee may refer for advice. One nominee must be external to the Research Degrees Sub-Committee.


FIRST REFEREE
NAME AND TITLE
Dr Helen Paris, Reader
POSTAL ADDRESS 
Drama Dept, School of Arts, Brunel University
Uxbridge, Middlesex UB8 3PH
EMAIL ADDRESS (preferably an institutional or official email address)
helen.paris@brunel.ac.uk
SECOND REFEREE
NAME AND TITLE
Jeremy Gardiner, Professor of Digital Arts
POSTAL ADDRESS 
School of Art and Design, TVU
Grove House 
1 The Grove 
Ealing, W5 5DX
EMAIL ADDRESS (preferably an institutional or official email address)
Jeremy.gardiner@tvu.ac.uk
8. DEAN OF SCHOOL’S DECLARATION


I CONFIRM THAT THE UNIVERSITY AND SCHOOL FACILITIES AND RESOURCES DETAILED IN THIS FORM, TOGETHER WITH OTHER APPROPRIATE RESOURCES, SUCH AS SUPERVISOR(S)’ TIME, WILL BE AVAILABLE FOR THE DURATION OF THE PROGRAMME OF RESEARCH

DEAN OF SCHOOL (or nominee)
 SIGNED:
PRINTED:
DATE:

9. DOCUMENT LOG
ONCE SECTIONS 1-8 HAVE BEEN COMPLETED, THIS FORM, ALONG WITH FORM(S) SDN, SHOULD BE SUBMITTED TO THE RELEVANT SCHOOL RDSC FOR CONSIDERATION AND THE REFERENCE FOR THE PERTINENT MINUTE(S) RECORDED BELOW. IF RECOMMENDED, THE FORM AND FORM(S) SHOULD THEN BE SENT TO THE PGR REVIEW SUB-COMMITTEE FOR APPROVAL ALONG WITH THE RELEVANT EXTRACT FROM THE MINUTES (EITHER ATTACHED TO THIS FORM OR PASTED AT ITS END). PLEASE NOTE THAT THE FORM WILL NOT BE PROCESSED UNTIL THE GRADUATE SCHOOL HAS RECEIVED THE MINUTES OF BOTH COMMITTEES IN THEIR ENTIRETY. 

RECOMMENDED BY SCHOOL RDSC
 DATE OF RDSC: 
MINUTE REF:
APPROVED BY PGR REVIEW SUB-COMMITTEE
DATE OF PGR REVIEW:
MINUTE REF:



























NOMINATION AS A SUPERVISOR/DIRECTOR OF STUDIES 
SHORT CURRICULUM VITAE FORM
(TO BE COMPLETED BY THE NOMINATED INDIVIDUAL)


In completing this form you should refer to the relevant sections of the Research Degree Regulations (Part 9 of the UEL Manual of General Regulations) and the UEL Code of Practice for Postgraduate Research Programmes.

This form should be typewritten wherever possible. 

Confirmation of appointment will be sent to the student’s and the supervisory team’s email addresses.

This form, to accompany Form REG: Application to Register for a Research Degree Programme or Form CSA: Application for a Change in the Supervisory Team, is to be submitted to the nominated individual in the School - usually the Research Administrator or Officer to the RDSC. 


1. SUPERVISOR TYPE

IS THE NOMINATION FOR A SUPERVISOR OR DIRECTOR OF STUDIES? (Please Tick)
SUPERVISOR

DIRECTOR OF STUDIES
x


2. PROPOSED SUPERVISOR’S DETAILS

NAME AND TITLE
Dr Leslie Hill
CURRENT POSITION, DEPARTMENT AND INSTITUTION
Principle Research Fellow UEL SMARTLab
PREVIOUS POSTS HELD 
Director Curious International Ltd 1996-present
NESTA Dream Time Fellow 2003-2005
Head of Media, London College of Music and Media, Thames Valley University
Sr. Lecturer / Artist Fellow, Institute for Studies in the Arts, Arizona State University 1997-2000

EMAIL ADDRESS (preferably an institutional or official email address)
leslie@smartlab.co.uk


3. PROPOSED SUPERVISOR’S QUALIFICATIONS AND EXPERIENCE

QUALIFICATIONS
PhD Representations of Women in British Drama 1890-1817, University of Glasgow
MA Shakespeare Text &amp; Performance, Shakespeare Institute, Stratford Upon Avon, University of Birmingham
Double BA English &amp; Philosophy Magna Cum Laude, University of New Mexico

CURRENT RESEARCH OR PROFESSIONAL PRACTICE
Socially engaged art projects, performance, filmmaking, ‘sci-art’ projects, biofeedback interfaces.  Academic writing in field of Performance Studies

THE PROPOSED SUPERVISOR IS NOT THEMSELVES BEING SUPERVISED ON A RESEARCH DEGREE PROGRAMME AT ANY HEI (Please Tick)
YES

NO X

PLEASE LIST UP TO SIX PUBLICATIONS WHICH ARE OF MOST RELEVANCE TO THIS PROPOSAL
PUBLICATION 1
Leslie Hill &amp; Helen Paris, HYPERLINK "http://www.placelessness.com/current/index.html"Performance and Place, (London: Palgrave MacMillan) 2006.
PUBLICATION 2
Leslie Hill &amp; Helen Paris, ‘Curious Feminists’ in Feminist Futures, Elaine Aston and Geraldine Harris (eds.), (London: Palgrave Macmillan) 2006.

PUBLICATION 3
Leslie Hill, ‘Red Lantern House’ in China Live: Reflections on Contemporary Performance Art, (London: Live Art UK) 2005.
PUBLICATION 4
Leslie Hill and Helen Paris, ‘One to One’ in We Love You: Reflections on Audiences (London: Goethe Institute and ACE) 2005.
PUBLICATION 5
Leslie Hill &amp; Helen Paris with Dr Upinder Bhalla, ‘On the Scent’ in Talking Back to Science: Art, Science and The Personal Bergit Arends and Verity Slater (eds.), (London: The Wellcome Trust) 2004.
PUBLICATION 6
Leslie Hill &amp; Helen Paris, 'On the Scent', Performance Research, (Cambridge: Routledge) September, 2003.


4. PROPOSED SUPERVISOR’S CONFIRMATION

I CONFIRM THAT THE ABOVE IS A TRUE AND ACCURATE RECORD OF MY EXPERIENCE AND QUALIFICATIONS
SIGNED:
PRINTED:
DATE:






































NOMINATION AS A SUPERVISOR/DIRECTOR OF STUDIES 
SHORT CURRICULUM VITAE FORM
(TO BE COMPLETED BY THE NOMINATED INDIVIDUAL)


In completing this form you should refer to the relevant sections of the Research Degree Regulations (Part 9 of the UEL Manual of General Regulations) and the UEL Code of Practice for Postgraduate Research Programmes.

This form should be typewritten wherever possible. 

Confirmation of appointment will be sent to the student’s and the supervisory team’s email addresses.

This form, to accompany Form REG: Application to Register for a Research Degree Programme or Form CSA: Application for a Change in the Supervisory Team, is to be submitted to the nominated individual in the School - usually the Research Administrator or Officer to the RDSC. 


1. SUPERVISOR TYPE

IS THE NOMINATION FOR A SUPERVISOR OR DIRECTOR OF STUDIES? (Please Tick)
SUPERVISOR
X
DIRECTOR OF STUDIES



2. PROPOSED SUPERVISOR’S DETAILS

NAME AND TITLE
Dr Esther MacCallum-Stewart
CURRENT POSITION, DEPARTMENT AND INSTITUTION
Postdoctoral Research Fellow, Games and Interactive Media, SMARTlab UEL.
PREVIOUS POSTS HELD 
Associate Tutor, University of Sussex (2001-2006)
Researcher, University of Sussex (2002-2006)
EMAIL ADDRESS (preferably an institutional or official email address)
neveah@gmail.com


3. PROPOSED SUPERVISOR’S QUALIFICATIONS AND EXPERIENCE

QUALIFICATIONS
PhD. ‘The First World War and Popular Culture’ (University of Sussex)
2006
BA English Literature and Cultural Studies. (1st Class)
2000
CURRENT RESEARCH OR PROFESSIONAL PRACTICE
Postdoctoral Research Fellow, Games and Interactive Media, SMARTlab UEL: digital games, players, player and game narratives and tribal behaviour. Along with Microsoft community affairs  at UELI am also developing the concept of Stealth Learning in online games
Board Member: DiGRA (Digital Games and Research Association)
THE PROPOSED SUPERVISOR IS NOT THEMSELVES BEING SUPERVISED ON A RESEARCH DEGREE PROGRAMME AT ANY HEI (Please Tick)
YES

NO
X
PLEASE LIST UP TO SIX PUBLICATIONS WHICH ARE OF MOST RELEVANCE TO THIS PROPOSAL
PUBLICATION 1
Tribal Fusions: Social Worlds, Players and Games. This book investigates the role of players and groups in online worlds, their behaviour as active agents in a developing narrative space and how the represent themselves online. (this is the project I am currently working on. Manchester University Press are reviewing the proposal. 50 000 words have been written so far)

PUBLICATION 2
Stealth Learning and the Digital Dividend (MIT Press) (Badshah, Goodman and MacCallum-Stewart) Part of the (E)Mergencies series, this book discusses how players can learn in online environments by drawing on an increasingly rich backdrop of virtual narrative experience. (accepted and first draft is completed - goes to press August 2009) 

PUBLICATION 3
Digital Culture, Play and Identity; A World of Warcraft Reader
MIT Press: ISBN: 0262033704 (2008)
Two chapters in this:

‘Never Such Innocence Again’, War and Histories in World of Warcraft 

The Playing of Roles: How does roleplay affect gameplay in  World of Warcraft? (with Justin Parsler, The University of Brunel)

PUBLICATION 4‘Real Men Carry Girly Epics – Normalising Gender Bending in Online Games
Eludamos Vol 1 #2 (2008)
 HYPERLINK "http://www.eludamos.org/index.php/eludamos/article/view/35/54" http://www.eludamos.org/index.php/eludamos/article/view/35/54


PUBLICATION 5
The International Journal of Performance Arts and Digital Media Vol 3 #2.1 (2007)
‘The Warfare of Imagined Homes – Building Identities in Second Life’ 
ISSN: 14794713

PUBLICATION 6
Lost on a Desert Island – Convergence Texts and Digital Games.
Games and Culture Vol 4 #3 2008. (This may change to vol 5 #1 depending on a preceding special edition)

4. PROPOSED SUPERVISOR’S CONFIRMATION

I CONFIRM THAT THE ABOVE IS A TRUE AND ACCURATE RECORD OF MY EXPERIENCE AND QUALIFICATIONS
SIGNED: 
PRINTED:
DATE: 



Agenda Item 5.4.
	Page  PAGE 8 of  NUMPAGES 20

Form REG  – Application to Register  for a Research Degree Programme 
Version 1.0 (August 2008)



</Text>
        </Document>
        <Document ID="156">
            <Title>8.7.1 Rhythm</Title>
        </Document>
        <Document ID="379">
            <Title>Unity_Virtual_Sound_Space</Title>
        </Document>
        <Document ID="268">
            <Title>Puppetry Bibliography</Title>
            <Text>[Bacon (1997)][#bacon97]

[Baird (1965)][#baird65]

[Bell (2000)][#bell00]

[Bicat (2007)][#bicat07]

[Blumenthal (2005)][#blumenthal05]

[Currell (2007)][#currell07]

[Currell (1999)][#currell99]

[Jurkowski (2008)][#jurkowski08]

[Jurkowski (1988)][#jurkowski88]

[Jurkowski and Francis (1996)][#jurkowski96]

[Jurkowski and Francis (1998)][#jurkowski98]

[Lecoq (2002)][#lecoq02]

[Lecoq (2006)][#lecoq06]

[Mack (1996)][#mack96]

[Meschke and Sörenson (1992)][#Meschke:1992la]

[Obraztsov (2001)][#obraztsov01]

[Schönewolf (1969)][#schonewolf69]

[Silk (1996)][#silk96]</Text>
        </Document>
        <Document ID="157">
            <Title>8.7.2 Gesture and Voice</Title>
        </Document>
        <Document ID="269">
            <Title>Latex Usage Notes</Title>
        </Document>
        <Document ID="158">
            <Title>Locative, Pervasive and Augmented Narratives</Title>
            <Text>6.8 Locative, Pervasive and Augmented Narratives
</Text>
        </Document>
        <Document ID="159">
            <Title>Artificially Intelligent Objects</Title>
            <Text>6.9 Artificially Intelligent Objects
</Text>
        </Document>
        <Document ID="410">
            <Title>Note (Quotations and Notes - Kozel), 7 Jun 2010, 19:09</Title>
        </Document>
        <Document ID="411">
            <Title>Worthington_ShadowMonsters_ORIG</Title>
        </Document>
        <Document ID="300">
            <Title>Article Ideas</Title>
        </Document>
        <Document ID="412">
            <Title>Future Software Developments and Ideas</Title>
            <Text>A brainstormed list of future (some planned) developments:

In performance, combine projected shadow play with actual silhouettes co-present on the same screen.

Combine computer generated augmentations with touch controlled silhouettes, like a feedback loop. Pass the video output of one process into the computer vision / augmentation environment.

Develop the UI for sceneography and character selection.

Develop the UI for transitions and other cinematic effects.

Like above, develop the UI for 'sequencing'

Other cinematic effects include:
Use the same middleware to develop touch interactions with soft bodies;

Explore the expressivity of non-figurative objects and shapes;


</Text>
        </Document>
        <Document ID="301">
            <Title>Light science: physics and the visual arts By Thomas D. Rossing, Christopher J. Chiaverina</Title>
            <Text>Light science: physics and the visual arts By Thomas D. Rossing, Christopher J. Chiaverina
1999
http://books.google.com/books?id=jpH1_dCT_UcC&amp;lpg=PP1&amp;pg=RA1-PA359#v=onepage&amp;q=&amp;f=false

￼</Text>
        </Document>
        <Document ID="413">
            <Title>Related New Media Work</Title>
        </Document>
        <Document ID="302">
            <Title>Chapter Call</Title>
            <Text>Introduction Traditionally, multimedia applications have primarily engaged two of the human senses – the audio and the visual – out of the five possible. With recent advances in computational technology, it is now possible to talk of applications that engage the other three senses, as well: tactile, olfaction, and gustatory. This integration leads to a paradigm shift away from the old multimedia towards the new mulsemedia – multiple sensorial media.
Recommended Topics Mulsemedia brings with itself new and exciting challenges and opportunities in research, industry, commerce, and academia. This book solicits chapters dealing with mulsemedia in all of these areas.
Topics of interest include, but are not limited to, the following:
Context-aware Mulsemedia
Metrics for Mulsemedia
Mulsemedia devices
Mulsemedia in distributed environments
Mulsemedia integration
Mulsemedia user studies
Multi-modal interaction
Mulsemedia and virtual reality
Olfactory enhanced media
Quality of service and Mulsemedia
Tactile/haptic interaction
User modelling and Mulsemedia
Mulsemedia and e-learning
Mulsemedia and e-commerce
Submission Procedure Researchers and practitioners are invited to submit on or before December 20, 2009, a 300 word chapter proposal clearly explaining the mission and concerns of his or her proposed chapter. Authors of accepted proposals will be notified by January 15, 2010 about the status of their proposals and sent chapter guidelines. Full chapters are expected to be submitted by March 15, 2010. All submitted chapters will be reviewed on a double-blind review basis. Contributors may also be requested to serve as reviewers for this project.
Publisher This book is scheduled to be published by IGI Global (formerly Idea Group Inc.), publisher of the “Information Science Reference” (formerly Idea Group Reference), “Medical Information Science Reference,” “Business Science Reference,” and “Engineering Science Reference” imprints. For additional information regarding the publisher, please visit www.igi-global.com. This publication is anticipated to be released in 2010.
Important Dates December 20, 2009: Proposal Submission Deadline January 15, 2010: Notification of Acceptance March 15, 2009: Full Chapter Submission April 15, 2010: Review Results Returned May 17, 2010: Revised Chapters Due July 30, 2010: Final Deadline
</Text>
        </Document>
        <Document ID="380">
            <Title>Unity3D_ConfigurableJoint_interface</Title>
        </Document>
        <Document ID="414">
            <Title>Emerging Themes</Title>
        </Document>
        <Document ID="303">
            <Title>Chapter Call</Title>
            <Text>￼

￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼

￼
￼
￼
￼
BOOKS
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼

￼
￼
￼
￼
BOOK SERIES
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼

￼
￼
￼
￼
JOURNALS
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼

￼
￼
￼
￼
PROCEEDINGS
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼

￼
￼
￼
￼
TEACHING CASES
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼

￼
￼
￼
￼
PAY-PER-VIEW
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼

￼
￼
￼
￼
REFERENCE
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼

￼
￼
￼
￼
E-RESOURCES
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼
￼

￼
￼
￼
￼
ABOUT IGI
￼
￼
￼
￼
￼
BECOME AN AUTHOR/EDITOR  |   MAILING LIST  |   HOW TO ORDER  |   LIBRARY SUGGESTION | EXAMINATION REQUESTS/COURSE ADOPTION | DISTRIBUTORS
￼
IGI Online Bookstore

	            	Search All...
	    Book Chapters
  Book Series
  Books
  E-Books
  Journal Articles
  Journals/Annals
  Proceeding Papers
  Proceedings
  Reference
  Teaching Cases

	            


Exact Search

￼
￼

￼

￼

  Browse Our Bookstore
•
IGI Catalogs &amp; Newsletters
•
Forthcoming Titles
•
Featured Book
•
By Category
•
Advanced Search

  Shop
•
My Profile
•
View My Cart

  Contact Us
IGI Global
Main Office
701 E. Chocolate Avenue
Hershey, PA 17033, USA
Tel: 717-533-8845 x100
Toll Free: 1-866-342-6657
Fax: 717-533-8661
    or 717-533-7115
Email: cust@igi-global.com
 

￼

CALL FOR CHAPTERS
Proposals Submission Deadline: 12/20/2009


Multiple Sensorial Media Advances and Applications:
New Developments in MulSeMedia
A book edited by Dr. George Ghinea (Brunel University), Dr. Frederic Andres (CVCE/NII), and Dr. Stephen Gulliver (University of Reading)

Introduction Traditionally, multimedia applications have primarily engaged two of the human senses – the audio and the visual – out of the five possible. With recent advances in computational technology, it is now possible to talk of applications that engage the other three senses, as well: tactile, olfaction, and gustatory. This integration leads to a paradigm shift away from the old multimedia towards the new mulsemedia – multiple sensorial media.
Recommended Topics Mulsemedia brings with itself new and exciting challenges and opportunities in research, industry, commerce, and academia. This book solicits chapters dealing with mulsemedia in all of these areas.
Topics of interest include, but are not limited to, the following:
Context-aware Mulsemedia
Metrics for Mulsemedia
Mulsemedia devices
Mulsemedia in distributed environments
Mulsemedia integration
Mulsemedia user studies
Multi-modal interaction
Mulsemedia and virtual reality
Olfactory enhanced media
Quality of service and Mulsemedia
Tactile/haptic interaction
User modelling and Mulsemedia
Mulsemedia and e-learning
Mulsemedia and e-commerce
Submission Procedure Researchers and practitioners are invited to submit on or before December 20, 2009, a 300 word chapter proposal clearly explaining the mission and concerns of his or her proposed chapter. Authors of accepted proposals will be notified by January 15, 2010 about the status of their proposals and sent chapter guidelines. Full chapters are expected to be submitted by March 15, 2010. All submitted chapters will be reviewed on a double-blind review basis. Contributors may also be requested to serve as reviewers for this project.
Publisher This book is scheduled to be published by IGI Global (formerly Idea Group Inc.), publisher of the “Information Science Reference” (formerly Idea Group Reference), “Medical Information Science Reference,” “Business Science Reference,” and “Engineering Science Reference” imprints. For additional information regarding the publisher, please visit www.igi-global.com. This publication is anticipated to be released in 2010.
Important Dates December 20, 2009: Proposal Submission Deadline January 15, 2010: Notification of Acceptance March 15, 2009: Full Chapter Submission April 15, 2010: Review Results Returned May 17, 2010: Revised Chapters Due July 30, 2010: Final Deadline
Editorial Advisory Board Members TBA
Inquiries and submissions can be forwarded electronically (Word document) or by mail to:
Dr. George Ghinea george.ghinea@gmail.com
with cc to:
Dr. Frederic Andres frederic.andres@cvce.lu and Dr. Stephen Gulliver s.r.gulliver@henley.reading.ac.uk
« Return to IGI's Invitation to Edit/Author a New Publication
 
￼
￼
Books  |  Book Series  |  Journals  |  Proceedings  |  Teaching Cases  |  Pay-Per-View  |  Reference  |  E-Resources  |  About IGI
Become An Author/Editor  |  Mailing List  |  How To Order  |  Library Suggestion  |  Examination Requests

IGI Global - All Rights Reserved ©2001-2009
</Text>
        </Document>
        <Document ID="270">
            <Title>Notes</Title>
            <Text>Hyperref and URL Formatting

There are controls to override the default formats for URLS in bibs. I have included this in the preamble but it is not 100% clear where the "URL: " comes from or where the original style is defined.

Google
-------
url.sty
redefine \harvardurl
\urlstyle{same}
</Text>
        </Document>
        <Document ID="381">
            <Title>Unity3D_Hinge_interface</Title>
        </Document>
        <Document ID="415">
            <Title>Multitouch and Expressive Gestures</Title>
        </Document>
        <Document ID="304">
            <Title>Title - Chapter Proposal</Title>
            <Text>Proposal

Multiple Sensory Media and Expressive Play
</Text>
        </Document>
        <Document ID="271">
            <Title>Email NOTES on application</Title>
            <Text>Please next time do get any paperwork to Esther and myself 2 weeks before any UEL deadline so we have time to work with you on them.

The members of the RDSC all felt that the project sounded very interesting and thought that your document was well written, so those are positives.

Here are the things the points the RDSC has asked you address before they approve your registration (and they will approve it next time):

1) narrow the focus - this is a point that has come up many times in supervisions.  the RDSC members were unanimous in their criticism that the scope is too board and the project too vague as it is currently presented and that in fact the aims and topics are so broad that the project is not do-able.  I argued that the specificity of the practice-based elements of the research will keep the project from being too broad as they will be specific case studies, but as you don't provide details about the practice based projects they shot down that defense.

2) you must clearly and explicitly articulate your research questions within the research document.  I summarised three main research questions for you which i argued were embedded in the project description but they say that in a research proposal readers shouldn't have to go looking for the research Qs - so present them as bullet points and make sure they are questions specific enough - i.e. that your research can answer them  (so instead of saying this thesis asks 'how can we cure HIV?' you would say 'this thesis examines the impact of ________ drug on recptors in a clinical study of males aged 30-45 conducted at _______ clinic over a ____ period")

3) the biblio in the registration doc is too short - i know you have a longer version for your annual review so just paste in the long version

4) paste your Gantt chart directly into the registration document

5) this is a panel of computer scientists, so they didn't like the part about innovative new software.  one of their niggles was that they say software design isn't innovative, it is commonplace (ha!) unless you spell out what you are going to do that is different and how it is different - so more specificity about the type of software and hardware interfaces you are planning to work with

I know it is a bit of a pain having to redo the registration doc, but lets think of it as an OPPORTUNITY to address a couple of the issues we've all noted have needed attention in your project all along - i.e. Focus, specificity and research questions you can answer within the scope of your thesis.  This work will be really valuable as you enter the 2nd year of your studies and it will be good to do this for your annual review as well, so there is no wasted effort.

The RDSC has agreed to backdate your registration to July 2008 once the requested changes are made and reviewed at the next meeting.

I'd like you to address the issues and send the registration doc AND all the annual review docs to Esther and myself two weeks prior to the July seminar - deadline = 6th July.

Leslie x



</Text>
        </Document>
        <Document ID="160">
            <Title>Conclusion</Title>
            <Text>6.10 Conclusion</Text>
        </Document>
        <Document ID="416">
            <Title>Software Scope and Design</Title>
        </Document>
        <Document ID="305">
            <Title>Body - Chapter Proposal</Title>
            <Text>What are the most effective designs for multiple sensorial media when creating and sustaining experiences of expressive play?

Computer vision for gesture recognition, touch surfaces (like the Apple's iPhone, Microsoft's Surface and TUIO systems), wireless control devices, accelerometers and game control devices, like data gloves, can be used beyond their originally designed purpose. Increasingly these technologies and the means to programme them are finding their way into the hands of non-specialists and are crossing boundaries of practice. The present chapter works in an interdisciplinary way between human-computer interaction/interface design, computer programming and the performance practice of puppetry, evaluating the reciprocal opportunity for innovative multi-modal practice.

The established puppetry traditions, as categorised by control mechanisms and form: rods, shadows and glove, (one may add the direct manipulation of objects and the related area of toys and automata), have fascinating parallels with moving objects in the virtual worlds of interactive art and games. The chapter will clearly establish and test the boundaries of these parallel forms.

An emerging methodological approach called media archeology, will be used to trace how traditional puppetry forms, often described by their mode of interaction/media, (rods, shadows, strings and gloves) - map into the emerging paradigm of multiple sensory interaction and multi-modal practice.

Puppetry and mulsemedia are broad areas. The chapter will find focus through particular case studies where artists, scientists and designers create computer forms for expression and improvisation centred around gestural interaction, touch, and the optical and physical capture of motion.

Puppetry has distinct practices in various world traditions, but in computer space can be defined as 'the expressive use of interfaces to control objects'. It is necessary, for the chapter, to offer an extended definition of 'object', 'expressive behaviour' and 'computational affect', which become interesting in the context of mulsemedia.


# 

defining object

digital puppetry

expressive play and expressive behaviour
computational affect 

entertainment applications

propositions
multi-touch

eye-control

</Text>
        </Document>
        <Document ID="382">
            <Title>Unity3D_Interfaced_With_Joints_Visualised</Title>
        </Document>
        <Document ID="272">
            <Title>Notes</Title>
        </Document>
        <Document ID="161">
            <Title>5.2 Gestural Interface Apparatus </Title>
        </Document>
        <Document ID="417">
            <Title>Rapid Prototyping: Using Unity 3D</Title>
            <Text>an integrated 3D authoring tool, middleware, a game engine

At the outset of the project, I had a desire to completely roll my own software

3D model import
Texturing and lighting control

Integrated Physics
In world sound and audio
interaction handling (single point of contact (mouse control) or multitouch
GUI

Just an evaluation of physics libraries, (and there are some delightful, seductive, samples for the digital puppeteer)  - most if not all, required the programmer to author a whole rendering pipeline, in order to use custom graphics, textures, or models.

OGRE
As the focus of the these is how digital puppets are expressive and enable expressive play, I needed a more agile, rapid approach to solving software related tasks 
Prototype</Text>
        </Document>
        <Document ID="306">
            <Title>Untitled</Title>
            <Text>Multi-modal interaction
Mulsemedia and virtual reality
Tactile/haptic interaction
Mulsemedia and Play
Mulsemedia and Expression
Mulsemedia and Performance
</Text>
        </Document>
        <Document ID="383">
            <Title>Unity3D_RigidBody_interface</Title>
        </Document>
        <Document ID="273">
            <Title>Note (Files), 13 Jul 2009, 21:44</Title>
            <Synopsis>Note on:  Files</Synopsis>
            <Text>Test	</Text>
        </Document>
        <Document ID="162">
            <Title>5.3 2D and 3D Output</Title>
        </Document>
        <Document ID="418">
            <Title>Expressive Gesture and Interaction</Title>
        </Document>
        <Document ID="307">
            <Title>Keywords</Title>
            <Text>Multi-modal interaction
Mulsemedia and virtual reality
Tactile/haptic interaction
Mulsemedia and Play
Mulsemedia and Expression
Mulsemedia and Performance
</Text>
        </Document>
        <Document ID="384">
            <Title>Untitled</Title>
        </Document>
        <Document ID="274">
            <Title>Aims</Title>
            <Text>The current study explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

In this context 'innovative' means both emerging, new, technology or established technologies that are being re-defined by their communities of use and are finding new applications within the performing arts, particularly puppetry performance.

I aim to explore the related contexts of digital puppetry, real-time animation, mimetic and non-mimetic kinetic objects, automata, 'cybernetic sculpture', performance systems and the technological interfaces to such phenomena. 

I aim to create evaluate and create puppet/object theatre performances/installations that use original software and hardware systems that are designed to explore 'performance expressivity', with reference to relevant historical, art, entertainment and technological precedents.

I wish to theorise and form a taxonomy of 'expressivity' in relationship to digital domains and puppetry. By 'expressivity', I refer to different domains of action including: voice, face, body, hands and gesture.

Scope and Focus: finding patterns in an interdisciplinary study

The scope may first appear broad and the research focus wide. However, the researcher agrees with the statement:

"a cross-disciplinary approach yields information about patterning that is not visible from any single discipline." (Dorosh, 2008, 14)

Initially the surveyed areas and practice are wide. The research progresses with the solid intent to articulate the patterns between (and the inter-relatedness of) cultural forms, and control systems, across computer media and performance art disciplines. In forming a taxonomy of 'expressivity' in relationship to digital control-systems and puppetry, a broad range of phenomena will need to be studied.

Focus is  be acheived through the practical work, detailed in the Proposed Plan of Research, below.
</Text>
        </Document>
        <Document ID="163">
            <Title>5.4 Screen</Title>
        </Document>
        <Document ID="419">
            <Title>Responsivity</Title>
        </Document>
        <Document ID="60">
            <Title>Introduction</Title>
            <Text>3.1 Introduction
</Text>
        </Document>
        <Document ID="308">
            <Title>MULSEMEDIA_chapter_proposal_ian_grant</Title>
        </Document>
        <Document ID="385">
            <Title>Multitouch</Title>
            <Text>Multitouch
Natural User Interface group [TODO make reference]
Human computer interaction is on an interesting trajectory from command line input (CLI) (keyboards and screens), through graphic user interfaces (GUI) windows, mice, keyboards and the innovations from Xerox Parc, Microsoft, Apple and others, to 'natural user interfaces' (NUI). NUI are best characterised by their lack of mice and keyboards, and can describe classes of input ranging from mono, dual to multitouch surfaces, whole body interaction (through camera based interfaces), to eye-gaze control, and various classes of gestural interaction.

Elsewhere in the thesis, I consider other physical forms of interaction with different classes of Human Interface Devices (HIDs) as interfaces for digital puppetry, but the current chapter focuses on multi-touch.

Shadow puppetry and shadow puppet objects often have numerous points of 'control' in the form of rods connected in a variety of manners, allowing different angles and different degrees of freedom (DOF):

(i) fixed joints constrained to 180 degrees of movement on a plane fastened to either static flat forms or articulated parts of a figure. See [][#fig:Shadow_puppet_hinge_001_currell] and  [][#fig:shadow_puppet_horse_001_currell] for a better visualisation of this design.

![fig:Shadow_puppet_hinge_001_currell][]
[fig:Shadow_puppet_hinge_001_currell]: Shadow_puppet_hinge_001_currell "Shadow puppet hinge 001 currell" width=418px
(ii) 'Universal joints' made from knotted cord allowing flexible directions of approach.

The rods are used to brace a puppet on a flat translucent screen, sometimes with a low 'foot-board' allowing the puppets to rest to a level. Animation happens by moving the rods, apply torque / rotation, lifting the rods to allow pendulous swing of connected bodies, or in some (exciting) cases the phenomenon of double-pendulous swing. A double pendulum hangs weighted objects around (at least) two pivot points and energies are stored, momentum spread through the connected bodies (in a more-or-less) chaotic way. Marionettes (string puppets) have similar properties.

(iii) Contra-movement of two rods can control the rotation around pivot points of fixed rigid bodies. [][#fig:shadow_puppet_horse_001_currell] is the simplest example of this style of articulation.
Across world shadow traditions, there are different approaches to the quantity of articulation a shadow puppet design has. Wayang Kulit, the Indonesian shadow tradition has characters with articulated arms only (a shoulder, elbow and wrist joint). Greek Karagiozis often has a pivot at the torso, and one articulated arm but an arm made of a chain of up to ten pivoted pieces.
Expressivity relates to the articulateness of a figure. But there is no easy correlation between the quantity of moving parts to the expressive potential of a figure. Gestural complexity increases with more moving parts, but control systems become more complex and the requisite 'techne' 

![fig:shadow_puppet_horse_001_currell][]
[fig:shadow_puppet_horse_001_currell]: shadow_puppet_horse_001_currell "Shadow puppet horse 001 Source: Currell" width=418px

The basic proposition of the ShadowEngine v001 is to replace the control rods of a physical 2D shadow puppet with multiple touches of a performer (or multiple performers) transmitted by a touch interface.

Gestures, Flick, Pinch, 

2D in 3D space...
</Text>
        </Document>
        <Document ID="61">
            <Title>Interdisciplinarity</Title>
            <Text>Interdisciplinary Theory

This is a sample footnote.[^somesamplefootnote]

[^somesamplefootnote]: Here is the text of the footnote itself.

How about a glossary entry: peanut[^peanut]
I like peanuts - mainly dry roasted.


[^peanut]: glossary: Peanut 
	A peanut is the nutty fruit of a peanut tree, not to be confused with walnut.

* Critique relevant theories and systems of objects from John Dewey the expressive object in `art as experience' to Baudrillards complex view of the object. Pop An Art of Consumption, the system of objects.

This is a paragraph after a list

:	"Everything is in motion, everything is changing, everything is being transformed and yet nothing changes. Such a society, thrown into technological progress, accomplishes all possible revolutions but these are revolutions upon itself." [p10-29, Baudrillard and Poster (1988)][#Baudrillard:1988la]
</Text>
        </Document>
        <Document ID="275">
            <Title>Details of Research in Lay Terms</Title>
            <Text>Many innovations in contemporary computing and the way we interact with machines have applications beyond the domains for which they are designed.

Computer vision for gesture recognition, touch surfaces (like the Apple’s iPhone, Microsoft’s Surface and TUIO systems), wireless control devices, accelerometers and game control devices, like data gloves, can be used beyond their originally designed purpose. Increasingly these technologies and the means to programme them are finding their way into the hands of non-specialists and are crossing boundaries of practice. The present study works in an interdisciplinary way between human-computer interaction/interface design, computer programming and the performance practice of puppetry, evaluating the reciprocal opportunity for innovative practice.

In my practice and research, I aim to create low-cost hardware and software that allows a skilled and unskilled puppeteers to control dynamic physical objects (like a robot) or a virtual object (like a game character) in ways that calibrate and test new interfaces for their expressive potential. Coming from a background in puppetry, where a special approach to gesture, movement and mechanism applies, I wish to find a playful fusion between emerging technology and performance traditions.

The established puppetry traditions, as categorised by control mechanisms and form: rods, shadows and glove, (one may add the direct manipulation of objects and the related area of toys and automata), have fascinating parallels with moving objects in the virtual worlds of interactive art and games. The thesis will clearly establish and test the boundaries of these parallel forms.

</Text>
        </Document>
        <Document ID="164">
            <Title>Evaluation of practice-based approach</Title>
            <Text>10.3 Evaluation of practice-based approach
</Text>
        </Document>
        <Document ID="309">
            <Title>New Material to Check</Title>
            <Text>New Books to Read or Buy and Put in Bibliography

Parables for the Virtual
Movement Affect Sensation
Brian Massumi

Simulation and Its Discontents 
(Simplicity: Design, Technology, Business, Life)
by S Turkle (Author) 



http://www.waset.org/journals/waset/v45/v45-40.pdf


Check Out (from :
[2] C. Darwin. The expression of emotions in man and animals: 3:rd ed. by Paul Ekman. Oxford University Press, 1872/1998. [3] Eden Davies. Beyond Dance, Laban’s Legacy of Movement Analysis. Brechin Books, 2001.
[4] P. Ekman. Emotion in the Face. New York, Cambridge University Press, 1982.
[10] R. Lazarus. Emotion and Adaptation. Oxford University Press, 1991.



Practice-as-Research: In Performance and Screen [Hardcover]
Ludivine Fuschini (Editor), Simon Jones (Editor), Baz Kershaw (Editor), Angela Piccini (Editor)
No customer reviews yet. Be the first.



Brian Massumi
http://www.brianmassumi.com/textes/Introduction.pdf

Art and Physics</Text>
        </Document>
        <Document ID="386">
            <Title>Shadow_puppet_hinge_001_currell</Title>
        </Document>
        <Document ID="62">
            <Title>Introduction</Title>
            <Text>6.1 Introduction
</Text>
        </Document>
        <Document ID="63">
            <Title>Introduction: Surfaces and Shadows</Title>
            <Text>The current chapter, Surfaces and Shadows, explores the interface between traditional shadow puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis as a whole will evaluate and test with *users* (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive digital puppetry, also know as performance animation systems. This chapter presents the design and testing of one such prototype application: the 'ShadowEngine' running on an multitouch portable device, the Apple iPad.

As a practice led project, the chapter presents an exegesis of the software design process for a prototype application called the 'ShadowEngine', that makes multi-touchable digital shadow theatre possible using physics based real-time animation and I indicate some preliminary design level insights and present potential approaches to project evaluation and testing. The animatable characters and objects are curiously expressive, and the analysis begins to refine the hermeneutic issues involved when operating and viewing complex multi-jointed characters in a physics-based 3D (pseudo 2D) environment. 

So the following chapter will start to:
\begin{itemize}
	\item Define The Expressive;
\begin{itemize}
	\item As a Quality Of Performance;
	\item As an Outcome Of Interpretation / Reception;
	\item As a Quality Of Design;
\end{itemize}
\end{itemize}
Then I seek to:
\begin{itemize}
	\item Define The Shadow Puppetry Context;
	\item Define Media Art Contexts Where Shadow Digital Puppetry Is Key
\end{itemize}
Then I present:
\begin{itemize}
	\item Multitouch As An Expressive Responsive/Interactive Approach
	\item Document The Design Processes of the Shadow Engine V001
	\item Evaluate The Expressive Properties Of The Software With Puppeteers And Audiences
	\item Some Theoretical Insights
\end{itemize}

In exploration of practices akin to puppetry, the research draws on instances of performance, games and installation art practice in wider cultural practice and the practical explorations of the author. This focus is multi-disciplinary. 

The current chapter seeks to historicise in and around the spaces occupied by digital shadow puppets and establish key definitions and articulate emerging theoretical ideas.

A methodological approach, resonant with a *media archeology*, traces how traditional puppetry forms, often described by their mode of interaction/media, (rods, shadows, strings and gloves) -- map into current paradigms of interaction with virtual worlds, digital objects and automated animation.

Sherry Turkle has developed a mode of enquiry to understand how we think and express through *evocative objects*. I argue digital puppets are a special class of evocative object. In addition to Turkle, I will consider multiple approaches to the theorisation of the object in cultural practice: again this is drawn from a variety of domains (e.g. John Dewey's *expressive object* in *Art as Experience* [][#Dewey:2005jy] to Baudrillard's complex *system of objects* [][#Baudrillard:1988la].

All styles of puppet manipulation rely on gestural interaction. The current chapter primarily considers the performative/expressive potential of multi-touch systems and the tactile control of on screen characters.

In the emerging domain of digital shadow puppetry, whole-body, face, hand interactions are also prominent. Due to the plurality of forms involved in shadow theatre (human shadowgraphs, cut or etched objects made of hide, paper, metal, card etc., different world traditions), I am consciously conceptually gregarious and will analyse examples drawn across these forms.

I ask a broader question: How do new and emerging technologies facilitate innovative techniques of design and control over puppet-like objects and create experiences of expressive play?

a receptions study of a performance system that uses a physics based animation system</Text>
        </Document>
        <Document ID="276">
            <Title>Proposed Plan of Research</Title>
            <Text>PROPOSED PLAN OF WORK, INCLUDING ITS RELATIONSHIP TO PREVIOUS WORK, MAXIMUM 4,000 WORDS. Please include in your discussion a description of the research methodologies and explain why these methodologies are the most appropriate for the task. Include a list of references for all works cited

Focus and Scope  
The proposed study is interdisciplinary and seeks to established patterns between diverse cultural and technological forms.

The current work has a practical, experimental and media archeological approach that plans to explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

Puppetry has distinct overtones in various world traditions, but in computer space can be defined as the expressive use of interfaces to control objects. It is necessary, in the thesis, to offer an extended definition of 'object', which becomes interesting in computational contexts as does 'expressive behavior' and 'affect'.

In exploration of practices akin to puppetry, the research draws on instances of performance, games and installation art practice in wider cultural practice and the practical explorations of the author. This focus is multi-disciplinary. 

Methods and Methodologies – A Media Archeology of Kinetic Behavioural Sculpture
The writing seeks to historicise in and around the spaces occupied by digital (or virtual) puppets, the avatar, automata, the robot, artificial life, and the animatronic. All these phenomena can be grouped in the realm of 'kinetic behavioural sculpture':

"A behavioral kinetic object is a dynamic system, meaning it changes with time. This system is composed of a source of energy, inputs, outputs, and a control architecture which converts the information from the inputs into information which stimulates the outputs. These elements sum to form the complete object, but other elements may be added to provide mass or form." (Reas 1996, 22)

An emerging methodological approach called media archeology, will be used to trace how traditional puppetry forms, often described by their mode of interaction/media, (rods, shadows, strings and gloves) - map into current paradigms of interaction with virtual worlds, digital objects, games, and automated animation; and associated areas of digitally enhanced dolls, toys and automata. Media Archeology is a field that reflects on today's technologies by linking them to the socio-technical histories out of which they emerged.

"There is a gang of artists, theoreticians, and artist-theoreticians who have a very strong affinity (moreover, one that links them to a figure such as Artaud): they burn and burn up in the endeavour to push out as far as possible the limits of what language and machines, as the primary instances of structure and order for the last few centuries, are able to express and in doing so to actually reveal these limits" (Siegfried Zielinski, 2009, my emphasis)

Ethnographic Methods - Thick Description of Performance and User Testing
My experience in ethnographic methodology also assists to bring the study within a particular social, cultural way of seeing.  How to systematically observe and perform thick description (Geertz, 1977) of contexts such as software design and study is relatively unique, but common in anthropology when interpreting performance culture and forms, such as puppetry.

Media archeology, in the way documents and artefacts are studied has an affinity with historical ethnography. The moment of study is past as well as present. I will apply ethnographic methods of theme analysis – observation, thick description, coding and dimensionalising -  to support a broader semiotic approach to the cultures and histories of digital puppetry.

"The concept of culture I espouse…is essentially a semiotic one. Believing with Max Weber, that a man [sic] is an animal suspended in webs of significance he himself has spun. I take culture to be those webs, and the analysis of it to be therefore not an experimental science in search of law but an interpretive one in search of meaning." (Geertz, 1977, 5)

Sherry Turkle (2005) has developed an approach to understand how we think and express through evocative objects. I argue digital puppets are a special class of evocative object. In addition to Turkle, I will consider multiple approaches to theorising the object in cultural practice: again this is drawn from a variety of domains (e.g. John Dewey's expressive object in Art as Experience (Dewey, 2005) to Baudrillard's complex system of objects (Baudrillard, 1988). I have started this work in relation to automata and talking toys (see the samples).

Kinetic Sculpture and Gestural Interaction
All styles of puppet manipulation rely on gestural interaction. The study considers the performative/expressive potential of numerous computer interaction systems that utilise tactility and touch, whole-body, face, hand interactions. In a cultural study of expressive automata and toys, the study considers the role of the sonic in puppetry: the rhythmic gesture, sound and voice.

I wish to explore: How do new and emerging technologies facilitate innovative techniques of design and control over puppet-like objects and create experiences of expressive play?

The human body in movement and the computational capture of such movement to make meaning through expressive acts mediate by evocative objects – is another way to state the primary exploration of the thesis.

"The world of objects and needs would thus be a world of general hysteria. Just as the organs and functions of a body in hysterical conversion become a gigantic paradigm which the symptom replaces and refers to, in consumption objects become a vast paradigm designating another language through which something else speaks," (Baudrillard:1988, 10-29)

Plan of Literature Review, Research, Practical Work and Writing
Below (FIG: GANTT), I supply a static image of a dynamic Gantt chart detailing an approximate time-plan of how the practical work relates to the evaluation of literary research material and writing activity.

FIG GANNT: dissertation plan

The GANNT chart is a snapshot produced by an interactive project management tool, OmniPlan, and is dynamic. It changes as research, practice and writing activities develop. 
&lt;http://www.daisyrust.com/phd/dissertation_timeplan_oct_2010_ian_grant.png>


Establishing Patterns through Practice
I will produce and evaluate: 

(i) Performance, Artwork and Kinetic Behavioural Sculpture; 
(ii) Original software, software art and computer based creative production techniques; 
(iii) Innovative control systems and interfaces that will be applicable hopefully beyond the domain of performance; 
(iv) Papers, videos, websites and published outcomes that will contribute to the thesis, documentation and other elements of the research.

As a minimum, I plan to make and test: 
(i) Several expressive, physical objects; 
(ii) Several virtual objects, that are controlled by
(iii) Several innovative control systems, in prototype, including touch surfaces, optical motion/expression/gesture capture, digital input devices.

It will be ideal if the systems are tested in performance or in installation contexts. Standard HCI user testing methods, including focus groups, cognitive walkthroughs, Goals, Operators, Methods, and Selection Rules (GOMS) analysis, will be employed, evaluated and discussed where relevant.

Relationship to Previous Work: Comparative Case Studies of Practice
The thesis will also analyse case study material, including software and performances by the author, other artists, puppeteers, animators and sculptors. Case studies include:

* Theo Jansen's Walker and his Staadcreatures: Covering ideas of Virtual Creatures, Object Orientedness, Automata and Computer Simulated Physics, cybernetic sculpture; See (Jansen 2007)

Figure 1: Theo Jansen Walker As Physical, Virtual and Textual (Code) Object

￼￼￼


* Surfaces and Shadows: Synchronous and Asynchronous Techniques: 
	- Lotte Reiniger, Stop motion animator
* Phil Worthington "Shadow Monsters". Animation and Computer Vision: AR installations;
 
Figure 2: Phil Worthington "Shadow Monsters" 2005

￼

* Golan Levin: New media artist. (Reface [Portrait Sequencer] 2007), Snout (2008) Double-Taker (Snout) (2008) Opto-Isolator (2007) and other projects: face and expression recognition; automata and robotics; video hybrids;

     
Figure 3: Golan Levin - physical and virtual projects (2006-2009)

￼￼￼

* Embodied Interactions: finger, hand, whole-body, gestural, eye and facial interactions;

Examples of how these areas map with my own work can be see in Ian Grant "Video Hybrids, explorations in digital puppetry", presented as a performance piece, software and documentary write up (see sample chapter).

</Text>
        </Document>
        <Document ID="165">
            <Title>Evaluation of questions, objectives and aims</Title>
            <Text>10.4 Evaluation of questions, objectives and aims
</Text>
        </Document>
        <Document ID="387">
            <Title>Shadow_puppet_hinge_002_currell</Title>
        </Document>
        <Document ID="64">
            <Title>Bibliography</Title>
            <Text>This is the bibliography</Text>
        </Document>
        <Document ID="277">
            <Title>dissertation plan</Title>
        </Document>
        <Document ID="166">
            <Title>Summary of findings</Title>
            <Text>10.5 Summary of findings 
</Text>
        </Document>
        <Document ID="388">
            <Title>shadow_puppet_horse_001_currell</Title>
        </Document>
        <Document ID="66">
            <Title>Chapter 2: Theories into Practice</Title>
        </Document>
        <Document ID="278">
            <Title>￼</Title>
            <Text>￼</Text>
        </Document>
        <Document ID="167">
            <Title>Summary of problems</Title>
            <Text>10.6 Summary of problems
</Text>
        </Document>
        <Document ID="67">
            <Title>Untitled</Title>
        </Document>
        <Document ID="389">
            <Title>Shadow_puppet_hinge_001_currell</Title>
        </Document>
        <Document ID="68">
            <Title>Chapter 3: Digital Puppetry</Title>
        </Document>
        <Document ID="279">
            <Title>Original Elements of the Research</Title>
            <Text>- A deepening of the material and thought and cultural philosophy surrounding puppetry studies and technology;

- Original Performance, Artwork and Artefacts;

- Original Software products and creative production techniques;

-  New hardware control systems and interfaces for expressive play that will be applicable hopefully beyond the domain of performance and digital puppetry;

- Papers and published outcomes that will contribute to the thesis, documentation and other elements of the PhD.
</Text>
        </Document>
        <Document ID="168">
            <Title>Future trends</Title>
            <Text>10.7 Future trends</Text>
        </Document>
        <Document ID="69">
            <Title>Chapter 5: Modes and Forms</Title>
        </Document>
        <Document ID="169">
            <Title>Conclusion</Title>
        </Document>
        <Document ID="420">
            <Title>Automation and Control in Performance Animation</Title>
        </Document>
        <Document ID="310">
            <Title>Turkle</Title>
            <Text>Who Am We?  We are moving from modernist calculation toward postmodernist simulation, where the self is a multiple, distributed system.
By Sherry Turkle
Children are comfortable with the idea that inanimate objects can both think and have a personality. But they no longer worry if the machine is alive. They know it is not. The issue of aliveness has moved into the background as though it is settled. But the notion of the machine has expanded to include its having a psychology. In talking about computers in a psychological way, children allow computational machines to retain an animistic trace, a mark of having passed through a stage in which the issue of the computer's aliveness was a focus of intense consideration. 
...
Children also grant new capacities and privileges to the machine world on the basis of its animation if not its life. They endow artificial objects with properties, such as having intentions and ideas, previously reserved for living beings.
Granting a psychology to computers can mean that objects in the category "machine," like objects in the categories "people" and "pets," are fitting partners for dialog and relationship. Although children increasingly regard computers as mere machines, they are also increasingly likely to attribute qualities to them that undermine the machine/person distinction. 
...
The creatures in simulation space challenge children to find a new language for talking about them and their status, as do mobile robots that wander about, making their "own decisions" about where to go. When MIT professor Rodney Brooks asked his 10-year-old daughter whether his mobots, or mobile robots, were alive, she said, "No, they just have control." For this child, and despite her father's work, life is biological. You can have consciousness and intentionality without being alive. At the end of the 1992 Artificial Life Conference, I sat next to 11-year-old Holly as we watched a group of robots with distinctly different "personalities" compete in a special robot Olympics. I told her I was studying robots and life, and Holly became thoughtful. Then she said unexpectedly, "It's like Pinocchio. First, Pinocchio was just a puppet. He was not alive at all. Then he was an alive puppet. Then he was an alive boy. A real boy. But he was alive even before he was a real boy. So I think the robots are like that. They are alive like Pinocchio [the puppet], but not like real boys."

...

Play has always been an important aspect of our individual efforts to build identity. The psychoanalyst Erik Erikson called play a "toy situation" that allows us to "reveal and commit" ourselves "in its unreality." 


Source: http://www.wired.com/wired/archive/4.01/turkle.html?pg=2&amp;topic= 
Print Source: http://www.wired.com/wired/archive/4.01/turkle_pr.html 
Viewed: 29/01/2010
Issue 4.01 | Jan 1996




http://web.mit.edu/sturkle/www/pdfsforstwebpage/ST_Relational%20Artifacts.pdf</Text>
        </Document>
        <Document ID="421">
            <Title>Myron Krueger and VideoPlace</Title>
        </Document>
        <Document ID="311">
            <Title>Chapter Notes</Title>
            <Text>Digital Silhouettes:   Shadow Puppetry

An Evaluation of Interactive Techniques for Shadows
Play with Light and Shadows (Schonewolf)

Check: http://www.cabinetmagazine.org/issues/24/Warner.php</Text>
        </Document>
        <Document ID="200">
            <Title>Ten Emerging Ideas Involving Talking Toys And Technology </Title>
            <Text>* Embedding talking technology in toys is more about control than play or exploratory learning. 
* Talking toys are first technological experiments, second, playthings. 
* Talking toys are monologic rather than dialogic. 
* Talking character-based toys, that are dependent on other media, are derivative and closed narrative systems. 
* Talking Toys represent an adult intervention into child-play. 
* What talking toys say is more important than how they say it. 
* Talking toys are of greater value when they are programmable and configurable by children. 
* Animated facial mechanisms attempt to re-embody disembodied voice and, in turn, over-concretise and limit imaginative play. 
* Talking toys are extraordinary simulators of intelligence and presence.
* Talking toys, traditional and digital puppets and animated media forms are more inter-connected than we may first think. </Text>
        </Document>
        <Document ID="422">
            <Title>Philip Worthington's Shadow Monsters</Title>
        </Document>
        <Document ID="312">
            <Title>BibDesk Markup for Latex</Title>
            <Text>In order to assist scrivener and latex, it is useful to markup bibdesk references in the following way:


Markup Capital Letters
{Q}uartz {C}omposer {R}eference {C}ollection 

Markup URLS
\url{http://developer.apple.com/documentation/GraphicsImaging/Reference/QuartzComposerRef/index.html}

Escape Special Characters
Technical Q\&amp;A 
</Text>
        </Document>
        <Document ID="201">
            <Title>5.4.1 Embedding talking technology in toys and Control</Title>
            <Text>Puppetry, the emerging forms of digital puppetry and puppet like talking toys are all about *control* in two important senses. There's the good old fashioned sense that such toys are 'cybernetic' systems where there is a feedback loop with the *movement -- action -- operator/audience* communication chain and in the sense that such toys embed pedagogical rules and structures. 

: "The thing about playing is always the precariousness of the interplay of personal psychic reality and control of actual objects" [Winnicot 1971 cited in Cassell and Ryokai, 1, 2001 ][#Cassell:2007uq]

Certain talking toys, the interactive series of *Actimates* from Microsoft, have been criticised for the empty way they encode, like passive vessels, content from other media channels. 'Barney the Dinosaur', for example is controlled by signals from PCs, TV broadcasts and video tape. The dolls mouth syncs and sings with the representation of Barney on screen. 

: "Most commercial applications in the domain of tangible personal technologies for children are variants on dolls, with increasingly sophisticated repertoires of behaviours. Microsoft Actimates' "Barney" and Mattel's "Talk with Me Barbie" have embedded quite sophisticated technology into familiar stuffed animals and dolls. These toys, however, deliver adult-scripted content with thin layers of personalization, and do not engage children in their own fantasy play. In both cases the toy is the speaker and the child is firmly in the position of listener." [Cassell and Ryokai, 1, 2001 ][#Cassell:2007uq]

Microsoft's learning toy theorist and actimate guru, Erik Strommen positions Barney as a mate, a learning pal, a friend. 

But another aspect of *control*, is the propensity of talking toys to lie: 

An extended extract from an interactive toy conference review, "Interactive Barney: Good or evil?" : 

: "When I hear Barney say, *You're my special friend* – that's a disingenuous statement," said Allen Cypher, a founder of Stagecast Software, which designs children's programs. "It's a fraudulent claim. It deceives kids into believing that Barney has some emotional attachment to them, and that's not true." Other panelists worried about Barney's "authoritarian tone," or that he discouraged imaginative play. And some said that, while Barney himself was basically harmless, he may be a harbinger of worse to come: an interactive Cartman from "South Park," perhaps, spewing expletives and insulting his owner"[^chcc_stitch]. And one member of the audience asked if a child could take Barney apart and "reprogram him to say, *Please slap me.*", "These products are designed to prevent that," Strommen said. [Newman][#Newman:2007kx]

[^chcc_stitch]: See Hasbro's *Aloha Stitch* doll for an example of such a moody toy.

When discussing *control* it is important to note that it is not meant in a purely sinister, ideologically manipulative, way. Play, and particularly play where children animate and give voice to objects that surround them, is about children asserting control and (dis)order over facets of their environment: 

: "One essential aspect of childrens' spontaneous storytelling play is that it is child-driven. And this is important since children feel a sense of achievement and empowerment when they know that they can create and control the content of their play objects. So, if technology is to encourage childrens' creativity and, in particular, play a role in childrens' storytelling play, it must not dampen that child-driven aspect of their play." [Cassell and Ryokai, pg..., 2001 ][#Cassell:2007uq]
</Text>
        </Document>
        <Document ID="423">
            <Title>Golan Levin's Messo Di Voce and Interstitial Space</Title>
        </Document>
        <Document ID="390">
            <Title>Archaic Craft and Emergent Techniques in the Digital Age</Title>
            <Text>Computers are seen as a preservation strategy for ancient craft and artisanal traditions...
theatre of paper, paper cutting etc.

working with leather, dyes
the anthropological contexts for puppetry root traditions in popular traditions

antipathy between contemporary technology and the demise of traditional practices...
US puppeteer and scholar, Paul McPharlin had a phrase 'handicraft in the machine age'

how could artistic craftperson-ship survive in the age of mechanised manufacture

art, skill, knowledge, 
a reification of the puppet object...
the act of modelling - sculpture
the act of rigging and jointing - calibration for performance</Text>
        </Document>
        <Document ID="313">
            <Title>To Interview</Title>
        </Document>
        <Document ID="202">
            <Title>5.4.2 Toys as technological experiments</Title>
            <Text>The history of talking toys and automata is clearly a story of technical innovation and development for the purposes of celebration, entertainment and play. According to Jasia Reichardt talking statues have been known since 2500 BC: 

: "Some incorporated concealed speaking trumpets through which someone hidden could address a gathering. The idea was that gods communicateed through the statues which represented them" [(Reichardt (1978), 9)][#Reichardt:1978qy]

Of interest here, Jacques de Vaucanson created a number of mechanical automata including a 'ﬂute player and defecating duck' (circa 1737-1738). On the *ﬂute player*: 

: "This automaton *breathed*. Even though the art of mechanics was sophisticated enough by then to make the machine perform many other movements, and even though Vaucanson unveiled the fact that this breath was created by bellows, the very act of breathing, seen in an inanimate figure, continued to cause a stir well into the following century." [(Wood (2002), 21-22)][#Wood:2002fk] 

The first talking doll was patented by Johann Nepomuk Maelzel in 1824. According to Gaby Wood "He designed a pair of bellows that, when attached to a tube, a widening oral cavity and a set of valves, could say *papa* or *maman*". [(Wood (2002), 118-119)][#Wood:2002fk]

Thomas Edison's *Talking Doll* (1891) -- conceived as an advertisement for his sound recording device -- embedded a miniaturised phonograph mechanism that played wax cylinder recordings of nursery rhymes, prayers and stories: 

: "[The phonograph] began by speaking the words of a child, and it was not long before a child was invented to give it shape, or to give it life. So the capturing and reproduction of speech were accompanied by a casing for it in human form" [(Wood (2002), 18)][#Wood:2002fk]

The context around Edison's toy development has shaped the industrialised processes surrounding technical innovation and toys ever since. There is little perceived difference between Edison grafting a mechanical phonograph into a toy and iTeddy's implanted mp3/mp4 playback device. Yet the former was an exercise in creating perfect representational forms of human (female) life, and the other a toy to placate media hungry children.

: "Edison's colleague, W.K.L. Dickson, wrote that it was *perhaps the daintiest and most suggestive of all the multiform uses to which the phonograph has been put.* He described *roseate lips* which would *lisp out the oft-conned syllables of nursery rhymes, pipe the familiar of Mother Goose's ballads, and give forth the cooing and wailing sounds of baby life*. Under such auspices into what enchanted realm will our ordinary toys be transformed." [(Dickson cited in Wood, 2002, 114)][#Wood:2002fk]

Duncan Bannatyne, on a broadcast of BBC TV's venture capital reality show [^chcc_dragonsden], said of iTeddy: 

: "I'm so sad. Reading bedtime stories is a father's [sic] job. I don't want to be replaced by a teddy bear." 

[^chcc_dragonsden]: See /url{http://www.bbc.co.uk/dragonsden/}

Talking toys that emerge from University research labs and university start ups are philosophically worlds apart from corporate toys from the likes of Microsoft, Disney franchises and the enormous toy companies like Hasbro and Mattel. The work of Justine Cassell at MIT with "StoryMat™", Dr Alison Druin with the "PETS" projects[^chcc_petsproject] (from the Human Computer Interaction Lab at the University of Maryland) are distinct in pedagogy and interactive strategy from most commercially available toys. The toys have a clear philosophy of use as *learning technologies* rather than simply embedding the latest speech recognition and synthesis chips in order to maximise rich play or to aim for 'realism', or to service a franchise. 

[^chcc_petsproject]: PETS -- "Personal Electronic Teller of Stories" robotic pets that support children in the storytelling process.

It should be noted that sponsorship relationships exist between the toy companies and innovative research groups in universities. An extended quotation from David Shenk's article Behold the Toys of Tomorrow (1999) illustrates the connections between technological innovation, the toy corporations and the University researcher:

: "The computerisation of toys also dovetails nicely with the ambitions of computer evangelists, those whose life's mission it is to deliver the power of computation into every aspect of every person's life. Nicholas Negroponte, the director of MIT's famously innovative Media Laboratory (the Vatican of techno-evangelism), noted last year in his Wired column that toys are the *fastest evolving vehicles on the infobahn*, meaning...they're the only class of objects that can truly keep up with the rapid pace of hardware and software innovation. ... [1999 saw] the formation of an industry-research consortium called *Toys of Tomorrow*. A dozen or so companies, including Mattel, Tomy, Intel, and Bandai...have signed up, committing to at least three years of the \$250,000 annual sponsorship fee. In return for the funding..., [the] sponsors get first crack at the new technology and ideas --... The Media Lab is a Willy Wonka factory for technophiles, where the only limitations are in the creators' imaginations. Intoxicated by the MIT fumes, one thinks: How could this not be a boon to society?" [(Shenk 1999. Date Accessed: 01/02/2007)][#Shenk:2007vn]

It should be noted that talking toys and animated toys are often adult orientated, rather than for children. This may be because such devices express the extraordinary fascination with what contemporary technologies can do. Jacques de Vaucanson's *defecating duck* was not a toy -- but a remarkable exploration of what clockwork and air power could do. Likewise, Edison's talking doll: 

: "One can only conclude that the [Edison's] dolls were not for children, and adults like [Albert Hopkins (Editor of *Scientific American* c1890)] were not alone in picking up on their aggressive horror. Formanek-Brunell quotes a survey taken at the time Edison's dolls were manufactured, in which a four year old girl, fusing the animate with the inanimate in a way that recalls Vaucanson's duck, said she didn't like talking dolls, because *the fixings in the stomach are not good for digestion*." [(Wood (2002), 118)][#Wood:2002fk]</Text>
        </Document>
        <Document ID="424">
            <Title>Experimental Methodology</Title>
            <Text>Abstract of Current Practical Project FROM THE WEB UPDATE 

The current chapter, Surfaces and Shadows, explores the interface between traditional shadow puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis as a whole will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of different modes of interactive digital puppetry. For this purpose, I are creating new performance animation systems and software. This chapter presents the design and testing of one such prototype application: the 'ShadowEngine' running on an multitouch portable device, the Apple iPad.
As a practice led project, the chapter presents an exegesis of the software design process for a prototype application called the 'ShadowEngine', that makes multi-touchable digital shadow theatre possible using physics based real-time animation and I indicate some preliminary design level insights and present potential approaches to project evaluation and testing. The animatable characters and objects are curiously expressive, and the analysis begins to refine the hermeneutic issues involved when operating and viewing complex multi-jointed characters in a physics-based 3D (pseudo 2D) environment.

Prototype and Development Plan

The web prototype, below, is based on the iPad version, and plays back in a web browser if you have the Unity Web Player, available here. The web player is 'monotouch' only, as most systems only have a single mouse pointer.

The primary aim was to prove the basic concept: that the 3D models, with UV and texture maps, could work in an orthogonal openGL context, simulating the interactions of shadow puppets in a multitouch environment. There is a rationale behind each of the figures and they all make it to the screen through different design processes. Each character has different rigs. All objects 'rigidbody' and have differently physical properties (configurable joints, hinges, mass, spring and damping properties) - and clearly visual designs - and all these parameters that effect 'expressivity'.

It is very satisfying that the proof of concept application works. Playing can lead to 'operator emoting', performer flow and rich expressive moments. In terms of performance animation, the combination between direct control and physical simulation (and it has to be said 'accidental physics glitches') is a ripe area for further exploration. For example, in the Lotte Reininger female figure, the different tension properties and rotation limits of each arm (the right, floppy and dead, the other stiff and pert) - are set to radically contrast to illustrate that expressivity is in part an act of tuning and configuration. Actually, in the current prototype, the whole figure is relatively 'uncontrained'. The radical kinetic/visual variation a few settings can make cannot be underestimated.

Development Plan for this Current Phase of Work on Shadows

The prototype will be developed in the following ways:
\begin{itemize}
\item user testing of different character set-ups (with performer and audience);
\item optimise the asset design, texturing, rigging and importing for mobile touch devices and, if possible, a table-top (TUIO) device;
\item tune and evaluate multitouch as a form of expressive control;
\item create a well-formed UI to configure global physical properties, character selection, scene selection and transitions, visual and lighting effects, collaborative control through networked play;
\item integrate computer vision techniques (in the desktop version) to produce real-time silhouette profiles of the operator and/or audience;
\item the direction of the thesis will be evaluated, and I will consider how far the present technology and platforms can fulfil the requirements for the rest of the practical exploration;
\end{itemize}</Text>
        </Document>
        <Document ID="10">
            <Title>Practice</Title>
        </Document>
        <Document ID="391">
            <Title>An Emerging Project in Digital humanities</Title>
            <Text>analysis and synthesis of modes of practice and understanding

publication
computational analysis
project design

archival and retrieval
animation studies
John Unsworth:  discovering, annotating, comparing, referring, sampling, illustrating and representing. 
new approaches to studying the 'enduring problems' relating to existing cultural artefacts  </Text>
        </Document>
        <Document ID="280">
            <Title>Research Questions</Title>
            <Text>Research Questions

Through practical, theoretical and historical research I wish to explore and answer the following research questions:

Primary Question
What is 'expressivity' in the context of computer controlled and user controlled physical and virtual objects?

Three main overarching questions
How do new and emerging technologies facilitate innovative techniques of design and control of puppet-like objects?

What are the most effective designs for interactive tools to create and sustain experiences of expressive play?

How do the domains of traditional puppetry and emerging interactive technologies relate?

Sub-Questions: Definitions and Detail

The following questions further refine and dimensionalise the key research questions.

What defines a digital puppet?
What defines a virtual puppet?
Can we describe the distinctions between virtual and physical kinetic expressive systems?
	- What is the difference between an automata and a puppet in the context of virtual creatures and a-life?

How do current performance technologies facilitate expression?

What is 'expressivity' in the context of computer controlled objects?

What happens to 'expressive acts' when mediated through interactive technology?
	- Gestural
	- Vocal
	- Whole Body (Viscerality and the wider sensorium of the body etc.)
	- Facial
	- kinetics, movement and touch

What are the qualitative distinctions between live and captured movement?

How can we use movement capture as an analytic tool within puppetry studies?

How can we use movement capture as a tool for play within puppet performance?

What are the most effective methodologies for studying interdisciplinary puppetry practice?

To what extent is it useful to compare diverse virtual and physical kinetic forms as forms of digital puppetry?

New Interfaces for Expression
What are the most effective interactive interfaces to capture the expressive acts of puppeteers?

How do we best describe the 'expressive potential' of custom interfaces for puppetry?

What are the perceptions of traditional puppeteers towards new technology and new interfaces for expression?

</Text>
        </Document>
        <Document ID="314">
            <Title>To Interview</Title>
            <Text>Craig Crane
</Text>
        </Document>
        <Document ID="203">
            <Title>5.4.3 Monologic and dialogic interaction</Title>
            <Text>Talking toys are rarely conversational agents, and interaction is heavily pre-determined. Randomising responses is one strategy that provides an illusion of knowing, active or predictive conversation. Such illusions are broken when the pattern or repetition is noticed.
 
Arguably, talking toys fix patterns and structure play and are, in nature, didactic and instructional. However, structured or programmatic experiences are crucial to learning, play and language development. I am not simply dismissing such toys dogmatically from some valorised over-emphasis on the value of *free play*. Lev Vygotsky states: 

: "Let us turn now to the role of play and its inﬂuence on a child's development. I think it is enormous. I think that play with an imaginary situation is something essentially new, impossible for a child under three; it is a novel form of behavior in which the child is liberated from situational constraints through his activity in an imaginary situation." [Vygotsky (First Published: 1933. Date Created: 2002 . Date Accessed: 01/02/2007)][#Vygotsky:2007ly]

I would assert that more *situational constraints* are imposed by over-structured, adult led, media inﬂuenced play, than child centred play. It may be a useful moment to introduce the term *metaxis* used in education drama contexts for describing the *dualness* of perception during role-play and *as-if* contexts. Metaxis has been defined as "the state of belonging completely and simultaneously to two different autonomous worlds" [(Boal (1995), 43)][#Boal:1995gf]. This definition has interesting resonance when considering the virtual realities created by digital talking toys.</Text>
        </Document>
        <Document ID="425">
            <Title>Synchronous and Asynchronous Techniques</Title>
        </Document>
        <Document ID="11">
            <Title>SmartLab Research Groups</Title>
            <Text>We are currently focusing our recruitment of students on our core research groups: 

Performance Technologies, 
Mobile Platform Games, 
Accessible Technologies &amp; Personal/Community Fabrication, 
Immersive Play, 
Multimodal Interfaces, 
Haptics &amp; Robotics, 
Assistive Technology, 
Wearable Computing and SMARTfashion, 
SMARTart &amp; gaming, 
Gender &amp; Interactivity, 
Interactive Screen, 
Digital Narratives, 
Community Building Online, 
IT for the Developing World.</Text>
        </Document>
        <Document ID="12">
            <Title>SmartLab PhDs</Title>
            <Text>￼
About
Projects
Events
People
PhD: docSMARTs
Media
Ethos for docSMARTs
Current PhD Students
Completed PhD Students


The SMARTlab Digital Media Institute PhD Programme
SMARTlab PhD Seminar
Running from 18-22 Febuary 2008
SMARTlab runs three annual seminar sessions for our students from around the world at UEL, London each year in February, July and October. The next seminar will take place from 18-22 February, with an intensive introduction for new and transfer students on the 17th.
As well as giving students ample time for both one-to-one peer support and ideas exchange within the international group, the SMARTlab seminar week also includes a series of talks and seminars designed to meet the needs of each student.
The seminars cater to the wide range of subject areas, fields of research and specialties of our students. The current session aims to get back to basics and explore the nature of a practice-based PhD and the aim for an original contribution to knowledge in cross-disciplinary fields. The week long programme also includes practical and academic advice on theory, writing and funding, as well as discussion, debate, and special events. Steve Di Paola will be speaking on modelling intelligent expression and cognitive knowledge systems, and students will be presenting their own research.
The seminar also gives students the chance to gain supervision with their supervisors across the different schools of UEL.
DocSMARTs: Practice-based PhDs (Live and Online) in Media Art http://www.SMARTlabphd.com 
￼
Artists, and indeed technologists working in artistic domains, have long encountered difficulties in placing their work in relation to the academy: in finding appropriate ways to 'measure' artistic practice in 'research exercises', in identifying appropriately flexible and experimental forms for artistic research processes and outcomes, and also in competing for academic funding.
The SMARTlab Digital Media Institute supports a highly selective group of PhD researchers. This groups works together live and online, with contributors from all around the world, to co-create and debate the nature of 'practice-based research'. Candidates are encouraged to work together on joint experiments, to share work in progress for group feedback, and to meet regularly with experts joining debates online and offering feedback to the cohort.
We meet 'live' three times a year (in February, July, and October) for intensive one-week retreat seminars here at our MAGIC Playroom and associated studios. These seminars focus on research methods and transdisciplinary critical practices, group critique, feedback and the relationship between practice and theory.
We accept applications on a rolling basis, at any time. SMARTlab sits outside the academic faculty structure at UEL, and has an unusual route to the application process whereby core SMARTlab Faculty do a first scan of proposals and then match selected projects with suitable supervisors from across the university, with Direction of Studies, and practice-based methods and group critique handled by the SMARTlab core staff.
We are currently focusing our recruitment of students on our core research groups: Performance Technologies, Mobile Platform Games, Accessible Technologies &amp; Personal/Community Fabrication, Immersive Play, Multimodal Interfaces, Haptics &amp; Robotics, Assistive Technology, Wearable Computing and SMARTfashion, SMARTart &amp; gaming, Gender &amp; Interactivity, Interactive Screen, Digital Narratives, Community Building Online, IT for the Developing World.
SMARTlab Faculty include professors &amp; senior researchers/post-doc fellows of Creative Technology Innovation, Informatics, Computing, Technology, Performance, Fashion, Art, Engineering, and cognate disciplines.
application form and info on how to apply:
http://www.uel.ac.uk/gradschool/prospective/apply.htm#howtoapply

how to register (after 6 months of enrolment)
http://www.uel.ac.uk/gradschool/current/mphil.htm
scroll down to registration and download the file and guidelines

info on funding
http://www.uel.ac.uk/gradschool/current/Researchstudentfunding.htm

FAQ
http://www.uel.ac.uk/gradschool/current/faq.htm



￼




The SMARTlab Digital Media Institute, University of East London, Docklands Campus, 4-6 University Way, London E16 2RD
Sitemap | feedback Updated Monday, 04 February 2008 
</Text>
        </Document>
        <Document ID="392">
            <Title>Untitled</Title>
        </Document>
        <Document ID="281">
            <Title>Title</Title>
            <Text>Expressivity and the Digital Puppet:
Mechanical, Digital and Virtual Objects
in Games, Art and Performance</Text>
        </Document>
        <Document ID="315">
            <Title>Motion Capture</Title>
            <Text>http://www.siggraph.org/education/materials/HyperGraph/animation/character_animation/motion_capture/history1.htm

</Text>
        </Document>
        <Document ID="204">
            <Title>5.4.4 Talking character-based toys</Title>
            <Text>Often talking toys become extensions of pre-existing media thatproject ideas outwards, from toy to child, rather than being empty vessels that facilitate projection from child to toy. This is particularly acute in any toy that represents known characters from other media productions, the huge so-called *character toy* market. 

What difference does it make if the imaginary topic of make-believe style play with talking toys is sourced from existing media,rather created from within than the child herself? 

Justine Cassell creates story environments and interactive objectsto research the quality of technology assisted spontaneous play and story creation. In [Cassell and Ryokai (Last Modified: 2001. Date Accessed: 01/03/2007)][#Cassell:2007uq], Cassell carefully documents and quantifies the generative, creative effects of certain (I would call *dialogical*) interactive technologies. Using quantitative and visualisation methods, she carefully annotates the original spontaneous vocal contributions offered by children while playing with interactive toys. She also transcribes and qualitatively analyses the text of stories children create using her *environment*.</Text>
        </Document>
        <Document ID="426">
            <Title>Testing and Evaluation: Expressive Shadows</Title>
        </Document>
        <Document ID="13">
            <Title>Routes for Social Engagement - pedagogy and networks</Title>
            <Text>enabling devices -> interface expressivity



tools
</Text>
        </Document>
        <Document ID="170">
            <Title>Appendices</Title>
            <Text>Appendices...</Text>
        </Document>
        <Document ID="14">
            <Title>PDFs</Title>
        </Document>
        <Document ID="393">
            <Title>Untitled</Title>
        </Document>
        <Document ID="282">
            <Title>Presentatyi</Title>
        </Document>
        <Document ID="316">
            <Title>BibDesk URL Online Refs</Title>
            <Text>The BIBDESK database:

URL / online references need harmonising throughout the whole database.
CAUTION: be careful with the URL field in general references. the style seems to extract the URLs on any reference and add them to the bibliography - desirable?

the AGSM style (although in the harvard family) has some stylistic differences to what is generally listed as harvard

constraints in ONLINE URL ELECTRONIC references.

Bibdesk Example

This entry:

￼

Results in: 

Chilvers, P. (2007), ‘Creature labs - norn babblings’, [online] (Updated: no- date). Available at: http://www.gamewaredevelopment.co.uk/creatures_more.php [Accessed: 01/02/2007]. 8

Note: remove URL field. 
Note: commas rather than periods (full stops)
Note: the UPDATED field may not be needed


Authorship or Source, Year. Title of web document or web page. [Medium] Available at: include web site address/URL(Uniform Resource Locator) and additional details such as access or routing from the home page of the source. [Accessed date]. 

National electronic Library for Health. 2003. Can walking make you slimmer and healthier? (Hitting the headlines article). [Online] (Updated 16 Jan 2005)
Available at: http://www.nhs.uk.hth.walking [Accessed 10 April 2005].

AGSM seems to have some stylistic differences - e.g. commas, bracketed year of pub etc.</Text>
        </Document>
        <Document ID="205">
            <Title>5.4.5 Adult interventions into child-play</Title>
            <Text>First, adults buy toys and design toys, and their associated pedagogy, for children. Children of a certain age exert pressure and express desires for certain toys, stoked by the marketing messages of the larger toy companies. Most talking toys enshrine messages and pedagogy from adults to children and on occasion seek to replace or act as surrogates for social parental contact: A selling point of a recent the UK designed toy, iTeddy, a strange *Tellie-Tubbies* and iPod hybrid, a bear with a media player embedded in it's stomach, was the *comforting* effect the toy had to placate children during the absence of a peer, buddie, parent or supervisor. The surrogate suckling/child rearing function of childrens media and TV are transferred into the toy itself. Like many toys of this ilk, iTeddy, refreshes its onboard content of nursery rhymes and stories using networked connectivity to a custom web-site. 

Erik Strommen is having a fascinating career that takes in companies as diverse as "The Childrens Television Workshop" creators of educational puppet-fest "Sesame Street", several game companies and Microsoft, where he worked on and promoted the Actimate series of interactive toys. In his 1999 paper *Learning from Television With Interactive Toy Characters As Viewing Companions* [Strommen (Online PDF of published work. Date Written: 1999. Date Accessed: 01/03/2007)][#Strommen:2007ul], and in later work[^chcc_strommenpubs], he builds an extended theory of how talking interactive toys can act as *scaffolding* for learning interactions, act as *buddies* and simulated co-learners. In more recent work, Strommen clearly delineates between interactive toys as surrogates for adult interventions, toys as establishing shared contexts for extended social interactions (i.e. such toys need adult supervision and interaction) and pure play without any pedagogical intent: 

: "Whos in charge? If the children are the ones setting things up not just physically but conceptually, if they are showing each other what to do, collaborating, its play. If the children are being told what to do, led, directed, or tested, its not play" (Strommen, 2004, ) 

[^chcc_strommenpubs]: *When the Interface is a Talking Dinosaur: Learning Across Media with ActiMates Barney* (1998). *Learning from Television With Interactive Toy Characters As Viewing Companions* (1999). *Interactive Toy Characters as Interfaces For Children* (2000).

The more interesting counter-examples of toys not obviously promoting language acquisition are speaking toys that babble and create their own non-human languages that parody child language. Such toys do not induct nor reinforce the adult designed language structures of nursery rhymes, alphabet led rote learning and traditional language learning games Such toys and characters, eg. the Norns in the Creatures series *language* [3] and the talking Furbies native language *Furbish*, communicate through prosody and gesture and are not limited by the need to process real language structures. Bizarrely (and wonderfully - in terms of creativity and useless play) such toys have lead to the players acquiring and learning nonsense languages. Peter Chilvers on developing "Nornish": 

: "We decided to look for a way of converting anything the Norns said into sounds, in such a way that (i) the words sounded like speech, and (ii) a word would sound the same every time it was spoken, and (iii) different words should have different pronunciations. Just to make life difficult for myself, I also added (iv) similar words should sound similar. The first step was to record some speech. Luckily, one of the artists working on the game had something of a gift for making bizarre noises, so we gave him a script full of gibberish and recorded him babbling away. This was then chopped up into individual syllables, and electronically treated to give male and female voices. Having been presented with a large collection of syllables, I went through them and decided whether they sounded like the start of a sentence, the middle of a sentence or the end of a sentence. Having established these groups, I let "nature" do the tricky bit for me. I came up with a way of using a random set of numbers to convert any group of three letters into one of these syllables. I then let these random numbers "breed" until I had a vocabulary that fitted all my requirements - all groups of letters had a corresponding sound, simi lar groups sounded similar, and I could recognise the starts and ends of sentences. Norns have a very small vocabulary, so in principle, it should be possible to learn to understand "Nornish" -- I confess I've never had the patience, though I'd love to hear if anyone has." [Peter Chilvers (Date Created: nodate. Date Accessed: 01/02/2007)][#Chilvers:2007wd]

People have learnt "Nornish". A critical act of non-utilitarian play.</Text>
        </Document>
        <Document ID="427">
            <Title>Process: Designing the Multitouch ShadowEngine</Title>
        </Document>
        <Document ID="15">
            <Title>muc2003-30-winkler</Title>
            <Text>G. Szwillus, J. Ziegler (Hrsg.): Mensch &amp; Computer 2003: Interaktion in Bewegung. Stuttgart: B. G. Teubner; 2003, S. 307-316
Creating digital augmented multisensual learning spaces – Transdisciplinary school education between aesthetic creating and building concepts in computer science
Thomas Winkler, Daniela Reimann, Michael Herczeg, Ingrid Höpel
Institute for Multimedia and Interactive Systems (IMIS), University of Lübeck Forum of the Muthesius-University of Arts, Design and Architecture, Kiel Institute of Art History of the Christian-Albrechts-University, Kiel
Abstract
This paper is about the rehabilitation of aesthetics in the context of teaching computer science and digital media in schools. It explains how interdisciplinary, digitally extended learning environments can be created with the help of free or low cost applications. Such learning environments focus especially on the idea that sensorial perception and co-construction of knowledge should be an integrated part of a creative learning process.
1	Introduction
Although more and more learn-media manufacturers appeal to constructive components, one of the most important moments in constructive oriented pedagogy is lost from sight: the importance that complex sensorial perception and co-operation in real physical world have in the learning process.
A creative, collaborative learning, which involves all human senses, even when the process is digital media-supported and computer science teaching-oriented, constitutes the linchpin in the ArtDeCom1 project, in which the authors of this paper are involved.
One of the seven teaching experiments of the last 11⁄2 years in the ArtDeCom project will be used as an example here. The teaching experiments try, in different approaches, to create a digitally enriched learn-environment where teaching goes beyond subject segmentation. In an eighth grade of a comprehensive school, children created a 3D hybrid space which they could all enter to- gether. In this space, 3D real/physical objects can be found as well as 3D virtual objects, embed- ded in and projected with self-constructed 3D virtual spaces in real/physical space.
Although the hybrid spaces are extremely complex, we worked on extreme low cost level; we succeeded doing that mainly by using freeware and otherwise using low cost materials.
1 ArtDeCom (Theory and Practice of integrated Arts, Design and Computer Science in Education) is funded by the German „Bund-Länder Commission for Educational Planning and Research Promotion“ (BLK) within the general funding program „Culture in the Media Age“ (KuBiM). See: http://artdecom.mesh.de
308	T. Winkler, D. Reimann, M. Herczeg, I. Höpel
Working with the interfaces between the real and the digital world was the central theme; this work included observing and reflecting about the interaction between children and computers in the real space and in the digital space as well as in the interfaces between the digital and the real space.
Because the linking of real/physical worlds with digital ones has a growing importance in all domains of human activity, in our opinion, the pedagogical orientation towards comprehending and decision making in aesthetics (self creation) as well as in computer science (self program- ming) is very important for us.
We think that introducing digital media in teaching should not be reduced to engrafting of digital technology in the different courses. By no means should it lead to students solving assignments following obsolete learning patterns. Teaching, especially if supported by digital media, should be perceived as co-construction of inter-subjective reality. In this co-constructive process, the models existent in computer science can be used as a teaching ground for interdisciplinary courses in sensual, aesthetical, practical oriented context.
2	Misunderstanding of constructive pedagogy – especially in the context of using digital media in the learning process
2.1	Constructivism and senses
Already during the pedagogy reform in the 1920’s (Piaget)2 and in the new pedagogy reform after 1968 (especially Beck and Wellershof)3 a new definition for the use of our multitude of senses in the learning process was born. Our understanding of constructive pedagogy theories begins here.
However the poor constructivism reception not only in pedagogy but also and more in pedagogi- cal tools, which imply the use of digital media, can be perceived a big problem. Constructivism is mainly reduced to cognition theory considerations. This is evident when we analyze the compu- ter’s dominating input devices: keyboard and mouse. The results of the actions performed by the user through these devices are usually rendered on the screen (sometimes additionally through speakers or earphones). But the screen lets us experience the world as spectators. That what we see behind the screen’s glass is an allegory of the world and it is not something that we experience in the middle of the world with our body. After all the input devices (keyboards and screens) are descendants of the analogue media and of the world seen through the Cartesian system: the ty- pewriter with the linear codes of written language and the framed Picture of an opposite world.
The important thing is though the correlation to the „Aisthesis“: the senses and the perceptions and the actual experience that one has, as handler, in the middle of the physical world.
The perception of an active handling process was already accentuated by phenomenologist Mer- leau-Ponty4 and the constructivists Maturana and Varela later formulated it in their most impor- tant work5 “Each act is acknowledging and each acknowledgment is acting”.
2 Piaget, J. (1975): Das Erwachen der Intelligenz beim Kinde, Stuttgart. 3 Beck, J. / Wellershof, H. (1989): SinnesWandel. Die Sinne und die Dinge im Unterricht. Frankfurt/M. 4 Merleau-Ponty, M. (1966): Phänomenologie der Wahrnehmung, Berlin. 5 Maturana, H. / Varela, F. (1990): Der Baum der Erkenntnis, Bonn.
Creating digital augmented multisensual learning spaces –Transdisciplinary school	309 education between aesthetic creating and building concepts in computer science
2.2	Linking aesthetical experience and construction of models in pedagogical context
The programs that are usually installed on our computers segment our experiences and reconstruct them for a narrowed clearly delimitated purpose. Most learning software (which limits to the traditional input and output devices) falls back on obsolete pedagogic methods, like frontal in- struction (chalk and talk), external motivation and working by oneself. Even the more experience- orientated game software usually reduces us to “the ones who sit in front of the computer”. As such we train the same behaviour pattern over and over; this pattern is characterized by a lack of independence and therefore a pedagogically narrowed one. This is also the reason why we focus on aisthesis, the sensual perception in ArtDeCom.
The pedagogical approach described here is therefore a consequence of the critical constructive oriented pedagogy as represented in Germany by Michael Göhlich6 or – more specifically, in relation with the use of new digital media in teaching – in the USA by Seymour Papert7 and Mit- chel Resnick.8 What they all have in common is the special attention they give to the aisthetic (physical perception) and of the creative deed in the learning process (especially in what abstract informatics model construction is concerned).
Following their approach, we assume that self education of informatics competencies, as we know it today, is much better accompanied by self handling of the concepts in a real context. This is how the children themselves created digitally enriched, real-physical 3D rooms based on creative- artistic criteria. In these rooms, they have at their disposal a set of different computer interfaces which support the use of our natural senses in order to achieve learning in group interaction.
We also denominate these rooms as Mixed Reality Learning Environments. Let us first see how 3D interactive rooms (for learning purposes also) look like.
3	Creating digital 3D spaces for educational purposes 3.1	CAVE-Technology in pedagogical context
The CAVE-Technology is fascinating: it gives the possibility to connect the user with the interac- tive, digitally produced 3D worlds.
In the pedagogical context a set of problems reveal themselves.
•	The CAVE-technology is very costly. Besides the cost of the IT- specialists who have to program the projection, the user must usually wear some technical equipment, like stereo-glasses or a data-glove, sometimes even more uncomfortable, a head mounted display
6 Göhlich, M. (1996): Konstruktivismus und Sinneswandel in der Pädagogik. In: Aisthesis/Ästhetik – Zwi- schen Wahrnehmung und Bewusstsein. Weinheim.
7 Papert, S. (1990): Mindstorms: Children, Computers, and Powerful Ideas. New York: Basic Books.
8 Resnik, M., Berg, R., Eisenberg, M, Turkle, S., and Martin, F. (1999): Beyond Black Boxes: Bringing transparency and aesthetics back to scientific investigation. International Journal of the Learning Sciences.
310
3.2
•
•
T. Winkler, D. Reimann, M. Herczeg, I. Höpel
One of the first steps towards an interface that does not molest the user in such a way and that will allow interactive access for more than one person is, for example, users wearing caps; these would be detected by an infrared-based device which could, this way, identify users’ positi- ons and movement. The system can create then an interactive projecti- on on the floor which can so be controlled by the users, just like in the Animax in Bonn9.
Both methods have an important deficiency, seen especially from the point of view of a responsible pedagogue: usually the user only fol- lows a program with a predetermined outcome. Not him, but others are the creators of the application which is used by a single user. The technology he uses remains unknown to him – it remains unreachable in the black box of the computer which creates the outcome for the u- ser in an unexplainable way.
Interactive, pedagogically valuable 3D experience rooms, beyond costly and strange technology
We set out from this critique points and we want to show:
•	that the use of digital technologies for introducing interactive, hybrid rooms is also possible with little effort and expenses and that it must not include the use of top digital technology;
•	that the use of “natural” interfaces is possible and that these interfaces come close to commu- nication and handling situations;
•	that the technical system can be brought closer to the user in this way, so that they can un- derstand it and they can use it to create their own creative playroom, in which they learn how to interact with the system in order to program it themselves.
4	Learning collaboratively in hybrid space at low cost 4.1	Freeware for creating immersive environments
It is amazing, that excellent applications for creating immersive 3D spaces by students themselves are free for download in the InterNet.
E.g. with the freeware PhotoModeler Lite you can create 3D models of real-world objects and scenes through the use of photographs simply and easily.
9 FX Factory Interactive installation for children. A project of the Bonn Development Workshop for Compu- ter Media in connection with the intention of establishing the model MEET (Multimedial Theater Education Environments). (2002), see: http://www.animax.de/animax_web/fxfactiry02_eng.htm
Creating digital augmented multisensual learning spaces –Transdisciplinary school	311 education between aesthetic creating and building concepts in computer science
Figure 1: Digital 3D model made with FotoModeler Lite
Figure 2: Digital 3D drawing made with Teddy
With the sketch-based 3D modelling freeware “Teddy”10 you can create 3D models by drawing several 2D freeform strokes interactively Teddy automatically constructs plausible 3D polygonal surfaces.
And with the freeware “chameleon”11 or “Alice Paint” you can paint these 3D models.
Figure 3: Painting a 3D model with Alice Paint	Figure 4: Converting a 3D model with the View- point Builder
10 Takeo Igarashi, Satoshi Matsuoka, Hidehiko Tanaka (1999): Teddy: A Sketching Interface for 3D Freeform Design ACM SIGGRAPH'99, Los Angels, 1999, pp.409-416
11 Takeo Igarashi, Dennis Cosgrove (2001): Adaptive Unwrapping for Interactive Texture Painting. 2001 ACM Symposium on Interactive 3D Graphics, I3D2001, Research Triangle Park, NC, March 19-21, 2001.
312	T. Winkler, D. Reimann, M. Herczeg, I. Höpel
Then one creates an immersive environment that offers 3D spaces with Adobe’s Atmosphere Buil- der. Atmosphere permits you the import of 3D viewpoint objects, converted with the freeware Viewpoint Builder. These objects and other objects created with Atmosphere can be animated with movement or sound by means of JAVA-script within self created Atmosphere 3D spaces for publishing in the InterNet.
Figure 5: Building a 3D space for the InterNet with Atmosphere
Figure 6: Imported viewpoint objects in Atmosphere
Finally the students can enter together these co-operatively created digital spaces. For this purpose they can incorporate avatars, which they created by themselves using their own image or, simply, their fantasy.
Figure 7: Creating an avatar of his own appea rance	Figure 8: Assigning gestures for communication
Creating digital augmented multisensual learning spaces –Transdisciplinary school	313 education between aesthetic creating and building concepts in computer science
with Avatar Lab
4.2	Self-made hybrid spaces in school
Taken only in itself, the work with the here presented digital tools and their combination already represents a new challenge. But our work it is not only heading for combining the singular appli- cations. On the one hand it is the necessary relation to real objects within the process of producti- on. And on the other hand it returns the projection of the virtual InterNet space back into the real/physical space. This use of the digital in the physical augments first of all the individual possibilities offered by these digital tools giving a total larger than the simple sum of the ele- ments. This way an immersive, out reaching hybrid room of high pedagogical value is created.
Figure 9: Seized by sensors of a microcompu-	Figure 10: Data projection rehearses in the
terhidden in a real object (spider), a student’s action causes the movement of this object
Hybrid space
With the use of image recognition software from the LEGO-Mindstorms®, the interface between reality and the digital world is picket out as a central theme in the interaction between the children and the computers in the real room.
•	The projection of InterNet world created with Atmosphere (among other things including 3D objects created out of digitalized photos using Photo Modeler Lite and digitally drawn objects using Teddy) has as central theme the difference between digital and physical world behavi- ors.
•	Verbal and gesture communication using avatars (programmed using JAVA-script) has as central theme new digital means of communication via Internet.
•	Because the 3D Internet world and the possibilities to communicate appear again as projecti- on within the real space with real objects of digital models, the central theme here is the pos- sibility of the extension of the reality by digital means in the physic world.
The central point of our project is the fact that children build de facto hybrid spaces themselves. They do this using freeware and/or low cost applications and creating the digital objects and mo- dels, again, themselves.
314
T. Winkler, D. Reimann, M. Herczeg, I. Höpel
Figure 10: Mixed Reality: Students in a data projection of an InterNet space
Figure 11: Real, 3D objects as master and point of reference for digital models in the Mixed Reality environmentConclusions
The focus on sensuality and perception has far reaching consequences, especially for digitally supported learning: The promising teaching attempts clearly show that learning by digital means is fascinating for the children and, at the same time, eased when multisensual, natural interfaces are used. Deep understanding of highly abstract relationships can be promoted through the direct connecting of program code and sensorial input respectively system output.
Returning to the senses and natural perceptions as ground for experience-oriented learning makes learning a creative process and connects it to natural forms of communication and cooperation. This is what co-constructivism means from the point of view of constructivist pedagogical theo- ries.
Besides the co-construction of knowledge, working in projects near to real life makes pupils learn how to read, write and calculate. Also their get a feel for - independence, teamwork, sensibility as well as for the ability to express oneself; which are important pedagogical goals.
The advantage of using freeware and low cost applications is not only ideal for the low budgets existent in schools but it also offers the possibility of creative, independent use of digital techno- logies. A very important consequence is that the children grasp the essence of digital media.
Creating digital augmented multisensual learning spaces –Transdisciplinary school	315 education between aesthetic creating and building concepts in computer science
Figure 12: Visitors in an interactive hybrid environment
Acknowledgements
The project ArtDeCom is funded by the German “Bund-Länder Commission for Educational Planning and Research Promotion” (BLK) within the general funding program “Cultural Educati- on in the Media Age”. The official title of the Project is “Theory and Practice of integrated Arts, Design and Computer Science in Education”, project number A 6681ASH01. The authors wish to thank the Integrierte Gesamtschule Schlutup in Lübeck, Germany. References
Beck, J. / Wellershof, H. (1989): SinnesWandel. Die Sinne und die Dinge im Unterricht. Frank- furt/M.
Göhlich, M. (1996): Konstruktivismus und Sinneswandel in der Pädagogik. In: Aisthesis/Ästhetik – Zwischen Wahrnehmung und Bewusstsein. Weinheim.
Merleau-Ponty, M. (1966): Phänomenologie der Wahrnehmung. Berlin.
Maturana, H. / Varela, F. (1990): Der Baum der Erkenntnis. Bonn
Papert, S. (1990): Mindstorms: Children, Computers, and Powerful Ideas. New York: Basic Books.
Piaget, J. (1975): Das Erwachen der Intelligenz beim Kinde, Stuttgart.
Resnik, M., Berg, R., Eisenberg, M, Turkle, S., and Martin, F. (1999): Beyond Black Boxes: Brin- ging transparency and aesthetics back to scientific investigation. In: International Journal of the Learning Sciences.
Igarashi T., Matsuoka S., Tanaka H. (1999): Teddy: A Sketching Interface for 3D Freeform Design ACM SIGGRAPH'99, Los Angels, 1999, pp.409-416
Igarashi T., Cosgrove D. (2001): Adaptive Unwrapping for Interactive Texture Painting. 2001 ACM Symposium on Interactive 3D Graphics, I3D2001, Research Triangle Park, NC, March 19-21, 2001.
Winkler, T., Reimann D., Herczeg M., Höpel I. (2002): Collaborative and Constructive Learning of Elementary School Children in Experien-tal Learning Spaces along the Virtuality Conti- nuum. In: Berichte des German Chapters of the ACM, Mensch &amp; Computer, Stuttgart.
316	T. Winkler, D. Reimann, M. Herczeg, I. Höpel
Winkler, T., Kritzenberger, H., Herczeg, M. (2002): Mixed Reality Environments as Collaborative and Constructive Learning Spaces for Elementary School Children. In: Proceedings of the World Conference on Educational Multimedia, Hypermedia and Telecommunications, Vol. 2002, Issue. 1, pp. 1034-1039.
FX Factory Interactive installation for children. A project of the Bonn Development Workshop for Computer Media in connection with the intention of establishing the model MEET (Mul- timedial Theater Education Environments, 2002). URL: http://www.animax.de/animax_web/ fxfactiry02_eng.htm
All pictures, except the one referring to the PhotoModeler Lite software, show aspects of a tea- ching attempt by ArtDeCom with eight grade students at the German comprehensive school Integrierte Gesamtschule Schlutup in Lübeck.
Contact Information
Dr. Thomas Winkler Institut für Multimediale und Interaktive Systeme (IMIS), Universität zu Lübeck Media Docks Willy-Brandt-Allee 31a 23554 Lübeck Germany
Email: winkler@imis.uni-luebeck.de</Text>
        </Document>
        <Document ID="171">
            <Title>￼</Title>
        </Document>
        <Document ID="172">
            <Title>Images</Title>
        </Document>
        <Document ID="394">
            <Title>Notes</Title>
        </Document>
        <Document ID="317">
            <Title>TODO</Title>
            <Text>Check all books have PLACE OF PUBLICATION</Text>
        </Document>
        <Document ID="206">
            <Title>5.4.6 What talking toys say: conversation analysis</Title>
            <Text>Talking toys enshrine a pedagogy that has, in effect, remained un-changed since Edison's *talking doll* of the late 19th Century'. The pedagogy is built on cautionary tales and stories, wrote learning of nursery songs and instruction led play. 

Elsewhere in this paper, I have mentioned Microsoft's Actimate *Barney the Dinosaur* being accused of lying - mainly because he doesn't have a consciousness yet talks freely of love. The ideological function of what talking dolls say is critical when evaluating talking toys. 

The New York based Barbie Liberation Organisation (BLO), a group of feminist hactivists formed circa December 1993, undertook a remarkable operation to swop the voice boxes of 300 Barbie and G.I. Joe dolls. 

: "When Barbie speaks, little girls listen, which is why controversy erupted in 1992 when Teen Talk Barbie exclaimed, "I love shopping," "Meet me at the mall," and "Math class is tough." This last phrase struck an especially sour note, given the under-representation of women in the sciences" [Dery (Date Created: 05/1994. Date Accessed: 1/2/2007)][#Dery:2007zr]

: "The Simpsons" episode Lisa vs. Malibu Stacey is a wonderful parody of the 'Teen Talk Barbie' controversy. The episode explores issues of what dolls are programmed to say and activist / feminist responses to such things[^chcc_simpsons]. There is even a subtle reference to the underground work of the Barbie Liberation Organisation when the girl doll "Malibu Stacey" is heard to say "My Spidey-sense is tingling. Anyone call for a webslinger?". 

Although the technologies of embedded speech are fascinating, whether using the latest embedded microprocessers for speech recognition and synthesis processors[^chcc_elvis], bellows and air, miniature phonographs, it seems that whenever toys talk the semiotic content, or the meaning of the iteration outweighs the possibilities inherent in technical act of production. 

[^chcc_simpsons]: First broadcast: in "The Simpsons" Season 5, September 30, 1993 May 19, 1994.

[^chcc_elvis]: See the wonderfully named E.L.V.I.S. “the Embedded Large Vocabulary Interface System platform” from Voice Signals Technologies.
[Voice Signals Technologies. (Date Created: 19/02/2002. Date Accessed: 1/2/2007.)][#Technology:2007ys]
</Text>
        </Document>
        <Document ID="283">
            <Title>Presentation 01</Title>
            <Text>Coding as Practice
------------------

Exploring touch/multitouch
Gesture and Object Recognition</Text>
        </Document>
        <Document ID="428">
            <Title>Notes on Practice as Research Barratt and Bolt</Title>
            <Text>[Barrett and Bolt (2007)][#barrett07]

(1)  Staging the Research
  
(2)  The literature and practice review
  
(3)  Materials Methods and Conceptual Frameworks
  
(4)  The Exegesis: Discussion of the Studio Research Process
  
(5)  Discussion of the Outcomes and Significance
  
(6)  Writing the Conclusion

(7)  The Abstract

(8)  Writing the Creative Arts Research Paper: A Summary

(9)  Citation and Referencing

(10) Appendix

--

(1)  Staging the Research
  
'What did the studio process reveal that could not have been revealed by any other mode of enquiry?'

locate the work in contexts of practice and contexts of theory

stablise the research discipline.

design studio enquiry as a research project
	- problem with the pre-determination of outcome
	- praxis - move between what is known and what will be revealed

introduction born of the proposal

The introduction asks:

What is the subject or topic to be investigated?
What are the specific areas of interest and what ideas and positions have other studio practitioners taken in relation to these?
What is the research question or hypothesis?
What is the research objective or aim -- what will be acheived at the end of the process?
What is the main thesis or argument?
How will this be developed in the research paper?

Stages of the Research

Stage 1: Establish the field: asserting the centrality and need for this enquiry - how is it significant?

State current practice and knowledge
Summarise what is know and formulate the problem

Stage 2: Summarise Previous Practice and Theory
Always from the perspective of THIS research, clearly stating the relationship between THIS research and the whole field

Stage 3: Justification: Find the GAP in the existing knowledge and raise a KEY question

Outline the studio process and chapters of the exegesis or sections for the paper

Stage 4: Introduce Present Research
State Purpose
State Hypothesis / thesis
Outline Approach: what methods of studio enquiry and analytical / conceptual frameworks
Outline Chapters (rationale for chapters)
Outline the research project itself

----
Move from the general to the specific
Make an argument to convince the reader of the importance of the research.



(2)  The literature and practice review
  
broaden the topic
and narrow the topic indicating the scope of the research
adjust critical focus

visual material
scholarly texts
referencing and citation

apprehension of and response to movement - tricky to capture

provide context and pedigree for the practice.

locate the research in a historical and contemporary context (DIACHRONIC)

methodological comparison and discussion

indicate gaps

demonstrate how practice informs theory

--

Reading should be:

Inspirational
raise immediate questions
inspire or clarify practice
advance the thesis

--

Apply the following questions to the literature review:

How does it support my position?

How can my postition be a critique or interrogation or clarification of the surveyed material?

How does it provide background or context?

Is the theoretical paradigm or fundamental concepts part of the material part of the analytical framework?

What gap does the project fill?

Practice is primary and not simply an illustration of theory.

the chinese room ...

(3)  Materials Methods and Conceptual Frameworks
  
bricolage

historical connections

media archeology

materials are encoded with historical knowledge and conventions and are inextricably linked to conceptual and theoretical frameworks


Materials, Methods and Assumptions

digital technologies provide new and different ways of revealing and modelling and simulating (adapted from Barrett and Bolt)

JUSTIFICATIONS
New forms of expression or way of revealing
Technical Solutions
Innovation
Fun

Genres
"The artist as researcher may want to celebrate critique, extend, revise or even incriminate the work of earlier practitioners." [192][#barrett07]

Observation
What happened? What changed; What was revealed? What is significant (what was revealed that could not have been revealed through any other method of enquiry?)

Ethical Considertations
Nature of Representation;
Appropriation of material and copyright issues;

Conceptual / Theoretical Framework

This is difficult for creative arts researchers.
[reception theory, semiotics]

(4)  The Exegesis: Discussion of the Studio Research Process
  
AIM: replicate the studio enquiry in the context of theoretical ideas and practices.

Outcome identification
Key moment identification
breakthroughs
outline details of methods/materials/processes that led to breakthroughs or new understandings...

Compare with the work of others


When to write the exegesis: there should be a sustained dialogue between studio work and writing.

MEMO: Visual Diary / Research Notes




(5)  Discussion of the Outcomes and Significance
  
complete studio work
journals / archives / and scholarly research

￼


The above is a good start for the creative arts researcher - but some more thematic arrangements of sections with titles 

CONCEPT: re-versioning / retelling retell the process within a wider context
the discussion should always reflect back to the HYPOTHESIS or thesis statement

Visuals - labelled and numbered. Referenced...

DISCUSSING THE SIGNIFICANCE OF THE WORK

significance relates to connections
significance relates to building theory and practice
whether the work challenges existing ideas
how has the work advanced or solve particular problems (expression)
how can the research be extended and applied (beyond the creative arts)



(6)  Writing the Conclusion

Nothing NEW;
Synthesis, set down broad generalisations that validate the thesis statement or hypothesis;
relate to AIMS and OBJECTIVES;
Point out implications;
Reiterate value and significance of the project;
Make recommendations for future work;
Critique limitations of the research process;



(7)  Practice as Research: The Abstract

Aims
Methods / Content
New Understandings / Outcomes
Significance and Relevance

Answer the following questions:
Why do the research?
How was the research conducted
What were the main outcomes and Results
What were the principle conclusions
What is the significance of the research

State what the practical enquiry will reveal that other modes may miss



(8)  Writing the Creative Arts Research Paper: A Summary


* What did the studio enquiry reveal that may not have been revealed through other modes of enquiry?
* What methods and approaches were developed through the studio production that allowed discoveries to be made?
* How does the completed work perform, model or demonstrate the new knowledge/understandings gained through the studio process?
* How might these understandings be applied both within the field and creative arts discipline as well as beyond it?
* How does practice inform theory?


(9)  Citation and Referencing

(10) Appendix




</Text>
        </Document>
        <Document ID="16">
            <Title>bbc-r-ar-2006-2007</Title>
            <Text>Preparing the BBC for Creative Futures and Digital Britain.
Ensuring it has the technology to remain relevant to licence fee payers and their evolving needs.
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	1
CONTENTS
Contents
Foreword	4
NEW SERVICES
High Definition Television Interactive Television for the On-demand World Online, On-demand, and On The Move Freeview Playback Serving our Audiences – User-centred Research
PRODUCTION
Production Magic D-3 Videotape Preservation System Programme Production Radio Spectrum for Production
DELIVERY
CORE TECHNOLOGIES
6	Networks for Programme Production
8	Radio Systems 10	Video Compression – Dirac and Dirac Pro 13	Audio Compression 14	Digital Rights Management
WORKING WITH US
16	Collaborative Projects 19	Standards 20	BBC Information &amp; Archives 23	The Innovation Forum
Future Media Innovation
24	More Information 26	Index 27	Credits and address
30 32 34 36 37
38 40 43 44 45
46 47 48
Digital TV – Switchover Digital TV – Architectures Audience Research DAB and Digital Radio Mondiale	28 Kamaelia	29
2
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	3
FOREWORD
Foreword by Huw Williams, Head of Research &amp; Innovation
A new name
This review reflects a year of important changes, with the renaming of our parent division to Future Media &amp; Technology (FM&amp;T), reflecting its role in transforming how the
BBC uses technology. The focus of our work is moving beyond digital broadcasting to incorporate the changing world created by the convergence of broadcasting and the new media technologies.
We have been joined during the year by the BBC’s Future Media Innovation and Information &amp; Archive research teams, whose work is introduced in this Review.
So we are now BBC Research &amp; Innovation (BBC R&amp;I).
Huw Williams Head of Research &amp; Innovation
Our achievements
I would like to pick out some of our significant achievements during the past year.
Encouraged by the increasing sales of HD-ready televisions, we helped the BBC prepare for the BBC’s HD test trans- missions, which began in summer 2006 on the three main digital platforms, terrestrial, satellite, and cable. While we await a decision on the BBC’s launch of a service, we continue to help fine-tune the system, to ensure that the public sees the best possible results from this improved technology.
It is now nearly nine years since we helped to put public digital TV broadcasts on air in the UK (not forgetting DAB three years earlier), and later this year the Digital Switchover will begin in earnest. Although the BBC’s spectrum planners themselves are no longer par t of the depar tment, we continue to advise on methods of technical assistance for our viewers, in particular the more vulnerable groups.We also work to ensure the BBC continues to make efficient use of its radio spectrum allocations, both for its broadcasts and its internal communications.
One of the themes that run through our work is that our audiences no longer simply restrict themselves to our linear television and radio broadcasts. In fact some rarely now watch or listen in this way. We have contributed to new developments such as iPlayer, new forms of interactive content on our digital broadcasts, Freeview Playback and other ways for our audiences to control their own viewing. While the PC and 3G phone might be obvious alternatives to the TV, another exciting prospect is using games consoles to create new forms of interactive story telling.
We continue to support our colleagues in production, with innovative solutions to make routine tasks easier and to encourage creative programming.The Piero sports graphics system won the award for Innovation in Content Creation at IBC 2006.
It is worth remembering that none of this technology will work reliably for consumers, or for the electronic media industry as a whole, unless the right technical standards are in place. We put significant effort into proposing, arguing and supporting standards, both within the UK and internationally, and value
the contacts we make through our collaborative work.
Governance
A new governance structure has been introduced to link the work of BBC R&amp;I closely to the business, formalising an innovation ‘funnel’ to nurture new ideas and research areas and their development into products. The link to the business is provided through regular Technology Executive Conferences, which set strategic technical priorities across the BBC and find sponsors for our projects. A newly created Research Board approves and oversees our workplan, deciding which research should be continued, and when and where it is appropriate to transfer it to open source or commercial developments.
The year ahead
Our work continues, concentrating on eight main research themes, with four long term projects that work across them – Digital Switchover, the challenges of the 2012 Olympics, and our academic and industrial collaborations.
I note as we finish writing this Review that the first products using our Dirac Pro open-source video compression codec
are about to be shown at the prestigious NAB broadcasters’ exhibition and conference in Las Vegas. We are also ver y close to trialling the delivery of digital TV and radio channels to 3G mobiles, bringing the BBC’s programmes to our audience
on the move.
In the coming year BBC R&amp;I will be key to a number of major BBC projects and initiatives most notably Freesat, iPlayer, further enhancements to Freeview, HD, and helping to define Web 2.0 services.
This translates into 14 separate portfolios of research, each with its own portfolio manager:
Production Magic,Wireless Connectivity, Future Broadcast Technologies, Blue Sky/Future Technologies, External Projects, Programme Production Technology, Interactive TV, Internet Distribution, Digital TV, Metadata Systems, Search &amp; Navigation, Networking &amp; Grid Technologies, Metadata Delivery &amp; Standards, and RF Transmission &amp; Reception.
4
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	5
NEW SERVICES
NEW SERVICES
High Definition Television
Many BBC programmes are already made in HD, and the amount will increase over the next few years as studios and other equipment are upgraded as part of the normal replacement cycle. The cost of HD production is becoming closer to SD, and it may soon be difficult to obtain SD production equipment.The BBC also has to protect its sales to other broadcasters.This is a valuable source of income, particularly from North America, where HD content is now insisted upon. HD production also ensures the BBC has a reasonable stock of programmes to show on a service, although a decision on a launch has yet to be taken.
In summer 2006 the BBC began broad- casting high definition television as a technical trial on digital satellite, on Telewest digital cable, and to a closed user group on digital terrestrial television in the London area.The trial began with coverage of the World Cup from Germany. This was rapidly followed by other outside broadcast events including Wimbledon and some of the Proms, together with a range of recorded HD programmes shown at peak times.
The trial has posed many challenges, and we are taking a leading role by providing advice and assistance to BBC Sport, BBC Resources, Red Bee Media and Siemens, and collaborating with equipment manu- facturers. HD technology is still relatively immature, and the entire signal chain from contribution, through production and post-production to play-out and distribution, has to work at much higher bandwidths and bit rates. The reference test system that we maintain at Kingswood Warren for digital television development is crucial. It enables us
to give advice based on practical experimentation in a controlled and repeatable environment on matters such as picture quality, bit rates, coding parameters, picture/sound timing,
multi-channel audio, and subtitles. We have led the work to edit the relevant DVB standard to enable subtitles at HD resolutions.
On satellite, the transponder with the most appropriate coverage for the trial was already full with standard definition services. A method had to be devised to move these into newly acquired satellite capacity (the BBC’s seventh transponder) without disturbing the service to the viewers. As on previous occasions, this was a collaborative effort between us, Siemens and BBC Distribution. At the same time, the BBC took the opportunity to expand its provision of interactive streams for a temporary period, allowing full interactive coverage in standard definition of Wimbledon and the World Cup, in addition to the simultaneous HD coverage of these events.
For the cable trial, we provided technical support to BBC Distribution in discussions with Telewest and helped on a number of technical matters such as specifying interfaces and trouble-shooting at the start of the trial.
For the terrestrial trial, we defined the technical requirements for the HD set
top boxes that were to be provided to the trialists, assessing a large number of submissions from manufacturers in response to a tender. The shor t time- scale, five months, and cost constraints meant that only a limited set of features could be built into the boxes. We therefore issued a list of essential requirements, in particular being able to receive Dolby Digital surround sound, and a longer list of desirable features, for example, reception of standard definition Freeview broadcasts.
Boxes from two manufacturers were shortlisted, on a combination of technical capability and price.We helped these manufacturers finalise the design of their boxes in time for the trials.
Trialists were required to possess displays that conformed to the EICTA ‘HD Ready’ standard. Such displays are fitted with either an HDMI or a DVI connector for the HD signal. Because these are relatively new interfaces, we carried out compatibility tests between a selection of typical HD Ready displays and the trial boxes, finding almost all pairings to be compatible. We also took the oppor- tunity to measure picture overscan,
‘Generally, the responses were positive, particularly concerning picture quaility’
to recommend a safe visible area for captions and other graphics.
Objectionable changes in loudness were found when switching between programmes using Dolby Digital and MPEG Layer II audio coding. We found this effect was being introduced by the mechanisms for signalling and modifying audio levels in Dolby Digital, and we are now working with manufacturers to reduce it.
Appreciation of HDTV has two main aspects, programme content and technical quality. Clearly, our professional interest is in the latter! Comprehensive questionnaires were put out by BBC Marketing, Communications and Audiences, and trialists were also able to post comments on a private web forum. Generally, the responses were positive, particularly concerning picture quality, and there was also a call for more programmes with surround sound. Critical comments included an occasional
lack of lip synchronisation, and some pops and clicks on the sound, these being due to teething troubles with the technical systems.
Over a few days in October 2006 we deliberately varied two technical para- meters; bit rate, between three different values, and the broadcast standard, between 1080i and 720p. From comments solicited on the web forum we found that some trialists were aware of reductions in bit rate in both the 1080i and 720p modes. A second longer series of tests has just been completed.
The latest upgrade to the video data- compression coder is being tested to assess the improvement in coding efficiency.
The BBC now needs to ensure that the technical requirements of an HD service can be met. The trial highlighted a number of areas where the specification needs more work to ensure that manufacturers can produce compatible receivers. The DTG has started to address this, and we are actively contributing on aspects such as the detailed syntax for the video encoding, system signalling, and requirements for receiver video and graphics processing.
What will replace HD?
It is not too early to ask this question. Even though HD broadcasting has barely begun, the basic research was started more than 20 years ago. Will the future be ultra-HD, 3D-TV, or something else? While it is clear that there is a continuing quest for ever higher quality pictures and sound, as can be seen from 4k digital cinema, equivalent to 2048 vertical lines, and NHK’s 4096-line Super HiVision system, there are serious difficulties in delivering this to the home.
Or will ultra-HD become a means to an end? There are benefits for pro- gramme production in starting with such high-quality material. If it is used correctly, it can help maintain HD quality at the end of the production chain, and this encourages more elaborate processes, such as the ‘Production Magic’ described later.
Currently we are exploring the state of research worldwide and investi- gating opportunities for collaboration with other organisations.
6
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	7
NEW SERVICES
Interactive Television for the On-demand World
The majority of the interactive services that the BBC has delivered to date have been based on content delivered over a unidirectional broadcast path. This path is shared with the main television pictures and sound, access services, schedule information and other data, and this generally limits the interactive content to relatively modest text and image files. However, two receiver technologies, mass local storage and broadband connectivity, can not only radically improve the nature of interactive content but also the way viewers can access it.
With mass storage available in the receiver, large amounts of data can be downloaded, perhaps at low speed overnight or at any other time when broadcast capacity is available.The inter- active services can then include richer graphics and even audio-visual clips.
To explore the possibilities, the BBC has been working in partnership with Cabot Communications, a supplier of digital TV software technologies, to extend the capabilities of a standard consumer digital TV recorder. The extensions allow the recorder to download and store MHEG-5 applications for on-demand access and to allow these applications to schedule conventional recordings.
This has allowed the BBC to undertake a three-month technical trial, with 300 of the modified recorders issued to members of the public.The con- sumer proposition for this was
a ‘catch-up TV’ service, where the recorder automatically records a selection of BBC programmes from the	broadcast	schedule. These	are	then made available for on-demand access via a rich graphical user interface written in MHEG-5 and enhanced with downloaded video clips.
‘The extensions developed to support the trial open up exciting possibilities for enhanced television services, such as DVD-style extras providing “the making of...” or “an interview with...”.’
TThe portable content format
The BBC provides interactive television on a number of different digital TV platforms. This is a challenge because each platform follows a different set
of technical standards for interactivity, and the BBC therefore has to produce several versions of every piece of interactive content.
This problem is not new, and the BBC has been exploring ‘author once, publish to many’ approaches for over five years. The result, in collaboration with other organisations, is a new DVB standard, the Portable Content Format (PCF).
The PCF, published by ETSI in January 2006, is being embraced by the BBC as both an authoring format and as an input format for platform-specific publication processes (often called more simply, ‘transcoders’). It is also hoped that PCF can make it easier
to take in interactive content from third parties, reducing both the ‘time to air’ and the management costs.
Interactivity at HD
Larger displays and greater screen resolution gives HD television a ‘wow’ factor that is hard to resist. But what part
do interactive services play alongside HD programmes? The BBC is researching ways to broadcast and present interactivity efficiently on HD, as far as possible with an awareness of the technology within HD receivers, but perhaps needing to influence it too.
Initially, HD programmes could make use of the interactive services transmitted for SD, such as BBCi and the popular television multiscreen services such as Wimbledon tennis. But how best should an HD receiver reproduce this content? Is it acceptable simply to render it at SD and then re-scale, or would it be better to have intelligent software rendering it directly at HD?
Beyond this some interactive services could benefit from being authored specifically for HD. This could be as straightforward as providing images at HD resolution, so that no up-conversion is required, or redesigning to make use of the extra resolution and the
larger screen.
We continue to work with other interested parties, to consider the impact of such ideas on the broadcast stream, in terms of need for additional capacity,
‘BBCR&amp;I aims to make the viewer’s experience more reliable and consistent’
and on the receivers, in terms of memory and processing demands.
Interoperability on DTT
Freeview transmissions conform to a set of published technical standards that the BBC helps to write. This allows any manufacturer to produce digital terrestrial television receivers for the UK. However, the lack of a single platform operator
and the open market in receivers continue to present a conformance and interoperability challenge, particularly in the context of interactivity. This can lead to receivers failing to display some interactive content correctly.
BBC R&amp;I plays a major role in the way that the BBC resolves this, by working closely with individual manufacturers and across the industry, through bodies such as the DTG MHEG Interoperability Group.Through this it aims to make
the viewers’ experience more reliable and consistent.
The extensions developed to support the trial open up exciting possibilities for enhanced television services, such as DVD-style extras providing ‘the making of...’ or ‘an interview with...’. We have successfully tested some examples of this in the laboratory, and are starting to include them in the technical trial.
The BBC is gaining valuable experience through the trial on the practicalities of running this type of service and work is underway to explore how to standardise the extensions that enable them.
The number of homes with broadband Internet connections is growing rapidly, and BBC R&amp;I has been looking at how a receiver with both an aerial input and a broadband connection might combine content from both sources. Some initial
ideas developed by the BBC were shown at IBC 2006. The demonstration showed text, graphics and video delivered over a broadband connection, using unicast and multicast, viewed alongside conven- tional broadcast-delivered content.
However, adding broadband access to set-top boxes and TVs in an open market such as Freeview presents a number of challenges. Home ADSL installations vary from a modem connected directly to a single PC, to complex networks with firewalls. Connections come with a range of connection speeds, often with usage restrictions and no guarantee of the quality of service. BBC R&amp;I is working with the industry on how to deal with these issues, through the DTG Interaction Channel Working Group.
8
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	9
NEW SERVICES
NEW SERVICES
Online, On-demand, and On the Move
The end of television delivered as a scheduled broadcast service has been widely predicted for some years now, with little evidence to back up the claim. The VCR allows viewers some freedom from the broadcasters’ programme schedules, but most models are notoriously difficult to program, generate a stack of tapes, and will only record one channel at a time.A new breed of on-demand technologies is finally emerging that we believe will disrupt the status quo in a more profound way.
Some broadcasters may view the new technologies as a threat to existing business models.The concept of advertising ‘spots’ is particularly at risk when the viewer can order individual programmes or simply skip through unwanted content.The BBC, on the other hand, is embracing this new non- linear landscape as an opportunity to exploit its content to the full. We can serve our licence fee payers better by allowing them to watch our programmes when they want to, not just at the times we choose to schedule them.
The BBC has been streaming audio-visual media on the Internet for some years. Indeed, the BBC was pivotal in the devel- opment of these types of services.The Radio Player service, based on technology originally developed by BBC R&amp;I, has proved popular and demonstrated a clear consumer demand for ‘listen again’ services. The challenge in the present Royal Charter period is to provide an equivalent ‘watch again’ service for television.
Television is a greater technical challenge than radio.The unicast streaming technology used to deliver the Radio Player service does not scale to the high bit rates and file sizes demanded by
‘ We can serve our licence fee payers better by allowing them to watch our programmes when they want to...’
ttelevision. Colleagues in BBC FM&amp;T developed a peer-to-peer Internet download service for distributing large media assets to personal computers for the ‘iMP’ trial of 2005/2006.
In 2004 we set up a public trial of live multicast streaming. Multicast uses the routers inside the network to duplicate the streams of data packets rather than sending separate streams all the way from the head end to each recipient. Such trials promote the roll-out of multicast, by encouraging users to set up their home equipment to receive
it, and Internet service providers to set up their networks to carry it.
This trial ran successfully until August 2006, at which time the rights we had negotiated for distribution of the content expired. It is now to be developed into
a full broadband ‘iPlayer’ service, following the approval of the BBC Trust.
Research continues on other delivery methods for on-demand video content, beyond the iPlayer environment and looking at future home networks delivering the same content transparently to the TV, PC, or mobile devices.
Syndicating on-demand content The BBC intends to syndicate on- demand content to cable operators, IPTV service providers, and 3G mobile phone networks, rather than delivering direct to these operators’ customers.
Each of these platforms uses a different combination of technologies, some of them proprietary. BBC R&amp;I has helped BBC Distribution to design the technical facilities for preparing the content for the various platforms.
The distribution system will be operated by the BBC’s playout contractor Red Bee Media. It is envisaged that a common content ingest and media management system called the On-Demand Production System (ODPS) will be shared with the broadband iPlayer to avoid needless duplication of effort. The infrastructure can acquire live programmes in real time as well as the more conventional method of ingesting tapes or files ahead of linear transmission. A suite of encoders will then convert the content into the appropriate format for each platform.The resulting media files will be staged to a ‘neutral drop-off zone’ operated by our
technology partner Siemens, from where the operators will retrieve them.
Metadata is sent direct to the platforms. This metadata will populate the content guides, and will include extras such as artwork relating to the programmes and promotional trailers; most on-demand platforms allow a sequence of promotional items to be played automatically before and after the programme that the viewer has selected.We hope to establish a profile of TV-Anytime for the metadata that is acceptable to all parties.
Convergence of walled gardens and the Internet Video-on-demand is an emerging battleground for established organisations to test their resolve in maintaining old business models when Internet start-up companies start to offer similar services; cable companies and telcos are building ‘walled garden’ video-on-demand systems to combat the pressure from Internet delivery of a broader, potentially more attractive set of content.The BBC’s public service remit means it is providing content to carriers in both camps, as well as direct delivery though projects like iPlayer.
True video-on-demand services
stream programme content in real time to a set-top box or personal computer via a broadband connection.The viewer selects programmes from an electronic catalogue and can start watching immediately; unlike near-video-on- demand there is no need to wait until the next showing starts.The on-demand receiver can even be combined with a digital TV recorder to provide the best of both worlds.
The digital TV recorder is a digital TV receiver that records onto a high capacity hard disc drive instead of tape. In addition to the usual VCR functions, programmes can be paused and resumed when watching ‘live’, and recordings can be viewed before they are complete. The digital electronic programme guides make recordings easy to set up, and if suitable extra signalling is included in
the broadcast, the machine can be set to record entire series just as easily as single programmes.The recording time can even adjust itself automatically if the schedule changes. On the latest models part of the capacity can be reserved for recordings selected by the service provider (‘push-video-on-demand’).
Download services offer programme content over a broadband connection to be stored for later viewing. Although usually associated with personal computers, these services have also been demonstrated with broadband- connected set-top boxes.The content is encrypted and the viewing permission can be set to expire after a fixed period.
The BBC iPlayer is a unified online player, to find and play on-demand and live-streamed audio and video.
10
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	11
NEW SERVICES
Online, On-demand, and On the Move continued
Freeview Playback
Freeview Playback brings a set of new features and the mark of quality assurance to the free-to-air Digital TV Recorder market. It is the first major change to the Freeview platform since the introduction of seven-day programme guides. Getting the service on air at the end of 2006 was the culmination of work that started for BBC R&amp;I more than two years ago.
Freeview Playback is much more than a marketing exercise. It is underpinned by significant enhancements to the signalling that accompanies DTT programmes. These enhancements were developed by BBC R&amp;I in consultation with other broadcasters, industry, metadata aggregators, and colleagues in BBC Marketing, Communications &amp; Audiences.This wide collaboration was essential because the data to support Freeview Playback runs through the whole broadcast chain.
An initial phase of the work was to agree an amendment to the UK guidelines for broadcaster and receiver equipment interoperability, to describe how Freeview Playback would work ‘under the hood’.This was coordinated and edited under the auspices of the DTG by BBC R&amp;I, and relied on our long term involvement with other standards bodies such as DVB.
To bring Freeview Playback to the viewer substantial changes were needed to the DTT technical systems. All electronic programme information for the UK DTT platform is generated by a system called the central collator, now owned and operated by our technology partner Siemens.When this came due for replacement BBC R&amp;I made sure that the new system would be capable of supporting Freeview Playback; our involvement in the replacement continued right through to the final stages of systems integration and testing.The central collator dated from the start of digital broadcasting, and it was a major achievement when this broadcast critical system was successfully changed over while services remained on-air.
Once the new system was in place the necessary extra metadata had to be generated and new feeds arranged to supply it. As the acknowledged experts in this area, BBC R&amp;I advised the BBC’s metadata supplier on suitable solutions.The Freeview Playback metadata is compliant with the TV-Anytime standard.
The BBC was the first broadcaster to transmit Freeview Playback information. A commitment has been made by the other UK DTT broadcasters to follow suit. This co-operation is encouraging manufacturers to produce the receivers, a market we expect to expand during Digital Switchover.The first two models have been available since December 2006.The features provided by Freeview Playback have also been recognised in Scandinavia, where the 'NorDig' broadcasters plan to implement the same underlying signalling.
The business relationships established through Freeview Playback continue, and are expected to lead to further developments. Currently, we are working on a new feature called ‘trailer booking’, which allows a Freeview Playback viewer to book a recording while watching a trailer for the programme.
TV and Radio on 3G mobiles Early in 2007 the BBC was given approval to launch trials to deliver BBC TV and Radio channels to a number of 3G mobile operators. The trial will be launched in April 2007 on Orange, Vodafone and 3, and to meet this tight timescale it has been necessary to reuse the technical infrastructure already deployed for the multicast trial. This will be replaced in due course, to allow for expansion of the trial to more operators and to include a wider range of BBC digital TV services and content,
At first, the trial will be carrying three BBC TV channels – BBC One, BBC Three, and BBC News 24 – and about eight Radio channels, all streamed live. It will be receivable on most modern 3G phone models, although usage may incur a charge from the user’s mobile phone operator.
Users will receive the appropriate regional variant of BBC One, depending on their location when calling in. (The BBC Three and BBC News 24 channels have no regional variations.) Some schedule data is carried, to display programmes titles and synopses.
Each video signal is encoded at 100 kbit/s, using H.264 coding.The image is clipped slightly to suit the narrower usable aspect ratio of the typical mobile phone screen. The audio is encoded using AMR (adaptive multi-rate coding), which is
already widely used on mobile phones. AMR is however optimised specifically for speech, and we hope later to move to AAC coding to improve the overall sound quality and make a further saving in bit rate.
Transmission is by the conventional mechanism used for IP streaming to mobile devices. At present a separate stream is set up for each viewer. The mobile phone operators may decide to change this in future, although the number of simultaneous calls to the same TV or radio channel within a single mobile phone cell may not always justify a different arrangement.
‘Early in 2007 the BBC was given approval to launch trials to devliver BBC TV and Radio channels to a number of 3G mobile operators.’
Because any 3G phone that can receive the service will also have Internet access, it is not essential to embed the inter- active content that accompanies the digital broadcasts.The same information, and more, can be obtained from the BBC website. Having a mix of content may be a catalyst for the way data services to mobile phones are charged.
• Accurate Recording – a Freeview Playback recorder continually monitors broadcaster information to control the start and end of a recording and does not just rely on a pre-published time.
• Series linking – a Freeview Playback recorder can automatically identify and record programmes from the same	series. There	is	no	longer
the need to rely on recording the same time and channel each week or to try to match programme titles.
• Repeat identification – when the viewer wants to record programmes that overlap, the recorder can check if the clash can be avoided by recording a repeat.
• Split programmes – this feature addresses a long term issue of programmes transmitted in multiple parts, for example a film split by the news. Viewers	often	fail	to	record the second part of the programme. Freeview Playback ensures nothing is missed.
• Recommended viewing – the recorder can offer the viewer other programmes to record, related to other recordings that have been made.
• Compliance – before carrying the Freeview Playback logo, a recorder must pass a rigorous set of tests to demonstrate it complies with the technical standards and features.
12
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	13
NEW SERVICES
NEW SERVICES
Serving Our Audiences – User-centred Research
How do we stay in touch with our audiences? How do we ensure that what we produce is relevant to them? How do we help them navigate the current maze of media offerings?
This set of projects is pursuing a number of ideas, under the general heading of user-centred research. It also includes our behavioural science work, where we try to understand the basic human needs that drive media behaviour, and to assess their impact on our audience.
Discovering content
Content is becoming easier to access, but harder to find. There are more devices supporting even more types of media, and there are more ways to deliver it to the viewer. In addition, viewers are expecting more from media – they want links between related items, and they want to navigate instantly to the parts that most interest them.
This is changing how we describe and provide our content. And of course it’s not just about BBC content – our audience are contributing more, not just in terms of images, audio and video
but also opinions, facts and data.
In order to encourage innovation in the methods of discovering our content we are continuing to supply feeds of electronic programme data based on the TV-Anytime standard to the developer community on BBC Backstage. We have created a simple means for developers to query this data via a publicly available experimental Web Application Programming Interface (API).
The Web API allows developers to create third party tools and ‘widgets’ to display schedule information for all our channels.
We have been experimenting with how we can combine the data and stored content to provide interesting means of finding content, creating a simple ‘player’ that combines live video/audio streams with programme information in a webpage.
Content packaging
The success of Podcasting – downloadable audio programmes – suggests that people like to acquire content for later consumption. However, the playback is very basic, with limited opportunities for enhancements or to promote other services and content.
The idea behind content packaging is to replace the download of individual items with a personalised content package, based on the profile of the user. The presentation of the content is created by the content provider, in the appropriate format for the playback device, using
a player or browser application.
We have developed two demonstrators for content packaging, the first designed for mobile phones, the second for high-end platforms such as laptops.
We are using these demonstrators to develop further ideas and test their usability.
Games consoles
One of the BBC’s primary goals is to reach and serve the entire British audience. A growing group that are seen by many to be under-served by the BBC are computer games enthusiasts. This group tend to feel that their game platform is the primary platform for relaxation and entertainment, rather than the TV; the BBC is felt to be insignificant, if not irrelevant. One way to reach this audience is to provide content for their platforms of choice, using the content grammars that are familiar to them.
As a first step we are assessing our existing content, to find what is most appropriate editorially and technically for games consoles. We have successfully demonstrated play-out of live streamed BBC video on Playstation2TM and Playstation3TM consoles, using an open source player, and we are now looking to cover a broader range of consoles. We are also considering how the BBC’s por tfolio	of	web-based	games	can	be adapted for console play.
Interactive story-telling
With the benefit of this experience with games	consoles	we	can	star t	to	tr y	new concepts. We	are	researching	how	to
bring the BBC tradition of rich narrative into this domain, looking at how to author and deliver truly interactive, open-ended stories.We are taking part in an EU-funded project, CALLAS, which is exploring combining emotion-sensing interfaces with interactive narrative techniques to produce a ‘bardic interactive storytelling engine’.
This work is highly experimental and in its early stages, and it is difficult to predict what might emerge. Early ideas centre on having a narrator with whom the viewer or viewers interact by speaking.The story is driven by the outcome of these interactions, with
the system working from not only what is said but also how it is said. Virtual actors could be used for the narrator and the cast, so that action need not be pre-recorded. Each viewer or group of viewers will therefore see a different story unfold.
Mass participation
Our audience’s ability to send content to us, as demonstrated by the waves of mobile phone images we receive when major news stories break, shows the willingness of our audience to participate.
We are involved in a three year DTI-funded collaborative project called,very appropriately,‘Participate’. This project is exploring the convergence of online and broadcast media to create new	kinds	of	mass	par ticipator y	events where a broad cross-section of the public contributes and shares content. This might be centred on one of the large outdoor displays the BBC operates from time to time, for example.
We are working with BBC Future Media Innovation and BBC Production to define some of the narrative aspects of the initial	trials. We	are	trying	out	potential technologies to allow large numbers of people to access the system simultane- ously, even when on the move without compromising technical quality.
Behavioural science
Participate is a good example of the need to understand human behaviour – how do people engage with a public event such as the one just described? Our immediate challenge in Participate is to decide how to measure this, and what metrics one might use.
Behavioural science factors, such as usability, accessibility, user experience and
interaction design are now common and critical features of the mainstream BBC business. We are continuing to support a number of projects across the BBC, including switchover and new interactive platforms.
Image classification
Opening up the BBC to user-generated content creates a practical problem in sorting the large volumes that can be submitted in a short space of time. The material is often topical and must be dealt with quickly.
We have been working on a project to classify user-generated content to allow rapid selections to be made.This work is a collaboration between ourselves and the University of Surrey.
We hope that ranking submitted images by their similarity to wanted images, clustering related images together, might achieve quite good results while being relatively simple to achieve. A more elaborate technique, which might ultimately achieve better results, is to generate metadata by object recognition. Although we started here with mathe- matical methods, we are now exploring techniques that try to emulate the recognition processes in the human brain.
14
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	15
PRODUCTION
Production Magic
We are developing a range of production tools that harness the power of computer image processing to analyse a scene in 3D. With these tools, we can insert virtual objects into a live scene, generate a new view of a real scene from a virtual viewpoint, or automatically steer a camera or spotlight to follow a person around the set.
This work ranges from relatively short-term projects for immediate production needs, through to longer-term fundamental research that could lead to radical changes in the way programmes are made. It may also lead to new forms of 3D or immersive content that
go well beyond what is possible today.
Inserting graphics into live images Whenever a production team plans to insert a virtual object into an image, the graphics system needs to know precisely how the camera is moving, so that the virtual object can be rendered in the correct place in every frame. Although there are several tools available commercially for doing this
in post-production, live (or as-live) operation is more difficult. For studio- based productions, it is usually necessary to shoot in a studio equipped with
a camera tracking system, such as the free-d system we developed some years ago, which uses markers mounted on the ceiling. For other kinds of production, such as outside broadcasts or shoots on location, new solutions are needed.
We have been working with several European companies and universities in the MATRIS project to develop markerless camera tracking using naturally-occurring features in the scene. This three-year project was funded by the EU’s 6th Framework Programme, and finished in January 2007. In addition to tracking directly from the camera images, the project also developed an inertial sensor, and looked at the use
of an auxiliary camera fitted with a fish-eye lens. Both of these techniques can improve the accuracy and stability of the tracking. We coordinated the project demonstration at IBC 2006, which attracted the interest of several potential licensees.
‘In September 2006, the Piero system won an IBC Innovation Award, in recognition of its range of novel features that came out of BBC R&amp;I work.’
As a part of our work in the MATRIS project, we developed a method that measures camera movement by tracking the lines on a football or rugby pitch.This has been licensed to Red Bee Media as a
part of the Piero sports graphics system, and is now in regular use by the BBC and a number of other broadcasters around the world. In June 2006 the system was used in High Definition for the Football World Cup, and for the first time it used a method to automatically lock onto the pitch lines when starting to track.
In some applications, it is useful for a presenter to be able to pick up a virtual object, rather than it being fixed in the studio or on the ground.We previously developed and licensed a system known as MixTV, which tracks a specially- patterned card which a presenter can move, so that a virtual object can be rendered on top of it. In April 2006,
a version of the system was used in a three-month trial by BBC Jam, the BBC’s broadband learning service for five to 16 year olds. This allowed users to interact with virtual 3D objects in real time, using their own hands rather than a mouse
or a keyboard.
We have been working with BBC News to develop a system that tracks the cameras using an array of marker patterns placed on the walls of the studio or on other par ts of the set. This avoids the expense and complication
of installing a dedicated tracking system, and can be used in almost any location. Another approach being investigated is to display the marker arrays on an existing rear-projection system, so they can easily be switched off when virtual graphics are not required. The system could also
be used in conjunction with hand-held markers to allow both fixed and moveable virtual graphics to be shown.
iview – capture of 3D scenes and generation of virtual views We are leading a DTI-funded collaborative project called iview, to develop methods to capture action from events such as
a football match in 3D and to provide a 360 degree free-viewpoint video replay of the action. During the replay the position of the virtual camera can be moved freely and crucial moments of
a game can be seen from viewpoints which are not normally open to a real camera. BBC Sport has an obvious application for such a system, as it would allow them to fly a virtual camera around the pitch, giving a new way of presenting key moments. It could also generate top- down views for tactical analysis, or views as seen by a linesman, player or referee.
In its first year, iview applied techniques formerly developed for use in a chroma- key equipped studio for use outdoors. New and robust techniques were required for calibration of the cameras, keying and generation of the 3D models of the action. First tests in collaboration with BBC Sport and BBC Outside Broadcasts were carried out for football using either a set of specially mounted fixed cameras or just the cameras normally present. From the fixed camera set, virtual camera positions (such as the goal keeper’s view) can be generated to provide new insights into the game that are not possible to achieve otherwise. Current work is focussing on tests with HD cameras.
The processing and generation of the 3D model of the action is currently carried out off-line, but options for real- time processing are being investigated. The replay uses image-based rendering methods that can be implemented at interactive rates on standard PCs. One goal of iview is to stream the replay data to an interactive platform such as a games console so that the user can determine his or her preferred viewpoint. We are also looking at applications in sports other than football, and in other
programme genres including studio- based programmes.
Tracking people and objects in 3D By tracking people in a scene, it is possible to automate some production processes. We have been investigating applications involving the automatic control of lights, by tracking the positions of the camera and presenter. The ability to automatically keep a presenter lit as he or she moves around a studio offers the prospect of reducing the overall level of lighting needed, whilst ensuring that a sufficient level of light is used. This is especially useful to satisfy the requirements of HD production where maintaining a good depth-of-field becomes particularly important.Working to requirements from BBC News, we have demonstrated a prototype system that can do this using information from the cameras.The system then adjusts the lighting by controlling the direction, beam width and brightness of a number of motorised spotlights.
We have also been investigating the use of audio location techniques to track the position of people. We have built a real-time demonstrator that uses two
16
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	17
PRODUCTION
Production Magic continued
pairs of microphones to estimate the location of a speaking person, using time delay analysis. This could be used to locate and track a TV presenter, or members of a studio audience when they speak, or in outdoor sporting events to help automate camera framing and
shot selection.
Although these developments are part of our long-term work to create new ways of making programmes, the technology we develop along the way can also be rapidly deployed to address short-term problems. One example of this was a requirement for a simple way of gathering votes from an audience. We developed a system to identify
a hand-held baton and determine its orientation, so audience members could vote by holding their baton either vertically or horizontally. The batons were covered in a highly retro-reflective material, illuminated by a ring of LEDs around the camera lens, allowing the system to operate independently of
the ambient light level.
D-3 Videotape Preservation System
Surround video
We have been looking at ways to use imaging technologies to present new forms of content to our audiences. Most people are already familiar with the concept of surround sound, where additional speakers are deployed around the room, fed by additional sound channels. We have been investigating whether a similar approach could be applied to video.
Rather than simply looking at the use of a very large screen display, we have investigated the concept of augmenting an existing conventional display with an image projected onto the walls and ceiling of the room. The projected image is generated in such a way as to extend the field-of-view of the existing display, helping to provide a ‘context’ for the main image, such as additional motion cues. We recorded a range of sequences with two rigidly-coupled cameras, one fitted with a conventional lens and the other with a wide-angle fish-eye lens. The image from the first camera was
shown on a flat-panel display, whilst the image from the second was projected onto the walls and ceiling, using a projector and a spherical mirror. Real- time image processing was used to compensate for the distortion introduced by the fish-eye lens and projection system, ensuring that it aligned in position and scale with the image on the flat panel display. We also investigated several methods of synthesising a surround image from existing content, shot without a secondary camera.
Initial tests have been very promising, with audiences reporting a much greater sense of immersion.
3D production planning software The quality and ease of use of 3D computer modelling has now reached the point where we can use it for pre- production planning. We are working with some of the developers of this software to see how it can best be applied to making production more creative and to reduce costs. We have started with studio layout and set design, although there are also other potential applications such as training.
Producers can test different options in a virtual space, helping them to make effective choices earlier in the production process. Our relationships with the software developers have allowed us to model real studio facilities such as cameras and cranes. This enables producers to see the exact shots they will get using particular equipment, so they can hire in without over-specifying.
Some programme makers have tried out the set design process. They found it not only useful in visualising the design, but also in communicating their ideas to others in the team. Decisions can be converted to floor plans for creating and building the real set. The virtual model is also easier to store than the traditional card- board models, and can be retrieved, for re-use or as a reference for the next series.
The BBC Television and Radio archives are among the largest collections of broadcast content in the world. They contain more than 300,000 hours of radio and 600,000 hours of television, and these figures grow daily.
The archives hold content on a variety of recording formats, which reflect technological advances made over the years. Great importance is placed on being able to retrieve content of any age, and it is often necessary to maintain access by transfer of content held on old formats to newer ones, because of physical media decay or obsolescence
of replay equipment.Transfers must be done in real time with human supervision, because a sub-optimum replay or poor conversion will irreparably damage the content. With such a sizeable archive, this can be costly.
In an attempt to improve matters, the advantages of migrating to file-based formats have been explored. When content is held as a data file the content format is no longer dictated by the storage medium, and in future it should be easier and faster to move to a new storage medium, perhaps at the touch of a button. Content on file servers is more accessible than that held on videotapes stored on shelves. Quality degradation ought no longer to be an issue because true digital clones can
be produced from data files.
The system is based around a high-end PC with an SDI capture card and specially developed software. D-3 is a digital composite PAL format, and BBC-designed Transform PAL decoders are used to give the best possible video quality. The captured signal is written into an MXF (material exchange format) file, together with descriptive and technical metadata that the system collects automatically from the BBC’s ‘INFAX’ records – for example, programme and series names, transmission date and duration. The system also adds the error log from the tape replay, and analyses the video signal for picture flashes and certain spatial patterns, to note the level of compliance against OFCOM recommen- dations for photosensitive epilepsy. It also produces a low bit rate ‘browse quality’ copy of the content, and includes soft- ware for performing quality checking
of the MXF file.
The system uses open standards wherever possible, to avoid technology ‘lock-in’ and to ensure that the files can continue to be accessed and understood. All the software written by the BBC in this work has been released as open source.This
is to promote interoperability and to encourage further development by other par ties,	in	par ticular	to	exploit	the	features of MXF for searching the content.
Initially the files are to be stored on LTO-3 data tape. When costs permit, they may be moved to file-servers. Up to ten programmes can be stored on each tape, a forty-fold saving on physical space over D-3, even though the video data is held uncompressed. LTO-3 is by far the most popular data tape format on the market, and is supported by a number of manu- facturers. It has an open architecture, and Ultrium, the consortium responsible for the LTO-3 standard, has decreed that new generations of the data tape drives must be able to read tapes from at least the previous two generations.
‘ The system uses open standards wherever possible, to avoid technology ‘lock-in’ and to ensure that the files can continue to be accessed and understood.’
Content held on D-3 videotape is next in line for preservation. The D-3 format is only 16 years old but is no longer supported by its manufacturer. The BBC holds about 380,000 D-3 tapes (some copied from the earlier analogue two- inch quadruplex format) and 100,000 have been selected for content retention.
We have devised a system to capture the content from the D-3 tapes, and to augment it automatically with metadata.
18
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	19
PRODUCTION
Programme Production
Our emphasis has continued to be on exploiting commodity IT hardware so that the cost of equipment used for production can be brought down.
Where, in the past, four videotape recorders might have been used in a studio, the trend of increasing computing power means that a single mid-range PC can now perform this entire task.
This capability will bring about a dramatic change to the way pro- grammes are recorded and pass through the production chain. Hence, we are continuing to work closely with colleagues in programme production, so that we can understand their requirements and see how the potential of the new technology can best be exploited.
Planning for music shows
Our work in previous years has produced a production planning system aimed at making the preparation of the camera script for a music show more efficient. It uses beat tracking to link the music breakdown to the lyrics and then to a standard video editing package, to allow camera shots to be planned against the music. Once the planning is complete a camera script is produced automatically.
We have now refined the software and developed quicker ways of working with it.We have encouraged more widespread testing of this improved version by production assistants and directors to
see if the benefits can be achieved in practice.This has brought some mixed responses. However, interest is growing and after a little guidance and ‘hands-on’ experience, some directors are now trying the system with a view to using
it for their productions.
In working more closely with production teams, we have also come to understand better what they require for different types of music show.This has led us to develop a more straightforward system where only a relatively simple annotation needs to be added.This is typical of
shows that have a large number of musical items, where full planning of each would be too time-consuming. As well as beat and bar indications, our simpler system allows markers with comments to be added anywhere in the music breakdown, and the standard production script is later generated from this.
We now have music production teams testing this simpler system, and we hope to be able to work with them on some major music productions over the forthcoming year.
Tapeless recorder
We have made several enhancements to the tapeless recording system that we trialled last year with the BBC Children’s programme BAMZOOKi.
We have now produced a tapeless recorder capable of recording four video and 16 audio channels simultaneously, compressing them in real-time to the required production format (typically DVCPro50 or Avid ‘2 to 1’), and storing them as MXF files. Although a PC equipped with dual-processors, and each with dual-cores, is required to achieve this, the overall cost is on a par with a mid-range server and is dominated by
four SDI capture cards. By equipping the recorder with 2 Tbytes of hard disc, it can record for up to two or three days in a studio.
Low cost server
A further development of this work has been a video server based on the widely- used SAMBA file system.This offers a way in which a low-cost server can be used to share content across a cluster of edit suites that are using either Avid or Apple editing software.
There are two difficulties in sharing content in this way, which we have been able to overcome by exploiting the virtual file system extension capabilities within SAMBA. Firstly, each Avid editing client expects exclusive use of the external storage. Shared access by multiple Avid clients leads to conflict, as each one attempts to create its own database files to index the video content that it finds. Our configuration of the server’s virtual file system keeps these files separate, so no conflicts arise.
The second obstacle is that not all editing systems can work directly with MXF files – for example, Apple’s Final Cut Pro. To overcome this, we have
implemented on-the-fly unwrapping of the MXF files within the SAMBA server. This again exploits the virtual file system extensions of SAMBA, but in this case, when the original MXF files are accessed by the editing system, the server removes the outer ‘MXF’ layers and provides only the encoded content,
e.g. DVCPro50, from within. In this way, the MXF files held on the server appear to the editing system as DV encoded files, which it can then use directly.
The next stage in the production flow is to make the recorded content available in a convenient way within the editing system. To	achieve	this,	we	are	making use of the AAF (‘Advanced Authoring Format’) standard. When required, an AAF file is produced which contains the details of all the MXF files recorded in the studio over the interval of interest. Opening the AAF file in the editing system then populates the editing ‘bin’ with this content, making it immediately available for editing.
Additionally, for multi-camera productions (where each camera view is recorded simultaneously), the resulting recordings are automatically grouped together, which saves time by avoiding the manual step of
grouping and synchronising the recordings within the editing system itself.
A typical multi-camera production method is to produce a ‘Main’ recording, which includes cuts between the cameras performed by the vision mixer, and separate, isolated recordings of some cameras (‘Isos’).These latter recordings may be used in the editing of the programme to give different views, e.g. an audience reaction shot, when appropriate. However, a drawback of this method is that the timing of the cuts done as the recording is underway may not be ideal and slight trimming in post- production	may	be	desirable. We	can offer this capability by recording the timings of the cuts applied by the vision mixer, and from these, creating corresponding edit entries in the AAF file. When this AAF file is opened, the editing timeline will show the original camera recordings with the edits between them, which can then be trimmed where necessary.
To understand better how these developments perform in practice, we are working alongside the production team for the BBC programme Eastenders, recording with our tapeless system in
parallel with their normal tape-based process. It is too early to have clear results but the advantages of easy access to the recordings are obvious, with immediate replay into the control room and studio being a notable benefit. In the edit suites, rapid access to the recordings also saves significant time. In this case, the use of the MXF standard for recordings is a particular advantage as the files can be used directly within the editing system, eliminating the need for
a file import stage. Although not as time consuming as ingesting from tape, file import would still slow down the process and reduce efficiency.
Editing from P2 memory cards Another development was at the request of BBC Sport. For the coverage of World Cup 2006 they needed to be able to edit content from Panasonic P2 camera memory cards in an Avid editor, and transfer the edited version back to a P2 card for playout on location.This last step was proving impossible because of the way MXF is used on P2 cards. After studying the problem, we were able to produce an application to perform the necessary conversion and copy back to the card.The application
20
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	21
PRODUCTION
Programme Production continued
Radio Spectrum for Production
was used with great success, and means our production colleagues can now work more flexibly with P2 cameras.
We have made our software available as open source so that others can benefit and share in its development.These packages, along with related software, can be found at the ‘Ingex’ project page http://ingex.sourceforge.net/
This work is described further in a paper that was presented at IBC 2006, where we also demonstrated the complete system in the New Technology Campus.
As our work in this area progresses, we are continuing to explore with other programme production teams how we can work with them to improve our mutual understanding of the requirements and benefits of tapeless production techniques.
Floorman
Floorman is a wireless picture monitor developed by BBC R&amp;I which may be used in one of two ways, with different technology for the two applications. We are now seeking to license the two versions.
•	On the studio floor or on location, by a producer/director, usually in preparing and rehearsing multiple camera shots. It is planned to commission a manufacturer
to produce and install a licensed system in a BBC studio.This will be available for use by production teams, to allow further assessment of its market potential.
•	By a presenter at a live outside broadcast, for reviewing the action on which they will be commenting. Our prototype has been used regularly by horse racing presenter Clare Balding, and BBC Outside Broadcasts are currently using it for a market trial. If the trial is successful we will enter into technical discussions with a manufacturer to hand over licensed designs.
Timed text and subtitle multicasting This covers our work in handling subtitles and similar text information where timing details are also included.The work is in two main areas.
Firstly, we have been providing the software tools to convert broadcast subtitle files to the formats required for use by the iPlayer, and for programmes provided online via bbc.co.uk. Subtitles used in each of the trials of iPlayer have been originated in this way and we have continued to refine the conversion process as the user requirements
have evolved.
Secondly, we have continued to participate in the ‘Timed-Text’ working group of the World Wide Web Consortium (‘W3C’), which is in the late stages of developing a standard for carrying timing details with text.This is applicable not only to subtitle exchange but also production processes, e.g. associating scripted text or indexing details with the recording of the content.
Building on this latter work, we are working within an AAF Association and EBU subtitling adhoc group which is producing a standard means for conveying subtitles in AAF and MXF files. This is based on the forthcoming W3C timed-text Recommendation but further work to specify details within the AAF and MXF standards has also been required to complete this functionality.
We are looking at ways for the BBC to continue to improve and expand its use of digital radio cameras despite changes in the spectrum we license from OFCOM.
The past year has seen great changes and challenges for wireless production, with the closure of our favoured band for PMSE (Programme Making and Special Events), 2500 to 2690 MHz. This loss of spectrum together with the increased popularity of radio cameras, the risk of yet more losses, increasing spectrum costs and the desire to make programmes in HD are presenting real challenges to the industry. Alternative spectrum between 2 GHz and 3.5 GHz is now being used instead, but this spectrum is fragmented and congested. Interference from high power CDMA services is causing concern at many sites.
An additional worry is the spectral purity of higher power COFDM systems that use these microwave bands. Commercially available systems have been measured, and some found to radiate excessive out- of-band energy in the adjacent channels. This limits adjacent channel operation, effectively halving the number of full performance channels that can be used at any one time.
allowing broadcasters to make better use of the limited PMSE spectrum that remains available.
Growth in broadcast applications, and the competing demands for 2 GHz spectrum from mobile and WiMax applications, is prompting us to consider a move to higher microwave bands. Initial work at 10 GHz has identified implementation, link budget and Doppler issues.We are now focussing on the
7 GHz band, working in collaboration with UK manufacturers on a solution for next generation HD Wireless Production. It is planned to exploit MIMO research work, described later, to provide new technology for PMSE use in these new microwave bands.
‘ We are now focussing on the 7 GHz band, working in collaboration with UK manufacturers on a solution for next generation HD Wireless Production.’
BBC R&amp;I has been working closely with JFMG Ltd (the company that manages the spectrum on OFCOM’s behalf for programme making and entertainment uses) to identify these problems
and alert the relevant manufacturers.
A measurement programme also looked at protection ratio issues and the effects of 3G CDMA cellular services on current receivers. By working with industry in this way, we hope to improve the performance of commercial equipment
22
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	23
DELIVERY
DELIVERY
Digital TV – Switchover
From autumn 2007, UK broadcasters including the BBC will start to switch off their analogue terrestrial television signals.This process, known as ‘Digital Switchover’, will take place progressively across the UK, and is expected to be complete by 2012.
During this process the two digital terrestrial television (DTT) multiplexes the BBC operates are expected to migrate from 16-QAM to 64-QAM modulation, with a consequent increase in deliverable bit rate, and our services will inevitably continue to develop and evolve.We have also been asked to accommodate services from several other public service broadcasters in
our multiplexes.
To these ends a significant re-engineering of our digital television delivery system has been started which will result in greater multiplex efficiency and network resilience. It will also greatly simplify both the procedures involved in Switchover and the BBC’s general ability to make technical changes to its digital services. We have contributed to the technical design of the new system, working with our technology partner Siemens and colleagues in BBC Operations.We are making extensive use of our digital TV test system to evaluate suitable techniques and technologies, modelling various multiplex occupancies to assess picture quality and strategic flexibility (the ability to change the composition
of the multiplex for new services or service components, or to enhance or transfer existing ones).
Helping the viewers
As the analogue transmitters are switched off, many digital transmitter powers will be increased and the digital coverage is predicted to be similar to the current analogue coverage, approaching 100%. However, it is estimated that about 10% of domestic aerial installations will need attention to receive the digital
transmissions. It is therefore important to give the best possible guidance to viewers to help them decide if their aerial needs to be checked.
The reception of existing analogue broadcasts can be a very useful guide to the condition and effectiveness of a domestic aerial installation, and we have been researching several techniques based on this.We first tried a test pattern that was designed to become less apparent as signal quality fell. Whilst this technique showed the correct average trend – those with good aerials tended to correctly identify the pattern – there was too much spread on individual results to give a reliable indication.
We have now created a test pattern using teletext. This gives an objective result, because the pattern is either received correctly or with obvious errors. The pattern has been designed to be as resistant as possible to delayed image interference (otherwise known as ‘multipath’ or ‘ghosting’), which normally affects teletext reception significantly, but which the digital television transmissions are designed to ignore. It also appears to be reasonably tolerant of co-channel analogue signals.
Tests have been successsful, with errors starting to show just at the point where an aerial installation ought to be checked. The test signal is about to be transmitted nationwide on the four original analogue services. (It would not be appropriate to transmit it on the analogue ‘five’ service, as the transmitter powers are at lower levels than for the other services, and
the results would be misleading).
Our work in this area forms a part of the overall work being conducted within the Digital Reception Prediction Group run by DigitalUK, the non-profit organisation leading the process of digital TV switchover in the UK.
The digital switchover assisted help scheme For the elderly and those with disabilities, Digital Switchover will present significant challenges. In September 2006 the UK Government announced the Digital Switchover Assisted Help Scheme, also known as the Targeted Help Scheme. This will offer people over 75 and those with significant disabilities help in the form of new receiving equipment, installation and follow-up support. Central to the scheme will be the procurement of equipment designed
particularly for these vulnerable viewers, to promote accessibility to digital services for everyone.
Building on specific needs identified by the Consumer Experts Group (representing the elderly and those with disabilities) and in consultation with the DCMS, DTI, Digital UK and receiver equipment manufacturers, we have collated and produced a set of core requirements.These are intended to maximise ease-of-use for the great majority of those eligible for help, without unduly prescribing any particular implementation that might be offered in a process of open tendering, and without stifling ingenuity and innovation.
After a period of consultation the DCMS has published an invitation to tender for the supply of equipment on the basis of these requirements. (The 2006 Core Receiver documents are available on the UK government’s digital switchover website: www.digitaltelevision.gov.uk).
Following the Licence Fee settlement, the BBC and DCMS are currently negotiating the commercial details of administering the Assisted Help Scheme, whilst some of the technical details of satisfying the
Core Requirements are under discussion in various industry fora.
Impulsive interference
Experience has shown us that the effects of impulsive interference need to be allowed for when specifying or designing systems for digital terrestrial television. Impulsive interference can come from
a wide variety of sources, for example car ignition systems, electric motors, light switches or room thermostats. Typically, it breaks the picture up into a mosaic of blocks, or causes a momentary loss of sound.
We have developed a receiver that measures the effects of impulsive interference against two key trans- mission parameters, signal level and transmission mode.The receiver can be left logging its results on a laptop PC over many days, to allow us to build up a statistical model.
The assembly of a pair of these receivers has only recently been completed, and the next stage will be to deploy them in domestic environments to make measurements under a range
of conditions.Tests in the laboratory have already confirmed one expectation
concerning transmission modes. There is a significant benefit in terms of resistance to impulsive interference by switching from the 2000 carrier mode (‘2K’) to the 8000 carrier mode (‘8K’), which we hope will be one of the changes implemented during switchover.
Mobile and handheld television broadcasting We have maintained a watching brief across the various emerging technologies that could be used for mobile and handheld television broadcasting, and provided advice and guidance where appropriate. If mobile transmissions are implemented using a cellular network radiating in the spectrum released by switching off analogue television, there is the possibility for interference to existing digital television reception. We have made measurements to analyse the conditions under which such interference will occur. We have kept the relevant standards organisations (mainly the DTG and EICTA) informed of our results and this should allow the effects from this type of interference to be minimised.
24
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	25
DELIVERY
Digital TV – Architectures
Audience Research
Freesat
The BBC has been investigating the potential for a free satellite service, to complement Freeview in areas where DTT is currently unavailable. From our experience of launching Freeview we know that most of the technical building blocks for a free satellite service already exist. However there are significant differences between DTT and DSAT that we are having to take into account. Another important part of our work has been to define appropriate behaviour for different classes of receiver (for example, some receivers may have storage or be capable of decoding HD transmissions) and to consider the test requirements for each of these. Currently the decision on the future of Freesat is under review by the BBC Trust.
Designing for change
When Nicam stereo sound and teletext were added to analogue television broadcasts, this could not be done within the existing signal standards. Although the changes were made successfully, considerable engineering ingenuity was needed and there were constraints on what could be achieved. Viewers had to buy new TVs if they wanted the new features, and the extra signals had to be added in a way that did not affect reception on existing sets.
When the DVB standard (the inter- national standard for digital television broadcasting) was drawn up, the engineers responsible made sure it could be extended in a systematic way to incorporate new features.The standard also defined a ‘multiplex’ structure that makes it possible to add, modify and rearrange TV channels and other broadcast components.This means that for many of the changes viewers do not have to replace their receiving equipment. Changes are picked up by
Centralisation of coding and multiplexing The BBC is planning to centralise the equipment that codes and multiplexes its digital TV services. In advance of this, we have taken a significant role
in the testing, configuration and technical sign-off for coding and multiplex equipment in BBC Scotland’s new headquarters in Pacific Quay.
This experience is proving valuable even though we know the technology will advance in the meantime. During the coming year, working with our technology partners Siemens, we will be setting up a new test-bed at Kingswood Warren to explore the options for the architecture and equipment for the centralised system.
digital TV receivers automatically, or by ‘rescanning’ or downloading new internal software.
The BBC continually exploits this flexibility to develop its digital services. Since the original launch, BBC R&amp;I have helped to develop a number of new features, including interactive services, enhanced electronic programme guides, new regional broadcasts, and audio descriptions of programmes for visually- impaired viewers.
However, a great deal of planning is needed even for a small change, especially as we always try to avoid any loss of service. BBC R&amp;I continues to carry out some of the development work, and to give advice and practical assistance. Using our experience and our specialised test facilities, we make sure new features work as intended, any new equipment is suitably chosen, and the best use continues to be made of the digital multiplex capacity.
DTT Multiplex B re-engineering In summer 2006 the BBC decided that BBC Parliament should be played out on DTT in full screen, instead of the quarter-screen ‘red button’ service that had been operating since the launch of Freeview. The only way to do this without sacrificing the picture quality of all the services in our Multiplex B was to change the coding and multiplexing system for newer and more efficient equipment. This meant going to another supplier for all the major technical functions, including the primary encoding of the video and audio components, the multiplexing, and the capacity switching. Capacity switching was being used to carry CBeebies during the day and BBC Four at night, using the same capacity. With the new equipment it proved possible to extend these time- sharing arrangements to other services, in	par ticular	to	make	some	capacity available for the trial of BBCi catch-up. The improved efficiency of the new equipment has also allowed the BBC to increase the number of News Loops from two to four. We modelled a number of different options to help our colleagues in BBC Distribution decide on the final capacity allocations, before helping to plan the changeover and put it into effect.
How do you measure audience viewing figures when your audience has many ways of watching your programmes? This is the challenge facing BBC Marketing Communications &amp; Audiences, with broadband and mobile phone access becoming increasingly popular and time-shifted viewing getting easier. Programmes
are augmented with subtitles, interactive content, and elec- tronic programme guides, and we need to track usage of these too, if we are to under- stand our audiences properly.
The ARENA project
The BBC has joined with nine other media organisations in Europe in a European Union collaborative project, ARENA, to try to devise a system that produces audience figures consistently across all these services and platforms. [Note that viewing data is collected only from household members and individuals who have their given informed consent to allow this.]
ARENA is planning four field trials on different platforms, linked through a common audience data collection system. The BBC has a particular interest in one of these field trials, on the new BT-Vision platform where we supply some of the on-demand content, as
well as operating two of the DTT multiplexes also receivable through the BT-Vision ‘V-box’.
This is an ambitious project, and the processes will be simulated before the system itself is built. Our work so far has been to establish the business and functional requirements for cross- platform audience research, talking to other organisations too to try to predict how	ser vices	will	develop.	The	project has used this information to create a formal data model. Our work is now
to ensure that the model is a good representation of the real-world situations and that it is tested correctly – the eventual outcome from the other partners will be software that can be ported into set-top boxes or the platform operators’ distribution systems.
Audiences for interactive services At the request of BBC MC&amp;A, and in parallel with the formation of ARENA, we have been developing an idea to measure ‘red button’ interactive TV usage	on	our	digital	television	ser vices. This uses a small ‘index mark’ added to each page of interactive content, created by the video rendering engine in the
digital set-top box or digital TV using instructions conveyed as part of the interactive page description sent to the receiver. Last year, we produced a specification for the index mark on the basis of laboratory tests, and the Broadcasters Audience Research Board (BARB) commissioned a prototype detector from their technical contractor AGB-Nielsen. About 200 of these detectors have now been installed in the homes of existing BARB panel members (these are households that have agreed to have their viewing patterns monitored by BARB), for a limited trial. With the co-operation of other UK broadcasters, the index marks were added to some interactive content on terrestrial and satellite broadcasts, and also on to a digital cable system. Tests in collaboration with AGB-Nielsen were successful, after some minor adjustments to accommodate variations in the different signal chains.
The index mark is designed to be just off-screen on the majority of television sets, and indeed, no complaints about visibility were received, although potentially all digital receivers will reproduce it, not just those in the BARB trial. BARB is currently re-tendering its technical contracts and a decision on the use of this technique will depend on the outcome of this process.
Audio watermarking
Audio watermarking, i.e. hiding infor- mation in the programme sound, has been tried unsuccessfully for radio audience measurement in previous years. The systems offered have been found to produce unacceptable impairments to the sound quality. Finally, this year, a system that had been tested several times before was found to have been improved sufficiently to be inaudible in our laboratory assessments. On-air trials were able to start with much less worry about the possible negative impact than has been the case previously.
26
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	27
DELIVERY
DAB and Digital Radio Mondiale
Kamaelia
Kamaelia is the software toolkit we created for exploring on-line delivery methods for the BBC’s archives. How do you make the BBC's considerable archives available on-line to, say, 20 million households at once, especially if they all want to watch something different? The solution needs to be scalable, acceptable to organisations that have rights over content the BBC holds, and flexible enough to exploit changes in technology.
AAC coding and DAB+
BBC R&amp;I submitted one of several proposals in response to the WorldDMB* call for contributions for adding a more efficient audio coding system to DAB. The work was co-ordinated by the WorldDMB Technical Committee in a collaboration across Europe, Asia, North America and Australasia.
Our proposal was adopted by WorldDMB after a careful analysis of all the competing solutions and after field tests carried out in the UK and Australia.
The result is DAB+, announced in March 2007.We have taken responsibility for standardising DAB+ through ETSI.
The main part of the proposal is an option to use high-efficiency AAC coding for radio broadcasts, creating capacity that could be used for more stations, interactive services or surround-sound broadcasting. Audio quality is improved at lower bit rates, and an improved error correction scheme gives slightly greater coverage, quicker tuning and no relative timing delays between stations.
For the BBC this work is decidedly long term, to ensure that technical standards that may one day allow us to introduce new services are carefully considered before being agreed.While countries that have yet to launch DAB could opt immediately for DAB+, the BBC remains committed to its MPEG Audio Layer II broadcasts. It is well aware of its responsibility to its listeners, many of whom will only recently have bought new DAB radios.
* Formerly the WorldDAB Forum.
Digital Radio Mondiale
Digital Radio Mondiale (DRM) offers clear, high-quality audio and data services on the long, medium and short wave bands. DRM is now a fairly mature standard and there are many hundreds of hours of transmissions on-air.
Consumer receivers are crucial to the large-scale uptake of DRM, and are beginning to become available.We have driven this development in two key ways.
Firstly, all the current receivers are based on code which we originally developed and licensed to manufacturers, and we continue to support our licensees with their implementations.
Secondly, we have built up an extensive receiver testing facility at Kingswood Warren, allowing us to perform tests including sensitivity, selectivity and linearity. In particular we have been able to expose receivers to known field strengths in order to determine if they will work in the field with the signals that broadcasters plan to deliver. This has revealed a number of shortcomings in the early prototypes, and
we have worked closely with manufacturers to improve their performance.
We have also continued our development of receiver algorithms, looking particularly into the possibility of seamless AFS (alternative frequency switching).This could enable a low-cost receiver with only one front-end to take advantage of the benefits of multiple-frequency transmissions, which we have previously demonstrated with our dual-front-end diversity receiver.
Whilst the BBC World Service is our main customer for DRM, we have recently helped BBC Distribution to prepare for a consumer trial of DRM on medium-wave, being carried out in partnership with BBC Radio Devon and National Grid Wireless. We have given advice based on our long experience of DRM as well as providing monitoring stations for the trial.As part of this we have developed a monitoring receiver network to allow reception conditions in various locations to be observed in real time.
We started to develop Kamaelia about two years ago, and at the end of 2005 BBC Radio &amp; Music Interactive had used it to produce a prototype system for transcoding all BBC radio output.
Over the past year, we have created a television equivalent for BBC New Media. This was a significant piece of work needing a large number of optimisations to produce a scalable system. Since then Kamaelia has been used in a number of other projects.Tools for re-framing video for mobile devices were created for researchers looking into mobile delivery. A system was created for capturing BBC broadcast content, storing it, transcoding for different bit rates, and preparing it for peer-to-peer transfers. A rather different application, cross-site white-boarding to allow people in different locations to collaborate more easily, was quickly created, then expanded to include audio and made peer-to-peer aware with local remixing for scalability.
We continue to move Kamaelia out of the laboratory and into the wider BBC, academia	and	industr y. We	have	given talks at a number of high profile conferences for software development,
and we ran a workshop in Brussels for interested	collaborators. We	mentored a number of students during Google’s Summer of Code 2006, resulting in tools for peer-to-peer online distribution, 3D user interfaces, and proofs of concept for trusted communications.This also proved that Kamaelia can be learnt and used effectively in a very short period
of time by the average developer.
Our next steps are to consolidate Kamaelia’s core and to increase testability, looking for a similar level of verifiability to hardware.We are also aiming to make some aspects of Kamaelia to the non-programmer – we want to take the graphical construction tools that are used in Kamaelia to sketch new systems, and extend them to create the code directly.
Because Kamaelia is open source, this enables	ever yone	from	schools	and colleges through to enterprising members of the public, to use it to create applications including ones to repurpose our	content	to	ser ve	their	needs.
To collaborate with us, visit http://kamaelia.sourceforge.net/Home
Kamaelia, parallelism and concurrency Distributing the BBC Archive content online is an inherently parallel process. Large numbers of people are simultaneously searching and downloading material, and this is a huge test of a distribution system, both its hardware and software.
Software engineers define concurrency as the real or apparent running of multiple processes at the same time. Modern computers, particularly professional machines and games consoles, have multiple processor cores, whereas computer software has its roots in single processors and sequential processes.The reason that concurrency has been made to work is down to putting fundamental rules in place.
Kamaelia was developed by BBC R&amp;I to make an online distribution system straightforward to create and easy to maintain.A happy side effect is that it can be used to develop other applications for communication where the biggest problems arise from parallel processes. It is this aspect that we are now exploiting, and encouraging others to exploit by making Kamaelia open source.
28
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	29
CORE TECHNOLOGIES
NEW SERVICES
Networks for Programme Production
Modern programme production tools no longer consist of specialised broadcast hardware interconnected by coaxial cables. They are now almost invariably computer applications running on PCs, and they use file-based media storage rather than tape. As a result, IT network connections are now an essential part of any production system.
Media files can be extremely large, putting patterns of demand on an IT network quite unlike conventional IT traffic. BBC R&amp;I has for a number of years investigated the problems this brings, and worked with suppliers, manufacturers, and standards bodies to establish solutions.
IT networks are usually packet switched, and their properties are significantly different from the circuit switched networks we have traditionally used to carry our signals. Being primarily designed for regular IT traffic, they do not always perform as well when faced with large media-content files. Over recent years we have bought or developed a number of highly specialised test tools, and we are continually studying the latest offerings to judge if the claims made are justified as far as media production is concerned. One of our developments is a software application that can be ported out to multiple PCs to simulate a busy production office or even an entire site. We are now investigating next-generation multicast streaming protocols, and, with HD production in mind, the performance of 10 Gbit/s IP routers.
Through this work, we ensure the BBC understands the issues surrounding net- work provision, and specifies appropriately from our technology supply partners.We regularly feed back the results of our tests to manufacturers, or do tests with them, to help tune products for media use.We have also found it important to take part in the work of the relevant Standardisation bodies, because of the
strong influence that standards have on the design of IT equipment.
We also run a number of research and innovation projects:
PRISM
We lead the DTI collaborative project PRISM, which is applying distributed- computing ‘GRID’ techniques to media production. Although the project itself will finish at the end of 2008, it is part of our longer term work to try to streamline and enhance the technical processes in programme production. The other aspect of the work is to encourage standardised interfaces, so that production tools will work together even if chosen from different suppliers. PRISM will publish the interfaces it develops as
a set of open specifications.
As a simple example of what PRISM is seeking to achieve, imagine creating a documentary programme from content which has been shot over a period of years on different systems. It would help if the editing system applied any necessary format conversion automatically, without the operator needing to intervene to take decisions or star t the process. With GRID, the editing
system could delegate the tasks of comparing formats and making the conversions, especially if the content was being retrieved from remote locations.
A more challenging example is adding a formal description of media content as it goes into repositories. Descriptive metadata	is	impor tant	for	searching	and retrieval. It is however extremely labour- intensive to create, and this is a definite disincentive. One of the partners in PRISM has considerable expertise on machine-interpretation of images, and we are very interested to find out if their techniques can be applied here. The interpretation is not entirely auto- matic, but needs only a small amount
of prompting on context from a human operator. It is nevertheless computationally intense, and can benefit from GRID techniques. PRISM aims to demonstrate
a scalable high performance system built on a standard IT core.
PRISM is working closely with BBC Northern Ireland, and will be tested in conjunction with a number of programmes that they produce.
WiFi for TV programme production WiFi is rapidly becoming a de facto standard for connecting Media Centre computers to TVs and audio systems.This is not an application WiFi was originally designed for, but some additions to the standard have made it just about suitable for domestic use.We have been looking with a number of companies at other ways of extending the standard, to allow us to use WiFi in TV production.
WiMax
WiMax is similar to WiFi, but works at higher powers and has greater range and capacity. Its transmission in the 5.8 GHz band is licence-exempt, and we have carried out a series of tests to see if it could be used for ad-hoc contribution and distribution links.Transmissions at this frequency can be affected by buildings, so WiMax uses COFDM to benefit from reflected signals.
We arranged for a transmitter aerial to be installed on a mast on high ground in Nor th	London,	and	mounted	a	receiver aerial on a 10 m mast on a radio van. Tests at points chosen at random within 5 km of the transmitter all gave a throughput of at least 10 Mbit/s, and
some achieved 20 Mbit/s.These bit rates are all adequate for high-quality contribution and distribution circuits.
We then moved to the Westminster area, where BBC News are trying to find a quick and economical way of setting up links back to their local studio. UrbanWiMax, a company that provides Internet services for business users, provided us with a temporary bidirectional link.Their WiMax network has a nominal maximum capacity of
4 Mbit/s (limited by the backbone rather than the WiMax itself), and is intended to provide spot capacity and diversity from cable connections. It covers most of the locations in Westminster that BBC News regularly use.
This is a much more built-up area, and tests with the radio van showed some locations where no signal was received. However, where a signal was obtained, in most instances it would suppor t up to 2.7 Mbit/s video and 192 kbit/s audio streamed simultaneously, definitely acceptable for news reports where a lower picture quality is acceptable and a video bit rate of about 1 Mbit/s is usually considered adequate. We also noted that the audio quality
exceeded our expectations even when reduced to 64 kbit/s.
Audio over IP
In 2004 some of the BBC’s local radio stations started to exchange audio content over standard IP networks, using broad- band ADSL connections. We were asked to help when it was found that some codecs were more affected by network conditions than others, and that codecs from different manufacturers did not always work together. This was worrying – we were hoping that IP technology would be an easy replacement for ISDN lines when these start to disappear.
We have therefore carried out a series of investigations, running tests on a range of commercial codecs.We are now working to establish an interoperability standard and a recommendation of good operational practices.This work is being done through the EBU, under whose authority the standard and recom- mendations will be published, with help from IRT in Germany and Sveriges Radio. (Sweden is already faced with losing all ISDN services in the next two years).
It is intended to produce a reference codec with which to test commercial products for compliance.
30
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	31
CORE TECHNOLOGIES
Radio Systems
The BBC is a major user of radio frequency spectrum, not just for its public broadcasts but also for many internal purposes such as wireless cameras, wireless microphones, and passing signals back from outside broadcasts. Spectrum is a scarce resource, and we are constantly looking for techniques to make more efficient use of it, as these three pieces of work demonstrate.
MIMO
MIMO stands for Multiple-Input- Multiple-Output and refers to a wireless system with more than one transmitter and receiver, all operating on the same frequency but with each transmitter carrying a different signal. Each receiver picks up a mix of the transmissions, depending on propagation conditions and the placing and polarisation both of its aerials, and those at the transmitters. If the receivers can determine the mixes in the form of a mathematical matrix, their outputs can be passed through the inverse of this matrix to recover the original transmissions. In the experimental system described here, the receivers calculate the matrix by measuring the pilot tones already present in the DVB-T signals.
The increased capacity is obtained at a cost. Depending on the amount of mixing, the received signal-to-noise ratios can be worse than for a single transmitter and receiver, and this might reduce range or require more transmitter power.
MIMO
MIMO is a method for increasing the capacity of a radio link by running multiple transmissions on the same frequency. To investigate its potential BBC R&amp;I has produced a prototype MIMO system using two transmitters and two receivers, with the transmissions based on DVB-T. An experimental pair of 50 W ERP transmitters has been installed at a site near Guildford. A survey vehicle has been equipped to investigate the coverage for both fixed and mobile reception.The transmitters can be switched remotely between dual-polarised or co-polarised transmission, to explore different configurations. We are doing this work in collaboration with OFCOM, Arqiva and National Grid Wireless within a new group set up for the purpose and chaired by the BBC, known as the Advanced Terrestrial Transmission Study Group, or ATTSG.
Initial results using 64QAM rate 2/3 modu- lation suggest that the coverage of dual- polar DVB-T MIMO is very nearly as good as that obtained for DVB-T itself, with a data throughput of 48 Mbit/s compared to 24 Mbit/s for the standard system.
Because MIMO is one option that is being considered for updating DVB-T,
the results of this investigation are being made available to the DVB-T2 technical working group (described opposite).
On-channel repeater
BBC R&amp;I has developed an on-channel repeater for the DAB and DVB-T frequency bands.This device, first shown by the BBC at IBC in 2005, allows a relay station to re-transmit a signal on the same frequency as it is received.This is normally difficult because the receiving antenna almost inevitably picks up
some of the signal from the transmitting antenna, causing howl-round. The on-channel repeater is particularly useful
for filling ‘holes’ in the service area of a transmitter, or extending the range.
We have protected the key technology with a number of patents, and we have issued a licence to use the algorithm to a major transmitter manufacturer. More enquiries for licences have been received both from the UK and overseas and are currently being processed.
Trials have been carried out in collaboration with network providers in the DAB band, with additional DVB-T trials scheduled for the near future. Information regarding the behaviour of
In trying to squeeze more capacity out of the precious UHF spectrum, the DVB-T2 project is bound by theoretical limits to what can be achieved.The Shannon capacity limit is well known; however, it is sometimes forgotten just how idealised it is. Once certain choices are made, such as picking a particular combination of constellation and bit mapping, more detailed application of Shannon’s information theory shows that rather less capacity is then achievable, even in an ideal case.This graph serves as a benchmark for studies of error-correcting codes together with the possible addition of higher- order constellations like 256-QAM to the system.
the system in complex single-frequency networks (SFNs) has allowed refinements to be made to the algorithms.
This year has seen the original system enhanced in a number of ways to improve signal-to-noise ratio and the ability to track rapidly time-varying coupling paths. So much so, in fact, that the system has been lab tested at 2.5 GHz in a PMSE (Programme Making &amp; Special Events) radio-camera mid-point application.
The results are very promising and we are close to having a solution to offer for this application.This would minimise the number of PMSE operations needing two frequencies at a time when this spectrum is likely to become increasingly expensive.
DVB-T2
In 2006 BBC R&amp;I submitted a paper to DVB, proposing the formation of a Study Mission to consider updating the standard for digital terrestrial television transmissions. The proposal was accepted and we were asked to lead the study, which identified several technologies that could be used to increase both efficiency and ruggedness.
We were then asked to lead a DVB technical study group to continue the
technical analysis, in parallel with another group preparing the commercial require- ments. The work of these groups is still underway. Currently it appears that a modest increase in capacity could be achieved through the use of advanced error correction codes, such as those used for DVB-S2, together with some changes to the underlying transmission system to eliminate other minor inefficiencies. Although a significantly greater increase in capacity (i.e. double or more) could probably be achieved through the use of MIMO this would require changes to both the transmitter aerial and the viewers’ receiver aerials.
The commercial attractiveness of these options is still being studied. Existing digital viewers would need to buy new set-top boxes or digital TVs, so the introduction of a new standard might have to be incorporated into the launch of a new service, for example HD onDTT.
It is expected that a first draft of a specification	for	fixed	and	por table reception will be completed around the beginning	of	2008,	with	a	fur ther specification for mobile receivers following later.
32
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	33
CORE TECHNOLOGIES
NEW SERVICES
Video Compression – Dirac and Dirac Pro
Almost everywhere video is handled digitally some form of data compression is applied. Although satisfactory forms of compression exist for most applications, some are subject to restrictive or expensive licensing arrangements. This can increase our costs, and potentially the cost to our audiences in receiving our programmes. The BBC as a publicly funded body is in a unique position to develop open compression technology where it has a specific need, and to make that technology freely available. For example, the BBC’s Open Archive initiatives will be making content available over the Internet, and we would like to be able to supply free software to schools and other educational bodies.
Dirac
Over the past few years we have drawn on practical experience that dates back to the 1960’s to develop an advanced video compression system called ‘Dirac’. Although we originally intended Dirac for high resolution images and high bit rates, its wavelet-based compression technique has proved suitable for applications from over 1 Gbit/s down to below 100 kbit/s. It is comparable with the latest standards H.264/MPEG-4 AVC and VC-1.
Measuring video quality
The technical quality of television pictures has been a frequent topic for discussion, even before the launch of the regular service. The debate continues, now largely over digital TV and latterly, HD.
Amongst the early broadcasts of the current HD trial were live programmes covering the FIFA World Cup from Germany. The broadcast arrangements were going to be complex using a chain of up to four MPEG-2 contribution links followed by transmission to the home using H.264 compression. Knowing that
Dirac is open source, relies on no other parties’ intellectual property, and can be used without payment of royalties. Its potential uses range from low resolution coders for mobile phones and Internet distribution of video clips through to HDTV and beyond with ultra-high resolution Digital Cinema. Dirac works under Linux and on Apple Mac PCs as well as Microsoft Windows.The ability to work on different operating systems is important to the BBC’s plans for
‘On Demand’ TV via the iPlayer and streaming content over the Internet.
passing video signals through cascaded compression systems can cause extra picture degradation we simulated the entire chain. By experimenting with bit rate settings we were able to measure the video quality that would be achievable at home and recommend a number of improvements.
This was perhaps an extreme example, but any production chain will include a number of different compression systems between camera and broadcast, and there may also be format conversions. Because of the range of compression
During the past year we have continued to improve Dirac.The algorithm has been made more efficient and easier to imple- ment. A detailed specification has been published and open-source software made available to allow anyone to develop implementations. A second implementation, called Schrödinger has been optimised for speed although because of this it is some- what harder to use as a basis for further development. Both implementations can decode standard definition TV in real time, with Schrödinger able to do this even on average specification PCs.
systems now available there is more than ever a need for a reliable and standardised method for measuring video quality.
Measuring video quality is an area we are continuing to research. In the coming year we plan to establish a test-bed to investigate and provide specialist advice on video quality within the BBC. We are continuing our programme of evaluating compression codecs and investigating issues of concatenated coding, focusing mainly on acquisition and production. We are contributing to collaborative work on this topic within the EBU.
Dirac Pro
We have now produced an implementation specifically for higher bit rates, which we have named Dirac Pro. It shares many features with Dirac and is most suitable above 100 Mbit/s.
Dirac Pro’s initial application was to trans- port 50 frames/second (fps) HD signals on the same cables and infrastructure as conventional 25 fps HDTV. Video
at 50 fps conveys a more ‘fluid’ motion and the BBC plans to move to this higher quality standard over the next few years. It is particularly suitable for sports coverage in HD, and could be used for the 2012 Olympic Games. Dirac Pro provides the 2:1 compression, sometimes known as ‘Mezzanine’ compression, with almost no loss in quality.
Another application of Dirac Pro is the transmission of HDTV signals between studios and production centres, using the existing distribution infrastructure. This requires more compression but Dirac Pro is sufficiently flexible to achieve this with little loss in quality. Many other applications of Dirac Pro are also possible, in programme production using networked infrastructure (‘desktop production’) and also in digital cinema
production. Dirac Pro was successfully demonstrated to film makers in January 2007 at the Hollywood Post Alliance.
Dirac is attracting considerable interest both from within the BBC and from other organisations. It was demonstrated publicly at IBC 2006, where content from the BBC’s HDTV trial was compared favourably with satellite broadcasts
using H264 at the same bit rate. We are currently formally standardising
Dirac Pro through the SMPTE. A draft of the specification, referred to as VC-2, has been submitted and we are now waiting for it to be ratified. Establishing the standard will encourage equipment manufacturers to incorporate Dirac Pro into their products. An implementation of Dirac Pro, developed jointly with our commercial partner NuMediaTechnology, will be launched at NAB 2007.
Where next?
We now regard our original implemen- tation as a reference and have released it to several universities as a develop- ment platform. We are collaborating with them to exploit Dirac further and push the boundaries of video compression.
As 3D displays move closer to reality, both for the home and mobile devices, we are working with the University of Surrey to develop methods for compressing 3D video content.These techniques also help in modelling motion in conventional 2D images, and could help in improving compression there.
Brunel University are adapting Dirac technology to produce bitstreams that can withstand poor quality transmission systems and the effects of network congestion, avoiding the complete collapse in quality that bedevils conventional compression techniques.
As bit rates are pushed ever harder, it becomes increasingly difficult to maintain picture quality. Because the relationship between encoder control mechanisms and perceived quality is very poorly understood, encoder designers rely
on ad-hoc, indirect, techniques to avoid quality collapsing when encoders are stressed. Manchester Metropolitan University have embarked with us on
a project to incorporate psycho-visual modelling techniques into video encoder designs to manage video quality directly, using Dirac software as a test-bed.
34
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	35
CORE TECHNOLOGIES
Audio Compression Audio bit rate reduction
A study has been made of MPEG Audio Layer II coding to find out how good it would be if an optimal coder could be developed. A ‘genetic’ coder was devised that could explore the myriad permutations of encoding that could be applied to each 24 ms frame of audio.The coder worked more slowly than real time but it was possible to record the results and listen at normal speed. The work was presented in a paper
to the AES European Convention in 2006. We concluded that the potential for further improvement was marginal, the best coders already being close to the performance of the genetic coder.
There is always pressure to reduce the bit rates of the audio signals on DAB to make room for new features. MPEG Audio Layer II coding is adjustable only in moderately coarse steps in bit rate, and it is difficult to make reductions without compromising the audio quality.BBC R&amp;I has continued to be closely involved in the assessment of proposed changes.
Multi-channel audio
BBC R&amp;I took part in a major series of subjective tests organised by the European Broadcasting Union’s B/MAE (Multi-channel Audio Evaluation) project group. In the first set of tests the subjective quality of multi-channel audio bit rate reduction systems such as Dolby Digital Plus, High Efficiency AAC MPEG Surround, and DTS was carefully assessed. These are systems that are, or could soon be, used for broadcast or Internet distribution of high quality surround sound, with or without accompanying video.
Early results were presented by IRT at the EBU’s Forecast ‘06 conference. A paper has been written by BBC R&amp;I together with colleagues from the EBU and IRT, and is to be presented at the 122nd AES convention. The conclusions reveal interesting performance evolution over the ten years since the previous comparable tests. With non-critical material, modern coders can achieve results as good as their predecessors but at a significantly lower bit rate. However, maintaining consistently high quality still requires the same, relatively high, bit rates.
Independent component analysis
Independent component analysis is a technique for splitting a sound signal into its component parts, and dealing with each one separately. As a simple example a duet might be split into the sounds of the two instruments, which in this instance would clearly be easiest from a stereo recording. We are fascinated by the applications that could be developed from this fundamental technology. A joint project is being set up with the University of Salford to examine how it might be applied in real broadcast audio environments.
Digital Rights Management
The Internet revolution encourages media businesses to find new ways to exchange content with their customers and between themselves. Digital Rights Management (DRM) is important in controlling and protecting these exchanges. However, some of the systems in use are proprietary, and as the number of connections multiplies, failure to develop interoperable solutions could be a major obstacle to the growth of the industry.
The requirements of DRM vary from limiting access, through ‘copy control’, to much more flexible permissions. In some cases DRM is used simply to protect the integrity of the content or authenticate authorship rather than limit distribution or collect remuneration.
BBC R&amp;I has been working with partners in industry and academia to develop open DRM solutions.We have supported the broadcast technology providers throughthe DVB organisation as they specify the technologies for protecting content delivered over broadcast networks.
We also participate in the Digital Media Project (DMP) as it develops specifications and builds software prototypes from its ‘Primitive DRM Tools’. These smaller DRM related functions can be used to assemble a variety of value chains according to individual business needs. The DMP aims to provide the technological means and organisational framework to allow participation in the media industry both by large content distributors and individual media creators wishing to distribute content with varying degrees of security. The DMP specifications bring a more inclusive model for all players in the value chain, from authors, performers, adapters and producers through to content providers and home users.
‘ The DMP aims to provide the technological means and organisational framework to allow participation in the media industry by both large content distributors and individual media creators wishing to distribute content with varying degrees of security.’
More recently we have begun working with a large group of academics and industrial partners within a mature and established project, funded partly by the European Union, that has developed a platform for cross media production and distribution; AXMEDIS.
In working with these international groups to develop and consider the issues of deploying DRM solutions we have come to a closer understanding of how our industry is changing and can change further to meet the demands of our modern world. By working with the wider industry we are better placed to control our own future and meet the needs of a new type of audience in the new digital age of broadcasting.
36
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	37
WORKING WITH US
Collaborative Projects
BBC R&amp;I works closely with other broadcasters, industry and universities in order to maximise the effectiveness of our research effort, to share knowledge, gain early insights, and influence emerging new systems and standards. Collaboration ranges from funding of students through to participation in major projects, many part- funded by government or the European Union.
We are pleased to acknowledge the effort and support given by all the collaborators and funding bodies mentioned in this review.
ARENA
www.ist-arena.org
AXMEDIS
www.axmedis.org
CALLAS
www.callas-newmedia.eu/
iview
bbc.co.uk/rd/projects/iview
MATRIS
www.ist-matris.org
Participate
www.participateonline.co.uk
PRISM
Arena is part funded by the European Union. It is exploring methods of measuring and aggregating audience figures on mobile phones, broadband connections and video recorders, as well as the conventional digital platforms. Arena runs for three years and is due to finish in December 2008.We are supported in this project by colleagues from BBC Marketing Communications and Audiences.
AXMEDIS is part funded by the European Union. The project provides an open solution which builds on technologies and tools to reduce costs and increase efficiency for the production, protection, management and distribution of content.
CALLAS is part funded by the European Union.The New Content Paradigms area focuses on building upon the BBC tradition of rich narrative. We are researching how to bring this BBC quality into the interactive domain, looking at techniques for both authoring and play-out of truly interactive (‘open-ended’) stories. CALLAS is combining the use of emotion-sensing interfaces with interactive narrative techniques to produce a ‘bardic interactive storytelling engine’.
We lead this DTI-funded project which is developing methods to capture action from events such as a football match in 3D and to provide a 360 degree free-viewpoint video replay of this action, as if from a virtual camera.
MATRIS was part funded by the European Union, and finished in January 2007. It developed markerless camera tracking technology that used naturally-occurring features in the scene. It also developed an inertial sensor and looked at the use of an auxiliary camera to improve the tracking. We coordinated the project demonstration at IBC 2006, which attracted the interest of several potential licensees.
Participate is a three-year DTI-funded collaborative project. It is exploring the convergence of pervasive, online and broadcast media to create new kinds of mass participatory events where a broad cross-section of the public contributes and shares content.
We lead this DTI-funded project which is developing workable production services based on GRID distributed computing techniques.
The BBC
Our research and innovation builds up a depth of knowledge in a technology, usually before it starts being rolled out as an application. When the BBC needs to	deliver	the	technology	our	exper tise is invaluable to a successful deployment. There are numerous examples in this Review where we are providing advice and active assistance, ranging from very visible projects such as the Digital Switchover and the test transmissions of HD, to more esoteric work on programme production and transmission technologies.
Other broadcasters
The European Broadcast Union is a central co-ordinating body that brings like minded organisations together, and many of our contacts with other European broadcasters are through its technical working groups. Extending our view outside Europe is also important, for new systems are often introduced first on other continents, HDTV being the best recent example.We have developed stronger links with, for example NHK in Japan and CCTV
in China. These different cultures traditionally view technologies differently from Europe and we can learn from this. Also these areas, together with India, have a very strong technical capacity and are influential in world markets, and the consumer marketplace.
Universities
Our links with Universities are numerous. As well as direct links, we also have DTI sponsored projects and many informal contacts. The	academic	knowledge	which this brings can underpin any development of a new service or system.
Collaboration in Europe
We contribute to NEM, the Networked Electronic Media initiative, with the aims of influencing and guiding the research agenda for the European Union’s Framework programme of research. This is an association of experts in the technology of media and their insight and	guidance	is	an	impor tant	pre- competitive indicator of future directions that	the	industr y	is	taking.
A number of our research projects are enhanced	by	joining	consor tia	funded under the European Union’s Framework 6 research programme. This allows us to contribute and gain from projects that are typically budgeted at many millions of Euros.
38
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	39
WORKING WITH US
Standards
The BBC has always worked in a so-called horizontal market where many players can share the same technology for different business purposes.The only way we can make this work is by adopting standards. Standards define almost every part of the broadcast chain, from before the capture of images and sound, all the way through to the display and loudspeakers at home.We therefore make a serious effort to help standards bodies to create proposals which will work well for public service broadcasters and in a complementary way to the commercial goals of private industry.
DVB-SUB
We chair this group on subtitling, for which we recently rewrote the relevant standard to enable subtitles at HD resolutions.
DVB-T2
We lead the technical study group, developing an updated specification for DVB-T to improve transmission efficiency and ruggedness.
ID3.org
ID3.org is a group which has been defining methods of tagging audio files with descriptive information or metadata. We have developed an addendum to the ID3 specification for the visually impaired, that allows an audio description of the textual information to be hidden inside the main audio file.
The Digital Media Project (www.dmpf.org) The Digital Media Project (DMP) is a non-profit Association. Its mission is to promote the successful development, deployment and use of digital media that respects the rights of creators and rights holders to exploit their works, the wish of end users to fully enjoy the benefits of digital media and the interests of value-chain players to provide products and services.
The DMP has wherever possible used existing technologies, for example MPEG-21, as the basis for a number of primitive functions. It is finalising the third release of its interoperable DRM platform specification IDP-3.
SMPTE and the AAF Association
The Society of Motion Picture and Television Engineers is based in the USA and has produced many relevant standards.We are active members of the SMPTE and the AAF Association, as they continue their work in refining standards relevant to television production. Our interest for this work lies mainly in the file formats used for capture and storage of content in tapeless production systems.
We are a participant in several SMPTE projects: Reg-XML, which is developing an XML representation of MXF and AAF, SMPTE 410M, which is developing the building blocks for incorporating other data, such as subtitles, in an MXF file and SMPTE 377M which is currently revising the core MXF standard.We are also working on the SMPTE metadata elements and groups registers which hold the definitions of the metadata carried in MXF and AAF. Additionally, we chair the SMPTE AAF Object Specification group, which is taking on the technical responsibility for the AAF specification.
We are similarly closely involved with the AAF Association, having a member on the Board of this trade body, and we lead its engineering	programme. The	AAF Association has, to date, been the group developing the AAF standard but with the technical responsibility for this being passed to the SMPTE, its work is now becoming more user and workflow focussed. Reflecting this change, the AAF Association has renamed itself as the Advanced Media Workflow Association.
We have also been highly active in a SMPTE working group standardising the Media Dispatch Protocol for coordinating file transfer of content between production centres over IP networks, including the Internet.The group has made good progress, and this work will result in an important standard allowing the BBC to share content with its production partners in an open way.
Our video coding technique Dirac is also being standardised within the SMPTE.
Audio Engineering Society (AES)
Most of the digital interfaces used in the audio industry are based on standards managed by the Audio Engineering Society.
We have contributed to two standards that have recently been ratified. AES 51 supports delivery of ATM cells over ethernet. AES 52 puts unique identifiers into AES 3 transport streams, so that
media managers can handle them much as they do pre-recorded audio files, automatically tying them to metadata for example. AES	52	also	opens	the	possibility of automatic routing.
AES 47, the audio-over-ATM standard extensively used in the BBC, has recently been re-ratified following a routine five- year review.
WorldDMB Forum and DRM Consortium There are many new platforms to deliver radio content that was once the preserve of analogue AM and FM technologies. The emerging technologies, such as DRM and DAB+, are seen as competitive by some and as complementary by others to our existing digital platforms. Our international presence in the world can also create conflicts between the domestic agendas of different countries. By engaging with a number of international fora, such as the WorldDMB Forum (formally WorldDAB), the DRM Consortium and conferences across Europe, Asia and the Americas, we have been able to put forward the positive and promote a reasoned co-operation between technologies that support our core	values. This	has	ensured	our continued success in bringing clarity to the messages delivered to our listeners from the companies that design and sell digital radios in all our markets.
Institute of Electrical and Electronics Engineers (IEEE) The IEEE has a wide range of standards under its remit.The main areas of interest to us are the wired and wireless networking standards, where the IEEE has become the body preferred to ETSI.
We are participating in the IEEE 802.11 standardisation process, with a particular interest in IEEE 802.11n, whose ratification is now not expected until late next year. We hope to influence the work so that new standards will enhance the potential of WiFi for video production.
Some standards endure for many years, otherwise we would find it difficult to operate. However, the situation is always fluid because of new consumer devices and services, technical developments and changes in business processes. New standards and additions to existing ones frequently need to be progressed, and the pace of change is accelerating.
In the early days standards were set nationally. For the past twenty years or so, we have seen a steady movement towards international and global bodies. But there is still no dominant body with responsibility for all the standards that are used in broadcasting, and indeed we now find there are standards from the telecommunications and IT industries that are equally relevant to our business.This means we often have to follow work through several standards bodies to
International
Digital Video Broadcasting Project (DVB) The DigitalVideo Broadcasting Project (DVB) is a consortium of over 260 companies and other organisations developing global standards for the delivery of digital television and data services.Their work has now extended beyond broadcasting to the convergence of Internet and mobile systems in the home. DVB specifications are offered to ETSI, CENELEC, the EBU, or the ITU to create the
standards themselves.
BBC R&amp;I has been instrumental in a number of DVB standards.
ensure we have the right tools for us at the end.
Because of our pro-active approach to standards, engineers from BBC R&amp;I are frequently invited to join standardisation groups, recognising not just their technical contribution, but the fair and reasonable balance we are able to broker when discussions become fractious and commercial. In many groups we are also asked to take the chair.
There are a number of other standardisation bodies and organisations, for example MPEG, the TV-Anytime European Users Group, and some technical groups in the EBU and SMPTE, where we hold membership or otherwise follow the proceedings, to make sure the BBC’s interests are not being compromised.
DVB MHP IPTV specification
The DVB Multimedia Home Platform (MHP) is an advanced platform for interactive TV.This year DVB has completed an IPTV profile for the MHP.
We contributed and edited the TV-Anytime parts of the specification, based on our earlier work in the myTV and Share-it collaborative projects. This allows the MHP to provide rich programme guides based on information delivered via broadband IP.
Where possible we would like to see standards that are:
Open
The standard is fully described in documents that are obtainable by anyone, so that any competent engineer can read the specification and recreate the technology.
and
Free
Or at least available on reasonable terms. Even a small charge for each instance of a standard being used can add up, given the volume of the BBC’s output and size of our audiences.Worryingly, we are starting to see instances of royalties being demanded after a standard has been committed to.
DVB-CPT
The first elements of the DVB Copy Protection &amp; Copy Management (DVB- CPCM) were released in November 2005 as the DVB ‘Blue Book’.This includes the DVB CPCM Reference Model and Usage State Information (USI) – an interoperable description format for the copy control conditions associated with
protected content.
DVB-GBS
This group defines the carriage and sig- nalling of data and SI (including TV-Anytime) in the DVB transport stream.
40
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	41
WORKING WITH US
Standards continued
W3C
The World Wide Web Consortium (W3C) develops interoperable tech- nologies to lead the web to its full potential.We are members of the ‘Timed-Text’ working group.
Europe
European Telecommunications Standards Institute (ETSI) ETSI is the dominant European body with responsibility for broadcasting and telecommunication standards. Most of the broadcast standards we use are standardised in ETSI, including DAB, the DVB suite of standards, and TV-Anytime.
European Broadcasting Union (EBU) The EBU is no longer the prime body for creating broadcast standards as it was in the days of MAC and NICAM. It has now changed its role to act as a consensus- building body, ensuring that public service broadcasters in Europe are able to develop a common viewpoint. Much of the EBU’s formal documentation is published and then referenced in other standards.
The EBU organises this work into a series of technical groups, and we contribute to all of these.
EBU Group B/MAE
We have recently taken part in a series of subjective quality tests organised by this group, which evaluates multi-channel audio coding technologies.
EBU Group N/ACIP
Set up early in 2006, this group is developing an interoperability standard for equipment for audio contributions over IP, in cooperation with manufacturers. It is building a test engine for verifying commercial equipment against the
International Electrotechnical Committee The IEC prepares standards for electrical, electronic and related technologies. In some respects it overlaps work in ETSI, IEEE and CENELEC, among others. Usefully, it seeks to lay down performance bounds, for example, the sensitivity one can expect from certain types of radio receiver, to ensure that systems work effectively.
standard, and is also compiling a set of recommendations for operational practice.
EBU Group N/CNCS
This group, which we chair, has been running for a number of years and is now contributing to an IEC group developing a common control standard for network connected broadcast and media production.
EBU Group N/VCIP
This is a new group set up at the start of 2007, with similar aims to N/ACIP but for video contributions over IP.
EBU Group P/CP
This is an interdisciplinary group, analysing common processes in TV production, to answer the increasing demand for economical systems integration. It is mostly concerned with file-based pro- duction and its related metadata issues.
EBU Group P/Display
We chair this group which is addressing the supply of professional TV monitors, used by all broadcasters to ensure consistent picture quality and colorimetry. CRT (cathode ray tube) TVs are now almost unobtainable, and the new flat panel technologies work in very different ways, not only from CRTs but also from one another.There are parameters to specify which simply did not exist with CRTs, or which could be taken for granted because the CRT TVs all worked in the same way.This group liaises closely with the SMPTE and the UK DTG.
CENELEC
CENELEC, the European Committee for Electrotechnical Standardisation prepares voluntary standards for electrical and electronic goods and services.The most immediate relevance to our work is their influence on the specifications
of receiver performance.
National
Digital TV Group (DTG)
The Digital TV Group is an industry association for digital television in the UK. This group defined the profile of the DVB standards which are now used for all Freeview services, and has a laboratory which is able to test compliance of consumer equipment with those standards. BBC R&amp;I engineers have worked closely with the Digital TV Group to set the standards.
Now the group is concentrating on high definition television, radio spectrum issues, mobile TV, and domestic systems.
BBC Information &amp; Archives
BBC Information and Archives (I&amp;A) became part of FM&amp;T in January 2007.
The project team working within I&amp;A is responsible for several internal delivery projects, with a forecast capital expenditure for 2007-8 of some £2.4 M.These projects are closely linked to the BBC’s Digital Media Initiative, the creation of processes and technology that will allow the BBC to produce and deliver content entirely digitally and without the use of tape.There can be no effective use of tapeless content across the BBC without a digital archive at the centre.
A significant part of I&amp;A’s project work is done in collaboration with otherorganisations:
Spoken Word
Spoken Word is a five-year Digital Libraries Content project that started in August 2003. It is jointly sponsored by the UK Joint Information Systems Committee and the US National Science Foundation.The purpose is to allow BBC radio archive content (and other audio) to be accessible online to UK university students. It is the only direct mechanism to get archive content into the higher educational sector, and has given us valuable experience of ‘digital repositories’ and associated access technology, all of which is useful for BBC’s iPlayer and for Open Archive initiatives. The other partners are the Glasgow Caledonian University, Northwestern University, Chicago, and Michigan State University.
PrestoSpace
PrestoSpace started in February 2004 and has just passed its third review. It is a large EU Integrated Project with 38 partners, running for four years. It is a ‘public value’ project; the BBC and its technology partners know how to preser ve	audiovisual	content	efficiently, using	a	factor y	approach	(for	example the BBC Archive Radio Digitisation project at Maida Vale). Our knowledge and	standing	in	audiovisual	preser vation means that the rest of Europe’s audiovisual collections look to us for guidance. PrestoSpace is the mechanism whereby the knowledge and approach of BBC and partners can be made available
to all audiovisual archives – to make preservation work ‘better, faster and cheaper’.
A number of other collaborations have recently begun, part-funded by either the DTI or the European Commission.
AVATAR
AVATAR is part funded by the DTI. It is defining IT storage solutions optimised for audio visual content and data from oil and gas exploration. It began in November 2006 and finishes in 2009.
ESTASTAR
ECTASTAR is part funded by the DTI. It is devising architectures to access exabytes of storage at terabit/s rates. It began in November 2006 and runs for two years.
SEMEDIA
SEMEDIA is part funded by the EU. It will develop techniques, environments and tools for media labelling, searching and retrieval from very large collections of	heterogeneous	data.	It	star ted	in Januar y	2007	and	finishes	in	2009.
Video active
Video Active is part funded by the EU. It is putting historical and heritage television material onto a shared, public website, and providing tools to support finding material in different languages. It started in September 2006 and runs for three years.
Other I&amp;A projects are:
Computer Assisted Indexing Computer Assisted Indexing is a semi automated indexing and taxonomy management tool for cataloguing and retrieving BBC archive material.
Off Air Compliance Recording Off Air Compliance Recording is the replacement of the VHS-based system for legal compliance recording of BBC TV transmissions with an automated system that will also capture interactive elements (from a concept developed by BBC R&amp;I).
Radio Digital Archive
Radio Digital Archive is the capturing of selected BBC radio output and related metadata, with the ability to search and retrieve as digital files.
Online Ordering
Development of new systems for online ordering of TV viewing material and music within the BBC, and for delivering commercial music direct to BBC users.
42
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	43
WORKING WITH US
The Innovation Forum In January 2007 we held our first FM&amp;T Innovation Forum at the Science Museum’s Dana Centre.
The Forum will meet three times per year and is part of the innovation process that FM&amp;T uses to discover, develop and implement its most important innovative ideas. It acts as a focus to trigger and socialise proposals as well as the start of the peer review process.
Future Media Innovation
The day-long event saw many interesting and varied ideas presented, including proposals to mount a high quality camera on athletes, use a Nintendo Wii controller to operate production systems, and put wind turbines on BBC premises. An audience of around eighty technologists attended and used the optical voting system developed by the Production Magic team to give immediate feedback after each presentation. Delegates also completed feedback forms for a more con- sidered view on the proposals. Following each Forum a more structured review process takes place, with volunteers discussing each idea in depth. Ideas that have been presented to the New Ideas Symposia, the forerunner to the Forum,
are also considered alongside those from the Forums.
April 2007 will see the start of work to build a Rapid Prototyping and Research Centre (R-PARC) at Kingswood Warren.This will be a wholly configurable space with generic equipment plus ‘per project’ specialist technologies, as required by the ideas flowing through the Centre.The aim is to spend short but intense periods of time researching and developing ideas, perhaps creating prototypes or demonstrators or investigating proofs of concept.
Huw Williams hosted the event and also presented the inaugural Innovation Awards:
Surround video project
Jigna Chandaria Paul Debenham Stephen Jolly Tim Sargeant Graham Thomas Bruce Weir.
Audio navigation visualisation project
Andrew Mason.
Augmented reality
Adrian Woolard.
New Ideas Symposium team
Michael Evans Matt Hammond Stephen Jolly Alia Sheikh.
The Future Media Innovation team aims to provide the BBC with product and service ideas from internal and external innovation networks. It aims to develop and support wide ranging and efficient innovation networks, and to drive new products into the business. Our current work is based on developing tools for engaging with innovators outside the organisation, loosely grouped in four sectors – corporate partners, academia, independent digital media production companies, and ‘lead users’. Here are some details about projects aimed at these sectors.
Participate
www.participateonline.co.uk
Participate explores convergence in pervasive, online and broadcast media to create new kinds of mass-participatory events in which a broad cross-section of the public contributes to, as well as accesses, contextual content – on the move, in public places, at school and at home.
Participate is a three-year collaborative project supported by the DTI and EPSRC blending expertise in online services, ubiquitous computing, broadcast media, sensors, pervasive gaming and events, and education. Our partners are BT, Microsoft Research Cambridge, Blast Theory, ScienceScope, University of Nottingham and the University of Bath.
The consortium is working together to develop scalable solutions for managed events and campaigns that engage and motivate participants over sustained periods of time. We are developing tools for the public to author, share and discuss content using their own devices, and for professionals	and	exper ts	to	collate	and edit contributions for publication over broadcast and interactive channels.
The BBC is working with a range of partners to develop a series of trials and events based on the theme of ‘the environment’ capturing and contributing
information about their local environ- ment to build a national picture across the UK.
Arts &amp; Humanities Research Council (AHRC) Partnership The aim of this partnership has been to discover whether there was scope for a long term partnership between our two communities, if there was a demand for enabling effective knowledge transfer and delivering mutually beneficial research, and how that research could turn into joint projects with a mutual strategic fit. The response after three initial summits was overwhelmingly positive and showed that	a	specialist	ar ts	and	humanities academic community which has a deep understanding of the drivers and inhibitors of change is an untapped resource for insight, expertise, and ideas for BBC FM&amp;T.
After the successful summits (documented at http://tell.memore.info) a joint funding call for collaborative projects was announced	in	Januar y	2007.	Over	60 applications were received and 26 made it to a shortlist. Five of these have now been chosen to go forward for joint funding, giving clear indications that there is a strategic fit for both communities.
Once the projects have begun we will be monitoring progress, disseminating outcomes and measuring success and impact on the business within both communities. This type of partnership is a first for the BBC and the AHRC and, should it prove successful, there will be further funding calls. Meanwhile, we are developing a sophisticated online brokerage tool which will track all the existing relationships as well as helping to develop new ones. We will be holding more summits in autumn 2007.
BBC Backstage
Backstage.bbc.co.uk is the developer/ designer network from the BBC. It is an opportunity for the BBC to engage with the external developer community, offer some of the data and services the BBC
produces, and share them with third party, non-commercial developers.
Over the last year we have gone out to developer communities all over the country in a series of meet-ups, events and university tours. We have established partnerships with O’Reilly,Yahoo!, The Obvious and the Guardian.We’ve launched our new in-depth podcasts and started on a process to allow the BBC much more flexibility in regards to development and prototyping into the near future. Our next major project is the Yahoo!/BBC Backstage ‘Hackday’ – a huge event involving over 400 developers at Alexandra Palace on 16 and 17 June, 2007 – more details at www.hackday.org
BBC Innovation Labs
http://open.bbc.co.uk/labs
BBC Innovation Labs is a process for working with the independent media production companies on early-stage prototypes for new products and services.The process has three stages: open days where BBC commissioners describe their strategic needs and the briefs for the Labs; an open submission process on the Labs site; and a five-day residential workshop where the companies work with BBC and external mentors to develop their ideas.
After successful pilots in 2005/6, we worked with ten regional partners to deliver four Labs in 2006/7, covering Scotland,	Nor th	England,	South	England and London. This year’s Labs were launched with 13 open days in cities across these regions, and we received over 500 proposals.These were filtered down to 40 ideas that were developed over the five-day Labs, with 18 ideas being selected for further development.
44
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	45
MORE INFORMATION
INDEX
More Information What to do if you
IIndex
3G telephony 4 – 5, 11– 12, 23 A
AAF (Advanced Authoring Format), 21–22, 41
AES (Audio Engineering Society), 36, 41 AHRC (Arts &amp; Humanities Research
Council), 45 ARENA (EU Project), 27, 39 Arqiva, 32 Audio over IP, 31 AVATAR (DTI Project), 43 AXMEDIS (EU Project), 37, 39
B
BAMZOOKi, 20 BARB (Broadcasters Audience Research
Board), 27 BBC Backstage, 14, 45 BBC Innovation Labs, 45 BBCi, 9, 26
C
Cabot Communications, 8 CALLAS (EU Project), 15, 39
D
DAB, 4, 28, 32 –33, 36, 42 DAB+, 28 Digital Switchover, (see Switchover) Dirac, 34 –35, 41 Dirac Pro, 34 –35 DMP (Digital Media Project), 37, 41 DRM (Digital Radio Mondiale), 28, 41 DRM (Digital Rights Management), 37, 41 DTG,7,8–9,13,25,42 DVB (Digital Video Broadcasting Project),
6,9,13,26,32–33,37,40–42
E
EBU (European Broadcasting Union), 22, 31, 34, 36, 40, 42
ECTASTAR (DTI Project), 43 EICTA, 6, 25 ETSI, (European Telecommunications
Standards Institute) 9, 28, 40, 40 – 42
FP
want to know more.
FFloorman, 22 free-d, 16 Freesat, 5, 26 Freeview, 4–5, 6, 8–9, 13, 26, 42
G
Games consoles, 4, 14, 29
H
HD (High Definition TV), 4 – 5, 6 – 7, 9, 16–7, 23, 26, 30, 33, 34–35, 38, 4–42
I
IBC 2006, 5, 8, 16, 22, 32, 35, 39 IEEE, (Institue of Electrical and Electronics
Engineers) 41–42 iMP, 11
iPlayer, 4–5, 10–11, 22, 34, 43 iview, (DTI Project), 17, 39
J
JFMG Ltd, 23 K
Kamaelia, 29
M
MATRIS, 16, 39 MHEG, 9 MHEG-5, 8 MIMO, 23, 32 – 33 MixTV, 16 MPEG Audio Layer II, 36 MXF (Material Exchange Format), 19,
20–22,41
N
NAB 2007, 5, 35 National Grid Wireless, 28, 32
O
OFCOM, 19, 23, 32
P2 memory cards, 21– 22 Participate (DTI Project), 15, 22, 37, 39,
45 Piero, 5, 9, 16
PMSE (Programme Making and Special Events), 23, 33
PrestoSpace (EU Project), 43 PRISM (DTI Project), 30, 39
R
Radio Digital Archive, 43 Radio Player, 10 Red Bee Media, 6, 11, 16
S
SAMBA, 20 – 21 Schrödinger, 34 SEMEDIA (EU Project), 43 Siemens, 6, 11, 13, 24, 26 SMPTE (Society of Motion Picture and
Television Engineers), 35, 40, 41– 42 Spoken Word (DLC Project), 43 Subtitles, 6, 22, 27, 41 Surround video, 18, 44
Switchover (Digital Switchover) 4 – 5, 13, 15,24–25,38
T
TV-Anytime, 11, 13, 14, 40, 42 U
User generated content, 15 V
Video Active (EU Project), 43
W
W3C, 22, 42 Web 2.0, 5 WiFi, 31, 41 WiMax, 23, 31 WorldDMB, 28, 41
This Annual Review has deliberately been aimed at a broad readership. As a consequence, the articles are considerably shorter than they might otherwise have been, and we have avoided going into deep technical detail.
For those who wish to learn more about any of the projects, there are several places you can find more. We have moved away from paper documentation, and now rely on the Internet to deliver our output.This makes it much easier for everyone to gain access to our work.You can find details of our projects
on our website. Website: bbc.co.uk/rd
PProjects
Many of the projects at BBC R&amp;I have a special set of pages, either on our website, or hosted by others such as our collaborative partners or SourceForge.The aim is to be open about our work whenever possible.
White Papers
We publish reports and tutorials about our work as a series of White Papers.
Our papers are published in places such as journals and conferences. We have a policy to retain copyright wherever possible on these pub- lications.This allows us to offer these works as White Papers too, making all our output accessible in one place.
Lectures
We receive many invitations to lecture. Our intention is to try to inform a wide range of interested groups, from the high level professional conferences and seminars, to small local specialist interest groups.
Whenever the lecture is being given in a public event, we try to publicise it in advance through our website.
46
APRIL 2006 – MARCH 2007
BBC RESEARCH &amp; INNOVATION ANNUAL REVIEW	47
Edited by Richard Marsden and Graham Thomas Designed by Kevin Claydon and Keren Greene
BBC Research &amp; Innovation Kingswood Warren TADWORTH Surrey
KT20 6NP Website: bbc.co.uk/rd
Tel: 01737 839500 Fax: 01737 839501
© BBC FM&amp;T 2007
48
APRIL 2006 – MARCH 2007</Text>
        </Document>
        <Document ID="17">
            <Title>IEEE07-mancini-bresin-pelachaud</Title>
            <Text>A virtual-agent head driven by musical performance Maurizio Mancini, Roberto Bresin and Catherine Pelachaud
Abstract—In this paper we present a system in which visual feedback of an acoustic source is given to the user using a graphical representation of an expressive virtual head. In this system we also included the notion of expressivity of the human behavior. We provide several mapping. On the input side, we have elaborated a mapping between values of acoustic cues and emotion as well as expressivity parameters. On the output side, we propose a mapping between these parameters and the behaviors of the virtual head. These mappings ensure a coherency between the acoustic source and the animation of the virtual head. After presenting some background information on expressivity of humans we introduce our model of expressivity. We explain how we have elaborated the mappings between the acoustic and the behavior cues. Then we describe the implementation of a working system that controls the behavior of a human-like head that varies depending on the emotional and acoustic characteristics of the musical execution. Finally we present the tests we conducted to validate our model of mapping between music performance emotions and expressivity parameters.
Index Terms—acoustic cues, music, emotion, virtual agent, expressivity
I. INTRODUCTION
WHAT happens when it is a computer listening to the music? In HCI applications affective communication plays an increasingly important role. It would be helpful if systems could express what they perceive and communicate it to the human user through visual and acoustic feedbacks.
Listening to music is an everyday experience. But why do we do it? For example one could do it for tuning her own mood. Research results show that we are not only able to recognize different emotional intentions used by musicians or speakers [1] but that we also feel these emotions. It has been found that when listening to music, people experience a change in bio-physical cues (such as blood pressure, etc.). This change may correspond to either the feeling of the emotion arising from listening the music or the recognition of the emotion evoked by the music [2].
Virtual agents with a human-like appearance and commu- nication capabilities are being used in an increasing number of applications for their ability to convey complex information through verbal and nonverbal behaviors like voice, intonation, gaze, gesture, facial expressions, etc. Their capabilities are useful when being a presenter on the web [3], a pedagogical agent in tutoring systems [4], a talking head helping hearing- impaired people to “listen” to a telephone call by lipreading [5], a companion in interactive setting in public places such as museums [6], [7], or even a character in virtual story- telling systems [8]. The expressivity of behaviors, that is the way behaviors are executed, is also an integral part of the communication process as it can provide information on the state of an agent, such as current emotional state, mood, and personality [9].
In our work we implemented a system that gives to the user a visual feedback by moving and modifying the expression of a virtual agent’s head. The agent’s behavior (movement plus expression) explicitly visualizes the emotional intentions of the musical execution. It is meant to show the direct connection between body motion and expressivity in music performance [10].
In the next section we present the state of the art. Then we give some background information on expressivity for human behavior, voice and music execution. Perceptual tests on expressivity of body and vocal cues are also provided. Then, in Section V we introduce our real-time application for visual feedback of musical execution. We provide information on the mapping between acoustic cues and animation parameters. In section VI we describe the tests we conducted to validate our model of mapping between music performance emotions and expressivity parameters. Finally we conclude the paper.
II. STATE OF THE ART
Some previous works [11]–[13] have addressed the gen- eration of synthetic human behavior depending on music (or sound) input. The works by Lee et al. [14] and by Cardle et al. [12] are mainly focused on adapting pre-calculated animations like walking or dancing to a given music input. These systems analyze the music and extract parameters such as tempo. Based on the values of the extracted parameters, the rhythm of the animation is changed. In the works by Cornwell et al. [15] and by Downie and Lefford [13] interaction between agents are modulated by music and sound. The emotive content of the acoustic source are positively correlated to the quality of the interaction between agents. For example a group of agents will tend to collaborate more if listening to a happy and positive piece of music [15]. [13] also underlines that music can help to give life to inanimate objects, increasing their credibility. Taylor et al. [16] developed a system that allows a user to adapt the way she plays a music instrument to the reaction of a virtual character. The user has to try to vary her execution to make virtual character reacts in some desired way.
Our work is most similar to DiPaola et al.’s work [11]. The authors emphasize that affective information can be de- livered through several means (music, facial expression, body movement, etc) by translating the original message into the language used by each mean. So if music is the starting mean and facial expression is the output mean, the system elaborates the information coming from the music and translates it into facial expressions and head movements. As in this work [11], we view that the translation, that is the mapping, between cues from one mean to another one is of high relevance.
1
III. EXPRESSIVITY
Human individuals differ not only in their reasoning, their set of beliefs, goals, and their emotive states, but also in their way of expressing such information through the execution of specific behaviors. We refer to these behavioral differences with the term expressivity. In the sub-section III-A we present definitions of expressivity in human behavior from the percep- tion studies point of view while sub-section III-B describes some work in voice and music expressivity.
A. Expressivity in behavior
Many researchers (Johansson [17], Wallbott and Scherer [9], Gallaher [18], Ball and Breese [19], Pollick [20]) have in- vestigated human motion characteristics and encoded them into categories. Some authors refer to body motion using dual categories such as slow/fast, small/expansive, weak/energetic, small/large, unpleasant/pleasant. Behavior expressivity has been correlated to energy in communication, to the relation between temporal/spatial characteristics of gestures, and/or to personality/emotion. For Wallbott [21] it is related to the notions of quality of the mental state (e.g. emotion) and of quantity (somehow linked to the intensity factor of the mental state). Behaviors encode not only content information (the What is communicating through a gesture shape for example) but also expressive information (the How it is communicating through the manner of execution of the gesture). There are evidence that some movement qualities are characteristic to emotions. These qualities are the spatial extension of the movement, its energy/power and the activity [21].
In a recent study by Dahl and Friberg (paper IV in [10]) a correlation between body motion and acoustic cues in ex- pressive music performance has been observed. For example, in angry and happy performances faster movements of the body correspond to faster tempi in the performances, and larger amount of movement to louder sound level, in sad performances more fluent movements correspond to a more legato articulation. In a perceptual test done within the same study, subjects could recognize the intended emotions by rating muted video clips, each showing one of three different musicians, a marimba player, a saxophone player and a bas- soon player, performing the same score with four emotional intentions (Fear, Anger, Happiness, Sadness). In particular this study highlights the importance of head movements in the communication of the emotional intentions of the player.
B. Expressivity in voice and music
Sound is an important mean of communicating emotions. Much of the essence of speech and music concerns the communication of moods and emotions. Sound can be charac- terized in terms of a number of physical variables (cues): onset time, decay time, pitch, loudness, timbre, and tempo as well as the rate of change of these variables. Combinations of these cues can be used for describing the expression of emotion in sound.
In the last 30 years, Klaus Scherer and co-workers at the Department of Psychology, University of Geneva, have been
conducting extensive work in the field of emotions and in particular in the area of perception of emotions in speech. They shed light on the multi-faceted problems related to emotions in vocal expression (for an overview see [22] [23]). Among other findings they clearly identified how cues are manipulated in the communication of emotions and specially during appraisal processes [24].
In the past decade, research in music communication has focused on the analysis and formalization of expressive com- munication [25] [26] [27]. Striking analogies between spoken and musical communication have been revealed, with respect to how emotions and moods are expressed using acoustic cues [1] [28]. It has been noticed that expressive rendering can help in marking more clearly the structure of the message being communicated [28]; and several acoustic cues involved in the communication of emotional expression have been identified [29] [1] [30] [31] [32] [33]. These cues can be combined in various ways for signaling the same emotion, thus affording robustness in communication via redundancy and variation. In a review of 101 papers on vocal expression and 41 on music performance [1], similarities were found between both channels in their use of acoustic cues for the communication of emotions.
IV. MODELING EXPRESSIVITY
A. Expressivity for virtual agents
1) Behavior expressivity parameters: In order to increase its credibility and life-likeness, a virtual agent should not only be able to show an emotional state but also to show it with a certain quality [34]; that is, the agent should be able to alter its way of expressing a given emotion through the application of some modifications on the quality of its movements. In our work we are more interested in what is visually perceived of a given behavior than in the internal reasons (for example mental state, personality, mood, etc) that have triggered that behavior. We base our model on perceptual studies. Starting from the results reported in [9] and [18], we have defined the expressivity of body movements over 5 dimensions. Four of these dimensions influence qualitatively the animation of our virtual agent [34]:
•	Overall Activity: amount of activity (e.g., passive/static or animated/engaged). This parameter influences the number of single behaviors happening during the communication. For example, as this parameter increases, the number of head movements per unit of time will increase. It is a single float-valued ranging from 0 to 1 where a value of zero corresponds to no activity, and a value of one corresponds to maximum activity.
• Spatial Extent: amplitude of movements (e.g., expanded versus contracted). This parameter determines the quan- tity of physical displacement of the body parts involved in the communication process (e.g., amplitude of head rotations or arms opening). This attribute, like all the following, is a real number defined in the interval [−1, 1], where zero corresponds to a neutral behavior, that is the behavior of our virtual agent without any expressivity control.
2
• Temporal Extent: duration of movements (e.g., quick versus sustained actions). This parameter modifies the speed of execution of behaviors. Low values produce very quick movements while higher values produce slower ones. For example low values produce very fast head rotations while higher values produce slower rotations.
• Fluidity: smoothness and continuity of movement (e.g., smooth, graceful versus sudden, jerky). Higher fluidity allows smooth and continuous execution of movements; while lower value creates a discontinuity in the move- ments. Figure 1 shows the same movement executed with different fluidity values.
• Power: dynamic properties of the movement (e.g., weak/relaxed versus strong/tense). Higher (resp. lower) values increase (resp. decrease) the acceleration of the muscles contraction, making movements become more (resp. less) powerful. Increasing this parameter will also produce movement overshooting. Figure 2 shows some examples of curves with different tensions.
2) Perceptual tests for behavior expressivity: To validate our expressivity model, we performed two perceptual tests [35]. In the first study we aimed to evaluate the implementation of each expressivity parameter while in the second study we aimed to understand if the set of expressivity parameters would allow us to model expressive behaviors. In both studies videos showing the same behaviors but with different expressivity parameters setting were created. Subjects had to evaluate the videos and find out, for the first study, which parameter has been modified and for the second study, which expressivity setting was the closest to reach a given movement quality. Both tests gave positive results. Subjects could perceive relatively well each expressivity parameter and which movement quality was intended [35].
B. Automatic extraction of expressivity in music performance
CUEX (CUe EXtraction) is an algorithm developed at KTH and Uppsala University for extracting acoustical cues from an expressive music performance [36] [37]. Acoustical cues that can be extracted by CUEX are articulation (legato or staccato), local tempo (number of events in a given time window), sound level (dB), spectrum energy above 1000 Hz, attack speed (dB/s), musical tone, and vibrato. The CUEX algorithm has been validated by testing it on real monophonic expressive performances1 played with electric guitar, piano, flute, violin, and saxophone. In average about 90% of tone onsets were correctly detected. CUEX has also been tested with voice and gave similar results.
Research in music performance has shown that musicians control acoustic cues for communicating emotions when play- ing [40] [41]. Particular combinations and relative values of the cues correspond to specific emotions. In Table I we present the use of acoustic cues by musicians when performing with happiness, anger, or sadness. Complete data have been reported by Juslin [41]. The acoustic cues extracted by CUEX
1These performances were collected, and rated in listening tests in previous experiments [38] [39]. Listeners were able to identify the intended emotions in musicians’ performances.
can be mapped into a 2-dimensional space that represents the expressivity of the performance. The 2-dimensional space is defined by the axes pleasure-displeasure (valence) and degree of arousal (activity) as proposed by Russell [42]. In the present work, acoustic cues extracted by CUEX are mapped onto this 2-dimensional activity–valence space using a fuzzy logic approach [43]. For example, if a piece of music is played with legato articulation, soft sound level, and slow tempo, then it will be classified as “sad”; while it will be classified as “happy” if the performance is characterized by a more staccato articulation, louder sound level, and faster tempo. CUEX is implemented both in Matlab and PD [44]. The latter is a simplified version of the Matlab one, with less precision, but it runs in real-time. In this study we used the PD implementation.
V. VISUALIZATION OF EXPRESSIVITY: FROM ACOUSTIC CUES TO AN ANIMATED VIRTUAL HEAD
In this section we turn our attention to an explicit visual representation of expressivity in music performances. The system, called Music2Greta, has been realized by interfacing the output of the acoustic features extraction system CUEX described in section IV-B with the input of the Greta vir- tual agent [45], see figure 3. Both components communicate through a TCP/IP socket. Acoustic cues extracted by CUEX and the corresponding emotional content are transmitted to Greta in real-time. After receiving them, the module called Acoustic params to expressivity applies a mapping (section V- A) between the acoustic cues and the expressivity parameters of the Greta agent (section IV-A). At the same time the Emotion blending module generates the facial expression that has to be assumed by the Greta’s face (section V-B).
A. Mapping acoustic cues to expressivity parameters
The acoustical cues extracted by CUEX (that is sound level, tempo, articulation) are linearly mapped into the behavior expressivity parameters using a scaling factor to adapt their ranges of variation. The variation of each expressivity param- eter is as follow:
• Sound level. The current sound level of the music per- formance is linearly mapped into the Spatial Extent and Power expressivity parameters. It influences the angle of rotation of head movements (Spatial Extent) as well as their acceleration and quantity of overshooting (Power).
•	Tempo. This parameter represents the local tempo of the musical performance and influences Temporal Extent and Overall Activity expressivity parameters. It acts on the duration of head movements (Temporal Extent), and on the frequency of head movements (Overall Activity).
•	Articulation. It reflects the style and the quantity of the articulation in the music performance, i.e. the amount of staccato or legato. It varies the Fluidity expressivity parameter. For example it acts on the continuity of head movements making them less continuous and less co- articulated as the articulation becomes more and more staccato.
3
Fig. 1.
Fig. 2.
Fig. 3.
Fluidity variation: left diagram represents high fluidity, right diagram represents low fluidity for the same behavior.
Power variation: left diagram represents movement executed with low power; while the right diagram represents the same movement with high power.
TABLE I MUSICIANS’ USE OF ACOUSTIC CUES WHEN COMMUNICATING EMOTION IN MUSIC PERFORMANCE (FROM [41])
Emotion	Acoustic cues	Emotion	Acoustic cues
Sadness	slow mean tempo	Anger	fast mean tempo
Music2Greta architecture
large timing variations low sound level legato articulation small articulation variability soft duration contrasts
dull timbre slow tone attacks flat micro-intonation slow vibrato final ritardando
small tempo variability high sound level staccato articulation spectral noise
sharp duration contrasts sharp timbre abrupt tone attacks accent on unstable notes large vibrato extent
no ritardando Happiness	fast mean tempo	rising micro-intonation
small tempo variability small timing variations high sound level little sound level variability large articulation variability
fast tone attacks bright timbre sharp duration contrasts staccato articulation
4
We can notice that our mapping establishes a mimicry between sound quality and movement quality: loud sound is matched by large head movement; fast speed tempo by rapid movement; staccato performance by discontinuous movement, etc. These positively correlated relations between acoustic cues variation and behaviors quality variation have been noticed between pitch accent on emphatic word(s) and nonverbal behaviors [46].
B. Mapping emotional intention onto the agent’s face
The emotional intention recognized in the music perfor- mance by the CUEX system is mapped onto the facial expres- sion to be displayed by the agent Greta. As described in section IV-B, the CUEX system determines the emotional content of a music performance in real-time. The result takes the form of a vector which coordinates are the amount of happiness, anger, or sadness contained in the actual performance. Thus it is possible that CUEX indicates that more than one emotion is present at the same time; that is, there is more than one emotion for which the level of arousal is greater than zero. In such a case, a blend of emotions is computed. The corresponding facial expression is obtained by applying the rules defined by P. Ekman and W. Friesen [47]. Two facial areas (upper face (eyes and eyebrows) and lower face (cheeks and mouth)) are considered [48], [49]. Facial expressions of blended emotions are computed by combining the expression shown on the upper face of one expression with the expression shown on the lower face of the other expression. Based on Ekman’s research [47], expressions of negative emotions are mainly recognized from the upper face (e.g., frown of anger) while positive emotions are from the lower face (e.g., smile of happiness). We applied these finding and elaborated the following rules:
• if anger and sadness are present: the lower face shows anger (tense lips) and the upper face displays sadness (inner raise eyebrows);
•	if anger and happiness are present: the lower face shows happiness (smile) and the upper face displays anger (frown);
• if happiness and sadness are present: the lower face shows happiness (smile) and the upper face displays sadness (inner raise eyebrow);
•	if anger, sadness and happiness are all three present: the lower face shows happiness (smile) and the upper face displays sadness (inner raise eyebrow). Anger will be revealed through rapid head movements.
Emotions may have visible effect on the agent through two other aspects: skin color and head movements quality. For example, when the music performance becomes “angry” (faster attack and higher spectrum energy) the face becomes redder and leans toward the user; while for sadness emotion (slow attack and low spectrum energy) the face leans backward and becomes paler.
VI. TESTING THE MAPPING BETWEEN EMOTIONAL INTENTION AND EXPRESSIVITY PARAMETERS
Our system is based on several mappings: from music performance to emotional intention, from emotional intention
to facial expression and expressive head movement, and from expressivity parameters to animation. In earlier studies we have already conducted studies related to the first [31] and third mappings [34] (see section IV-A.2 and section IV-B). In the present tests our aim was to find out if the mapping between the emotional intention (as extracted from the acous- tic cues) and the expressivity parameters of the virtual agent Greta were perceived by subjects.
A. Experimental setup
Two groups of subjects took part at the tests. Group 1 consisted of researchers and doctoral students of music acous- tics and speech technology at KTH, 3 females and 4 males, aged 25–46 (average 32), who played a musical instrument in average for 14 years. Group 2 was composed of researchers and doctoral students at the University of Paris8, 2 females and 4 males, aged 24–44 (average 32), who played a musical instrument in average for 5 years. In total, subjects were 13 and of 10 different nationalities.
Fig. 5. there is the agent’s head; below there is the slider used by the subjects for controlling the quality of the head movement.
Screen-shot of interface used for the tests: on top of the window
As musical stimuli, we used performances of two melodies, Brahms’ 1st theme of the poco allegretto 3rd movement Symphony Op.90 No.3, in C minor, and Haydn’s theme from first movement of Quartet in F major for strings, Op. 74 No. 2. The two melodies were performed by a professional guitar player with three emotional intentions: anger, sadness, and happiness. The testing set of musical pieces consisted of 3 ∗ 2 = 6 stimuli (3 emotions, 2 melodies).
Participants were asked to sit in front of a PC where the test application was running. They were also instructed to read the explanation of the test procedure before starting the test.
For the purposes of this test we realized a slightly different version of the Music2Greta system. The test consisted in listening to the 6 pieces of music while watching the virtual agent’s head moving on the screen and having the possibility of altering its movement quality (e.g., faster movement, lower amplitude) by moving a slider on the screen (figure 5 shows the interface used for the tests).
For each musical stimulus, the agent’s face displayed the appropriate emotion. Each subject was instructed to change the position of the slider on the screen and see how the agent’s
5
Fig. 4.	This sequence shows an example of output of the Music2Greta feedback system. From left to right and top to bottom we can see the agent tilting her head on the side while displaying happiness. Then this facial expression fades to sadness as the head rotates downward. Finally its expression changes into anger with the skin reddening and the head leaning towards the user.
head changed its movement quality (the slider did not affect the emotion shown on the agent’s face, which was fixed for each piece of music). When the subject found a good match between the musical stimulus and the agent’s head movement she could go ahead to the next stimulus.
The slider value influenced the agent’s head movement by altering the value of the 5 expressivity parameters described in IV-A. To do so, we had to create a mapping from a 1- dimension variable (the slider value) to a 5-dimension space (the expressivity parameters values).
At first, we had to decide which expressivity values should be considered as the expected expressivity values for each of the 3 emotions in our tests (anger, sadness, and happiness). We based our decision on perceptual studies conducted by Wallbott [21] and Gallaher [18]. We then associated this set of expected expressivity values to a randomly chosen value of the slider which could be anywhere along the range of variation of the slider. The position of the slider associated to the expected expressivity values was unknown to the subjects.
Let us give an example. Figure 6 shows a graph with the correspondence between the slider position (X axis) and the 5 expressivity values (Y axis). In this figure, the value X = −0.3 (that is slider = −0.3) corresponds to the pre- decided expected expressivity values (the dashed box) for the emotion for a given piece of music. Let us see the variation of just one of the expressivity parameters, e.g. Power (PWR). Position X = 0 in figure 6 corresponds to PWR = 0.5. By moving the slider towards the left, i.e. smaller X values, the value of P W R tends to 0, while by moving the slider towards the right PWR tends to 1. Similarly the other parameters Overall Activity (OAC), Fluidity (FLD), Temporal Extent (TMP), and Spatial Extent (SPC) are simultaneously varied when adjusting the slider position.
At the end of the test, the subjects’ choice, that is the final position X of the slider, is compared to the value of X =
−0.3 to check whether our set of expected expressivity values, PWR, OAC, FLD, TMP, and SPC, is correctly perceived by the subjects.
Subjects could listen to each musical stimulus as many times as they liked to, and they could constantly change the position of the slider, while watching the corresponding head’s behavior on the screen. The order of the 6 musical stimuli was randomized for each subject, and the right and left extremes of the slider were randomly switched between subjects.
Fig. 6. of the slider (X) and the values of the 5 expressivity parameters (Y). In the dashed box we highlighted the expected expressivity values for the given example. P W R = Power, OAC = Overall Activity, F LD = Fluidity, T M P = Temporal Extent, and SPC = Spatial Extent.
B. Results and discussion
Since the data collected for the two groups of subjects were not significantly different, they were pooled together in the analysis that follows. The emotional intention of the face and
Example of graph showing the correspondence between the value
6
the performances (the independent variable) had a considerable effect on the listeners positioning of the slider, that controls Overall Activity, Spatial Extent, Temporal Extent, Fluidity and Power (the dependent variables). Main results for the angry, sad, and happy emotional intentions are plotted in Figure 7. As one can observe, there is an interaction of the tonality of the musical stimuli with the expressive movements chosen by the subjects. It is well know that minor tonality is often associated to sadness and major tonality to happiness. This can explain why in this experiment, for the emotional intentions sadness and happiness, subjects tend to choose similar but different expressive movements for the same facial expression depending on the musical stimulus. In particular when a facial expression was presented together with the Haydn’s melody, in Major tonality, subjects tend to choose expressive movements with higher Overall Activity. For the emotional expression of anger, subjects preferred the same expressive movements for both melodies, but that differ from the expected ones.
The expected expressivity values were originally chosen for facial expressions that were not necessarily associated to music listening. This could partly explain the deviations from expected values observed in this experiment. Subjects’ choices suggest that Greta’s expressive movements should be controlled differently when providing feedback to expressive music than when speaking. Greta’s expressivity settings, as given by subjects, were also influenced by the tonality, and the overall character of the musical piece.
VII. CONCLUSION
In this paper, we have presented an application in which an animated virtual head is used as visual feedback on an expressive music performance. The acoustic parameters in the performance, such as tempo, sound level, and articulation, are extracted and analyzed. Their values are used to identify the emotional intention of the performer. The values of these parameters are also mapped onto the behavior expressivity pa- rameters controlling the movement quality of the head. Finally the emotional intention and the expressivity parameters are given in input to an animated virtual agent’s head that shows facial expressions of emotion, and expressive head movement. In this way we have a visualization of what we hear in a music performance, and generally in any audio stream including voice. A possible application could therefore be a virtual butler whose expressive behavior is driven by the acoustic input from the local, remote, or virtual environment. The butler would give real-time, silent, and informative feedback about the acoustic environment.
ACKNOWLEDGMENT
The present work has been supported by the EU funded Human-Machine Interaction Network on Emotion Network of Excellence (HUMAINE http://emotion-research.net/) and COST 287 Action “Gesture CONtrolled Audio Systems” (ConGAS http://www.cost287.org). We are very thankful to Bjo ̈rn Hartmann for developing the gesture expressivity model and to Ste ́phanie Buisine for conducting the evaluation test of this model.
Fig. 7. Fluidity and Power as resulted from the test. Empty circles represent the expected values. Full triangles represent the mean values when the major melody was played. Full squares represent the mean values when the minor melody was played.
REFERENCES
[1] P. N. Juslin and P. Laukka, “Communication of emotions in vocal expression and music performance: Different channels, same code?” Psychological Bulletin, vol. 129, no. 5, pp. 770–814, 2003.
[2] C. L. Krumhansl, “An exploratory study of musical emotions and psy- chophysiology,” Canadian Journal of Experimental Psychology, vol. 51, no. 4, pp. 336–352, 1997.
[3] H. Welbergen, A. Nijholt, D. Reidsma, and J. Zwiers, “Presenting in virtual worlds: Towards an architecture for a 3D presenter explaining 2d-presented information,” in Lecture Notes in Computer Science, vol. 3814, 2005, pp. 203–212.
[4] W. L. Johnson, “Animated pedagogical agents for education training and edutainment,” in ICALT, 2001, p. 501.
[5] J. Beskow, I. Karlsson, J. Kewley, and G. Salvi, “Synface - a talking head telephone for the hearing-impaired,” in Computers helping people with special needs - ICCHP 2004, K. Miesenberger, J. Klaus, W. Zagler, and D. Burger, Eds., 2004, pp. 1178–1186.
[6] L. Chittaro, L. Ieronutti, and R. Ranon, “Navigating 3D virtual envi- ronments by following embodied agents: a proposal and its informal evaluation on a virtual museum application,” PsychNology Journal (Special issue on Human-Computer Interaction), vol. 2, no. 1, pp. 24– 42, 2004.
[7] S. Kopp, L. Gesellensetter, N. Krmer, and I. Wachsmuth, “A conversa- tional agent as museum guide – design and evaluation of a real-world application,” in Intelligent Virtual Agents, P. et al., Ed.	Springer-Verlag, 2005, pp. 329–343.
Mean values for Overall Activity, Spatial Extent, Temporal Extent,
7
[8] E. Figa and P. Tarau, “The VISTA project: An agent architecture for vir- tual interactive storytelling,” in TIDSE’2003, N. Braun and U. Spierling, Eds., Darmstadt, Germany, 2003.
[9] H. G. Wallbott and K. R. Scherer, “Cues and channels in emotion recognition,” Journal of Personality and Social Psychology, vol. 51, no. 4, pp. 690–699, 1986.
[10] S. Dahl, “On the beat: Human movement and timing in the production and perception of music,” Ph.D. dissertation, Speech, Music and Hear- ing, KTH, Royal Institute of Technology, Stockholm, Sweden, 2005.
[11] S. DiPaola and A. Arya, “Affective communication remapping in mu- sicface system,” in Electronic Imaging &amp; Visual Arts, 2004.
[12] M.Cardle,L.Barthe,S.Brooks,andP.Robinson,“Music-drivenmotion editing: Local motion transformations guided by music,” EGUK 2002 Eurographics UK Conference, June 2002.
[13] M. Downie and N. Lefford, “Underscoring characters,” Massachusetts Institute of Technology, May 1999.
[14] H.-C. Lee and I.-K. Lee, “Automatic synchronization of background music and motion in computer animation,” in Eurographics 2005 pro- ceedings, Dublin, Ireland, 2005.
[15] J. Cornwell and B. Silverman, “A demonstration of the pmf-extraction approach: Modeling the effects of sound on crowd behavior,” in 11th BRIMS, SISO, May 2002.
[16] R. Taylor, D. Torres, and P. Boulanger, “Using music to interact with a virtual character,” in The 2005 International Conference on New Interfaces for Musical Expression.
[17] G. Johansson, “Visual perception of biological motion adn a model for its analysis,” Perception and Psychophysics, vol. 14, pp. 201–211, 1973. [18] P. E. Gallaher, “Individual differences in nonverbal behavior: Dimen- sions of style,” Journal of Personality and Social Psychology, vol. 63,
no. 1, pp. 133–145, 1992. [19] G. Ball and J. Breese, “Emotion and personality in a conversational
agent,” in Embodied Conversational Characters, S. P. J. Cassell, J. Sul-
livan and E. Churchill, Eds.	Cambridge, MA: MITpress, 2000. [20] F. E. Pollick, “The features people use to recognize human movement style,” in Gesture-Based Communication in Human-Computer Interac- tion - GW 2003, ser. LNAI, A. Camurri and G. Volpe, Eds.	Springer,
2004, no. 2915, pp. 10–19. [21] H. G. Wallbott, “Bodily expression of emotion,” European Journal of
Social Psychology, vol. 28, pp. 879–896, 1998. [22] K. R. Scherer, “Vocal communication of emotion: A review of research
paradigms,” Speech Communication, vol. 40, pp. 227–256, 2003. [23] K. R. Scherer, T. Johnstone, and G. Klasmeyer, “Vocal expression of emotion,” in Handbook of the Affective Sciences, R. J. Davidson, H. Goldsmith, and K. R. Scherer, Eds.	New York and Oxford: Oxford
University Press, 2003, pp. 433–456. [24] K.R.Scherer,A.Schorr,andT.Johnstone,Eds.,AppraisalProcessesin
Emotion: Theory, Methods, Research, ser. Series in Affective Science.
Oxford University Press, 2001. [25] A. Gabrielsson, “The performance of music,” in The Psychology of
Music, D. Deutsch, Ed.	San Diego: Academic Press, 1999, pp. 501–
602. [26] ——, “Music performance research at the millennium,” Psychology of
Music, vol. 31, no. 3, pp. 221–272, 2003. [27] A.FribergandG.U.Battel,“Structuralcommunication,”inTheScience
and Psychology of Music Performance: Creative Strategies for Teaching and Learning, R. Parncutt and G. E. McPherson, Eds.	New York and Oxford: Oxford University Press, 2002, pp. 199–218.
[28] J. Sundberg, “Emotive transforms,” Phonetica, vol. 57, pp. 95–112, 2000.
[29] P. N. Juslin and P. Laukka, “Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion,” Emotion, vol. 1, no. 4, pp. 381–412, 2001.
[30] P. N. Juslin and J. A. Sloboda, Eds., Music and emotion: Theory and Research.	New York and Oxford: Oxford University Press, 2001.
[31] P. Laukka, P. N. Juslin, and R. Bresin, “A dimensional approach to vocal expression of emotion,” Cognition and Emotion, vol. 19, no. 5, pp. 633–653, 2005.
[32] K. R. Scherer, “Emotion expression in speech and music,” in Music, language, speech, and brain, J. Sundberg, L. Nord, and R. Carlson, Eds.	London: Macmillan, 1991, pp. 146–156.
[33] ——, “Expression of emotion in voice and music,” Journal of Voice, vol. 9, no. 3, pp. 235–48.
[34] B. Hartmann, M. Mancini, and C. Pelachaud, “Implementing expressive gesture synthesis for embodied conversational agents,” in The 6th International Workshop on Gesture in Human-Computer Interaction and Simulation, VALORIA, University of Bretagne Sud, France, 2005.
[35] B. Hartmann, M. Mancini, S. Buisine, and C. Pelachaud, “Design and evaluation of expressive gesture synthesis for embodied conversational agents,” in Third International Joint Conference on Autonomous Agents &amp; Multi-Agent Systems (AAMAS), Utretch, July 2005.
[36] A. Friberg, E. Schoonderwaldt, P. N. Juslin, and R. Bresin, “Automatic real-time extraction of musical expression,” in International Computer Music Conference - ICMC 2002.	San Francisco: International Computer Music Association, 2002, pp. 365–367.
[37] A. Friberg, E. Schoonderwaldt, and P. N. Juslin, “Cuex: An algorithm for extracting expressive tone variables from audio recordings,” in Acoustica united with Acta Acoustica, in press.
[38] A. Gabrielsson and P. N. Juslin, “Emotional expression in music perfor- mance: Between the performer’s intention and the listener’s experience,” Psychology of Music, vol. 24, pp. 68–91, 1996.
[39] P.N.JuslinandE.Lindstro ̈m,“Musicalexpressionofemotions:Model- ing composed and performed features,” in Abstracts of the 5th ESCOM Conference, 2003.
[40] A. Gabrielsson and P. N. Juslin, “Emotional expression in music,” in Handbook of affective sciences, H. H. Goldsmith, R. J. Davidson, and K. R. Scherer, Eds.	New York: Oxford University Press, 2003, pp. 503–534.
[41] P. N. Juslin, “Communicating emotion in music performance: A review and a theoretical framework,” in Music and emotion: Theory and research, P. N. Juslin and J. A. Sloboda, Eds.	New York: Oxford University Press, 2001, pp. 305–333.
[42] J. A. Russell, “A circumplex model of affect,” Journal of Personality and Social Psycholog, vol. 39, no. 6, pp. 1161–1178, 1980.
[43] A. Friberg, “A fuzzy analyzer of emotional expression in music perfor- mance and body motion,” in Music and Music Science 2004, J. Sundberg and W. Brunson, Eds., Stockholm, 2005.
[44] M.Puckette,“Puredata,”inInternationalComputerMusicConference- ICMC1996. SanFrancisco:InternationalComputerMusicAssociation, 1996, pp. 269–272.
[45] C. Pelachaud and M. Bilvi, “Computational model of believable conver- sational agents,” in Communication in Multiagent Systems, ser. Lecture Notes in Computer Science, M.-P. Huget, Ed.	Springer-Verlag, 2003, vol. 2650, pp. 300–317.
[46] D. Bolinger, Intonation and its Part.	Stanford University Press, 1986. [47] P. Ekman and W. Friesen, Unmasking the Face: A guide to recognizing
emotions from facial clues.	Prentice-Hall, Inc., 1975. [48] T. D. Bui, D. Heylen, M. Poel, and A. Nijholt, “Generation of facial expressions from emotion using a fuzzy rule based system,” in Proceed- ings of 14th Australian Joint Conference on Artificial Intelligence (AI 2001), D. C. M. Stumptner and M. Brooks, Eds.	Adelaide, Australia:
Springer, 2003, pp. 83 – 94. [49] M. Ochs, R. Niewiadomski, C. Pelachaud, and D. Sadek, “Intelligent
expressions of emotions,” in Affective Computing and Intelligent Inter- action, First International Conference, ser. Lecture Notes in Computer
Science, J. Tao, T. Tan, and R. W. Picard, Eds., vol. 3784. 2005, pp. 707–714.
Springer,
Maurizio Mancini IUT de Montreuil - Universite ́ de Paris 8 - 140 rue de la Nouvelle France 93100 Montreuil, France phone: +33 (0) 148703463 email: m.mancini(at)iut.univ-paris8.fr
Roberto Bresin KTH - Royal Institute of Technology, CSC - School of Computer Science and Communication, TMH - Department of Speech, Music and Hearing - Lindstedtsva ̈gen 24 - 100 44 Stockholm, Sweden phone: +46 (8) 7907876 email: roberto(at)kth.se
Catherine Pelachaud IUT de Montreuil - Universite ́ de Paris 8 - 140 rue de la Nouvelle France 93100 Montreuil, France phone: +33 (0) 148703702 email: c.pelachaud(at)iut.univ-paris8.fr
8</Text>
        </Document>
        <Document ID="173">
            <Title>Ian_grant_2</Title>
        </Document>
        <Document ID="395">
            <Title>The Augmented Silhouette</Title>
        </Document>
        <Document ID="318">
            <Title>Physics-based Animation</Title>
            <Text>
physical simulation
physics engines

evaluation of different techniques

box2D, chipmunk, bullet, PhysX,

PRACTICAL TEST - switching animation and physical simulation...
between primary /secondary motion
PROCESS - Speedy prototyping - [TODO - USE OF UNITY3D]
DEFINE - soft bodies
DEFINE - rigid bodies
DESCRIBE - aegia physx engine (now owned by nvidia)
http://developer.nvidia.com/object/physx_features.html#soft

\begin{itemize}
\item design factors - multitouch interface

\item primitive geometry

\item arbitrary collision shapes.
\end{itemize}

Select work of Jean Baudrillard is an important theoretical starting point for the ideas explored in this chapter. Not the totality of his work but select concepts resonate with the sense of object, simulation and interactions with technology. Particularly, the theorisation of objects [Baudrillard (2005 (1996))][#Baudrillard:2005vn] and his definitions of the orders of simulacra [Baudrillard (1994)][#Baudrillard:1994fk]:

: "Three orders of simulacra:

simulacra that are natural, naturalist, founded on the image, on imitation and counterfeit, that are harmonious, optimistic, and that aim for the restitution or the ideal institution of nature made in God's image;

simulacra that are productive, productivist, founded on energy, force, its materialization by the machine and in the whole system of production-- a Promethean aim of a continuous globalization and expansion, of an indefinite liberation of energy ... .

simulacra of simulation, founded on information, the model, the cybernetic game-- total operationality, hyperreality, aim of total control." [p.121][#Baudrillard:1994fk]
On holograms, 

: "Three-dimensionality of the simulacra-- why would the simulacra with three dimensions be closer to the real than the one with two dimensions?" [p.107][#Baudrillard:1994fk]

[UNPACK] Closer to the real-
[UNPACK] aniconic / God's image - observe the historic connection with Java / Malaysian shadow forms where a deity could only be seen to be represented by a shadow and not a physical form.
[UNPACK] simulations that are productive could refer to those that generate emotion engagement, those that are used instrumentally as tools for expression.
[UNPACK] energy, force - though only poetically connected, simulations of 3D worlds that attempt to conserve simulations of Newtonian physics mechanical / optical
[UNPACK] [DEFINE] the cybernetic game and make reference to 'total control' - which is a significant part of the dynamic of the puppeteer.

[THOUGHT] There is a dialectical relationship between total control and the accidental expressivity of chaos;

Physical simulation in computer space is a broad and involved area. In the domain of digital puppetry, that can easily occupy a fused sense of how 3D spaces get 'projected' upon 2D surfaces, the simulation of Newtonian forces

:"In computer graphics we are given great freedom - we can develop scenes in which the fundamental laws of physics that we associate with our surroundings are followed or we can adjust, change and even disregard such laws. For example, in the case of animated images we need not adhere to Newton's Laws of motion - we can create scenes in which action and reaction are not equal and opposite (however, the repercussions may be somewhat surprising...)" [p.352][#Blundell:2008vn]


</Text>
        </Document>
        <Document ID="207">
            <Title>5.4.7 Programmable and conﬁgurable talking toys </Title>
            <Text>Talking toys are of greater value when they are programmable and configurable by children 
</Text>
        </Document>
        <Document ID="284">
            <Title>Statement of Originality</Title>
            <Text>I will deliver original:

- Performance, Artwork and Artefacts;
- Software product and creative production techniques;
- Innovative control systems and interfaces that will be applicable hopefully beyond the domain of performance;


I hereby declare that this thesis, or parts thereof, has not been and will not be submitted in whole or in part to another University for the award of any other degree.


Signature:




Ian John Grant</Text>
        </Document>
        <Document ID="18">
            <Title>aisb04-bevacqua</Title>
            <Text>E. Bevacqua
Department of Computer and System Science University of Rome “La Sapienza” elisabetta.bevacqua@libero.it
Abstract
We aim at the realization of an Embodied Conversa- tional Agent able to interact naturally and emotionally with user(s). In previous work [23], we have elaborated a model that computes the nonverbal behaviors associated to a given set of communicative functions. Specifying for a given emotion, its corresponding facial expression will not produce the sensation of expressivity. To do so, one needs to specify parameters such as intensity, ten- sion, movement property. Moreover, emotion affects also lip shapes during speech. Simply adding the facial ex- pression of emotion to the lip shape does not produce lip readable movement. In this paper we present a model that adds expressivity to the animation of an agent at the level of facial expression as well as of the lip shapes.
1. Introduction
With the development of 3D graphics, it is now possible to create Embodied Agents that can talk simulating that kind of communication that people know since they are born. Moreover, nonverbal communication is as important as verbal one. Facial expressions can provide a lot of information. In particular they are good window on our emotional state [9]. Emotions are a fundamental aspect of human life influencing how we think and behave and how we interact with others. Facial expressions do improve communication [13]; they can make clear what we are thinking, even without speaking. For example wrinkling our nose in front of something that we dislike communicates very clearly our disgust. Therefore, believable synthetic characters make the interaction between users and machine easier and more fulfilling, providing a more human-like communication. Experiments have shown that interface with facial displays reduces the mental gap between users and computer systems [25].
Most of the ECA systems developed so far have been concentrated in defining the appropriate nonverbal behav- ior linked to speech. But nonverbal behaviors are charac- terized not only by the signals of the expression itself but
M. Mancini and C. Pelachaud
LINC - Paragraphe IUT of Montreuil - University of Paris 8 m.mancini@iut.univ-paris8.fr c.pelachaud@iut.univ-paris8.fr
also by temporal parameters (e.g. time of appearance and disappearance of an expression) and by muscular activi- ties quality (such as tense movement). In the aim of cre- ating believable embodied conversational agent (ECA) able to serve as bridge in the communication between hu- mans and the machine, ECA ought to be empowered with human-like qualities. In this paper we present a compu- tational model of expressivity for facial expression and for the lip movements. The work presented in this pa- per is part of the Greta system that have been developed within the EU project MagiCster 1 . The system takes as input a text tagged with communicative functions in- formation. The language of tags is called Affective Pre- sentation Markup Language, APML [6]. APML is used as a script language to control the agent’s animation. To endow agent with expressivity quality we have extended APML.
In the next section we prevent an overview of the state of the art. We then present our extension of APML. We follow by describing the lip shape model.
2. Stateoftheart
There is not a single way to approach the issue of emo- tions in ECAs. S. Kshirsagar and her colleagues have developed a 3D virtual characters with emotions and per- sonality [8]. The agent can maintain a basic dialogue with users. Through a personality and emotion simulators the agent responds naturally to, both, the speech and the fa- cial expressions of the user that are tracked in real time.
M. Seif El-Nasr, J. Yen and T. Ioerger [11] imple- mented a new computational model of emotions that uses a fuzzy-logic representation to map events and observa- tions to emotional states. They based their work on the research on emotions that shows that memory and expe- rience have a major influence on the emotional process.
A. Paiva and her research team approached the prob- lem of modelling emotional states of synthetic agents
1IST project IST-1999-29078, partners: University of Edinburgh, Division of Informatics; DFKI, Intelligent User Interfaces Department; Swedish Institute of Computer Science; University of Bari, Diparti- mento di Informatica; University of Rome, Dipartimento di Informatica e Sistemistica; AvartarME.
Speaking with Emotions
when implementing a computer game (FantasyA) where two opponents fight each other [14]. The battle is played in a virtual arena by two characters, one controlled by the user (through the doll SenToy), and the other by the system. They made their own decisions but are influ- enced by the emotional state induced by the user. So the agent’s actions depends on its emotions, on its oppo- nent’s emotions, and on its personality. Carmen’s Bright IDEAS [19] is an interactive drama where characters ex- hibit gestures based on their emotional states and per- sonality traits. Through a feedback mechanism a ges- ture made by of a character may modulate its affective state. A model of coping behaviors has been developed by Marsella and Gratch [20]. The authors propose a model that embeds information such as the personality of the agent and its social role.
All those works aim to define models being able to decide what kind of emotion synthetic agents should “feel” in a particular situation.
Another area of the research on emotions in virtual characters is concerned with the expressivity of emotions. M. Byun and N. I. Badler [3] proposed a method for facial animation synthesis varying preexistent expres- sions by setting a small number of high level parameters (defined based on the Laban notation [15] that drive low level facial animation parameters (FAPs from MPEG-4 standard [7]). The system is called FacEMOTE [3] and derives from EMOTE [4] which has been origanally de-
veloped for arm gestures and postures. Very few computational models of lip shape take
into account emotion. M. M. Cohen and D. W. Mas- saro developed a coarticulation model [5], based on Lo ̈fqvist’s gestural theory of speech production [16]. To model lip shapes during emotional speech, they add the corresponding lip shapes of emotion to viseme definition [21].
Our work is somewhat related to facEMOTE but we do not modified directly the animation stream; rather we create a new FAP stream to animate our 3D agent from an input text where tags specifying communicative functions are embedded [24, 6]. Our approach differs from other ones as we have added qualifiers parameters to simulate expressivity in facial expressions as well as in lip move- ments. Moreover we drive our computational model of emotional lip shapes from real data [17].
3. Temporalcharacteristicsoffacial expressions
A facial expression is not only identified by a configura- tion of facial muscles but is also important how this con- figuration will be temporally activated. For example we can consider, starting from an initial neu- tral state, the time (or, identically, the speed) needed for the facial muscles to contract and reach the final state cor-
responding to the final expression; we call it onset time. Similarly, the corresponding time needed for the muscles to change from their current state back to the rest position is called offset time.
So a single expression (in the sense of ”muscular config- uration”) can assume different expressivity depending on the manner it appears (onset), the time it remains on the face (called the apex time) and finally the speed it disap- pears (offset). We have chosen this specification to rep- resent the temporal characteristics of facial expressions [22]. One example of this representation is shown in Fig- ure 1. The time is on the x axis while on the y axis there is the intensity of the expression; where 0 means ”muscles in rest position” and 1 represents ”muscles in final posi- tion”.
Figure 1: Temporal course of a facial expression.
Although these three parameters are extremely important in delivering expressivity of an expression, few studies assigning values to them exist; but vision systems could be used to extract them [12, 28].
It has been shown that, for example, expressions of sad- ness usually disappear slowly from the face. Thus, they have a long offset time [9]. On the other hand, expres- sions of joy appear very rapidly; this is characterized by a short onset time. Sometimes expressions may also differ in duration (apex time). Finally, fake expressions gener- ally appear either too late or too early (e.g., polite smile versus real happiness smile); thqt is the value of delay varies.
4. Intensityoffacialexpressions
Facial movements corresponding to an expression can be produced with different intensity. An eyebrow can raise a little or a lot. A mild happiness will produce a small smile, while a great happiness will be shown by a large smile. Besides, P. Ekman found that if the emotion is felt very lightly, not every facial movement correspond- ing to the emotion will be visibly displayed; in the sense that changes expressed in the face may not be perceiv-
able [10]. For example, in the case of mild fear like ap- prehension, only slight expression around the mouth may be shown; while for extreme fear both areas, the muscles around the eyes and the mouth, are very tense [9].
Thus, the intensity of an emotion controls not only the amount of movements (strong or light) but also the ap- pearance of some movements.
5. Givingexpressivitytofacialexpressions
Until now we have been concentrating in elaborating computational models that define the most appropriate non verbal behaviors from an input text. The nonver- bal behaviors as specified within APML correspond to frozen facial expressions; a facial expression is linked to an intensity; its temporal course is given by the XML span... Aiming at adding life characteristics to the agent, we have realized some modifications to the APML lan- guage in order to allow the Greta agent to communicate a wider variety of facial expressions of emotion as well as to allow for a more flexible definition of facial expres- sions and their corresponding parameters. Expressivity may be expressed differently depending on the consid- ered modality: a face has different variables (timing vari- ations, muscular intensity, as we have seen in sections 3 and 4) compared to gaze (length of mutual gaze, ratio of gaze aversion and looking at, ...). These modifications re- fer mainly to facial expressions timings as well as to their intensity.
An APML tag defines the meaning of a given commu- nicative act [23]; the Greta engine looks up in a library of expression to instantiate this meaning into the corre- sponding facial expression. A facial expression has 3 temporal parameters as defined in Section 3: onset, offset and apex. In the previous version of the Greta engine, the value of the onset and of the offset were set as constants. An expression was set to start at the beginning of the tag and to finish at its end. That is the apex of an expression was set to be the total time length of a tag (computed as the duration of the speech embedded in the tag; this dura- tion being provided by the speech synthesizer) minus the onset and offset times. APML provides a scheme to spec- ify the mapping between meaning and signals for a given communicative act. We have extended APML to allow one to alter the expressivity of a communicative act. We have introduced a new set of 5 attributes that act both on expressions timings and intensities.
For each APML tag it is possible to specify one or more of the following attributes:
Delay : specifies the percentage of delay before an ex- pression arises; it forces the Greta engine to delay the start of an expression for a certain time. This time is specified by a percentage of the total de- fault animation time (that is the time of the speech embedded in the XML tag). If not specified, the
default delay value is 0% (that is there is “no de- lay”); this corresponds to the previous version of APML.
Duration : specifies the total duration of an expression. It is specified as a percentage of the default expres- sion duration. The Greta engine will set the dura- tion of an expression (the apex of an expression) to last for this new value. The default value is 100% (that is “normal duration”); this corresponds to the previous version of APML.
Onset : specifies a value for the onset. This value is given as a number of animation frames that the en- gine have to use to render the “onset” phase of an expression. The default value is 0 and this tells to the engine to set the onset value to constant as defined in the previous version of APML and ex- plained before.
Offset : specifies a value for the offset. This value is given as a number of animation frames that the en- gine have to use to render the “offset” phase of an expression. The default value is 0 and this tells to the engine to set the onset value to constant as defined in the previous version of APML and ex- plained before.
Intensity : corresponds to a factor that multiplies the quantity of movement of each FAP involved in the facial expression. Until now the corresponding fa- cial expression to a meaning for a given commu- nicative act was explicitly defined. It was not pos- sible to modify on the fly such a mapping. In order to have a facial expression with lower or greater intensity, one had to create a new entry in the li- brary of expressions. This was quite cumbersome. To remedy this lack of flexibility, we introduce an intensity factor that can modify automatically any defined expression. Our facial model is MPEG-4 compliant [7]. An expression is defined by a set of FAPs (Facial Animation Parameter). The varia- tion of their intensity corresponds to the modifica- tion of the value of each FAP. The default intensity value is 1 meaning that values of the FAPs defining an expression in the library of expressions are not changed.
Let us consider the following example:
&lt;theme belief-relation="gen-spec" affect="anger">
some text &lt;/theme>
The timings of the expression as evaluated in the tag are given in the Figure 2.
Let us consider now the same example with the intro- duction of the new tags. For example we may have:
Figure 2: Temporal course of a facial expression.
&lt;theme belief-relation="gen-spec" affect="anger" delay="40%" duration="40%" onset="4" offset="4" intensity="1.5" >
some text &lt;/theme>
The type of expression evaluated in the tag remains unchanged; what change are the temporal and intensity parameters of the expression. Figure 3 illustrates these changes. Figure 4 shows the resulting expressions as they can be seen on the agent’s face.
Figure 3: Temporal course of a facial expression when taking into account the new modifiers of APML.
Another example of how intensity can modify the ap- pearance of an expression is given in Figure 5, in which an expression of joy is computed with an intensity of 1 in the first image and an intensity of 1.5 in the second.
6. Computationallipmodel
Our lip model is based on captured data of triphones of the type ′V CV where the first vowel is stressed whereas the second is unstressed. The data has been collected with the optical-electronic system ELITE that applies passive markers on the speaker’s face [17]. The data covers not only several vowels and consonants for the neutral ex- pression but also different emotions, namely joy, anger, sadness, surprise, disgust and fear [17]. The original
(a) intensity=1	(b) intensity=1.5
Figure 4: Anger expression: in figure (b) the frown is more intense, the lips are more tense and the teeth are clenched.
(a) intensity=1
(b) intensity=1.5
Figure 5: Joy expression: in figure (b) the cheeks are more raised, the lips are more widely open and the smile is larger.
curves from the real speech data represent the variation over time of the 7 phonetically and phonologically rele- vant parameters that define lip shapes: upper lip height (ULH), lower lip height (LLH), lip width (LW), upper lip protrusion (UP), lower lip protrusion (LP), jaw (JAW) and lip corners (LC). On such curves we have selected the maximum or the minimum (target point) to characterize each viseme. To get a good representation of the charac- teristics of a vowel and of the breadth of its original curve, we choose two more points; one between the onset of the phoneme and its target and the other between its target and the offset of the phoneme. Instead, consonants are well represented considering just the target point. Since consonants, and at a slightest degree the vowels, are in- fluenced by the context, we collect their targets from ev- ery ’V CV contexts (for instance, for the consonant /m/, the targets points are extracted from the contexts /′ama/, /′eme/, /′imi/, /′omo/ and /′umu/).
Targets data of vowels and consonants have been stored in a database. Besides targets values, other infor-
mation have been collected like the vowel or the conso- nant that defines the surrounding context, the duration of the phoneme and the time of the targets in this interval. A similar database for each of the six fundamental emotions and for the neutral expression have been created.
6.1. Lipshapealgorithm
We have developed an algorithm that determines for each viseme the appropriate values of the 7 labial parameters by applying coarticulation and correlation rules in order to consider the vocalic and the consonantal contexts as well as muscular phenomena such as lip compression and lip stretching [2].
Our system works as follow. It takes as input a text (tagged with APML) an agent should say. The text file is decomposed by Festival [26] into a list of phonemes with their duration. The first step of our algorithm consists in defining fundamental values, called target points, for ev- ery parameter of each viseme associated with the vowels and consonants that appear in this phoneme list. A tar- get point corresponds to the target position the lips would assume at the apex of the viseme (which may not always correspond to the apex of the phoneme production [17]). These targets have been collected analyzing the real data described above and are stored in a database, one for each emotion.
Afterwards, the algorithm modifies the targets ac- cording to the emotion in which the phonemes are ut- tered, to the coarticulation and correlation rules and to the speech rate.
Finally, the lip animation is computed on those tar- gets.
Table 1: Matrix of the emotion Mild-Joy. 7. Emotionmodel
We describe each emotion through a 7x7 matrix. The rows correspond to the seven recorded emotions, whereas the columns are the lip parameters. A value in the matrix represents the percentage of dependence that the corre- sponding lip parameter has on the corresponding emo- tion. Therefore, the value of the targets for each labial parameter will be an interpolation among the targets in the emotions that have a value on the column different
from zero. Obviously the seven emotions have all 1 in the row corresponding to the emotion itself.
Let us consider the consonant /b/ in the triphone /′aba/ uttered with the emotion ‘mild-joy’. The matrix of this emotion is shown in Table 1. Now, let Naba be the target of the lip width parameter of /b/ uttered in a neutral emotion and let Jaba be the target of the same lip parameter of /b/ uttered in the ‘joy’ emo- tion. The new value of this target MJaba in the mild-joy emotion will be:
MJaba =0.2∗Naba +0.8∗Jaba As consequences, the lip width will be less wide in
the ‘mild-joy’ emotion than in the ‘joy’ emotion.
7.1. Expressivityqualifiers
We have also defined two qualifiers to modulate the ex- pressivity of a lip movement. The first one, Tension De- gree, can be Strong, Normal and Light. It allows one to set different intensities of muscular strain. Such a tension may appear for the expressions of emotions like fear and anger. Such a tension can also appear, for example, when a bilabial consonant (as /b/) is uttered and lips compress against each other, or when labial width increases and lips stretch them getting thinner. The second qualifier, Artic- ulation Degree, can take the values Hyper, Medium and Hypo. During hypo articulation, it may happen that lip targets may not reach their apex.
LOGISTIC FUNCTION
(a)	Anticipatory	coarticula-	(b) Carry-Over coarticulation. tion.
Figure 6: Logistic Function - Strong Degree of influence.
8. Coarticulationandcorrelationrules
Once all the necessary visemes have been loaded from the database and modified according to the emotions and
ULH
LLH
JAW
LW
UP
LP
CR
Neutral
0
0
0
0.2
0.1
0.1
0.2
Joy
1
1
1
0.8
0.9
0.9
0.8
Surprise
0
0
0
0
0
0
0
Fear
0
0
0
0
0
0
0
Anger
0
0
0
0
0
0
0
Disgust
0
0
0
0
0
0
0
Sadness
0
0
0
0
0
0
0
the expressiveness qualifiers, coarticulation and correla- tion rules are applied. In fact, to be able to represent visemes associated to vowels and consonants, we need to consider the context surrounding them [2]. Firstly, let us consider consonants. Since vowels are linked by a hi- erarchical relation for their degree of influence over con- sonants(u>o>i>e>a)[18,27],wefirstdetermine which vowels will affect the consonants in V1 C1 . . . Cn V2 sequence and which labial parameters will be modified by such an influence. Vowels act mainly over those lip parameters that characterize them. The new targets of the consonants for each lip parameter are computed through a linear interpolation between the consonantal targets V1 CiV1	in the context deriving from the vowel V1 and the consonantal targets V2 CiV2	in the context of the vowel V2:
V1 CiV2 = k∗V1 CiV1	+(1 − k)∗V2 CiV2	(1)
The interpolation coefficient k is determined through a mathematical function, called logistic function, whose analytic equation is:
f(t) =	a	+ c 1+e−bt
This function represents the influence of a vowel over adjacent consonants, on the labial parameters that charac- terize it, and allows one to obtain carry-over coarticula- tion and anticipatory coarticulation (see Figure 7.1). The constants a and c force the function to be defined between 0 and 1 both on the abscissa and on the ordinate simpli- fying the computation. The constant b defines the slope of the curve that represents different degrees of influence. Time t=0 corresponds to the occurrence of V1, and time t=1 corresponds to V2. The consonants Ci are placed on the abscissa depending on their temporal normalized dis- tance from the vowels.
INF E MOSTRA
For example, let us consider the sequence /’ostra/ taken from the Italian word ’mostra’ (’exhibition’) and the lip parameter UP. To obtain the curve representing the upper protrusion, the consonantal targets of /s/, /t/ and /r/ in the context oCa must be calculated. The vowel /’o/ exerts a strong influence over the following three con- sonants and the algorithm chooses the steepest influence function. Figure 7(a) shows how the logistic function is applied to define the interpolation coefficients and Figure 7(b) shows how the targets are modified through equation (1).
Once all the necessary visemes have been calculated, correlation rules are applied modifying the value of the targets to simulate muscular tension. For example, when a bilabial consonant (as /b/) is uttered, lip compression must appear. Thus when a strong lip closing occurs the FAPs on the external boundary of the lips must be further lowered down.
Vocalic targets are modified in a very similar way, ac- cording to the consonantal context in which appear. Con- sonants are grouped on the base of which labial param- eters they influence. For example, /b/, /m/ and /p/ will affect vowels on ULH and LLH parameters.
Finally, lip movement over time is obtained interpo- lating the computed visemes through Hermite Interpola- tion.
8.1. Speechrate
Speech rate strongly influences labial animation. At a fast speech rate, lip movement amplitude is reduced while at a slow rate it is well pronounced (lip height is wider when an open vowel occurs or lip compression is stronger when a bilabial consonant is uttered).
To simulate this effect, at a fast speech rate the value of targets points is diminished to be closer to the rest po- sition; while at a slow speech rate, lips fully reach their targets.
9. Evaluationtests
As evaluation tests, we compare the original curves with those computed by our algorithm. As first example let us consider the triphone /’aba/ uttered either in joy and in neutral expression. Figure 8 shows the curves that repre- sent Lip Opening. We have differentiated the movement of the lower lip from the movement of the upper lip (to get a finer movement description). So the Lip Opening curves are obtained as a sum of ULH and LLH curves. The generated curves are shown in Figure 8(a) whereas the original ones in Figure 8(b). In both figures the red dotted line represents the lip movement in joy emotion, while the blue solid line describes the lip opening in neu- tral expression. As one can see the joy emotion causes a reduction of the lip opening (8(b)). The same behavior is also shown in generated curves (see Figure 8(a)).
(a) influence of /’o/ on /s/, /t/ and /r/.
(b) Alteration of UP targets for the Italian word /mostra/.
Figure 7: Coarticulation effects on consonants.
The second example is quite similar to the first one but the word uttered is /mamma/ and the emotions con- sidered are disgust and neutral. Lip Opening curves are shown in Figure 9. The red dotted line represents the lip movement in disgust emotion and the blue solid one is the lip opening in neutral expression. Figure 9(a) shows the computed curves whereas Figure 9(b) the original ones. Like joy, disgust emotion causes a diminution of lip open- ing. Both, the original and the computed curves display the same behavior.
In both examples, phonemes segmentation is identi- fied by vertical lines.
For the generated curves, times are given by Festival, while for the original ones times come from the analy- sis of real speech. Moreover the generated curves always start at the neutral position. This is not necessarily true in the original data. Those differences can make the calcu- lated curves slightly different from the original ones.
MAMMA
(a) Generated curves.
(b) Original curves.
(a) Generated curves.
(b) Original curves.
ABA
Figure 9: Lip Opening for the Italian word /mamma/ for the disgust and neutral expression.
11. References
[1] Benguerel, A. P. and Cowan, H. A., “Coarticulation of upper lip protrusion in french,” Phonetica, vol. 30, pp. 40–51, 1974.
[2] E.BevacquaandC.Pelachaud.Modellinganitalian talking head. In Auditory-Visual Speech Processing AVSP’03, Saint-Jorioz, France, 2003.
[3] M. Byun and N. Badler, “FacEMOTE: Qualitative parametric modifiers for facial animations,” Sym- posium on Computer Animation, San Antonio, TX, July 2002.
[4] D.M. Chi, M. Costa, L. Zhao, and N.I. Badler. The EMOTE model for effort and shape. In Kurt Ake- ley, editor, Siggraph 2000, Computer Graphics Pro- ceedings, pages 173–182. ACM Press / ACM SIG- GRAPH / Addison Wesley Longman, 2000.
[5] M.M.CohenandD.W.Massaro.Modelingcoartic- ulation in synthetic visual speech. In M. Magnenat- Thalmann and D. Thalmann, editors, Models and Techniques in Computer Animation, pages 139– 156, Tokyo, 1993. Springer-Verlag.
[6] B. DeCarolis, C. Pelachaud, I. Poggi, and M. Steed- man.	APML, a mark-up language for believ- able behavior generation.	In H. Prendinger and M.	Ishizuka,	editors,	Life-like	Characters.	T ools, Affective Functions and Applications, pages 65–85. Springer, 2004.
[7] P. Doenges, T.K Capin, F. Lavagetto, J. Ostermann, I.S. Pandzic, and E. Petajan. MPEG-4: Audio/video
Figure 8: Lip Opening for the triphone /’aba/ for the joy and neutral expression.
10. ConclusionandFutureDevelopment
We have presented a model that adds expressivity to the animation of an agent. Expressivity is related not only to the specification of a facial expression but also how this expression is modulated through time and depend- ing on the context. In this paper we have presented parameters to characterize expressivity along the face modality. We have also described a computation model of emotional lip movement. Our next step is to per- form some perceptual evaluation tests to further check the feasibility of our models. Movies illustrating our method may be seen at the URL: http://www.iut.univ- paris8.fr/ ̃pelachaud/AISB04.
and synthetic graphics/audio for real-time, interac- tive media delivery, signal processing. Image Com- munications Journal, 9(4):433–463, 1997.
[8] A. Egges, S. Kshirsagar and N. Magnenat- Thalmann, “A Model for Personality and Emotion Simulation,” KES 2003: 453-461.
[9] P. Ekman and W. Friesen. Unmasking the Face: A guide to recognizing emotions from facial clues. Prentice-Hall, Inc., 1975.
[10] P. Ekman and W. Friesen. Felt, false, miserable smiles. Journal of Nonverbal Behavior, 6(4):238– 251, 1982.
[11] M. S. El-Nasr, J. Yen and T. Loerger, “FLAME - Fuzzy Logic Adaptive Model of Emotions,” Inter- national Journal of Autonomous Agents and Multi- Agent Systems, 3(3):1-39.
[12] I. A. Essa. Analysis, Interpretation, and Synthesis of Facial Expressions. PhD thesis, MIT, Media Labo- ratory, Cambridge, MA, 1994.
[13] A. J. Fridlund and A. N. Gilbert, “Emotions and facial expression,” Science, 230 (1985), pp. 607- 608.
[14] K. Ho ̈o ̈k, A. Bullock, A. Paiva, M. Vala and R. Prada, “ FantasyA - The Duel of Emotions,” in Proceedings of the 4th International Working Con- ference on Intelligent Virtual Agents - IVA 2003.
[15] R. Laban and F.C. Lawrence. Effort: Economy in body movement. Plays, Inc, Boston, 1974.
[16] A. Lo ̈fqvist’s, “Speech as audible gestures,” Speech Production and Speech Modeling, pp. 289–322, 1990.
[17] E. Magno-Caldognetto, C. Zmarich, and P. Cosi. Coproduction of speech and emotion. In ISCA Tuto- rial and Research Workshop on Audio Visual Speech Processing, AVSP’03, St Jorioz, France, September 4th-7th 2003.
[18] E. Magno-Caldognetto, C. Zmarich and P. Cosi, “Statistical definition of visual information for Ital- ian vowels and consonants,”	in International Conference on Auditory-Visual Speech Process- ing AVSP’98, D. Burnham, J. Robert-Ribes, and E. Vatikiotis-Bateson, Eds., Terrigal, Australia, 1998, pp. 135–140.
[19] S. Marsella, W.L. Johnson, and K. LaBore. Interac- tive pedagogical drama. In Proceedings of the 4th International Conference on Autonomous Agents, Barcelona, Spain, June 2000.
[20] S. Marsella and J. Gratch. Modeling coping be- havior in virtual humans: Don’t worry, be happy. In proceedings of the /2nd International Confer- ence on Autonomous Agents and Multiagent Sys- tems, Melbourne, Australia, 2003.
[21] D. Massaro. Perceiving Talking Faces : From Speech Perception to a Behavioral Principle. Brad- ford Books Series in Cognitive Psychology. MIT Press, 1997.
[22] C. Pelachaud, N.I. Badler, and M. Steedman. Gen- erating facial expressions for speech. Cognitive Sci- ence, 20(1):1–46, January-March 1996.
[23] C.Pelachaud,V.Carofiglio,B.deCarolis,F.deRo- sis and I. Poggi, “Embodied Contextual Agent in Information Delivering Agent,” in Proceedings of AAMAS, 2002, vol. 2.
[24] I. Poggi. Mind markers. In C.Mueller and R.Posner, editors, The Semantics and Pragmatics of Everyday Gestures. Berlin Verlag Arno Spitz, Berlin, 2001.
[25] A. Takeuchi and K. Nagao, “Communicative fa- cial displays as a new conversational modality,” ACM/IFIP INTERCHI ’93, Amsterdam, 1993.
[26] P. Taylor, A. Black, and R. Caley. The architecture of the Festival Speech Synthesis System. In Pro- ceedings of the Third ESCA Workshop on Speech Synthesis, pages 147–151, 1998.
[27] E.F. Walther, Lipreading, Nelson-Hall, Chicago, 1982.
[28] Y. Yacoob and L. Davis.	Computer Vision and Pattern Recognition Conference, chapter Comput- ing spatio-temporal representations of human faces, pages 70–75.</Text>
        </Document>
        <Document ID="19">
            <Title>Patrik N. Juslin</Title>
            <Text>Five myths about expressivity in music performance and what to do about them
Patrik N. Juslin Department of Psychology Uppsala University Box 1225 SE- 751 42 Uppsala Sweden
E-mail: Patrik.Juslin@psyk.uu.se
One of the recurring themes in treatises on music is that music is expressive. Yet, of all the skills that make up music performance, the ones to do with expressivity are often regarded as the most elusive. Research has indicated that expressive skills are frequently neglected in music education. The goal of this paper is to provide a critical discussion of musical expressivity. First, I consider five myths about musical expressivity which have had a negative impact on music education: (a) Expressivity cannot be studied objectively; (b) You must feel the emotion in order to convey it to your listeners; (c) Explicit understanding is not beneficial to learning expressivity; (d) Emotions expressed in music are very different from everyday emotions; and (e) Expressive skills cannot be learned. Then, I outline criteria for a useful teaching strategy aimed at expressive skills. Finally, I describe a new and empirically based approach to learning expressivity that meets these criteria. The method is termed cognitive feedback, and aims to allow performers to compare their use of acoustic cues to express emotions with an “optimal” model based on how listeners use the same cues to judge emotional expression. Implications for music education are discussed.
Five myths about expressivity in music performance and what to do about them
Patrik N. Juslin Department of Psychology Uppsala University Box 1225 SE- 751 42 Uppsala Sweden
E-mail: Patrik.Juslin@psyk.uu.se
The title of this paper does not simply reflect the present author’s hubris. It is rather an attempt to arouse interest and emotion in the reader, much like a music performer tries to arouse interest and emotion in a listener. In treatises on music, one of the recurring themes is that music is perceived as expressive of emotion (e.g., Davies, 1994). This idea applies not only to the notated or implied structure of the music but also to the way it is performed. The performance is essential in shaping the expression of the music. Yet, of all the subskills that make up music performance, the ones to do with expressivity are often regarded as the most elusive. Moreover, research has indicated that expressive skills are often neglected in music education (Juslin &amp; Persson, 2002). Why is this so? First, music teachers lack a theory that can guide their teaching of expressive skills. Second, there are certain myths about musical expressivity that are shared by many teachers and students.
The goal of this paper is to provide a critical discussion of musical expressivity. First, I consider five myths (in my view) about musical expressivity, which have had a negative impact on music education. Then, I outline some criteria for a useful teaching strategy aimed at expressive skills. Finally, I describe a new teaching strategy that meets these criteria.
Five myths about musical expressivity
“Expressivity cannot be studied objectively”
A lot has been written about expressivity by philosophers, musicologists, and musicians – often with the implication that there is something mystical about expressivity. Different authors have described surprisingly different facets of expressivity. This has led to the belief that expressivity is an entirely “subjective” quality which cannot (or at least should not) be described in scientific terms. Musicians are often unable, or unwilling, to define the concept of expressivity or to probe its underlying mechanisms. Does this mean that it is impossible to study expressivity objectively? Not so. Acoustic correlates of perceived expressivity can readily be obtained and manipulated in musical performances, and listeners’ judgments of expressivity can be systematically and reliably related to acoustic correlates. (For an example of empirical findings, see Figure 1.)
One example of how performance expressivity might be formally conceptualized is provided by the GERM model (Juslin, Friberg, &amp; Bresin, 2002). This is a so-called computational model that decomposes patterns of expression into different sub-components. Drawing on previous research, Juslin et al. propose that expression derives from four primary components: (a) Generative rules, which mark the structure in a musical manner; (b) Emotional expression, which serves to convey a particular mood; (c) Random fluctuations, which reflect human limitations in timing precision;
and (d) Motion principles, which postulate that tempo changes should follow patterns of human movement. The four components were simulated in synthesized performances, and listening tests revealed that all of these components contribute to the perceived expressivity of a performance.1 Studies like this one suggest that expressivity is an empirically tractable problem, and lend some hope to the notion that teaching of expressive skills can be informed by science (Juslin, 2002).
“You must feel the emotion in order to convey it to your listeners”
‘A musician cannot move others unless he too is moved’, C. P. E. Bach argued in his treatise on the true art of playing keyboard instruments. ‘He must of necessity feel all of the affects that he hopes to arouse in his audience’ (1778, p. 152). This (romantic) notion is common, even among today’s music students (Lindström, Juslin, Bresin, &amp; Williamon, in press).
True enough, focusing on felt emotions may help a performer to naturally translate emotions into appropriate sound properties. But felt emotion is no guarantee that the emotion will be conveyed to listeners, neither is it necessary to feel the emotion in order to convey it successfully. After all, it is the acoustic features of the performance (not the performer’s emotions as such) that reach the listener. As observed by Sloboda (1996) students rarely monitor the expressive outcomes of their own expressive performances. Instead, they monitor their own intentions, and ”take the intention for the deed” (p. 121). Emotional engagement is not a bad thing. However, it cannot substitute for teaching that addresses the actual playing in specific and informative ways.
“Explicit understanding is not beneficial to learning expressivity”
It seems generally agreed that music performers are usually not aware of the details of how their expressive intentions are realized in a performance (Sloboda, 1996), even though there are large individual differences among performers in this regard. To the extent that acoustic cues are used implicitly, this presents a problem for the teaching of expression, which relies predominately on verbal instruction (Tait, 1992). Furthermore, because expert performers do not consciously think about how to apply expressive cues, we may wrongly conclude that students do not benefit from consciously thinking about expressive cues, even in the learning stages.
The notion that the learning of musical expressivity is best left untouched by conscious thought reflects a misunderstanding that pervades commonsense teaching based on tradition and folklore. It is not realized that goal-directed strategies that initially are willfully applied normally undergo automation as a result of practice. That is, although a performer may initially have to use cues in a conscious manner, soon the associations among cues and emotions become internalized by the performer and no longer in need of conscious control. Recent studies have suggested that explicit instruction is very beneficial to learning expressive skills (Juslin &amp; Laukka, 2000; Woody, 1999).
“Emotions expressed in music are very different from everyday emotions”
Some authors have argued that emotions expressed in music are very different from the emotions expressed in everyday life. (Indeed, some authors have denied that music can express emotions at all!) It is often said that music expresses emotions so subtle and complex that we cannot describe them in words. They are ”ineffable” to use a popular term. This view has not proved very helpful in music education. How could teachers say anything about the ineffable?
1 Notably, the “emotion” component correlated most strongly with the ratings of expressivity in this experiment.
A more fruitful approach regards emotions expressed in music as in many ways similar to those expressed outside music (cf. Davies, 1994; Juslin, 2001; Sloboda, 1996). The acoustic cues used to express emotions in performance (e.g., tempo, sound level, articulation, timbre) gain much of their power from the nonverbal communication of emotions (e.g., voices, gestures, movements). Hence, when teachers say that musical emotions are different from everyday emotions, they are robbing their students of their most powerful sources of expressive form. Teaching of expressive skills must allow everyday emotions back into the classroom. Musical emotions touch us deeply not because they are so different from everyday emotions, but because they are so similar.
“Expressive skills cannot be learned”
The four myths about musical expressivity described so far will tend to reinforce the fourth one, namely that expressive skills cannot be learned. If expression is entirely subjective and passive, and has nothing to do with explicit understanding or emotions as we know them, it is obviously difficult to teach expression to students.
Expertise in musical performance is often seen as the synthesis of technical and expressive skills. But technical aspects of playing are often regarded as learnable skills, whereas expressive aspects are regarded as instinctive. Many teachers regard expression as something that cannot be taught – a view that is sometimes shared by their students: ”there is no technique to perform expressively. You have to use your soul” (cited in Woody, 2000, p. 21). Consequently, expressivity is wrongly believed to reflect merely talent, which is beyond learning and development.
True enough, knowledge about expressivity is mostly tacit and difficult to convey through verbal means, making it somewhat elusive to both research and teaching practice. Also, expressive skills may to some extent reflect the emotional sensitivity of the performer. But this does not imply that it is impossible to develop expressive skills through training. This is really an empirical question. Studies which have addressed this question have demonstrated that expressive skills can be much improved by instruction and training (Marchand, 1975; Juslin &amp; Laukka, 2000; Woody, 1999).
What can be done?
First, music teaching should incorporate up-to-date theories and findings concerning expressivity. Second, if current teaching of expressive skills is not optimal, perhaps we need to consider using new ways of teaching expressive skills. If so, what are the criteria for a useful teaching strategy? First, the strategy should include the three elements required for deliberate practice (cf. Ericsson, Krampe, &amp; Tesch-Römer, 1993), namely (a) a well-defined task, (b) informative feedback, and (c) opportunities for repetition and correction of errors. Second, it should provide some objective means of measuring the accuracy of the musical communication. Third, it should relate the actual sound properties of the performance to experiential concepts (e.g., emotions, metaphors). Finally, it should allow the student to compare his or her playing to an “optimal” model. In the following, I outline a teaching strategy that meets these criteria.
In a project called Feedback-learning of Musical Expressivity (Feel-ME) we aim to develop new methods for teaching expressive skills based on recent advances in psychology, technology, and music acoustics. A new and empirically based approach to learning expressivity termed cognitive feedback is developed and implemented in user-friendly computer software. The purpose of this
method is to allow performers to compare their use of acoustic cues to express specific emotions with an “optimal” model based on how listeners utilize these same cues to judge the expression.
Cognitive feedback proceeds as follows: In the first phase, performers play a piece of music with various emotional expressions (see Figure 2). Their performances are recorded and automatically analyzed by the software with regard to acoustic characteristics (e.g., tempo, sound level, timbre). Then, statistical analysis is used to model the correlations between the performer’s intentions and the acoustic cues. The performer’s model is mathematically related to a stored model of listeners’ emotion judgments. This makes it possible to directly compare how performers and listeners use the same cues in the performances (Juslin, 2000).
In the second phase, the performers receive feedback from the software. This consists of a verbal description of the performer’s cue utilization, the listeners’ cue utilization, the degree of matching between performer and listeners. Instances of poor matching are explained to the performers. For example, a performer may be told that ”you use staccato articulation in your sadness expressions, but listeners associate legato articulation with sadness. Thus, you should try to play your sadness expressions with more legato articulation”. The accuracy of communication is also indexed.
In the final phase, the performers repeat the first task once again. The goal is to see whether the performers have improved their accuracy by changing the cue utilization in the ways suggested by the feedback. A recent study evaluated the efficacy of this procedure (although using manual acoustic measurements), and showed that the method is highly effective (Juslin &amp; Laukka, 2000).
The goal of cognitive feedback should be to provide students with the tools they need to develop a personal expression. Knowledge concerning the relationships between acoustic cues and their perceptual effects will help the performers to reliably achieve desired listener responses. The fact that software is used brings certain advantages. Computer feedback (a) provides critical feedback, but in a non-threatening environment, (b) is available 24 hours, (c) offers possibilities for flexible and individually based learning, and (d) involves expressive features that teachers only have tacit knowledge about.
Concluding remarks
This paper suggests that new strategies for teaching expressive skills are within reach. Adopting these strategies in music education will require openness among teachers. Some of the dominant views on expressivity may need to be revised in the light of empirical findings. But regardless of whether the reader agrees with this author’s conclusions, I hope that he or she will at least agree that the issues addressed in this paper merit closer consideration than has hitherto been the case.2
References
Bach, C. P. E. (1778/1985). Essay on the true art of playing keyboard instruments (Transl. and ed. by W. J. Mitchell). London: Eulenburg Books.
Davies, S. (1994). Musical meaning and expression. Ithaca, NY: Cornell University Press.
2 This research was supported by the Bank of Sweden Tercentenary Foundation through a grant to Patrik Juslin.
Ericsson, K. A., Krampe, R. Th., Tesch-Römer, C. (1993). The role of deliberate practice in the acquisition of expert performance. Psychological Review, 100, 363-406.
Juslin, P. N. (2000). Cue utilization in communication of emotion in music performance: Relating performance to perception. Journal of Experimental Psychology: Human Perception and Performance, 26, 1797-1813.
Juslin, P. N. (2001). Communicating emotion in music performance. A review and a theoretical framework. In P. N. Juslin, &amp; J. A. Sloboda (Eds.), Music and emotion: Theory and research (pp. 309-337). New York: Oxford University Press.
Juslin, P. N. (2002). Four facets of musical expressivity: A psychologist’s perspective on music performance. Invited paper presented at the conference Investigating Music Performance, 12-13 April 2002, Royal College of Music, London, UK.
Juslin, P. N., Friberg, A., &amp; Bresin, R. (2002). Toward a computational model of expression in music performance: The GERM model. Musicae Scientiae, Special Issue 2001-2002, 63-122.
Juslin, P. N., &amp; Laukka, P. (2000). Improving emotional communication in music performance through cognitive feedback. Musicae Scientiae, 4, 151-183.
Juslin, P. N., &amp; Persson, R. S. (2002). Emotional communication. In R. Parncutt, &amp; G. E. McPherson (Eds.), The science and psychology of music performance. Creative strategies for teaching and learning (pp. 219-236). New York: Oxford University Press.
Lindström, E., Juslin, P. N., Bresin, R., &amp; Williamon, A. (in press). Expressivity comes from within your soul: A questionnaire study of music students’ perspectives on expressivity. Research Studies in Music Education.
Marchand, D. J. (1975). A study of two approaches to developing expressive performance. Journal of Research in Music Education, 23, 14-22.
Sloboda, J. A. (1996). The acquisition of musical performance expertise: Deconstructing the ”talent” account of individual differences in musical expressivity. In K. A. Ericsson (Ed.), The road to excellence (pp. 107-126). Mahwah: Erlbaum.
Tait, M. (1992). Teaching strategies and styles. In R. Colwell (Ed.), Handbook of research on music teaching and learning (pp. 525-534). New York: Schirmer Books.
Woody, R. H. (1999). The relationship between explicit planning and expressive performance of dynamic variations in an aural modeling task. Journal of Research in Music Education, 47, 331- 342.
Woody, R. H. (2000). Learning expressivity in music performance: An exploratory study. Research Studies in Music Education, 14, 14-23.</Text>
        </Document>
        <Document ID="174">
            <Title>5.11 Conclusion</Title>
            <Synopsis>5.11 Conclusion</Synopsis>
            <Text>5.11 Conclusion</Text>
        </Document>
        <Document ID="396">
            <Title>Expressive_Box2D_Creature_Empathy_001</Title>
        </Document>
        <Document ID="319">
            <Title>Jurkowski</Title>
            <Text>Translate

Jurkowski, H. (2008). Métamorphoses : La marionnette au XXe siècle. L’Entretemps  éditions.

[Jurkowski (2008)][#jurkowski08]

Vers le début des années 80, le théâtre d'ombres, autre théâtre, va vivre une profunde transformation. L'essor du dessin animé avait entraîné, après-guerre, une perte d'intérêt pour cette forme de "spectacle impersonnel" qui semblait n'avoir aucune chance de survivre à l'exception de quelques variantes traditionnelles comme le wayang et les ombres indiennes avec les héros  du Râmâyana, les ombres chinoises avec le Roi des Singes,  les Ombres turques et grecques avec Karagöz et Karagiosis. (Jurkowski 2008, 132)

GOOGLE TRANSLATE

By the early '80s, the shadow theater, another theater, goes through a profound transformation. The rise of anime had led after the war, loss of interest in this form of "impersonal spectacle" that seemed to have no chance to survive with the exception of a few alternatives like traditional wayang shadow and Indian with the heroes of the Ramayana, the shadow with the Monkey King, the Turkish and Greek Shadow with Karagöz and Karagiosis.


Names / Places

Australian  Henri Séraphin
Richard Bradshaw
France Jean-Pierre Lescot

Le Théâtre Gioco Vita (1970) see Gilgamesh (1982)
Compagnie Amoros et Augustin

des ombres impressionnistes


</Text>
        </Document>
        <Document ID="208">
            <Title>5.4.8 Animated facial mechanisms</Title>
            <Text>Animated facial mechanisms attempt to re-embody dis-embodied voice and, in turn, over-concretise and limit imaginative play 

: "Phenomenologically, there is a close relationship between the voice and the face; both the voice and the face are parts of us that are turned outwards and by which the world knows us, but which we can ourselves only see or hear partially. They signify intimacy and vulnerability. We are our faces and we are our voices . " (Conner, 2000, 401) [4] 

It is a trend in recent talking toys, to have complex animated faces.This is in part due to a desire to a create an illusion of an *embodied* voice. Disembodied voices tend towards the *uncanny*. 

In the history of puppet theatre, there is a pronounced difference between forms that attempt to locate the voice *within* an object by rhythmically mapping gesture and movement to voice and articulated face parts, and those forms that rely on the play of light on static sculpted forms of faces to suggest expression and facial motion. The misbegotten primary aim of 'more' articulation is *more* realism -- greater verisimilitude in the imitation of living forms. 

This is echoed in the aims of makers Hasbro and Voice Signals recent technology expressed in a press release. The seek to offer a more interactive, *richer* play experience through speech activation and recognition: 

: "We are extremely pleased that Hasbro has selected Voice Signal's MicroREC speech recognition software for this product [Aloha Stitch]," commented Stewart Sims, Voice Signal's executive vice president of marketing. "Hasbro has a well-deserved reputation for creating fun, innovative, quality products, and we are delighted that they have chosen Voice Signal to supply the speech recognition software that increases the interactivity of their toys and brings *Aloha Stitch* to life." [17] 

In Hasbro's *Aloah Stich*, the Voice Signal voice chip creates a *bi-polar* toy, that varies its responses to a set number questions according to one of two moods. I quote an online review of the toy by a parent at length as it is hard to access information about the performance and interactive sequences of most of the toys under consideration in the present paper: 

: "So just what kind of smack does Stitch talk? Here's a rundown of cues and replies
You say -- "What's your badness level" He says -- "Mostly Good Today" or a sheepish, "I'm having a pretty good day" when he's nice and Naughty! Blarrrtghll! when he's rotten. 
You say, "Are you hungry?" He says, "Coconut cake and coffee, please." when he's nice and "Not anymore, I ate your dinner" when he's rotten. 
You say, "Sing a Song." He sings Aloha Oe when he's happy and burps it when he's rotten (way too funny). 
You say, "I know you can talk." He says, "Okay Okay" when he's nice and "Doggies cannot talk" or "Bark! Bark!" when he's rotten. 
You say, "Got to Sleep" He says, "Very Sleepy. . Snore" when he's nice and barks, "No, make me a sandwich!" when he's rotten. 
You say, "Where are you?" He says, "I'm with my family" or a very sad, "I'm lost" when he's nice and "Stinky Planet Earth" (which comes out yarth) when he's rotten. 
You say, "Will you play?" He replies, "Surf's Up! Cowabunga!" or "I can't I have nothing to wear." when he's happy and "I'm busy get lost!" or "I'm busy you go away, okay?" when's he's rotten." [Parent (Date Created: 28/11/2004. Date Accessed: 01/02/2007)][#AParent:2004rr]

Inanimate children's toys, which the child enlivens with movement and either unspoken or spoken voice are, to me, more enduring playful simulations - as the mutability, the changeability, the ﬂuidity of roles are emphasised through imaginative projections on a static face, rather than the repetitive 'fixed movements' of most automated articulated movement. 

In an article on 'Mike the Talking Head' (an extraordinary head mounted facial performance capture system and CG digital puppet documented circa 1988), Valarie Hall comments: "When it all comes together, the quest for realism in character animation is but a test for the animator to find out how good he/she is at using the tools they have at their disposal." [(Hall (Date Created: nodate. Date Accessed: 01/02/2007))][#Hall:2007fk]

Any aesthetic assessment about *realism* in talking toys and how it effects play needs to balance manufactures promotional excitements with an assessment of the whole interactive system.</Text>
        </Document>
        <Document ID="285">
            <Title>Bibliographic Notes</Title>
        </Document>
        <Document ID="175">
            <Title>Front</Title>
            <Text>A thesis submitted to the
School of Computing
in partial fulfilment of the
requirements for the degree of PhD
Doctor of Philosophy
SmartLab Digital Media Institute
University of East London
December 2012</Text>
        </Document>
        <Document ID="397">
            <Title>Expressive_Box2D_Creature_Empathy_002</Title>
        </Document>
        <Document ID="209">
            <Title>Conclusion</Title>
            <Text>For now, I will simply state the two remaining ideas and leave them hanging for further cogitation. Also, this survey has uncovered a topic richer and more varied than I had initially speculated and it is a pregnant ground for further research and maybe will provide, at last, a focus for my thesis.
</Text>
        </Document>
        <Document ID="286">
            <Title>Bibliographic Searches</Title>
            <Text>Google Scholar: search "puppetry"

MERL mitsibushi

GOOGLE SCHOLAR /  BOOK search "dance motion control of a humanoid robot"

ACM / Springer / Journal / Series: Entertainment Computing Computers in Entertainment (CIE)
[#Nakahara:2009p1530][][#Torpey:2009p1547][]


torben grodal - games theorist
</Text>
        </Document>
        <Document ID="176">
            <Title>ToC</Title>
        </Document>
        <Document ID="398">
            <Title>Expressive_Box2D_Creature_Empathy_003</Title>
        </Document>
        <Document ID="287">
            <Title>Myron Krueger</Title>
            <Text>See YouTube videos.
See book..

Video:

￼

Shadows interacting

Scale

links between art shadow play (Schonewolf) and Krueger

</Text>
        </Document>
        <Document ID="177">
            <Title>MMD - Useful Snippets</Title>
            <Text>Add a Reference

(1) Simple reference after direct quotation

[p.145][#Cubitt:1998fk]

Creates

 #

(2)

: "This is an attempt to add some latex" \citep[Summarised in][924]{Cubitt:1998fk}

In the final PDF this generates: 

￼

\citep[Margot cited in][p.24]{Cubitt:1998fk}


(3) [#Cubitt:1998fk][]

generates

￼



Further \citet, \citet* and \citep, \citep* examples can be found here:

http://merkel.zoneo.net/Latex/natbib.php

You can use bibdesk to 'drag' styles elements from the 'cite string drawer'.




Footnote

This is a sample footnote.[^somesamplefootnote]

[^somesamplefootnote]: Here is the text of the footnote itself.

Add to Glossary

\glossary(a){Peanuts}{Chewy salty things}

The advertised syntax doesn't work

[^glossary]: glossary: Glossary 
A section at the end ...


Add an Image (and a label to cross reference)


![Nautilus Star][]
[Nautilus Star]: Nautilus_Star "Nautilus Star" width=798px height=599px


To cross reference, include:

[Nautilus Star][#nautilusstar]

or

[][#nautilusstar] to use the form figure/section/chapter number.

The anchor is created by lowercasing the label and removing white space.
[Check with other characters, eg. underscores, etc.].






Adding a Quotation
It seems quotations do not indent as a FRONTISPIECE. There needs to be a paragraph before the first quotation in a section.

: "This is an indented quotation "</Text>
        </Document>
        <Document ID="399">
            <Title>Expressive_Box2D_RagDoll_Empathy_001</Title>
        </Document>
        <Document ID="288">
            <Title>Touch</Title>
            <Text>Excellent set of URLs and resources

xTuio
http://www.billbuxton.com/multitouchOverview.html
Buxton: Input Theories, Techniques and Technology
Input Device Sources &amp; Resources
http://www2.io.tudelft.nl/id-studiolab/research/pdfPool/2000/00Humthesis.pdf
http://www.billbuxton.com/InputSources.html
DIY Touchpanels - Maker Faire 2006

http://www.hf.uio.no/imv/forskning/forskningsprosjekter/musicalgestures/participants/Soundlines/index.html


PDF
Multi-Touch_Technologies_v1.01


http://www2.io.tudelft.nl/id-studiolab/research/pdfPool/2000/00Humthesis.pdf
Gestural design tools:
prototypes! experiments and scenarios
Caroline Hummels
ISBN 90-9014013-1
Pub: 2000

--
The Media Computing Group : Multi-Touch Framework
http://hci.rwth-aachen.de/multitouch
multitouch.framework -- mac and iphone multitouch sdk


---
Hand Tracking, Finger Identification, and Chordic Manipulation on a Multitouch Surface
By Wayne Westerman
Date: 1999
Type: PhD Thesis</Text>
        </Document>
        <Document ID="178">
            <Title>Declaration</Title>
            <Text>To be completed</Text>
        </Document>
        <Document ID="3">
            <Title>Title</Title>
            <Text>Expressivity and the 
Mechanical, Digital and Virtual Object 
in Games, Art and Performance </Text>
        </Document>
        <Document ID="289">
            <Title>Haptics</Title>
            <Text>
University of Pennsylvania	Year 1981
Real Time Control of a Robot Tacticle Sensor
Jeffrey A. Wolfeld University of Pennsylvania

http://repository.upenn.edu/cgi/viewcontent.cgi?article=1713&amp;context=cis_reports
fulltext</Text>
        </Document>
        <Document ID="4">
            <Title>Expressivity</Title>
            <Text>Source: http://www.abigaildolan.com/Research.htm 
vibrato, timbre, timing, and dynamics
An analysis of ‘performance expressivity’ is based on recent research into heightened speech (Cohen, 1983; Cohen and Inbar, 2001; Leech-Wilkinson, forthcoming). Analytic tools used in heightened speech research are adapted and implemented in an analysis of musical gestures - the embodiment of musical text by shaping the simultaneous and synergistic constituents (e.g. vibrato, timbre, timing, and dynamics) of performed music. The analysis is contextualized in terms of cognitive significance and the resultant impact on the perception of expression
Emerging Concepts: 
Domains of Expressivity
Somatic / whole body
Face
	- Eyes
Voice - prosody and semantics
Gestural
Kinetic - qualities of movement
Sonic / Musical
Compound Expressive Behaviour
Semantic / Linguistic
[performance theory - Barba, Schechner and others]
Simplified / Complex Expressive Systems

Source: http://www.hichumanities.org/AHproceedings/Patrik%20N.%20Juslin.pdf 
See PDF


Source: http://www.aiga.org/resources/content/1/9/2/3/documents/a_foundation_for_emotional_expressivity.pdf
Internal Link: a_foundation_for_emotional_expressivity
See PDF

Source: http://www.sp2008.org/events/EMUS-conferences.pdf 
EMUS - Expressivity in Music and Speech  
Prosody and expressivity in speech and music 
Models and conditions for expressive machine talk 
Some Terminology: 
Enaction - embodiement - empathy - co-experienciation 
Prosody and expressivity through musical performance and speech 
Term: CHIRONOMIC - gestural communication of dynamics, rhythm etc.
Source: http://interletras.com/canticum/Eng/Rhythm_chironomy.htm 

Source: http://hct.ece.ubc.ca/nime/2005/proc/nime2005_228.pdf 
New Interfaces for Musical Expression
Poepel, Cornelius. (2005) On Interface Expressivity: A Player-Based Study. Available: http://hct.ece.ubc.ca/nime/2005/proc/nime2005_228.pdf. Viewed: 28/4/2008.
See PDF

EMERGING CONCEPT: Gestural Expressivity

Catherine Pelachaud, IUT de Montreuil
Source: http://webperso.iut.univ-paris8.fr/~pelachaud/ 
See Movie Archives:
http://linc.iut.univ-paris8.fr/greta/ 
http://webperso.iut.univ-paris8.fr/~pelachaud/greta-movie/ 

Projects
2008-2010: 
STREP, SEMAINE: Sustained Emotionally coloured Machine-human Interaction using Nonverbal Expression
2006-2010:
Integrated Projet, CALLAS: Conveying Affectiveness in Leading-edge Living Adaptive Systems
2006-2009: 
COST Action 2102: Cross-Modal Analysis of Verbal and Non-verbal Communication
2004-2008: 
Network of Excellence, Humaine. Responsible of Workpackage 6: Emot 

Research              
Embodied Conversational Agent
Nonverbal behavior: facial expression, gesture, gaze
Emotion: model of expressive and emotional behavior
Feedback: model of feedback behavior, of listener
Interactive and multimodal system
Human-machine Interaction
Audio-visual speech: lip movement, coarticulation model
AISB 2004 Convention: Motion, Emotion and Cognition, University of Leeds.

Speaking With Emotions PDF: http://linc.iut.univ-paris8.fr/greta/papers/aisb04-bevacqua.pdf 
An expressive virtual agent head driven by music performance: PDF
Emerging Concept: Bio Mimetics
Emerging Concept: 
Emoting
Empathic Objects - Virtual Companions e.g. AISB
Kelly Dobson
Source: http://web.media.mit.edu/~monster/ 
Link to related paper: Internal: dobson_whitman_ellis_machinevoices
Emerging Concept: Machine Therapy (watch out for Weizenbaum)
e.g. Omo is an artefact that shares empathic relationships with humans. The invasiveness of the machine dissolves in the direction of an organic allegory that enables new subconscious feelings. In that sense, Omo might also be seen as a friend or a companion.
Source: http://www.fundacion.telefonica.com/at/vida/popUpPremiados/html/OMO-en.html 
 
Emerging Concept: Affective Input: 
SAFIRA - Supporting Affective Interactions for Real-time Applications
See Deliverables
See SenToy
Source: http://www.sics.se/safira/what-is.html 



</Text>
        </Document>
        <Document ID="5">
            <Title>Hybridity in Games</Title>
            <Text>Source: http://www.half-real.net/about.html 

Little Big Planet
Source: http://www.mediamolecule.com/ 
Emoting 

Real-time character design and performance

Augmented realities

Projection technologies

Physical hyperlinks 

Combinations of physical and virtual forms

Network distributed participants.

Spectacle and Immersive / Experiential properties - giant screens

</Text>
        </Document>
        <Document ID="179">
            <Title>Annotated Bibliography</Title>
        </Document>
        <Document ID="6">
            <Title>Improvisation and Technology</Title>
            <Text>Source: http://music.arts.uci.edu/dobrian/ICIT/ 

Integrated Composition, Improvisation &amp; Technology (ICIT) Master of Fine Arts Degree


Improvisation and Digital Technology

Source: http://mdm.gnwc.ca/?q=blog/20080404/improvisation-digital-media-2-convergificationing 

Source: Improvisation In Digital Media http://mdm.gnwc.ca/?q=blog/spontaneous 

Synthesis
Re-generation
Spontaneity



Emerging Concept: Creative Flow

Source: http://biomusic.wordpress.com/ 
PhD Outline: http://biomusic.files.wordpress.com/2008/01/outline2.pdf 
Internal Link: outline2

Concepts: Immediate and Mediated Interaction / Manipulation.
</Text>
        </Document>
        <Document ID="7">
            <Title>Style</Title>
        </Document>
        <Document ID="8">
            <Title>Computer Sampling</Title>
            <Text>Movement / Kinetic
Gesture
Emotion

Linguistic / Semantic


Audio / Music


Real-time sampling


</Text>
        </Document>
        <Document ID="430">
            <Title>Seminar - Automatic Writing</Title>
            <Text>Frame

A frame b frame c frame de-frame
no-frame, same frame, game frame, main frame

//in the frame through the frame,

blow-frame slow game, 

Blustery

bluster : filibuster : blowing hot air 

//blowing air and gas
//parachute  

</Text>
        </Document>
        <Document ID="70">
            <Title>Chapter 6: Rods and Strings</Title>
        </Document>
        <Document ID="9">
            <Title>Scope - Case Studies</Title>
            <Text>Source: http://mc.informatik.uni-hamburg.de/konferenzbaende/mc2003/konferenzband/muc2003-30-winkler.pdf 

Internal Link: muc2003-30-winkler

Creating digital augmented multisensual learning spaces 

Object Theatre (Dennis Silk)
Puppet Theatre
Animatronics
Robot Performance and 'robots in art' - e.g. Louis-Philippe Demers

Cybernetic Sculpture 
	- Edward Ihnatowicz - SAM (Sound Activated Mobile) and The Senster http://www.senster.com/ 
	- Ken Feingold - http://www.kenfeingold.com/ 
Ars Electronica - Corebounce's Digital Marionette (http://www.corebounce.org/wiki/Projects/Marionette) 

Affective Computing and Mimetic Approaches to Agents and Objects
Virtual Pets.
Virtual Companions
Talking Toys

Expressivity and Avatars / Character Design for Emoting and Performance in Games and Virtual Environments

Interfaces for Performance Motion Capture / Face Capture
-----------
Performer Flow, Spontaneity and Improvisation 
Collaborative interaction in the control of expressive objects (co-present in a space and networked)
Translations and calibration - e.g. sound to motion, voice to motion, brain-waves to motion and vica versa.
Automated computer control of 'performance expressivity' via motion analysis, gestural interfaces - computer vision
Human computer control systems - gloves, haptic devices, experimental interfaces e.g. IBVA Brain wave detection - (http://www.ibva.com/)
Animation Software Metaphors - Pixar's Marionette - animation design and control for expressivity.
Maverick machines and artificial creatures - analogue computers and automata - autonomous mechanical agents (e.g. Theo Jansen's Beach Creatures)

Emphatic

Source: http://portal.acm.org/citation.cfm?id=1164820.1164845 Massive List Of Interactive And Gestural Music, automated expressivity recognition - dance and music
Internal Link: 1346</Text>
        </Document>
        <Document ID="71">
            <Title>Chapter 7: Shadows</Title>
        </Document>
        <Document ID="431">
            <Title>Seminars</Title>
        </Document>
        <Document ID="320">
            <Title>Quotations and Notes - Cubitt</Title>
            <Text>On retro-engineering / remaking the hardware to re-configure artistic and creative practice and the evolving of the 'apparatus of making'

: "No single artwork, however luminous, however achieved, however passionate, carries in itself the purpose or function of the electronic media arts as a new cultural domain. The achievement of such a practice is to create the terms under which the apparatus of making is constantly re-evolved." [p.145][#Cubitt:1998fk]
Exploration of new subjectivities blocked by the limits of existing designs. 

Tropes in existing designs.
On Amateurism: unrecognised cultures, homespun cosmologies. It is it's own legimate activity in a tradition of the underground \citep{Cubitt:1998fk}

[summarised from][p.144]

On digital appropriation: "a celebration of the syncretism and hybridity per se, if not articulated in conjunction with questions of hegemony and neo-colonial power relations, runs a risk of appearing to sanctify the *fait accompli* of colonial violence" 
\citep{Cubitt:1998fk}

[Shohat cited in][p.144]

: "The heirarchies of multimedia design have prioritised certain elements of the body -- - eyes, ears, hands - -- over others, distracting and disassembling the body in the interests of coherence now centred outside the body, in a pure communication between mind and object." [p.151][#Cubitt:1998fk]
**Glossary:** Heterotopia - a plurality of forms and practices
Chapter - Virtual Realism

about train windows / journeys the picturesque is proto-cinematic 30

'every visualisation is a symbol system' 31

Baudrillard - materiality and the virtual

Heim - Six technical definitions of the virtual
the appearance of simulated 3D space on 2D monitors
interaction with electronic representations
immersion in hard- and software environments
telepresence
'full body immersion'
immersive networked communications

Jaron Lanier's RB2 - reality built for two

: "...proponents of virtuality like Lanier or Myron C. Krueger \citeyearpar{Krueger:1991uq} embrace Baudrillard's culture of artifice." [p.32][#Cubitt:1998fk]

perception - clarity and renewal
optical / psychological 
'the machine ensemble' 30

The presence of 'humanity', the humane.
Co-presence of the image and the creator of the image.

A PULSE - a RHYTHM - disembodiment and recorporealisation 32

Study of Perception - [][#marr2010vision]

peripheral movements
registering objective depth

Ponty mentioned [][#merleau-ponty02]

[][#Sobchack:1997kx]

about analogue / computer films of John Whitney Youngblood cited in [p.37][#Cubitt:1998fk] 'Kinaesthetic and optical dynamism'

abstraction / expression
'dispersal', randomness, the decay of meaning

Jargon - 'remaking the apparatus'

Eisenstein - 'overtonal montage' 

: "the interlinking of thematics across the whole film through associations of composition, motif, rhythm and sound that would articulate the shots in ever more complex webworks of interrelation." [p.44][#Cubitt:1998fk]
**Chapter - Spatial Effects**

'perspective as special effect' 74

[][#George:1990vn]
cartoons and spatial characteristics

: "The central techniques of the classical cinema -- pan, tilt, zoom, dolly, track -- appear absurd to Méliès, master of the fixed camera and the focal length. The answer is, of course, that the 'contemporary views' of the late 1920s were no more natural than Méliès's grotesques: we have long since abandoned nature. What the mobile camera and editing techniques have provided is a new mobility for the audience, filling the perspectival space between the screen and the disembodied eye of contemplation." [p.78][#Cubitt:1998fk]

'Such mobility is the secret of perspective as special effect.' [p.78][#Cubitt:1998fk]

Navigation
Smoothness
diegetic depth of field and actual surface [][#George:1990vn] 79
cut (jump cut)
seamless space for exploration

the modelled playworld

**emulation**

software handles the 'consistency of spatial representation'

Computer animation compared with hand drawn animation and the pursuit of verisimilitude.

: "for the algorithmists, the engineering is inseparable from the art" [p.82][#Cubitt:1998fk]

: "The VDU is a grid, every pixel identifiable as a numerical address, and its states likewise encoded. This grid derives its onscreen presentations from modernist design practice, which itself can be traced back to Descartes's invention of a neutral space defined only by coordinates rather than contents. ... But behind this insipid, unmarked space can be glimpsed an older grid deployed in the mediaeval scriptoria, a grid made up not of modular, transferable boxes, but of points and cruxes, in each of which the divine and the sublunary met." [p.89][#Cubitt:1998fk]

</Text>
        </Document>
        <Document ID="72">
            <Title>Chapter 8: Multi-Modal Practice</Title>
        </Document>
        <Document ID="73">
            <Title>Introduction</Title>
            <Text>4.1 Introduction
</Text>
        </Document>
        <Document ID="210">
            <Title>Abstract</Title>
            <Text>This chapter will outline developing trends and scenarios where talking and listening to the speech of humanoid and non humanoid objects, toys, robots is a part of play and other imaginative work. Working from an expertise in puppetry, automata and the emerging new field of *digital puppetry*, the author will (eventually) take concepts from many disciplines, embedded computing, voice synthesis, performance studies and educational drama (metaxis[^chccmetaxis], role-play, absorption, projection, performativity) and apply them to the world of interactive toys. To organise the discussion, I will present and discuss ten emerging ideas in relation to speaking puppets, digital technology and talking toys.
[^chccmetaxis]: See [Boal (1995)][#Boal:1995gf]</Text>
        </Document>
        <Document ID="432">
            <Title>Seminar - Automatic Writing-1</Title>
            <Text>Seminar Week Journal

A glossary

hype cycle

research
res
res arch
re-search
"slow the fuck down": an appeal for slowness: valuing the luxury of time to indulge on thinking, making work, interacting maybe in a non-productive ways. Resisting commodification and a new sense of industrialisation

object hood - the thingness
analytical usefulness of etymology
hinge / un-hinge
all the metaphors/language of mechanics and movement 
all the descriptive usefulness of sound: stutter
scratch
sonic vibrations
umwelt

stutter
scratch
sonic vibrations



archives And Aura

	
permaculture

how much information a minute can hold?</Text>
        </Document>
        <Document ID="74">
            <Title>Chapter 9: Audience Response</Title>
        </Document>
        <Document ID="75">
            <Title>Experiments in Digital Puppetry -- Video Hybrids in Quartz Composer</Title>
            <Text>*Digital Puppetry* is a hybrid art form that includes a broad range of creative practices. The current chapter explores real-time video montages and avatar control using wireless game controllers while exploring what is meant by the term *digital puppet* and raises issues surrounding the virtual and tangible body in performance. Real-time media objects are viewed as *extensions* to the human performer -- sympathetic with the traditions and conventional definitions of puppetry.

I document the workings of a prototype performance system made using Apple's innovative and free development tool *Quartz Composer*. It encompasses screen-based digital puppetry and scenography, *mixed-reality* video composites and custom software programming and the gestural control of an on-screen avatar using the popular game controller, the Nintendo *Wii-remote*.

**Chapter Keywords**: Animated Performance; Digital Puppetry; Digital Scenography, Real-Time Performance Capture and Control; Video Processing;</Text>
        </Document>
        <Document ID="211">
            <Title>Merging Article</Title>
        </Document>
        <Document ID="100">
            <Title>Gestural Interface Apparatus </Title>
            <Text>4.2 Gestural Interface Apparatus 
</Text>
        </Document>
        <Document ID="433">
            <Title>Untitled</Title>
            <Text>


http://www.semihemisphere.com/lucid_living.html

http://www.semihemisphere.com/</Text>
        </Document>
        <Document ID="322">
            <Title>Quotations and Notes - Tillis</Title>
            <Text>: "The power of the puppet as a metaphor is an implicit confirmation of the idea that double-vision is the central process to the puppet. In much puppetry ... the operator and/or speaker is not present on-stage, and yet the puppet is perceived to be an intentional creation subjected to intentional control. Even when the puppet is presented in the most imitative manner possible, it is perceived by its audience to be an object" [p.159][#Tillis:1992lr]

cosmological / mystical level 

lots of stuff peppered through the book:

: "There are people who weep, are sad and aroused watching puppets, though they know they are merely carved pieces of leather manipulated and made to speak. These people are like men [sic.] who, thirsting for sensuous pleasures, live in a world of illusion; they do not realize the magic of hallucinations they see are not real." \citep{Tillis:1992lr}

[court poet of King Airlangga Java (AD 1035-1049) cited in][p.6]
the designed figure, the envoiced figure, the kinetic figure

double vision - "an audience sees the puppet in two ways at one time: as a perceived object and as an imagined life." [p.7][#Tillis:1992lr]
cross cultural observation and analysis of 

phenomenon of the puppet: cross contexts, technical, performative, aesthetic, illusory, kinetic.
theoretical problems of the puppet: issues of definition: 'how is the puppet to be defined; or, what is to be considered a puppet?' [p.7][#Tillis:1992lr]
descriptions and taxonomies; metaphoric use of the term 'puppet'. 

the puppet

: "Contemporary puppet theatre is a rich and differentiated totality, taking in  cultural elements of different provenance and from different epochs. [p.62][#jurkowski88]

a synchronic study of the puppet may reveal insights about the 'means of expression' of the puppet.
: "We must not forget how many people think [puppetry] is not worth taking seriously, that those who study it are wasting their time." \citep [Obstratov cited in][p.6]{Tillis:1992lr}

</Text>
        </Document>
        <Document ID="76">
            <Title>Introduction</Title>
            <Text>Introduction to the section</Text>
        </Document>
        <Document ID="212">
            <Title>5.4.9 Simulating intelligence and presence </Title>
        </Document>
        <Document ID="77">
            <Title>Aims and Objectives</Title>
            <Text>I aim to explore the related contexts of digital puppetry, real-time animation, mimetic and non-mimetic kinetic objects, automata, *cybernetic sculpture*, performance systems and the technological interfaces to such phenomena. 

I aim to create performances that use original software and hardware systems that are designed to explore *performance expressivity*, with reference to relevant historical, art, entertainment and technological precedents.

I wish to theorise and form a taxonomy of *expressivity* in relationship to digital domains, ideas familiar in contexts that relate to physical performance and, particularly musical performance. By *performance expressivity*, I refer to different domains of *expressivity* including:

- Somatic / whole body
- Face / Mouth / Eyes
- Breath / Voice - prosody and semantics
- Gestural
- Kinetics - qualities and direction of movement
- Sonic / Musical - vibrato, timbre, timing, rhythm and dynamics
- Linguistic / Semantic
- Simplified / Complex Expressive Systems
- Compound Expressive Behaviour  

I will make systematic observations of the interplay between performer and object, audience and object, player/user and virtual object where dynamics surrounding the projection of human qualities [anthropomorphism], emoting and expressive relationships are formed. *Object* is used to avoid the figurative and historical overtones of the 'puppet'. To date, my focus has been on voice and speaking objects.</Text>
        </Document>
        <Document ID="101">
            <Title>6.2.1 Gestural Interfaces</Title>
        </Document>
        <Document ID="434">
            <Title>New Folder</Title>
        </Document>
        <Document ID="323">
            <Title>Practice Notes</Title>
            <Text>Plexiglass Sources

Rear Projection
http://www.plexiglas-shop.com/GB/en/sheet/plexiglas-rp-7qve7vsnwdc.html
xtuio 
nui
touch monitor drivers

unity 3D
iphone and ipad
Check out new prof: Research Prof Peter McOwan
http://www.dcs.qmul.ac.uk/researchgp/vision/


</Text>
        </Document>
        <Document ID="78">
            <Title>Research Questions</Title>
            <Text>Research Questions
</Text>
        </Document>
        <Document ID="290">
            <Title>Multi-Touch_Technologies_v1.01</Title>
            <Text>Multi-Touch Technologies NUI Group Authors 1st edition [Community Release]: May 2009
This book is © 2009 NUI Group. All the content on this page is pro- tected by a Attribution-Share Alike License. For more information, see http://creativecommons.org/licenses/by-sa/3.0
Cover design: Justin Riggio Contact: the-book@nuigroup.com
Contents
Preface......................... i
About this book                        About the authors	                     
Chapter 1: Hardware Technologies . . .	.	...
 
 
.
 
.
  ii 	iii
.1
  2
  3
. . 3 . . 5   9 . .10 . .11 	13 .	.13 .	.13 	15 	17 	18 	20  23 . .23 . .24
11 Introduction to Hardware	                12 Introduction to Optical Multi-Touch Technologies 
1.2.1 Infrared Light Sources . . . . . . . . . . . . . . .
 
 
    
. .
. .
. .
. .
 
 
    
1.2.2 Infrared Cameras. . . . . . . . . . . . .
13 Frustrated Total Internal Reflection (FTIR)
1.3.1 FTIR Layers . . . . . . . . . . . . . . .
.

. .  . .      . .
. .
 
. . . .   .	. .	.
  	      
. . . .
. .
 
. .
. .
 
.	.
.	.
  	      
. . . .
1.3.2 Compliant Surfaces . . . . . . .
14 Diffused Illumination (DI) 						
1.4.1 Front Diffused Illumination . . .
1.4.2 Rear Diffused Illumination . . .
. . . .
   
.	.	.	. .	.	.	.     			    
15: Laser Light Plane (LLP)								 16: Diffused Surface Illumination (DSI) 17 LED Light Plane (LED-LP)      18Technique Comparison          19: Display Techniques            
1.9.1 Projectors	. . . . . . . . . . . . . 1.9.2 LCD Monitors . . . . . . . . . . . .
Chapter 2: Software &amp; Applications . . .
21: Introduction to Software Programming    22Tracking                      2.2.1 Tracking for Multi-Touch . . . . . . . . 23: Gesture Recognition               2.3.1 GUIs to Gesture . . . . . . . . . . . . . 2.3.2 Gesture Widgets . . . . . . . . . . . . .
. . . . . 25
        26         27 . . . . . . . .28      	30 . . . . . . . .30 . . . . . . . .31

     
.
. . . .
2.3.3 Gesture Recognition . . . . . . . . . . . . . . . . . .	32
2.3.4 Development Frameworks. . . . . . . . . . . . . . .	34
24Python 37
2.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . 37
2.4.2 Python Multi-touch Modules and Projects
. . . . . . . . .	. . .
. . . . . . . . .	. . .
. . 38 . . 39 . .	39 . .	40 .	.	41 . . 45 . . 46 .	.	47
2.4.3 PyMT . . . . . . . . . . . . . . . . . 2.4.4 PyMT Architecture. . . . . . . .. . . 2.4.5 OpenGL . . . . . . . . . . . . .. . . 2.4.6 Windows, Widgets and Events.	..	.	. 2.4.7 Programming for Multi-Touch Input .
. . . . . . .	. . .
2.4.8 Implementing the Rotate/Scale/Move 2.4.9 Drawing with Matrix Transforms . .
Interaction .
......	.
2.4.10 Parameterizing the Problem and Computing the Trans- formation........................... 48
2.4.11 Interpret Touch Locations to Compute the Parameters 50
25 ActionScript 3 &amp; Flash              
2.5.1 Opening the Lines of Communication. . .
26NET/C#                      
2.6.1 Advantages of using .NET .	.	.	.	.	.	. . . 2.6.2 History of .NET and multi-touch . . .	.	. 2.6.3 Getting started with .NET and multi-touch
  
. . .
  
. . . .	.	. . . .
  
. . . . . . . . . . . .
      		 		   
 

    
.
. . .
. . . .
 

    
.
. . .
. . . .
  52
. 52
  56
.	56 .	56 . 57
  59
.	59 .	61 .	61 .	63
  65   67 		75 		81   87
27: Frameworks and Libraries          
2.7.1 Computer Vision . . . . . . . . . . . . 2.7.2 Gateway Applications . . . . . . . . . 2.7.3 Clients . . . . . . . . . . . . . . . . . 2.7.4 Simulators . . . . . . . . . . . . . . .
Appendix A: Abbreviations             Appendix B: Building an LLP setup        Appendix C: Building a Rear-DI Setup  		 Appendix D: Building an Interactive Floor		 Appendix E: References              
 
. . . . . . . .     	 	  
Introduction to Multi-Touch
In this chapter:
About this book Authors
i
ii	Multi-Touch Technologies Book
AFbout this book
ounded in 2006, Natural User Interface Group (~ NUI Group) is an interactive media community researching and creating open source ma- chine sensing techniques to benefit artistic, commercial and educational applications. It’s the largest online (~5000+) open source community
related to the open study of Natural User Interfaces.
NUI Group offers a collaborative environment for developers that are interested in learning and sharing new HCI (Human Computer Interaction) methods and concepts. This may include topics such as: augmented reality, voice/handwriting/ gesture recognition, touch computing, computer vision, and information visualiza- tion.
This book is a collection of wiki entries chapters written exclusively for this book, making this publication the first of its kind and unique in its field. All contribu- tors are listed here and countless others have been a part the community that has developed and nurtured the NUI Group multi-touch community, and to them we are indebted.
We are sure you’ll find this book pleasant to read. Should you have any commnts, criticisms or section proposals, please feel free to contact us via the-book@nu- igroup.com. Our doors are always open and looking for new people with similar interests and dreams.
Introduction to Multi-Touch	iii
About the authors
Alex Teiche is an enthusiast that has been working with Human-Computer Interaction for several years, with a very acute interest in collaborative multiuser interaction. He recently completed his LLP Table, and has been contributing to the PyMT project for several months. He is now experimenting with thin multi- touch techniques that are constructable using easily accessible equipment to the average hobbyist.
Ashish Kumar Rai is an undergraduate student in Electronics Engineering at Institute of Technology, Banaras Hindu University, India. His major contribution is the development of the simulator QMTSim. Currently his work consists of development of innovative hardware and software solutions for the advancement of NUI.
Chris Yanc has been a professional interactive designer at an online market- ing agency in Cleveland, OH for over 4 years. After constructing his own FTIR multi-touch table, he has been focusing on using Flash and Actionscript 3.0 to develop multi-touch applications from communication with CCV and Touchlib. Chris has been sharing the progress of his multi-touch Flash experiments with the NUIGroup community.
Christian Moore is a pioneer in open-source technology and is one of the key evangelists of Natural User Interfaces. As the founder of NUI Group, he has led a community that has developed state-of-the-art human sensing techniques and is working on the creation of open standards to unify the community’s efforts into well-formatted documentation and frameworks.
Donovan Solms is a Multimedia student at the University of Pretoria in Gauteng, South Africa. After building the first multitouch screen in South Africa in the first quarter of 2007, he continued to develop software for multitouch us- ing .NET 3 and ActionScript 3. He can currently be found building commercial multitouch surfaces and software at Multimedia Solutions in South Africa.
Görkem Çetin defines himself an open source evangelist. His current doctor- ate studies focus on human computer interaction issues of open source software. Çetin has authored 5 books, written numerous articles for technical and sector magazines and spoken at various conferences.
Justin Riggio is a professional interactive developer and designer that has been working freelance for over 5 years now. He lives and works in the NYC area and
iv	Multi-Touch Technologies Book
can be found at the Jersey shore surfing at times. He has designed and built a 40” screen DI multi touch table and has set up and tested LLP LCD set ups and also projected units. You can find on his drafting table a new plan for a DSI system and AS3 multi touch document reader.
Nolan Ramseyer is a student at University of California, San Diego in the United States studying Bioengineering: Biotechnology with a background in computers and multimedia. His work in optical-based multi-touch has consumed a larger part of his free time and through his experimentations and learning has helped others achieve their own projects and goals.
Paul D’Intino is the Technical Training Manager at Pioneer Electronics in Melbourne, Australia. In 2003 he completed a Certificate III in Electrotechnol- ogy Entertainment and Servicing, and went onto repairing Consumer Electronics equipment, specializing in Plasma TV’s. His work in both hardware and software for multi-touch systems grew from his passion for electronics, which he aims to further develop with help from the NUI Group.
Laurence Muller (M.Sc.) is a scientific programmer at the Scientific Visualiza- tion and Virtual Reality Research Group (Section Computational Science) at the University of Amsterdam, Amsterdam in the Netherlands. In 2008 he completed his thesis research project titled: “Multi-touch displays: design, applications and performance evaluation”. Currently he is working on innovative scientific software for multi-touch devices.
Ramsin Khoshabeh is a graduate Ph.D. student in the University of Cali- fornia, San Diego in the Electrical and Computer Engineering department. He works in the Distributed-Cognition and Human-Computer Interaction Labora- tory in the Cognitive Science department. His research is focused on developing novel tools for the interaction with and analysis of large datasets of videos. As part of his work, he has created MultiMouse, a multi-touch Windows mouse interface, and VideoNavigator, a cross-platform multi-touch video navigation interface.
Rishi Bedi is a student in Baltimore, Maryland with an interest in emerging user interaction technologies, programming in several languages, and computer hard- ware. As an actively contributing NUI Group community member, he hopes to build his own multi-touch devices &amp; applications, and to continue to support the community’s various endeavors. Also interested in graphic design, he has designed this book and has worked in various print and electronic mediums.
Mohammad Taha Bintahir is an undergraduate student at University of On-
Introduction to Multi-Touch	v
tario Institute of Technology, in Toronto Canada studying Electrical Engineering and Business and focusing on microelectronics and programming. He’s currently doing personal research for his thesis project involving, both electronic and optical based Multi-touch and multi-modal hardware systems.
Thomas Hansen is PhD. Student at the University of Iowa and open source enthusiast. His research interests include Information Visualization, Computer Graphics, and especially Human Computer Interaction directed at therapeutic applications of novel interfaces such as multi-touch displays. This and his other work on multi-touch interfaces has been supported by the University of Iowa’s Mathematical and Physical Sciences Funding Program.
Tim Roth studied Interaction Design at the Zurich University of the Arts. Dur- ing his studies, he focussed on interface design, dynamic data visualization and new methods of human computer interaction.His diploma dealt with the cus- tomer and consultant situation in the private banking sector. The final output was a multitouch surface, that helped the consultant to explain and visualize the com- plexeties of modern financial planning.
Seth Sandler is a University of California, San Diego graduate with a Bachelors degree in Interdisciplinary Computing and the Arts, with an emphasis in music. During Seth’s final year of study he began researching multitouch and ultimat- ley produced a musical multi-touch project entitled ‘AudioTouch.’ Seth is most notably known for his presence on the NUI Group forum, introduction of the ‘MTmini’, and CCV development.
Editors
This book was edited &amp; designed by Görkem Çetin, Rishi Bedi and Seth Sandler.
Copyright
This book is © 2009 NUI Group. All the content on this page is protected by a Attribution-Share Alike License. For more information, see http://creativecom- mons.org/licenses/by-sa/3.0
vi	Multi-Touch Technologies Book
Chapter 1
Multi-Touch Technologies
In this chapter:
1.1	Introduction to Hardware 1.2	Introduction to Optical Techniques 1.3	Frustrated Total Internal Reflection (FTIR) 1.4	Diffused Illumination (DI) 1.5	Laser Light Plane (LLP) 1.6	Diffused Surface Illumination (DSI) 1.7	LED Light Plane (LED-LP) 1.8	Technqiue Comparison 1.9	Display Techniques
1
2	Multi-Touch Technologies Handbook
1M.1 Introduction to Hardware
ulti-touch (or multitouch) denotes a set of interaction techniques that allow computer users to control graphical applications with several fingers. Multi-touch devices consist of a touch screen (e.g., computer display, table, wall) or touchpad, as well as software that
recognizes multiple simultaneous touch points, as opposed to the standard touch- screen (e.g. computer touchpad, ATM), which recognizes only one touch point.
The Natural User Interface and its influence on multi-touch gestural interface design has brought key changes in computing hardware design, especially the creation of “true” multi-touch hardware systems (i.e. support for more than two inputs). The aim of NUI Group is to provide an open platform where hardware and software knowledge can be exchanged freely; through this free exchange of knowledge and information, there has been an increase in development in regards to hardware.
On the hardware frontier, NUI Group aims to be an informational resource hub for others interested in prototyping and/or constructing, a low cost, high resolu- tion open-source multi-input hardware system. Through the community research efforts, there have been improvements to existing multi-touch systems as well as the creation of new techniques that allow for the development of not only multi- touch hardware systems, but also multi-modal devices. At the moment there are five major techniques being refined by the community that allow for the creation of a stable multi-touch hardware systems; these include: Jeff Han’s pioneering Frustrated Total Internal Reflection (FTIR), Rear Diffused Illumination (Rear DI) such as Microsoft’s Surface Table, Laser Light Plan (LLP) pioneered in the community by Alex Popovich and also seen in Microsoft’s LaserTouch prototype, LED-Light Plane (LED-LP) developed within the community by Nima Mota- medi, and finally Diffused Surface Illumination (DSI) developed within the com- munity by Tim Roth.
These five techniques being utilized by the community work on the principal of Computer Vision and optics (cameras). While optical sensing makes up the vast majority of techniques in the NUI Group community, there are several other sens- ing techniques that can be utilized in making natural user interface and multitouch devices. Some of these sensing devices include proximity, acoustic, capacitive, re- sistive, motion, orientation, and pressure. Often, various sensors are combined to form a particular multitouch sensing technique. In this chapter, we explore some of the mentioned techniques.
Multi-Touch Technologies	3
1O.2 Introduction to Optical Multi-Touch Technologies
ptical or light sensing (camera) based solutions make up a large per- centage of multi-touch devices. The scalability, low cost and ease of setup are suggestive reasoning for the popularity of optical solutions. Stereo Vision , Overhead cameras, Frustrated Total Interal Reflec-
tion, Front and Rear Diffused Illumination, Laser Light Plane, and Diffused Sur- face Illumination are all examples of camera based multi-touch systems.
Each of these techniques consist of an optical sensor (typically a camera), infrared light source, and visual feedback in the form of projection or LCD. Prior to learn- ing about each particular technique, it is important to understand these three parts that all optical techniques share.
1.2.1 Infrared Light Sources
Infrared (IR in short) is a portion of the light spectrum that lies just beyond what can be seen by the human eye. It is a range of wavelengths longer than visible light, but shorter than microwaves. ‘Near Infrared’ (NIR) is the lower end of the infrared light spectrum and typically considered wavelengths between 700nm and 1000nm (nanometers). Most digital camera sensors are also sensitive to at least NIR and are often fitted with a filter to remove that part of the spectrum so they only capture the visible light spectrum. By removing the infrared filter and replacing it with one that removes the visible light instead, a camera that only sees infrared light is created.
In regards to multitouch, infrared light is mainly used to distinguish between a visual image on the touch surface and the object(s)/finger(s) being tracked. Since most systems have a visual feedback system where an image from a projector, LCD or other display is on the touch surface, it is important that the camera does not see this image when attempting to track objects overlyaed on the display. In order to separate the objects being tracked from the visual display, a camera, as explained above, is modified to only see the infrared spectrum of light; this cuts out the visual image (visible light spectrum) from being seen by the camera and therefore, the camera is able to see only the infrared light that illuminates the object(s)/finger(s) on the touch surface.
Many brands of acrylic sheet are intentionally designed to reduce their IR trans- mission above 900nm to help control heat when used as windows. Some cameras
4	Multi-Touch Technologies Handbook
sensors have either dramatically greater, or dramatically lessened sensitivity to 940nm, which also has a slightly reduced occourance in natural sunlight.
IR LEDs are not required, but a IR light source is. In most multitouch optical techniques (especially LED-LP and FTIR), IR LEDs are used because they are efficient and effective at providing infrared light. On the other hand, Diffused Illumination (DI) does not require IR LEDs, per se, but rather, some kind of infrared light source like an infrared illuminator (which may have LEDs inside). Laser light plane (LLP) uses IR lasers as the IR light source.
Most of the time, LEDs can be bought in forms of “single LEDs” or “LED rib- bons”:
•	Single LEDs: Single through-hole LEDs are a cheap and easy to create LED frames when making FTIR, DSI, and LED-LP MT setups. They require a knowledge in soldering and a little electrical wiring when constructing. LED calculators make it easy for people to figure out how to wire the LEDs up. The most commonly through-hole IR LED used are the OSRAM SFH 485 P. If you are trying to make a LCD FTIR, you will need brighter than normal IR LEDs, so these are probably your best bet.
•	LED ribbons: The easiest solution for making LED frames instead of solder- ing a ton of through-hole LEDs, LED ribbons are FFC cables with surface mount IR LEDs already soldered on them. They come with an adhesive side that can be stuck into a frame and wrapped around a piece of acrylic with a continuous LED ribbon. All that is required is wrap and plug the ribbon into the power adapter and done. The best quality ribbons can be found at envi- ronmentallights.com.
•	LED emitters: When making Rear DI or Front DI setups, pre-built IR emit- ters are mush easier than soldering a ton of single through-hole LEDs to- gether. For most Rear DI setups, 4 of these are usually all that is needed to completely cover the insides of the box. By buying the grouped LEDs, you will have to eliminate “hot spot” IR blobs caused by the emitters by bouncing the IR light off the sides and floor of the enclosed box.
Before buying LEDs, it’s strongly advised to check the data sheet of the LED. Wavelength, angle, and radiant intensity are the most important specifications for all techniques.
•	Wavelength: 780-940nm. LEDs in this range are easily seen by most cameras
Multi-Touch Technologies	5
and visible light filters can be easily found for these wavelengths. The lower the wavelength, the higher sensitivity which equates to a higher ability to determine the pressure.
•	Radiant intensity: Minimum of 80mw. The ideal is the highest radiant inten- sity you can find, which can be much higher than 80mw.
•	Angle for FTIR: Anything less than +/- 48 will not take full advantage of total internal reflection, and anything above +/- 48 degrees will escape the acrylic. In order to ensure there is coverage, going beyond +/- 48 degrees is fine, but anything above +/- 60 is really just a waste as (60 - 48 = +/- 12 de- grees) will escape the acrylic.
•	Angle for diffused illumination: Wider angle LEDs are generally better. The wider the LED angle, the easier it is to achieve even illumination.
For the DI setup, many setups bump into problem of hotspots. In order to elimi- nate this, IR light needs to be bounced off the bottom of the box when mounted to avoid IR hot spots on the screen. Also, a band pass filter for the camera is re- quired – homemade band pass filters such as the exposed negative film or a piece of a floppy disk will work, but they’ll provide poor results. It’s always better to buy a (cheap) band pass filter and putting it into the lens of the camera.
1.2.2 Infrared Cameras
Simple webcams work very well for multitouch setups, but they need to be modi- fied first. Regular webcams and cameras block out infrared light, letting only vis- ible light in. We need just the opposite. Typically, by opening the camera up, you can simply pop the filter off, but on expensive cameras this filter is usually applied directly to the lens and cannot be modified.
Most cameras will show some infrared light without modification, but much bet- ter performance can be achieved if the filter is replaced.
The performance of the multi-touch device depends on the used components. Therefore it is important to carefully select your hardware components. Before buying a camera it is important to know for what purpose you will be using it. When you are building your first (small) test multi-touch device, the requirements may be lower than when you are building one that is going to be used for demon- stration purposes.
6	Multi-Touch Technologies Handbook
Resolution: The resolution of the camera is very important. The higher the reso- lution the more pixels are available to detect finger or objects in the camera image. This is very important for the precision of the touch device. For small multi-touch surfaces a low resolution webcam (320 x 240 pixels) can be sufficient. Larger sur- faces require cameras with a resolution of 640x480 or higher in order to maintain the precision.
Frame rate: The frame rate is the number of frames a camera can take within one second. More snapshots means that we have more data of what happened in a specific time step. In order to cope with fast movements and responsiveness of the system a camera with at least a frame rate of 30 frames per second (FPS) is recom- mended. Higher frame rates provide a smoother and more responsive experience.
Interface: Basically there are two types of interfaces that can be used to connect a camera device to a computer. Depending on the available budget one can choose between a consumer grade webcam that uses a USB interface or a professional camera that is using the IEEE 1394 interface (which is also known as FireWire). An IEEE 1394 device is recommended because it usually has less overhead and lower latency in transferring the camera image to the computer. Again, lower la- tency results in a more responsive system.
Lens type: Most consumer webcams contain an infrared (IR) filter that prevents IR light from reaching the camera sensor. This is done to prevent image distor- tion. However for our purpose, we want to capture and use IR light. On some webcams it is possible to remove the IR filter. This filter is placed behind the lens and often has a red color. If it is not possible to remove the IR filter, the lens has to be replaced with a lens without coating. Webcams often use a M12 mount. Professional series cameras (IEEE 1394) often come without a lens. Depending on the type it is usually possible to use a M12, C or CS mount to attach the lens.
Choosing the right lens can be a difficult task, fortunately many manufactures provide an online lens calculator. The calculator calculates the required focal length based on two input parameters which are the distance between the lens and the object (touch surface) and the width or height of the touch surface. Be sure to check if the calculator chooses a proper lens. Lenses with a low focal length often suffer from severe image distortion (Barrel distortion / fish eye), which can complicate the calibration of the touch tracking software.
Camera sensor &amp; IR bandpass filter: Since FTIR works with IR light we need to check if our webcam is able to see IR light. Often user manuals mention
Multi-Touch Technologies	7
the sensor name. Using the sensor name one can find the data sheets of the camera sensor. The data sheet usually contains a page with a graph similar as below. This image is belongs to the data sheet of the Sony ICX098BQ CCD sensor [Fig. 1].
Figure 1: Sensitivity of a Sony CCD sensor
The graph shows the spectral sensitivity characteristics. These characteristics show how sensitive the sensor is to light from specific wavelegths. The wave length of IR light is between 700 and 1000 nm. Unfortunately the image example only shows a range between 400 and 700 nm.
Before we can actually use the camera it is required to add a bandpass filter. When using the (IR sensitive) camera, it will also show all other colors of the spectrum. In order to block this light one can use a cut-off filter or a bandpass filter. The cut- off filter blocks light below a certain wave length, the bandpass filter only allows light from a specific wavelength to pass through.
Bandpass filters are usually quite expensive, a cheap solution is to use overexposed developed negatives.
Recommended hardware
When using a USB webcam it is recommended to buy either of the following:
•	The Playstation 3 camera (640x480 at >30 FPS). The filter can be removed and higher frame rates are possible using lower resolution.
•	Philips SPC 900NC (640x480 at 30 FPS). The filter cannot be removed, it is
8	Multi-Touch Technologies Handbook
required to replace the lens with a different one.
When using IEEE 1394 (Firewire) it is recommended to buy:
•	Unibrain Fire-i (640x480 at 30 FPS) a cheap low latency camera. Uses the same sensor as the Philips SPC 900NC.
•	Point Grey Firefly (640x480 at 60 FPS) Firewire cameras have some benefits over normal USB webcams: •	Higher framerate •	Capture size •	Higher bandwidth •	Less overhead for driver (due to less compression)
Multi-Touch Technologies	9
1F.3 Frustrated Total Internal Reflection (FTIR)
TIR is a name used by the multi-touch community to describe an opti- cal multi-touch methodology developed by Jeff Han (Han 2005). The phrase actually refers to the well-known underlying optical phenom- enon underlying Han’s method. Total Internal Reflection describes a
condition present in certain materials when light enters one material from another material with a higher refractive index, at an angle of incidence greater than a spe- cific angle (Gettys, Keller and Skove 1989, p.799). The specific angle at which this occurs depends on the refractive indexes of both materials, and is known as the critical angle, which can be calculated mathematically using Snell’s law.
When this happens, no refraction occurs in the material, and the light beam is totally reflected. Han’s method uses this to great effect, flooding the inside of a piece of acrylic with infrared light by trapping the light rays within the acrylic us- ing the principle of Total Internal Reflection. When the user comes into contact with the surface, the light rays are said to be frustrated, since they can now pass through into the contact material (usually skin), and the reflection is no longer total at that point. [Fig. 2] This frustrated light is scattered downwards towards an infrared webcam, capable of picking these ‘blobs’ up, and relaying them to tracking software.
Figure 2: FTIR schematic diagram depicting the bare minimum of parts needed for a FTIR setup.
10	Multi-Touch Technologies Handbook
1.3.1 FTIR Layers
This principle is very useful for implementing multi-touch displays, since the light that is frustrated by the user is now able to exit the acrylic in a well defined area under the contact point and becomes clearly visible to the camera below.
Acrylic
According to the paper of Han, it is necessary to use acrylic for the screen. The minimum thickness is 6 mm however large screens should use 1 cm to prevent the screen from bending.
Before a sheet of acrylic can be used for a multi-touch screen it needs to be pre- pared. Because acrylic often gets cut up roughly, it is required to polish the sides of the sheet. This is done to improve the illumination from the sides. To polish the sheet it is recommend to use different kinds of sandpaper. First start with a fine sandpaper to remove most of the scratches, after that continue with very fine, super fine and even wet sandpaper. To make your sheets shine you can use Brasso.
Baffle
The baffle is required to hide the light that is leaking from the sides of the LEDs. This can be a border of any material (wood/metal).
Diffuser
Without a diffuser the camera will not only see the touches, but also all objects behind the surface. By using a diffuser, only bright objects (touches) will be visible to the camera. All other ‘noise data’ will be left out.
Compliant layer
With a basic FTIR setup, the performance mainly depends on how greasy the fingertips of the user are. Wet fingers are able to make better contact with the surface. Dry fingers and objects won’t be able to frustrate the TIR. To overcome this problem it is recommended to add a ‘compliant layer’ on top of the surface. Instead of frustrating the total internal reflection by touch, a compliant layer will act as a proxy. The complaint layer can be made out of a silicon material such as ELASTOSIL M 4641. To protect and improve the touch surface, rear projection material such as Rosco Gray #02105 can be used. With this setup it is no longer required to have a diffuser on the rear side.
Multi-Touch Technologies	11
1.3.2 Compliant Surfaces
The compliant surface is an overlay placed above the acrylic waveguide in a FTIR based multi-touch system. The compliant surface overlay needs to be made of a material that has a higher refractive index than that of the acrylic waveguide, and one that will “couple” with the acrylic surface under pressure and set off the FTIR effect, and then “uncouple” once the pressure is released. Note that compliant sur- faces are only needed for FTIR - not for any other method (DI, LLP, DSI). The compliant surface overlay can also be used as a projection screen.
The compliant surface or compliant layer is simply an additional layer between the projection surface and the acrylic. It enhances the finger contact and gives you more robust blobs, particularly when dragging as your finger will have less adhe- sion to the surface. In the FTIR technique, the infrared light is emitted into side the acrylic waveguide, the light travels inside the medium (due to total internal refection much like a fibre optic cable), when you touch the surface of the acrylic (you frustrate this TIR effect) causing the light to refract within the medium on points of contact and creating the blobs (bright luminescent objects). There is much experimentation ongoing in the quest for the ‘perfect compliant layer’.
Some materials used to success include Sorta Clear 40 and similar catalyzed sili- con rubbers, lexel, and various brands of RTV silicone. Others have used fabric based solutions like silicon impregnated interfacing and SulkySolvy.
The original successful method, still rather popular, is to ‘cast’ a smooth silicone surface directly on top of your acrylic and then lay your projection surface on that after it cures. This requires a material that closely approximates the optical proper- ties of the acrylic as it will then be a part of the acrylic as far as your transmitted IR is concerned, hence the three ‘rubber’ materials mentioned earlier...they all have a refractive index that is slightly higher but very close to that of acrylic. Gaining in popularity is the ‘Cast Texture’ method. Tinkerman has been leading the pack in making this a simple process for DIY rather than an involved commercial process. But essentially, by applying the compliant layer to the underside of the projection surface, and texturing it, then laying the result on acrylic, you gain several benefits.
The compliant surface is no longer a part of the acrylic TIR effect so you are no longer limited to materials with a similar refractive index to that of acrylic, al- though RTV and Lexel remain the most popular choices, edging out catalyzed silicons here. Since it is largely suspended over the acrylic by the texture, except
12	Multi-Touch Technologies Handbook
where you touch it, you get less attenuation of your refracted IR light, resulting in brighter blobs.
Fabric based solutions have a smaller following here, and less dramatic of a proven success rate, but are without question the easiest to implement if an appropriate material can be sourced. Basically they involve lining the edges of your acrylic with two sided tape, and stretching the fabric over it, then repeating the process to attach your display surface. Currently the hunt is on to find a compliant surface overlay that works as both a projection surface as well as a touch surface. Users have been experimenting with various rubber materials like silicone. Compliant surfaces, while not a baseline requirement for FTIR, present the following advan- tages:
•	Protects the expensive acrylic from scratches
•	Blocks a lot of light pollution.
•	Provides consistent results (the effectiveness of the bare acrylic touch seems to be down to how sweaty/greasy yours hands are).
•	Zero visual disparity between the touch surface and the projection surface
•	Pressure sensitive
•	Seems to react better for dragging movements (at least in my experience)
•	Brighter blobs to track, as there is no longer a diffuser between the IR blob light and the camera.
Developing a compliant surface for a LCD FTIR setup is difficult, as the surface must be absolutely clear and distortion-free, so as to not obscure the LCD im- age. This difficulty is not present in projection-based FTIR setups, as the image is projected onto the compliant surface itself. To date, no one has successfully built a LCD FTIR setup with a 100% clear compliant surface. However, several individuals have had success with LCD FTIR setups, with no compliant surface whatsoever. It appears that the need for a compliant surface largely depends on how strong blobs are without one, and specific setups.
Multi-Touch Technologies	13
1D.4 Diffused Illumination (DI)
iffused Illumination (DI) comes in two main forms: Front Diffused Illumination and Rear Diffused Illumination. Both techniques rely on the same basic principles - the contrast between the silent image and the finger that touches the surface.
1.4.1 Front Diffused Illumination
Visible light (often from the ambient surroundings) is shined at the screen from above the touch surface. A diffuser is placed on top or on bottom of the touch surface. When an object touches the surface, a shadow is created in the position of the object. The camera senses this shadow.
1.4.2 Rear Diffused Illumination
Depending on size and configuration of the table, it can be quite challenging to achieve a uniform distribution of IR light across the surface for rear DI setups. While certain areas are lit well and hence touches are deteced easily, other areas are darker, thus requiring the user to press harder in order for a touch to be detected. The first approach for solving this problem should be to optimize the hardware setup, i.e. positioning of illuminators, changing wall materials, etc. However, if there is no improvement possible anymore on this level, a software based approach might help.
Currently, Touchlib applies any filter with the same intensity to the whole input image. It makes sense, though, to change the filter’s intensity for different areas of the surface to compensate for changing light conditions. A gradient based ap- proach (http://eis.comp.lancs.ac.uk/~dominik/cms/index.php?pid=24) can be ap- plied where a grayscale map is used to determine on a per-pixel-base how strong the respective filter is supposed to be applied to a certain pixel. This grayscale map can be created in an additional calibration step.
Infrared light is shined at the screen from below the touch surface. A diffuser is placed on top or on bottom of the touch surface. When an object touches the surface it reflects more light than the diffuser or objects in the background; the extra light is sensed by a camera. [Fig. 3] Depending on the diffuser, this method can also detect hover and objects placed on the surface. Rear DI, as demonstrated in the figure below, requires infrared illuminators to function. While these can be
14	Multi-Touch Technologies Handbook
bought pre-fabricated, as discussed in the Infrared Light Source section, they can also be constructed manually using individual LEDs. Unlike other setups, Rear DI also needs some sort of a diffuser material to diffuse the light, which frequently also doubles as the projection surface.
Figure 3: Rear DI schematic
Multi-Touch Technologies	15
1I.5 Laser Light Plane (LLP)
nfrared light from a laser(s) is shined just above the surface. The laser plane of light is about 1mm thick and is positioned right above the surface, when the finger just touches it, it will hit the tip of the finger which will register as a IR blob. [Fig. 4]
Infrared lasers are an easy and usually inexpensive way to create a MT setup using the LLP method. Most setups go with 2-4 lasers, postioned on the corners of the touch surface.The laser wattage power rating (mW,W) is related to the brightness of the laser, so the more power the brighter the IR plane will be.
The common light wavelengths used are 780nm and 940nm as those are the wavelengths available on the Aixiz.com website where most people buy their laser modules. Laser modules need to have line lenses on them to create a light plane. The 120 degree line lens is most commonly used, so as to reduce the number of lasers necessary to cover the entire touch surface.
Safety when using lasers of any power is important, so exercise common sense and be mindful of where the laser beams are traveling.
Figure 4: LLP schematic
1.5.1 Laser Safety
Infrared lasers are used to achieve the LLP effect, and these lasers carry some in- herent risk. For most multi-touch setups, 5mW-25mW is sufficient. Even lasers of this grade, however, do possess some risk factors with regard to eye damage. Vis-
16	Multi-Touch Technologies Handbook
ible light lasers, while certainly dangerous if looked directly into, activate a blink- response, minimizing the laser’s damage. Infrared lasers, however, are impercepti- ble to the human eye and therefore activate no blink response, allowing for greater damage. The lasers would fall under these laser safety categories [Wikipedia]:
A Class 3B laser is hazardous if the eye is exposed directly, but dif- fuse reflections such as from paper or other matte surfaces are not harmful. Continuous lasers in the wavelength range from 315 nm to far infrared are limited to 0.5 W. For pulsed lasers between 400 and 700 nm, the limit is 30 mJ. Other limits apply to other wavelengths and to ultrashort pulsed lasers. Protective eyewear is typically required where direct viewing of a class 3B laser beam may occur. Class-3B lasers must be equipped with a key switch and a safety interlock.
As explained in Appendix B, a line lens is used to expand a laser’s 1-dimensional line into a plane. This line lens reduces the intensity of the laser, but risk is still present. It is imperative that laser safety goggles matching the wavelength of the lasers used (i.e. 780 nm) are used during setup of the LLP device. Following initial setup and alignment, caution should be exercised when using a LLP setup. No objects that could refract light oddly should be placed on the setup - for example, a wine glass’s cylindrical bottom would cause laser light to be reflected all over the area haphazardly. A fence should be erected around the border of the setup to enclose stray laser beams and no lasers should ever be looked at directly. Goggles vary widely in their protection: Optical density (OD) is a measure of protection provided by a lens against various wavelengths. It’s logarithmic, so a lens provid- ing OD 5 reduces beam power by 100 times more than an OD 3 lens. There is merit to setting up a system with low-power visible red lasers for basic alignment, etc, since they are safer and optically behave similarly. When switching into IR, “detection cards” are available that allow observation of low power IR lasers by absorbing the light and re-emitting visible light - like an index card to check a vis- ible beam, these are a useful way to check alignment of the IR laser while wearing the goggles.
To reiterate the most important safety precautions enumerated above: always use infrared safety goggles when setting up, and never shine a laser directly in your eye. If extreme caution is exercised and these rules not broken, LLP setups are quite safe. However, violation of any of these safety guidelines could result in serious injury and permanent damage to your retina.
Multi-Touch Technologies	17
1D.6 Diffused Surface Illumination (DSI)
SI uses a special acrylic to distribute the IR evenly across the sur- face. Basically use your standard FTIR setup with an LED Frame (no compliant silicone surface needed), and just switch to a special acrylic. [Fig. 5] This acrylic uses small particles that are inside the
material, acting like thousands of small mirrors. When you shine IR light into the edges of this material, the light gets redirected and spread to the surface of the acrylic. The effect is similar to DI, but with even illumination, no hotspots, and same setup process as FTIR.
Evonic manufactures some different types of Endlighten. These vary in their thickness and also in the amount of particles inside the material. Available thick- ness ranges between 6-10mm, follwing “L”, “XL” and “XXL” for particle amount. The 6 mm (L) is too flexible for a table setup, but the 10 mm (XXL) works nicely.
Figure 5: DSI Schematic
18	Multi-Touch Technologies Handbook
1L.7 LED Light Plane (LED-LP)
ED-LP is setup the same way as an FTIR setup except that the thick acrylic that the infrared light travels through is removed and the light travels over the touch surface. This picture [Fig. 6] shows the layers that are common in an LED-LP setup.
Figure 6: LED-LP 3D Schematic created in SecondLife
The infrared LEDs are placed around the touch surface; with all sides being sur- rounding preferred to get a more even distribution of light. Similar to LLP, LED- LP creates a plane of IR light that lays over the touch surface. Since the light coming from the LEDs is conical instead of a flat laser plane, the light will light up objects placed above the touch surface instead of touching it. This can be adjusted for by adjusting filter settings in the software (touchlib/Community Core Vision) such as the threshold levels to only pick up objects that are lit up when they are very close to the touch surface. This is a problem for people starting with this type of setup and takes some patience. It is also recommended that a bezel (as can be seen in the picture above) is put over the LEDs to shield the light into more of a
Multi-Touch Technologies	19
plane.
LED-LP is usually only recommended when working with an LCD screen as there are better methods such as Rear DI when using a projector that usually don’t work with an LCD screen. Like Rear DI and LLP the touch surface need not be thick like in FTIR, but only as strong as it needs to support the forces from work- ing on the touch surface.
1.7.1 Layers used
First, a touch surface, which is a strong, durable surface that can take the pressure of user interaction that is optically clear should be used. This is usually acrylic or glass. If using in a projector setup, the image is stopped and displayed on the projection layer. If using in an LCD setup, the diffuser is placed below the LCD screen to evenly distribute the light from the LCD backlight.
The source of infrared light for an LED-LP setup comes from infrared LEDs that are placed around at least 2 sides of the acrylic right above the touch surface. Typically the more sides surrounded, the better the setup will be in IR prevalent lighting conditions. Refer to the LED section for more information on IR LEDs.
A computer webcam is placed on the opposite site of the touch surface so that is can see the blobs. See the camera section for more information on cameras that are commonly used.
Figure 7: Picture of LED-LP setup
20	Multi-Touch Technologies Handbook
1A.8 Technique comparison
frequent question is “What is the best technique?” Unfortunately, there is no simple answer. Each technique has its advantages and dis- advantages. The answer is less about what is best and more about what is best for you, which only you can answer.
F TIR
Rear DI
Advantages
Disadvantages
•	An enclosed box is not required •	Blobs have strong contrast •	Allows for varying blob pressure
•	With a compliant surface, it can be used with something as small as a pen tip
•	Setup calls for some type of LED frame (soldering required)
•	Requires a compliant surface (sili- cone rubber) for proper use - no glass surface
•	Cannot recognize objects or fidu- cial markers
Advantages
Disadvantages
•	No need for a compliant surface, just an diffuser/projection surface on top/bottom
•	Can use any transparent material like glass (not just acrylic)
•	No LED frame required
•	No soldering (you can buy the IR- Illuminators ready to go)
•	Can track objects, fingers, fidu- cials, hovering
•	Difficult to get even illumination
•	Blobs have lower contrast (harder to pick up by software)
•	Greater chance of ‘false blobs’ •	Enclosed box is required
Multi-Touch Technologies	21
Front DI
LLP
Advantages
Disadvantages
•	No need for a compliant surface, just an diffuser/projection surface on top/bottom
•	Can use any transparent material like glass (not just acrylic)
•	No LED frame required
•	No soldering (you can buy the IR- Illuminators ready to go)
•	Can track fingers and hovering •	An enclosed box is not required •	Simplest setup
•	Cannot track objects and fiducials •	Difficult to get even illumination •	Greater chance of ‘false blobs’
•	Not as reliable (relies heavily on ambient lighting environment)
Advantages
Disadvantages
•	No compliant surface (silicone)
•	Can use any transparent material like glass (not just acrylic)
•	No LED frame required •	An enclosed box is not required •	Simplest setup
•	Could be slightly cheaper than other techniques
•	Cannot track traditional objects and fiducials
•	Not truly pressure sensitive (since light intensity doesn’t change with pressure).
•	Can cause occlusion if only using 1 or 2 lasers where light hitting one finger blocks another finger from receiving light.
22	Multi-Touch Technologies Handbook
DSI
LED-LP
Advantages
Disadvantages
•	No compliant surface (silicone)
•	Can easily switch back and forth between DI (DSI) and FTIR
•	Can detect objects, hovering, and fiducials
•	Is pressure sensitive
•	No hotspots
• Even finger/object illumination throughout the surface
• Endlighten Acrylic costs more than regular acrylic (but the some of the cost can be made up since no IR illuminators are needed)
•	Blobs have lower contrast (harder to pick up by software) than FTIR and LLP
•	Possible size restrictions due to plexiglass’s softness
Advantages
Disadvantages
•	No compliant surface (silicone)
•	Can use any transparent material like glass (not just acrylic)
•	No LED frame required
•	An enclosed box is not required
•	Could be slightly cheaper than other techniques
•	Hovering might be detected •	Does not track objects or fiducials •	LED frame (soldering) required
•	Only narrow-beam LEDs can be used - no ribbons.
Multi-Touch Technologies	23
1.9 Display Techniques
1.9.1 Projectors
The use of a projector is one of the methods used to display visual feed- back on the table. Any video projection device (LCDs, OLEDs, etc.) should work but projectors tend to be the most versatile and popular in terms of image size.
There are two main display types of projectors: DLP and LCD.
•
•
LCD (Liquid Crystal Displays) are made up of a grid of dots that go on and off as needed. These use the same technology that is in your flat panel moni- tor or laptop screen. These displays are very sharp and have very strong color. LCD projectors have a screen door effect and the color tends to fade after a few years.
DLP (Digital Light Processing) is a technology that works by the use of thousands of tiny mirrors moving back and forth. The color is then created by a spinning color wheel. This type of projector has a very good contrast ratio and is very small in physical size. DLP may have a slower response rate.
One thing to consider with projects is brightness. It’s measured in ANSI lumens. The higher the number the brighter the projector and the brighter the setting can be. In a home theater setting you alway want a brighter screen but in a table setup it differs.
With the projector close to the screen and the screen size being smaller the pro- jection can be too bright and give you a hotspot in the screen. This hotspot effect can be dazzling after looking at the screen for several minutes.
One of the biggest limiting points of a projector is the throw distance. This is the distance that is needed between the lens of the projector and the screen to get the right image size. For example in order to have a screen size of 34in, then you may need to have a distance of 2 feet between you projector and the screen. Check to see if your projection image can be adjusted to fit the size of the screen you are using.
In case you want to make your multi touch display into a boxed solution, a mirror can assist in redirecting the projected image. This provides the necessary throw
24	Multi-Touch Technologies Handbook
length between the projector and screen while allowing a more compact configu- ration.
The aspect ratio is a measure of image width over image height for the projected image. for example if you have a standard television then your aspect ratio is 4:3. If you have a wide screen tv then you have a 16:9 ratio. In most cases a 4:3 ratio is preferred but it depends on the use of the interface.
1.9.2 LCD Monitors
While projection-based displays tend to be the most versatile in terms of setup size, LCD displays are also an option for providing visual feedback on multi-touch setups. All LCD displays are inherently transparent – the LCD matrix itself has no opacity. If all of theplastic casing of the display is removed and IR-blocking dif- fuser layers are discarded, this LCD matrix remains, attached to its circuit boards, controller, and power supply (PSU). When mounted in a multi-touch setup, this LCD matrix allows infrared light to pass through it, and at the same time, display an image rivaling that of an unmodified LCD monitor or projector.
Naturally, in order for an LCD monitor to produce an image when disassembled, it must be connected to its circuit boards. These circuit boards are attached to the matrix via FFC (flat flex cable) ribbon that is of a certain length. A LCD monitor is unusable for a multi-touch setup if these FFC ribbons are too short to extend all components of the LCD monitor far enough away from the matrix itself so as to not inhibit transparency of infrared light through it. Databases of FFC- compliant LCD monitors exist on websites such as LumenLabs, up to 19”. LCD monitors that are not FFC compliant are still potentially usable for multi-touch setups, if the FFC ribbons are extended manually. FFC extensions can be found in electronics parts stores, and can be soldered onto existing cable to make it the necessary length.
Chapter 2
Software &amp; Applications
In this chapter:
2.1	Introduction to Software 2.2	Blob Tracking 2.3	Gesture Recognition 2.4	Python
2.5	ActionScript3 2.6	.NET/C# 2.7	List of Frameworks and Applications
25
26
Multi-Touch Technologies Handbook
2P.1: introduction to Software Programming
rogramming for multi-touch input is much like any other form of cod- ing, however there are certain protocols, methods, and standards in the multi-touch world of programming. Through the work of NUI Group and other organizations, frameworks have been developed for several
languages, such as ActionScript 3, Python, C, C++, C#, and Java. Multi-touch pro- gramming is two-fold: reading and translating the “blob” input from the camera or other input device, and relaying this information through pre-defined protocols to frameworks which allow this raw blob data to be assembled into gestures that high-level language can then use to interact with an application. TUIO (Tangible User Interface Protocol) has become the industry standard for tracking blob data, and the following chapters discuss both aspects of multi-touch software: touch tracking, as well as the applications operating off of the tracking frameworks.
Software &amp; Applications	27
2O.2. Tracking
bject tracking has been a fundamental research element in the field of Computer Vision. The task of tracking consists of reliably being able to re-identify a given object for a series of video frames contain- ing that object (estimating the states of physical objects in time from
a series of unreliable observations). In general this is a very difficult problem since the object needs to first be detected (quite often in clutter, with occlusion, or under varying lighting conditions) in all the frames and then the data must be associated somehow between frames in order to identify a recognized object.
Countless approaches have been presented as solutions to the problem. The most common model for the tracking problem is the generative model, which is the basis of popular solutions such as the Kalman and particle filters.
In most systems, a robust background-subtraction algorithm needs to first pre- process each frame. This assures that static or background objects in the images are taken out of consideration. Since illumination changes frequent videos, adaptive models, such as Gaussian mixture models, of background have been developed to intelligently adapt to non-uniform, dynamic backgrounds.
Once the background has been subtracted out, all that remains are the foreground objects. These objects are often identified with their centroids, and it is these points that are tracked from frame to frame. Given an extracted centroid, the tracking al- gorithm predicts where that point should be located in the next frame.
For example, to illustrate tracking, a fairly simple model built on a Kalman filter might be a linear constant velocity model. A Kalman filter is governed by two dynamical equations, the ‘state’ equation and the ‘observation’ equation. In this example, the set of equations would respectively be:
The state equation describes the propagation of the state variable, ‘xk,’ with pro- cess noise, ‘k’ (often drawn from a zero-mean Normal distribution) If we say ‘xk-1’ represents the true position of the object at time ‘k-1,’ then the model predicts the position of the object at time ‘k’ as a linear combination of its position at time ‘k-1,’ a velocity term based on constant velocity, ‘Vuk,’ and the noise term, ‘k.’ Now since, according to the model, we don’t have direct observations of the precise object positions, the ‘xk’s,’ we need the observation equation. The observa-
28
Multi-Touch Technologies Handbook
tion equation describes the actual observed variable, ‘yk,’ which in this case would simply define as a noisy observation of the true position, ‘xk’. The measurement noise, ‘k,’ is also assumed to be zero-mean Gaussian white noise. Using these two equations, the Kalman filter recursively iterates a predicted object position given information about its previous state.
For each state prediction, data association is performed to connect tracking pre- dictions with object observations.Tracks that have no nearby observation are con- sidered to have died and observations that have no nearby tracks are considered to be a new object to be tracked.
2.2.1 Tracking for Multi-Touch
Tracking is very important to multi-touch technology. It is what enables multiple fingers to perform various actions without interrupting each other. We are also able to identify gestures because the trajectory of each finger can be traced over time, impossible without tracking.
Thankfully, today’s multi-touch hardware greatly simplifies the task of tracking an object, so even the simplest Kalman filter in actuality becomes unnecessary. In fact much of the performance bottleneck of tracking systems tends to come from gen- erating and maintaining a model of the background. Computational costs of these systems heavily constrict CPU usage unless clever tricks are employed. However, with the current infrared (IR) approaches in multi-touch hardware (e.g., FTIR or DI), an adaptive background model turns out to be overkill. Due to the fact that the captured images filter out (non-infrared) light, much of the background is removed by the hardware. Given these IR images, it is often sufficient to simply capture a single static background image to remove nearly all of the ambient light. This background image is then subtracted from all subsequent frames, the resul- tant frames have a threshold applied to them, and we are left with images contain- ing ‘blobs’ of foreground objects (fingers or surface widgets we wish to track).
Furthermore, given the domain, the tracking problem is even simpler. Unlike gen- eral tracking solutions, we know that from one frame of video to another (~33ms for standard refresh rate of 30Hz), a human finger will travel only a limited dis- tance. In light of this predictability, we can avoid modeling object dynamics and merely perform data association by looking for the closest matching object be- tween two frames with a k-Nearest Neighbors (k-NN) approach. Normal NN ex- haustively compares data points from one set to another to form correspondences
Software &amp; Applications	29
between the closest points, often measured with Euclidian distance. With k-NN, a data point is compared with the set of ‘k’ points closest to it and they vote for the label assigned to the point. Using this approach, we are able to reliably track blobs identified in the images for higher-level implementations. Heuristics often need to be employed to deal with situations where ambiguity occurs, for example, when one object occludes another.
Both the Touchlib and CCV frameworks contain examples of such trackers. Using OpenCV, an open-source Computer Vision library that enables straightforward manipulation of images and videos, the frameworks track detected blobs in real- time with high confidence. The information pertaining to the tracked blobs (posi- tion, ID, area, etc.) can then be transmitted as events that identify when a new blob has been detected, when a blob ‘dies,’ and when a blob has moved. Application developers then utilize this information by creating outside applications that listen for these blob events and act upon them.
30
Multi-Touch Technologies Handbook
2.3: Gesture Recognition
The Challenge of NUI : “Simple is hard. Easy is harder. Invisible is hardest.” - Jean-Louis Gassée
2.3.1 GUIs to Gesture
The future of Human-Computer-Interaction is the Natural User In- terface which is now blurring its boundary with the present. With the advancement in the development of cheap yet reliable multi-touch hardware, it wouldn’t be long when we can see multi-touch screens not only in highly erudite labs but also in study rooms to drawing rooms and maybe kitchens too.
The mouse and the GUI interface has been one of the main reasons for the huge penetration of computers in the society. However the interaction technique is in- direct and recognition based. The Natural User Interface with multi-touch screens is intuitive, contextual and evocative. The shift from GUI to Gesture based inter- face will further make computers an integral but unobtrusive part of our lifestyle.
In its broadest sense, “the notion of gesture is to embrace all kinds of instances where an individual engages in movements whose communicative intent is para- mount, manifest, and openly acknowledged” [3] Communication through ges- tures has been one of the oldest form of interaction in human civilization owing to various psychological reasons which, however, is beyond the scope of present discussion. The GUI systems leverages previous experience and familiarity with the application, whereas the NUI interface leverages the human assumptions and its logical conclusions to present an intuitive and contextual interface based on gestures. Thus a gesture based interface is a perfect candidate for social and col- laborative tasks as well for applications involving an artistic touch. The interface is physical, more visible and with direct manipulation.
However, only preliminary gestures are being used today in stand-alone applica- tions on the multi-touch hardware which gives a lot of scope for its critics. Multi- touch interface requires a new approach rather than re-implementing the GUI and WIMP methods with it. The form of the gestures determines whether the type of interaction is actually multi-touch or single-touch multi-user. We will dis- cuss the kind of new gesture widgets required, development of gesture recognition
Software &amp; Applications	31
modules and the supporting framework to fully leverage the utility of multi-touch hardware and develop customizable, easy to use complex multi-touch applications.
2.3.2 Gesture Widgets
Multi-touch based NUI setups provide a strong motivation and platform for a gestural interface as it is object based ( as opposed to WIMP ) and hence remove the abstractness between the real world and the application. The goal of the in- terface should be to realize a direct manipulation, higher immersion interface but with tolerance to the lower accuracy implied with such an interface. The popular gestures for scaling, rotating and translating images with two fingers, commonly referred as manipulation gestures, are good examples of natural gestures. [Fig. 1]
Figure 1: Examples of gestures in multi-touch applications
New types of gesture-widgets [Fig. 2] are required to be build to fully implement the concept of direct manipulation with the objects ( everything is an object in NUI with which the user can interact ) and a evocative and contextual environment and not just trying to emulate mouse-clicks with a gesture.Gesture widgets should be de-
Figure 2: Gestural widget examples
signed with creative thinking, proper user feedback keeping in mind	the context of the applica-
32
Multi-Touch Technologies Handbook
tion and the underlying environment. These gesture widgets can then be extended by the application developer to design complex applications. They should also sup- port customizable user defined gestures.
2.3.3 Gesture Recognition
The primary goal of gesture recognition research is to create a system which can identify specific human gestures and use them to convey information or for device control. In order to ensure accurate gesture recognition and an intuitive interface a number of constraints are applied to the model. The multi-touch setup should provide the capability for more than one user to interact with it working on inde- pendent (but maybe collaborative) applications and multiple such setups working in tandem.
Figure 3: Gesture examples - (1) interpreted as 1 gesture, (2) as 3.
Gestures are defined by the starting point within the boundary of one context, end point and the dynamic motion between the start and end points. With multi- touch input it should also be able to recognize meaning of combination of gestures separated in space or time. The gesture recognition procedure can be categorized in three sequential process:
•	Detection of Intention: Gestures should only be interpreted when they are made within the application window. With the implementation of support for multi-touch interface in X Input Extension Protocol and the upcoming
Software &amp; Applications	33
XI2, it will be the job of the X server to relay the touch event sequence to the correct application.
•	Gesture Segmentation: The same set of gestures in the same application can map to several meanings depending on the context of the touch events. Thus the touch events should be again patterned into parts depending on the ob- ject of intention. These patterned data will be sent to the gesture recognition module.
•	Gesture Classification: The gesture recognition module will work upon the patterned data to map it to the correct command. There are various techniques for gesture recognition which can be used alone or in combination like Hid- den Markov Models, Artificial Neural Networks, Finite State Machine etc.
One of the most important techniques is Hidden Markov Models (HMM). HMM is a statistical model where the distributed initial points work well and the output distributions are automatically learned by the training process. It is capable of modeling spatio-temporal time series where the same gesture can differ in shape and duration. [4]
An artificial neural network (ANN), often just called a “neural network” (NN), is a computational model based on biological neural networks [Fig. 4]. It is very flex- ible in changing environments. A novel approach can be based on the use of a set of hybrid recognizers using both HMM and partially recurrent ANN. [2]
The gesture recognition module should also provide functionality for online and
Figure 4: Blob detection to gesture recognition framework outline
34
Multi-Touch Technologies Handbook
offline recognition as well as shot and progressive gestures. A model based on the present discussion can be pictorially shown as:
The separation of the gesture recognition module from the main client architec- ture, which itself can be a MVC architecture, helps in using a gesture recognition module as per requirement eg say later on speech or other inputs are available to the gesture recognition module then it can map it to appropriate command for the interface controller to update the view and the model.
2.3.4 Development Frameworks
A number of frameworks have been released and are being developed to help in the development of multi-touch applications providing an interface for the manage- ment of touch events in an object-oriented fashion. However the level of abstrac- tion is still till the device input management, in the form of touch events being sent through TUIO protocol irrespective of the underlying hardware. The gesture recognition task which will realize the true potential of multi-touch surfaces, is still the job of the client. Often, however, some basic gestures are already included, in particular those for natural manipulation (e.g. of photos), but in general these frameworks aren’t focused on gestural interfaces. They rather tend to port the GUI and WIMP canons to a multitouch environment.[1]
Gesture and gesture recognition modules have currently gained a lot of momen- tum with the coming up of the NUI interface. Some of the important frameworks are:
Sparsh-UI - (http://codegooglecom/p/sparsh-ui/)
Sparsh-UI [30], published under LGPL license, seems to be the first actual mul- titouch gesture recognition framework. It can be connected to a a variety of hard- ware devices and supports different operating systems, programming languages and UI frameworks. Touch messages are retrieved from the connected devices and then processed for gesture recognition. Every visual component of the client interface can be associated to a specific set of gestures that will be attempted to be recognized. New gestures and recognition algorithms can be added to the default set included in the package.
Software &amp; Applications	35
Gesture Definition Markup Language - (http://nuigroupcom/wiki/ Getsure_Recognition/)
GDML is a proposed XML dialect that describes how events on the input surface are built up to create distinct gestures. By using an XML dialect to describe ges- tures, will allow individual applications to specify their range of supported gestures to the Gesture Engine. Custom gestures can be supported. This project envisages
1.	Describing a standard library (Gesturelib) of gestures suitable for the majority of applications
2.	A library of code that supports the defined gestures, and generates events to the application layer.
Grafiti
Grafiti is a C# framework built on top of the Tuio client that manages multi- touch interactions in table-top interfaces. The possible use of tangible objects is particularly contemplated. It is designed to support the use of third party modules for (specialized) gesture recognition algorithms. However a set of modules for the recognition of some basic gestures is included in this project.
NUIFrame
NUIFrame is a C++ framework based on the above discussed model (currently under development). It provides a separate gesture recognition module to which besides the touch-event sequence, contextual information regarding the view of the interface is also provided by the client application. This ensures that the same gesture on different objects can result in different operations depending on the context. It will also support custom gestures based on user specification for a par- ticular command.The set of gesture widgets will also support automatic debugging by using pattern generation, according to the gestures supported by it.
AME Patterns Library
The AME Patterns library is a new C++ pattern recognition library, currently fo- cused on real-time gesture recognition. It uses concept-based programming to ex- press gesture recognition algorithms in a generic fashion. The library has recently been released under the GNU General Public License as a part of AMELiA (the Arts, Media and Engineering Library Assortment), an open source library col- lection. It implements both a traditional hidden Markov model for gesture rec- ognition, as well as some reduced-parameter models that provide reduced train-
36
Multi-Touch Technologies Handbook
ing requirements (only 1-2 examples) and improved run-time performance while maintaining good recognition results.
There are also many challenges associated with the accuracy and usefulness of Gesture Recognition software. These can be enumerated as follows:
•	Noise pattern in the gestures
•	Inconsistency of people in use of their gestures.
•	Finding the common gestures used in a given domain instead of mapping a difficult to remember gesture.
•	Tolerance to the variability in the reproduction of the gestures
•	Tolerance with respect to inaccurate pointing as compared to GUI system.
•	Distinguishing intentional gestures from unintentional postural adjustments- known as the gesture saliency problem.
Gorilla arm: Humans aren’t designed to hold their arms in front of their faces making small motions. After more than a very few selections, the arm begins to feel sore, cramped, and oversized -- the operator looks like a gorilla while using the touch screen and feels like one afterwards. It limits the development and use of vertically-oriented touch-screens as a mainstream input technology despite a promising start in the early 1980s. However it’s not a problem for short-term us- age.
Further improvement to the NUI interface can be added by augmenting the input to the gesture recognition module with the inputs from proximity and pressure sensing, voice input, facial gestures etc.
Software &amp; Applications	37
2.4. Python
Python is a dynamic object-oriented programming language that can be used for many kinds of software development. It offers strong sup- port for integration with other languages and tools, comes with extensive standard libraries, and can be learned in a few days. Many Python pro- grammers report substantial productivity gains and feel the language en- courages the development of higher quality, more maintainable code.
2.4.1 Introduction
This section describes two great technologies: multi-touch user interfac- es and the Python programming language[1]. The focus is specifically directed towards using Python for developing multi-touch software. The Python module PyMT[2] is introduced and used to demonstrate some of the paradigms and challenges involved in writing software for multi- touch input. PyMT is an open source python module for rapid prototyping and development of multi-touch applications.
In many ways both Python and multi-touch have emerged for similar reasons of making computers easier to use. Although as a programming language Python certainly requires advanced technical knowledge of computers, a major goal is to let developers write code faster and with less hassle. Python is often touted as being much easier to learn than other programming languages[3]. Many Python programmers feel strongly that the language lets them focus on problem solving and reaching their goals, rather than deal with arcane syntax and strict language properties. With it’s emphasis on readability and a plethora of available modules available from the open source community, Python tries to make programming as easy and fun as multi-touch user interfaces make computers more natural and intuitive to use.
Although the availability of multi-touch interfaces is growing at an unprece- dented rate, interactions and applications for multi-touch interfaces have yet to reach their full potential. Especially during this still very experimental phase of exploring multi-touch interfaces, being able to implement and test interactions or prototypes quickly is of utmost importance. Python’s dynamic nature, rapid devel- opment capabilities, and plethora of available modules, make it an ideal language for prototyping and developing multi-touch interactions or applications quickly.
38
Multi-Touch Technologies Handbook
2.4.2 Python Multi-touch Modules and Projects
The following is a brief overview of some multi-touch related python projects and modules available:
PyMT [2]
PyMT is a python module for developing multi-touch enabled media rich OpenGL applications. It was started as a research project at the University of Iowa[10], and more recently has been maintained and grown substantially thanks to a group of very motivated and talented open source developers [2]. It runs on Linux, OSX, and Windows. PyMT will be discussed in further detail in the fol- lowing subsections. PyMT is licensed under the GPL license.
touchPy [4]
Python framework for working with the TUIO[8] protocol. touchPy listens to TUIO input and implements an Observer pattern that developers can use or sub- class. The Observer pattern makes touchPy platform and module agnostic. For example it does not care which framework is used to draw to the screen. There are a series of great tutorials written for touchPy by Alex Teiche [9].
PointIR [5]
PointIR is a python based multi-touch system demoed at PyCon 2008. While it certainly looks very promising, it seems to have vanished from the (searchable?) realms of the Internet. Specific license information is unknown.
libAVG [6]
According to its web page, “libavg is a high-level media development platform with a focus on interactive installations” [4]. While not strictly a multi-touch module, it has been used successfully to receive TUIO input and do blob tracking to work as a multi-touch capable system. libavg is currently available for Linux and Mac OS X. It is open source and licensed under the LGPL.
pyTUIO [7]
A python module for receiving and parsing TUIO input. pyTUIO is licensed under the MIT license.
Software &amp; Applications	39
2.4.3 PyMT
PyMT is a python module for developing multi-touch enabled media rich OpenGL applications. The main objective of PyMT is to make developing novel, and cus- tom interfaces as easy and fast as possible. This section will introduce PyMT in more detail, discuss its architecture, and use it as an example to discuss some of the challenges involved in writing software for multi-touch user interfaces.
Every (useful) program ever written does two things: take input and provide output. Without some sort of input, a program would always produce the exact same output (e.g “Hello World!”), which would make the program rather useless. Without output, no one would ever know what the program was doing or if it was even doing anything at all. For applications on multi-touch displays, the main method of input is touch. Graphics drawn on the display are the primary output. PyMT tries to make dealing with these two forms of input and output as easy and flexible as possible. For dealing with input, PyMT wraps the TUIO protocol [8] into an event driven widget framework. For graphical output PyMT builds on OpenGL to allow for hardware accelerated graphics and allow maximum flex- ibility in drawing.
2.4.4 PyMT Architecture
Figure 5 displays the main architecture of PyMT. As already mentioned it uses TUIO and OpenGL for input/output. The current version of PyMT relies on py- glet[11], which is a a cross-platform OpenGL windowing and multimedia library for Python. Especially pyglet’s multimedia functionality makes dealing with im- ages, audio and video files very easy. Most media files can be loaded with a single line of python code (and drawn/played to an OpenGL context with a single line as well). This subsection briefly discusses the role of OpenGL as a rendinering engine as well as the event system and widget hierarchy at the heart of PyMT.
40
Multi-Touch Technologies Handbook
Figure 5: PyMT Architecture: PyMT builds on top of pyglet [11], which provides OpenGL windowing and multimedia loading for various file formats. It also imple- ments a TUIO client that listens for input over the network. PyMT glues these tech- nologies together and provides an object oriented library of widgets and a hierarchical system for layout and event propagation.
2.4.5 OpenGL
Using OpenGL as a drawing backend has both advantages and disadvantages. While it allows for ultimate performance and flexibility in terms of 2D and 3D rendering, it is also very low-level. Therefore it’s learning curve can be rather steep and requires a good understanding of basic of computer graphics.
PyMT tries to counteract the need for advanced OpenGL knowledge by pro- viding basic drawing functions that act as wrappers to OpenGL. For example PyMT includes functions such as drawCircle, drawRectangle, drawLine, draw- TexturedRectangle and many others that require no knowledge of OpenGL. Us- ing OpenGL as an underlying rendering engine however, lets more advanced users take full control over the visual output of their applications at peek performance. PyMT also provides helper functions and classes to aid advanced OpenGL pro- gramming. Creating, for example, a Frame Buffer Object or shaders from glsl source can be done in one line. PyMT also handles projection matrices and layout transformations under the hood, so that widgets can always draw and act in their local coordinate space without having to deal with outside parameters and trans- formations. The main idea is to do things the easy way most of the time; but if advanced techniques or custom drawing is required to provide easy access to raw OpenGL power.
Software &amp; Applications	41
Describing OpenGL itself far exceeds the scope of this document. For gen- eral information about OpenGL see [12][13], the standard reference “The Red Book”[14], or the classic Nehe OpenGL tutorials on nehe.gamedev.net [15]
2.4.6 Windows, Widgets and Events
Most Graphical User Interface (GUI) toolkits and frameworks have a concept called the Widget. A Widget is considered the building block of a common graphical user interface. It is an element that is displayed visually and can be manipulated through interactions. The Wikipedia article on Widgets for example says the following [16]:
In computer programming, a widget (or control) is an element of a graphical user interface (GUI) that displays an information arrange- ment changeable by the user, such as a window or a text box. [...] A family of common reusable widgets has evolved for holding general information based on the PARC research for the Xerox Alto User Interface. [...] Each type of widgets generally is defined as a class by object-oriented program- ming (OOP). Therefore, many widgets are derived from class inheritance.
Wikipedia Article: GUI Widget[16]
PyMT uses similar concepts as other GUI toolkits and provides an array of wid- gets for use in multi-touch applications as part of the framework. However, the main focus lies on letting the programmer easily implement custom widgets and experiment with novel interaction techniques, rather than providing a stock of standard widgets. This is motivated primarily by the observations and assump- tions that:
•	Only very few “Widgets” and Interactions have proven themselves standard within the natural user interface (NUI).
•	The NUI is inherently different from the classic GUI / WIMP (Window, Icon, Menu, Pointing device).
•	The NUI is very contextual. i.e. visual information and interactions should adapt to the context of user interaction.
•	Real time multi-user and collaborative interactions have been largely impos-
42
Multi-Touch Technologies Handbook
sible on Mouse/Keyboard systems. They may require a complete rethinking of the user interface.
PyMT organizes widgets at runtime in a tree like hierarchy. At the root of the tree is an application window (usually of type MTWindow). Widgets can be added to such a window or to other widgets by use of the add_widget method which is de- fined as part of the base class of all widgets: MTwidget. Events, such as on_draw, on_resize, mouse and touch events are propagated along this tree to reach all of the Widgets. Programmers can take advantage of the hierarchy by defining container elements, which can handle layout or prevent unnesesary processing of events by widgets not affected by a particular event.
PyMT provdides a quite extensive list of widgets and utility objects with various functionality. For example all widgets can be styled using CSS. Instead of pro- gramatically building the widget tree using add_widget, the programmer can sup- ply XML describing the hierarchy to be built automatically. Because listing all of the functionality included in PyMT would take too much space at this point, the readers should consult the PyMT API documentation for further information[2]
This following short example program will attempt to demonstrate some of the key ideas of PyMT. Figure 2 shows the result of running this example code in Listing 1 using a hand to give 5 input touches. The program logic can be divided into roughly four main sections.	* The only way to truly evaluate novel interac- tions and interfaces for the NUI, is to implement them and actually experience them on a device. (PyMT also provides event logging functionality for the pur- poses of documenting and analyzing formal user studies)
1.	Importing PyMT and initializing variables: The very first import line tells python to use PyMT, this loads all the PyMT objects and functions into the namespace. The program also creates a variable called touch_positions. This variable is a python dictionary (hashmap of key value pairs) that is used to store touch positions of individual touches.
2. Defining a new TestWidget class: The widget inherits from MTWidget and defines 4 event handlers. The on_touch_down and on_touch_up event handlers update the touch positions. The on_touch_up handler removes the touch from the touch_positions dictionary. The draw method is responsible for drawing a red circle of radius 40 for each touch at its current position. It does so by using a PyMT functions drawCirlce, and using the values stored in touch_positions.
Software &amp; Applications	43
3.	Creating a window to hold the widget: An MTWindow is an application window, you can add widgets to this window using the add_widget function. An added widget will receive touch and draw events from the window and render to it.
4.	Starting the application/main loop: The runTouchApp function starts PyMT’s main event loop. This instatiates the window, the TUIO listener, and starts the dispatching of events.
from pymt import * #a dictionary/hash map to store touch positions touch_positions = {} #A widget that will draw a circle for each touch class TestWidget(MTWidget):
#these functions handle the touch events def on_touch_down(self, touches, touchID, x, y):
touch_positions[touchID] = (x,y)
def on_touch_move(self, touches, touchID, x, y): touch_positions[touchID] = (x,y)
def on_touch_up(self, touches, touchID, x, y): del touch_positions[touchID]
#at each frame, the draw method draws the widget def draw(self):
44
Multi-Touch Technologies Handbook
set_color(1,0,0) for position in touch_positions.values():
drawCircle(position, radius = 40)
#create a window and add our widget to it win = MTWindow() widget = TestWidget() win.add_widget(widget)
#run the program runTouchApp()
Listing 1. Example source code for a PyMT application that draws a red circle at each touch position.
Figure 6: A screen shot of the example from the program in Listing 1. Taken with 5 concurrent touches (on a very small screen: 640x480).
Software &amp; Applications	45
2.4.7 Programming for Multi-Touch Input
Programming applications and interactions for multi-touch devices is very dif- ferent from developing classic mouse based interfaces. The previous subsection discussed some reasons the NUI is inherently different from the classical WIMP based GUI. The major differences lies in having to handle multiple simultaneous touches/cursors. While being able to interpret multiple touches hugely expands the possibilities for potential interactions and interfaces, it also adds a layer com- plexity to the programming and computational aspect of development that is far from trivial.
The way TUIO, and most other multi-touch protocols and frameworks handle the issue of receiving event information about touch input defines three types of basic messages/events. These signal a new touch, movement of an existing touch, or the removal of a touch. The messages are always annotated with a “touch ID”, so that the program and its callback function can handle the event in the context of how that specific touch and others have been presented in prior events. As seen in the example from Listing1, these events arrive in PyMT as:
• • •
on_touch_down(touches, touchID, x, y) on_touch_move(touches, touchID, x, y) on_touch_up(touches, touchID, x, y)
Each event carries in addition to the touchID the x and y location in pixels coor- dinates of the event relative to the containing window. Also a dictionary with tou- chID’s of all currently alive touches as keys is provided with the touches hashmap. Using the touches dictionary, the programmer can inspect all current touches and their locations at the time of the current event. In fact the touches dictionary holds TUIO2DCursor objects as its values, which hold additional information such as relative movement and acceleration as defined by the TUIO protocol. PyMT also provides on_object_* events for TUIO object messages (e.g. used with fiducial markers).
It quickly becomes clear that interpreting multiple simultaneous touches becomes much more complex than handling a single pointer for e.g. a mouse. The context of the user interface can change with each individual touch, and subsequent event handlers must be aware of all the interactions happening before making a decision about how to deal with a touch event.
46
Multi-Touch Technologies Handbook
A couple of general programming techniques have proven helpful while experi- menting with PyMT. For example the concept of giving a widget ownership of a specific touch has proven useful in various cases. When using this technique other widgets ignore subsequent touch_move or touch_up events with a specific touchID. Only the widget that has taken ownership will handle these events.
Also, the use of hashmaps (key value mappings), or dictionaries as they are called in Python can aid significantly in keeping track of the various states and contexts. A simple example of this was shown in Listing 1. More useful applications of dic- tionaries can be achieved when multiple dictionaries are used to seperate touches into certain groups responsible for one part of the interaction or a certain widget.
Ultimately, the possibilities for new interactions and way to interpret multiple touches is limited only by the creativity and imagination of the programmer. The multitude of configurations and states that a multi-touch display can find itself in at any given moment is gigantic even before considering prior context. As the next example will show, this allows for some very natural and intuitive interactions to be programmed, but it also means the logic and algorithmic complexity becomes much bigger and harder to cope with.
2.4.8 Example: Implementing the Rotate/Scale/Move Interac- tion
For it’s conclusion, this section discusses the implementation of a well known interaction technique. The Rotate/Scale/Move interaction [Fig. 7] is often seen in various multi-touch demos. This intuitive way to rotate, move and scale a two dimensional objects using two fingers mimics the way one can e.g. move a piece of paper on a table top. It feels so natural and intuitive because the two rela-
tive points of the object being touched remain underneath the points of contact wherever the fingers move.
Figure 7: Rotate/ Scale/Move Interaction. A user can touch two points on an object and move it another place.
Software &amp; Applications	47
In PyMT this interaction is implemented as the ScatterWidget object, to which the programmer can add any additional widgets to make them behave as part of the scatter widget. Granted, this interaction has been implemented many times. Some might argue that it has been overused or lost its novel appeal. It is included here not necessarily to showcase the particular interaction, but because it is a great example of some of the complexities that can arise in programming multi-touch interactions. Although this interaction uses only two concurrent pointers/touches and feels very natural, the computations and math involved in achieving it are somewhat more complex than most mouse interactions.
To tackle the implementation of this interaction, some basic understanding of matrix transformations is required. By using matrix multiplication, arbitrary transformations can be modeled by a single matrix (usually 4x4 in the context of computer graphics). For example a matrix representing a translation of 5 units along the x axis, can be multiplied by a matrix representing a rotation of 90 degrees around the y axis. The resulting matrix represents a transformation that does both a translation by 5 units on x and a rotation around the y axis by 90 degrees. To transform a point (its coordinates) one simply multiplies the matrix by the point. For more information on matrix transformations see [18].
2.4.9 Drawing with Matrix Transforms
Listing 2a, describes the first part needed for the rotate/scale/move interaction. Here the drawing function draws the object its working with. transform_mat is a transformation matrix. So far it only holds the identity matrix, which leaves points unchanged when multiplied. Implementing the interaction is now a question of modifying the transformation matrix appropriately based on the touch input.
#a matrix that holds the transformation we are apply- ing to our object
self.transform_mat = IdentityMatrix() #a hashmap to keep track of the touchID’s we have seen
self.touches = {}
#this function draws the object using the transfor- mation matrix
def draw(self): glPushMatrix() #save the current matrix state
48
Multi-Touch Technologies Handbook
glMultMatrixf(self.transform_mat) #apply transfor self.draw_object() # draw the object as usual
glPopMatrix() #restore the current matrix state
Listing 2a. Example code for Rotate/Scale/Move interaction. When drawing the object, a transformation matrix is applied. The matrix is modified by following func- tions on touch events. For complete code of entire class see pymt/ui/scatter.py from [2].
2.4.10 Parameterizing the Problem and Computing the Trans- formation
Next, the problem lies in determining how to transform the transformation matrix of the object. Figure 8 depicts some of the parameters needed to transform the object. An important observation to make is that for any given event only one of the two touches can have moved. This is because every event only caries informa- tion for one touch at a time.
To compute the new transformation the following parameters are required. The distance between the two fingers before and after the event (d1 and d2). The angle R by which to rotate the object. And the point Pi around which to rotate/scale (By our observation this is one of the two original points).
Figure 8. Rotate/Scale/Move Interaction. From two touches (A and B), various pa-
Software &amp; Applications	49
rameters have to be computed. The scale by which to enlarge/shrink the object (d2/d1), the angle by which to rotate the object (R), and the point around which to perform these transformations (Pi). An important observation is that only one of the two touch locations will change at each received event (i.e wither A=Pi or B=Pi) Assuming the parameters are computed somehow, the appropriate changes will need to applied to the transformation matrix. Listing 2b shows PyMT / OpenGL code to do this. In OpenGL, matrices are always pre-multiplied, so that the trans- formation closest to the drawing call is the first one applied (although called last).
Here the new transformation is computed and stored as follows:
1.	Start with the old transformation stored in transform_mat
2.	Translate so that the point around which to rotate is at the origin (0,0), since rotations and scales are always relative to the origin in OpenGL
3.	Rotate by the computed amount of degrees around the z axis (going into/out of the screen)
4.	Apply the computed scale factor
5.	Move the entire thing back to the point it came from
Another noteworthy observation is that no translation (plain move) is ever applied. Using this technique, the movement of the of the object is achieved by rotating around the point that didn’t change in the last event, or scaling and shrinking it in different directions subsequently. The translation happens automatically as events occur after each other. However, to do e.g. a one finger drag, which translates the object, a translation would have to be added as it is in the actual implementation of ScatterWidget in PyMT. It is left out here for keeping the example as simple as possible.
#this functions applies the newly computed transformations #to the old matrix
def apply_angle_scale_trans(self, angle, scale, point): glPushMatrix() #save the current matrix state
glLoadIdentity()
glTranslated(point.x, point.y,0)	#5. move back to intersection
glScaled(scale, scale,1) #4. Scale around (0,0)
50
Multi-Touch Technologies Handbook
glRotated(angle,0,0,1) #3. Rotate around (0,0)
glTranslated(-point.x, -point.y,0) #2. Move intersec- tion to (0,0)
glMultMatrixf(self.transform_mat) #1. apply transform as was
form_mat#now save the modified matrix back to self.trans-
mat)
glGetFloatv(GL_MODELVIEW_MATRIX,self.transform_ glPopMatrix() #restore the current matrix state
Listing 2b. Example code for applying transformation parameters. This function ap- plies the transformation parameters to the transformation matrix. For complete code of entire class see pymt/ui/scatter.py from [2].
2.4.11 Interpreting Touch Locations to Compute the Param- eters
Finally, the program must compute the parameters solely based on the location provided by the touch events. Listing 2c shows code to do this. For simplicity, this code assumes a function get_second_touch to get the information about the second touch involved in the interaction. A function like this can be implemented e.g. by using a dictionary to keep track of the touchID’s, making sure only two touches are in it at any given point in time, and then returning the one not equal to the argument passed.
The code also assumes some basic vector functions for computing lpoint distances and angles between lines. Implementations for these can be found in PyMT’s vector class [2]. Alternatively see [20] [21]. The computations basically boil down to the magnitude of a vector and the dot product between two vectors.
To angle of rotation is given by the angle between the lines of A1-B (where the current touch used to be (A1) and the second touch) and A2-B (where the touch
Software &amp; Applications	51
is now and the second touch). The scale factor is easily computed by the ratio of the new distance d2 divided by the old d1. Where d1 a= |A1, B| and d1 a= |A2, B|.
Listing 2c. Example code for computing parameters of Rotate/Scale/Move inter- action. For complete code of entire class see pymt/ui/scatter.py from [2].
def rotate_zoom_move(self, touchID, x, y): intersect = Vector(0,0) rotation = 0 scale = 1
#we definitly have one point, so if nothing else we drag/translate
A1= Vector(*self.touches[touchID]) A2= Vector(x,y) #get the second touch #(not the on with touchID of current event) second_touch = self.get_second_touch(touchID) B = Vector(*self.touches[second_touch])
#compute scale factor (d1 and d2 are floats) d1= A1.distance(B)
d2= A2.distance(B) scale = d1/d2 #compute rotation angle old_line = A1 - B new_line = A2 - B rotation = -1.0 * old_line.angle(new_line) #apply to our transformation matrix self.apply_angle_scale_trans(rotation, scale, B) #save new position of the current touch self.touches[touchID] = Vector(x,y)
52
Multi-Touch Technologies Handbook
2B.5 ActionScript 3 &amp; Flash
ack to about 1996, the web was relatively young and people were look- ing for a tool to help make website animated and vibrant. FutureS- plash was invented, mentioned as a simple way to add vector-based animation and release it on sites through Macromedia’s Shockwave
player. Macromedia itself soon acquired FutureSplash Animator and re-dubbed it Macromedia Flash. As the years went on, Flash evolved, its capabilities increased and spread across computers all over the world. In 2000, Macromedia added Ac- tionscript 1.0 a way to edit and manipulate objects with a programming language rather than only using the timeline animation. In 2005, Actionscript 2.0 started to shift the idea of programming to an object oriented programming model rather than a straight forward syntax. In 2007, Adobe acquires the rights to Macromedia, including Flash which is released as Adobe Flash CS3. Adobe adds a fully revised version of the last version of Actionscript and labels this one 3.0 [1].
By now, Flash is one of the most powerful tools in a web designer’s arsenal, but it no longer stops at the web. Because of new coding platforms, namely Adobe Flex and Adobe AIR, developers can use Actionscript 3.0 to create cross-platform desktop applications. And now, Flash can be used to create multi-touch applica- tions via communication with a computer vision and multi-touch sensing applica- tion, such as Touchlib, CCV and reacTIVision.
2.5.1 Opening the Lines of Communication
Before we start developing applications in Flash, it’s important to understand the flow of information from the multi-touch sensing applications. When you open the applications it’s all prepared to start transmitting the blob information for any application that can read it using Tangible UI Object protocol (TUIO). However, Flash doesn’t understand the blob data right away. Why is this? “TUIO is a very simple UDP-based protocol. So if you wish to create responsive applications using Adobe Flash Professional or ActionScript 3.0, you need to have a bridge that reads this UDP socket and converts it to a TCP connection.” [2]
So what can make a UDP to TCP connection? Flash OSC can. [3] It makes the bridge we need between Touchlib and Flash so that you can create multi-touch Flash applications.
Software &amp; Applications	53
Inside MyFirstAppas, paste in this code:
After your Flash file’s properties are adjusted, there is one more piece to add to it. Find the Document Class property and type in app.demo.MyTouchApp.MyFir- stApp this is the Flash file’s connection to the MyFirstApp.as file.
There’s only one last thing to do, right now the Flash file is empty, and the way TUIO works, there needs to be something on the stage for it to recognize that a user is touching the screen. All we need to do now is put a Shape over the stage. So use the Rectangle Tool and cover the entire stage area. And set your color, in Flash CS3, you can even chance the alpha to be fully transparent. TUIO will still be able to recognize the shape and react to it.
Now it’s time to run a test movie. In the Control drop down.
This is to verify that the Flash file and the Actionscript file are in sync. So long as you don’t get any errors, you should see an empty Flash movie and the output panel will pop up and read “MyFirstApp Loaded”.
Now we need to add the TUIO class to the .as file.
package app.demo.MyTouchApp{ import flash.display.*;
public class MyFirstApp extends Sprite { public function MyFirstApp():void { trace(“MyFirstApp Loaded”); }
}
}
...
public function MyFirstApp():void { TUIO.init(this,’localhost’,3000,’’,true); } }
}
54
Multi-Touch Technologies Handbook
Test the movie again, you should see the white, red and green squares in the upper left corner, the touch information in the upper right, and circles under your fingers where you touch on the stage.
So, everything it ready to go. Now we need a solution to gather the touch in- formation into an array. We could create an array in BlobLines.as, and on every touch down add that blob to the array, then on a touch up remove the blob from an array, and on a touch movement update all the information in that array. We could do that, and I’ve done that before, it’s not a pleasant thing to write. BUT! I just so happens that TUIO.as actually contains that information in an array, OB- JECT_ARRAY, and is much easier to pass through to our AS file than one may think. And since TUIO.as is gathering the information directly from FLOSC, it seems to me that it is more reliable and accurate to work with.
So how do we do that? We need to add a little something to flash/events/TUIO.as
OBJECT_ARRAY is a private variable. The creator of this file intended only for this AS file to be able to change that value. Which makes sense, you don’t want some rogue function changing value in the array when your trying to use it specifi- cally for multi-touch objects. But, we don’t want to change it, we only want to be able to get the value at any time. So, around line 119 or so, add in this function.
Save the file and return to MyFirstApp.as Now we want to test that the information we want is coming through. What has been added/changed?
addEventListener(Event.ENTER_FRAME, test_returnBlobs); is added to run a function every frame to do so, import flash.events.Event; needs to be added to the imports so this AS file knows what Event.ENTER_FRAME is. and the function is added to trace TUIO.returnBlobs().
Test the Movie. You should see the number of touch points on the screen for each frame displayed
public static function returnBlobs():Array{ return OBJECT_ARRAY;
}
Software &amp; Applications	55
in the document’s Properties panel. This is good, one we have this information coming through, we can begin using however we need to.
package app.demo.MyTouchApp{ import flash.display.Sprite; import flash.events.TUIO; import flash.events.Event; public class MyFirstApp extends Sprite {
public function MyFirstApp():void { TUIO.init(this,’localhost’,3000,’’,true);
turnBlobs); }
addEventListener(Event.ENTER_FRAME, test_re-
{
}
public function test_returnBlobs(e:Event):void trace(TUIO.returnBlobs().length);
}
}
56
Multi-Touch Technologies Handbook
2A.6 .NET/C#
ccording to Wikipedia, Microsoft .NET Framework is a software framework available with several Microsoft Windows operating sys- tems. It includes a large library of coded solutions to prevent common programming problems and a virtual machine that manages the ex-
ecution of programs written specifically for the framework. The .NET Framework is a key Microsoft offering and is intended to be used by most new applications created for the Windows platform.
2.6.1 Advantages of using .NET
The most noticable advantage of the .NET framework is that if you write an application using the framework, then that code is guaranteed to run on other machines that have the .NET framework installed. From the release of Microsoft Windows Vista, the .NET framework will be included with the operating system installation.
Another advantage is that the code runs as ‘managed’. This means that it cannot crash your system, nor can it make it less stable. Further, compatibility issues are less that that of native C++ based application.
.NET also allows you to write code in a variety of languages. C# and Visual Basic are the most common but J# and C++ is also supported.
2.6.2 History of .NET and multi-touch
.NET 2.0 was never a real contender for multi-touch applications due to its lack of user-interface ‘freedom’. It was designed to be a business application frame- work and not for rich user interfaces. Only with the coming of .NET 3, WPF and Silverlight provided enough attention for use in multi-touch applications. The XAML markup allows for the extensibility and ‘freedom’ that developers need to build rich, interactive interfaces that are aesthetically pleasing.
.NET 3 was not a supported platform from the beginning. Flash was the most commonly used design and development tool for many enthusiasts. The reason for that was lot of development took place using Touchlib and TUIO. This allowed for easy sending of touch information to client applications.
Software &amp; Applications	57
In 2007, Donovan Solms created the C# Touchlib Interface (CsTI). It allows touchdatafromTouchlibtobesentto.NETusingabinaryconnection.CsTIisin its core a Touchlib application of sorts. It converts the touch events to actual .NET events that .NET programmers are so used to. Another common approach these days is to use the same TUIO that Flash uses to get the touch data into .NET.
Many .NET multi-touch frameworks have been created since the starting days. UsingMultiTouchVista,youcannowcontrolWindows7withaCCVorTouchlib based setup. The Microsoft Surface uses .NET as its application base. .NET 3, WPF and Silverlight all support 3D.
XNA, the new managed graphics API from Microsoft, is also another .NET possibility because of the better 3D support. But that has not yet been explored enough.
2.6.3 Getting started with .NET and multi-touch
It’s required to decide whether it’s best to use an existing framework, create your own or extend an existing one, as most of them are open-source. Below are listed the three options and how to get started on each:
1 Use an existing framework
There are quite a few .NET multi-touch frameworks available. They are also com- monly named WPF multi-touch frameworks. As an example, MultiTouchVista allows multi-touch interaction with Windows 7.
2 Create your own
This option is for the experienced .NET developer. It requires to work with the raw touch data and one needs to figure out how to build an event system for the framework, as well as write algorithms to determine all the low level stuff that .NET usually handles.
There are two ways to get the raw touch data into .NET: 1.	C# Touchlib Interface (CsTI) or 2.	TUIO via XMLSocket
Other CsTI is a binary connection with only touchlib. On the other hand, TUIO via XMLSocket is a network connection with either Touchlib, CCV or ReacTIVi-
58
Multi-Touch Technologies Handbook
sion. You can find a basic implementation on the ReacTIVision website (http://re- activision.sourceforge.net/). Last but not least, the first .NET multi-touch frame- work in the community (http://code.google.com/p/dotnetmtf/) is now deprecated but the code on googlecode still shows some basics for a starting point. But it should only be used as a starting point. The correct way is to use IInputProvider. More on this topic can be found on MSDN.
3	Extend an existing framework
This is for the developer who finds a framework that he likes but it lacks one or two features. Using this approach you can only add what you need and spare some time when compared to writing your own. Make sure that the license used for the framework gives you a freedom to work on, either commercially or non- commercially. Also pay attention to the way it works, and whether it’s still an ac- tive project or not.
4	Tools
Most .NET programmers prefer to use Microsoft’s Visual Studio, which is a ro- bust and versatile IDE for .NET development. One can also get free version la- belled as “Express Editions”. It is available here in web install format and offline ISO on Microsoft’s web page.
Software &amp; Applications	59
2.7: Frameworks and Libraries
2.7.1 Computer Vision
BBTouch
BBTouch is an open source, OSX based tracking environment for optical multi- touch tables.
Programming Language: Cocoa (Mac) License: GPL license Page: http://benbritten.com/blog/bbtouch-quick-start/ Bespoke Multi-Touch Framework
The Bespoke Multi-Touch Framework is a feature-rich and extensible software framework for developing multi-touch interfaces. Licensed under the BSD open- source license, you are free to use and extend the source code to suit your purposes. The framework can be paired with any vision-based multi-touch hardware plat- form (e.g. FTIR, or Diffused Illumination). The package includes a number of example applications, a Windows mouse emulator, 2D Ink/Symbol recognition, 4-point calibration, and independent presentation layer (support for XNA and WinForms included), and OSC network support using unicast, multi-cast, and broadcast UDP/IP.
Programming Language: C# License: BSD License Page: http://www.bespokesoftware.org/multi-touch/ reacTIVision
reacTIVision is an open source, cross-platform computer vision framework for the fast and robust tracking of fiducial markers attached onto physical objects, as well as for multi-touch finger tracking. It was mainly designed as a toolkit for the rapid development of table-based tangible user interfaces (TUI) and multi-touch interactive surfaces.
Programming Language: C++
60
Multi-Touch Technologies Handbook
License: GPL license Page: http://reactivision.sourceforge.net/ Community Core Vision (CCV )
Community Core Vision, CCV for short, and formerly tBeta, is a open source/ cross-platform solution for computer vision and multi-touch sensing. It takes an video input stream and outputs tracking data (e.g. coordinates and blob size) and touch events (e.g. finger down, moved and released) that are used in building multi-touch applications. CCV can interface with various web cameras and video devices as well as connect to various TUIO/OSC enabled applications and sup- ports many multi-touch lighting techniques including: FTIR, DI, DSI, and LLP with expansion planned for the future (custom modules/filters).
Programming Language: C++ License: MPL or MIT (not defined) Page: http://tbeta.nuigroup.com Touché
Touché is a free, open-source tracking environment for optical multitouch tables. It has been written for MacOS X Leopard and uses many of its core technologies, such as QuickTime, Core Animation, Core Image and the Accelerate framework, but also high-quality open-source libraries such as libdc1394 and OpenCV, in order to achieve good tracking performance.
Programming Language: Cocoa (Mac) License: LGPLv3 Page: http://gkaindl.com/software/touche http://code.google.com/p/touche/ Touchlib
Touchlib is a library for creating multi-touch interaction surfaces. It handles tracking blobs of infrared light, and sends your programs these multi-touch events, such as ‘finger down’, ‘finger moved’, and ‘finger released’. It includes a configura- tion app and a few demos to get you started, and will interace with most types of webcams and video capture devices. It currently works only under Windows but efforts are being made to port it to other platforms.
Software &amp; Applications	61
Programming Language: C++ License: New BSD License Page: http://nuigroup.com/touchlib/ http://code.google.com/p/touchlib/
2.7.2 Gateway Applications
FLOSC
FLOSC is an AS3 library that communicates with a “Flosc server” to enable flash apps to get OSC information
Programming Language: Java License: MIT Page: http://code.google.com/p/flosc/
2.7.3 Clients
Creative multi-touching
Creative Multitouching is a tool on a multi-touch environment to enhance cre- ative projects together. Things to do are drawing, simple writing and search for photo’s and video’s on Flickr and Youtube and combining them together into a creative collage.
Programming Language: Actionscript 3 (Adobe Air) Status: active License: not specified Page: http://www.multitouching.nl/page.asp?page=148 Grafiti
A multi-platform, extensible Gesture Recognition mAnagement Framework for Interactive Tabletop Interfaces. Built on top of the TUIO client, it supports the development of multitouch gestural interfaces, possibly including the use of tan- gible objects as targets.
62
Multi-Touch Technologies Handbook
Programming Language: C# License: GNU General Public License (GPL) v3 Page: http://code.google.com/p/grafiti Multi-Touch Vista
Multi-Touch Vista is a user input management layer that handles input from vari- ous devices (touchlib, multiple mice, wii remotes etc.) and normalises it against the scale and rotation of the target window. It will allow standard applications to be scaled and rotated in a multi-touch style and receive standardised input. It will also provide a framework on which to build multi-input WPF applications. It supports Windows Vista and XP.
Programming Language: C# License: GNU General Public License (GPL) v2 Page: http://www.codeplex.com/MultiTouchVista PyMT
PyMT is a python module for developing multi-touch enabled media rich OpenGL applications based on pyglet. Currently the aim is to allow for quick and easy interaction design and rapid prototype development. There is also a focus on logging tasks or sessions of user interaction to quantitative data and the analysis/ visualization of such data.
Programming Language: Python License: GPL v3 Page: http://pymt.txzone.net/ TouchPy
TouchPy is a bare bones light weight Python Multi-Touch framework that does not bind you to any specific GUI Toolkit. It is simple to use, and is the most ver- satile Python Multi-Touch framework.
Programming Language: Python License: GPL
Software &amp; Applications	63
Page: http://touchpy.googlecode.com
2DCur
A project for triggering events from OSC / TUIO Protocol 2DCur (2D cursor) messages. Currently a Python library is in the works, as is a set of externals for the Lily visual programming environment for Firefox.
Programming Language: Python, Lily (Javascript Visual Language on Mozilla Framework)
License: GPL3 Page: http://2dcur.googlecode.com
2.7.4 Simulators
SimTouch
SimTouch is another TUIO simulator build using the Adobe Air runtime. The core benefit to using SimTouch is the transparent background allowing the ap- plication developer to have a better grasp of what he/she is ‘touching’.
Programming Language: Action Script 3 (Adobe Air) License: MIT License Page: http://code.google.com/p/simtouch/ ReacTIVision
reacTIVision is an open source, cross-platform computer vision framework for the fast and robust tracking of fiducial markers attached onto physical objects, as well as for multi-touch finger tracking. It was mainly designed as a toolkit for the rapid development of table-based tangible user interfaces (TUI) and multi-touch inter- active surfaces. This framework has been developed by Martin Kaltenbrunner and Ross Bencina at the Music Technology Group at the Universitat Pompeu Fabra in Barcelona, Spain as part of the the reacTable project, a novel electronic music instrument with a table-top multi-touch tangible user interface.
Since version 1.4, reacTIVision implements basic multi-touch finger tracking, identifying simple white blobs as finger tips on the surface. This generally works
64
Multi-Touch Technologies Handbook
with DI (diffuse illumination) as well as with FTIR solutions. Additionally the overall robustness and speed of the object tracking has been improved significantly.
Programming Language: Java License: GNU General Public License Page: http://mtg.upf.es/reactable/?software QMTSim
The aim of this project is the development of a New TUIO Simulator for fast development and debugging of Multi-touch Applications.TUIO is a versatile protocol, designed specifically to meet the requirements of table-top tangible user interfaces.In order to develop applications one has to wait till one gets the required multi-touch screen.
Programming Language: c++ License: GNU General Public License Page: http://code.google.com/p/qmtsim
Appendix A: Abbreviations
Multi-touch: An interactive technique that allows single or multiple users to control graphical displays with more than one simultaneous finger.
Multi-point: An interactive technique that makes use of points of contact rath- er than movement. A multi-point kiosk with buttons would be an example.
Multi-user: A multi-touch device that accepts more than one user. Larger multi-touch devices are said to be inherently multi-user.
Multi-modal: A form of interaction using multiple modes of interfacing with a system.
Tabletop Computing: Interactive computer displays that take place in the form of tabletops.
Direct manipulation: The ability to use the body itself (hands, fingers, etc) to directly manage digital workspaces.
Blob tracking: Assigning each blob an ID (identifier). Each frame we try to determine which blob is which by comparing each with the previous frame.
Blob detection: Process of detecting regions or areas of interest in an image that are either lighter or darker than their surrounding.
Tracker: The program which takes images from a camera, puts them through several filters, and finally reports the position, size, and relative movement of blobs over some protocol
TUIO: Tangible User Interface Objects - a protocol used for communicating the position, size, and relative velocity of blobs. It is built on OSC, which is built on UDP.
Touch event: A term used to describe when a system knows that an object has touched the multi-touch device.
Gesture: A physical movement that can be sensed, and often an action assigned to it. Some common gestures are single finger panning, and two finger zoom- pinching.
Sensor: A device that measures changes in an environment. 65
ZUI: Zoomable User Interface - a user interface which is infinitely zoomable. In theory this would give you infinite workspace, but memory constrictions limit this.
Diffuser: Something that spreads and scatters light. A diffuser is used in various multi-touch techniques to create even lighting.
FTIR: Frustrated Total Internal Reflection - a multi-touch technique that ex- ploits the phenomena of Total Internal Reflection. Light within a transparent channel of low refractive index will reflect internally until an object with a higher refractiveindex, such as a finger, touches or frustrates the surface thus lighting up the frustrated area.
DI: Diffused Illumination - a multi-touch technique that makes use of a diffused surface to help filter shadows (Front DI) orilluminated fingers (Rear DI) from a touch surface. Sometimes this is referred to as Direct Illumination.
LLP: Laser Light Plane - a multi-touch technique that uses a laser and line gen- erating lens to cast a beam over a touch surface.When the beam plane is broken by an object, the area is lit up.
DSI: Diffused Surface Illumination - a multi-touch technique that uses a special acrylic Endlighten to help disperse even light supplied by edge lighting the acrylic. The effect is similar to DI.
Stereo Vision or Stereoscopic: A two camera multi-touch technique.
Zero force: Refers to the amount of force or pressure needed to trigger a touch event. In this case, ‘zero’ means ‘little.’
66
Appendix	67
ALppendix B: Building an LLP setup
LP (Laser Light Plane) multi-touch varies from other technologies mentioned in this book in the sense that instead of using LEDs it uses lasers. These lasers are used to create a plane of infrared light above the surface, instead of inside it (like FTIR), or from below it (like DI). In
this sense it can be seen as similar to LED-LP. LLP is known for its easy of setup and amazing (very high contrast) blobs with very little effort.
Step 1: Materials Needed
•	Clear Surface (Acrylic, Glass, Air) •	IR Goggles •	Infrared (780 to 880nm) Lasers with Line Lenses •	Projection Surface/LCD Monitor •	Projector (Not necessary if using LCD Monitor) •	Camera with IR Bandpass Filter to match lasers
As shown in the diagram, all the IR light in an LLP setup is above the surface, as close to the surface as can be. When an object obstructs this plane, the light is scattered downward and picked up by the camera. [Fig. 1]
Figure 1: LLP schematic
68	Appendix
A major pitfall with LLP is occlusions. Occlusion simply means “blockage or con- striction”, and it applies to LLP in the sense that with only one or two lasers, ob- jects on the table can stop light from reaching other objects, and some objects may not be visible to the camera (See illustration below).
Step 2: Safety and infrared goggles
Before we go into the theory behind LLP and actual construction of an LLP device, some safety tips must be mentioned. When working with LLP setups you will inevitably be working with Infrared Lasers, which can be dangerous. Never point a laser at yourself or others, even with line lens attached.
Figure 2: Infrared blocking goggles example
Now that that is over with, here are two tips to make sure your table is safe dur- ing construction and use. The first is to use IR blocking goggles [Fig. 2] to match your lasers. These are essentially the opposite of a bandpass filter for your eyes, and let all light through except your laser light. This cannot be stressed enough; make sure your IR goggles and bandpass filter match your lasers, or they will be useless.
Another thing people often do is build a non-reflective “fence”, or border around the touch area on their setup, just a few millimeters high. This will stop the IR light from escaping, and keep it contained within the touch area. People often use wide masking tape around the border, leaving a good inch or so sticking up.
Appendix	69
Step 3: Lasers
Obviously if one took a standard laser and stuck it over a glass surface it would not work very well. What would result is a single-touch single-dimension slider, like a fader on an audio interface. There are two reasons for this: First, the touch closest to the laser would cause total occlusion, and any objects past that object would have no light to scatter downward (Fig. 3). As shown in the third panel, Finger one is scattering all the laser light down towards the camera, and not leaving any for Finger two. Finger one would show up as a brilliant blob, however Finger two would not show up at all. The obvious solution to this is to add another laser [Fig. 4]. While then would only allow for two touches, we can ignore this issue because when we move to 2D touching this becomes a non-issue.
Figure 3: Demonstration of occlusion issues in LLP.
The most obvious problem to this setup is that it is single dimensional. In order to make this sense touches in the x and y plane we need a plane of light. To get this plane of light we use an optical lens called a Maddox Rod. A Maddox rod is a bunch of little semicircles lined up, and when a line of light enters it gets split up into a plane on light. From now on the Maddox Rod will be referred to as a “Line Lens”, as that is the common term used around NUI Group.
The illustration above shows what a normal laser beam looks like, the type used in our example above. The next illustration shows a laser with the line lens attached. This laser is generating a plane of light, and we are only viewing one slice of it. It would be impossible for us to view the whole plane with current technology. This is the kind of laser that is used in an LLP multi-touch setup.
70
Appendix
Figure 4: How occlusion can be avoided using multiple lasers
When using this there is still occlusion problems (see figure below). As seen in the illustration, the lighter object would show up to the camera, however it is blocking any light from reaching the darker object. The same obvious solution we used on the last example works here as well, use more lasers. Most people use four lasers in their LLP setup, and that provides protection from all but the weirdest and rarest occlusions. You generally do not have to worry about occlusion being too big of a deal when using two or more lasers, unless your setup is very large with more than one person using it at a given time. [Fig. 5]
Figure 5: Line lenses and occlusion
Step 4: Selecting Your Lasers
Appendix	71
There are many different kinds and specifications of lasers, and it is important to get the right for your setup. Like anything else, higher quality lasers cost more money. There are four key things to consider when buying lasers.
• • • •
Number of lasers being used Power of Lasers (in Milliwatts) being used Wavelength of light being emitted by lasers Angle of Line Lens
A common place people get their lasers is AixiZ (http://www.aixiz.com/). The first thing to think about is number of lasers. We recommend between two and four depending on the size of your setup, placed in the corners. It is possible to have occlusion problems if not using more than one laser. The ideal setup has a light plane generated from a laser in each of the four corners. When selecting lasers it is always important to have more rather then higher quality ones.
The next thing is the power of lasers. Common powers are 5mW, 10mW, and 25mW. Anything above 25 (and even 25 is most cases) is way overkill, and more dangerous then necessary. The higher power the lasers, the brighter the beam, and inherently the more dangerous that beam is. In order to maintain safety, the small- est laser that will work for your setup is desirable. A general consensus is 5mW and 10mW lasers work great as a starting point for most setups. For any screen over 20 inches (~51 cm) diagonal, 10mW should be preferred.
Less important, but still worth considering is your bandwidth. As long as it is between 780 and 900 it should work fine. Make sure you get matching IR goggles and a bandpass filter for your lasers. If your lasers are 780 nm (This is what most people use, they are very cheap), make sure to get 780 nm goggles and a 780 nm bandpass filter. Lots of people recommend going to 850 nm lasers, but there is no real reason for this. It will not work any better then a setup using 780 nm or 900 nm lasers.
Step 5: Choosing a Surface
Nothing really has to be said here, it is a complete matter of personal choice. As long as it allows IR light to pass through it should work fine.
72	Appendix
Step 6: Getting a Power Supply
You will need a way to power these lasers. The first thing is voltage. Most lasers are either 3.3V or 5V. Your power supply must match the voltage of your lasers, or you risk damaging your lasers. Running a 5V laser at 3.3 volts probably won’t hurt it, it just won’t turn on, and if it does it will not be bright. Running a 3.3V laser at 5V is almost guaranteed to fry it.
The next thing that needs to be considered is current. Your power supply will have to be able to supply enough current for all the lasers, and it is recommended that your power supply can supply more current then required. Current is measured in amps or milliamps (abbreviated A or mA respectively). Assume (very roughly) 500mA for each laser. If you have four lasers, you roughly need a 2A power supply. All you need to do is get the power supply running, and then extract the black wire (for ground), and either the red (5V), or orange (3.3V). Including a kill switch in series with the power supply is a good idea, and so is a visible indicator light in parallel with the lasers. Most people use some kind of modeling clay, putty, or similar to be able to adjust the laser in the mount but then let it dry and become permanent.
Step 7: Hooking up your lasers
Wiring up lasers is very different from wiring up LEDs. Lasers are almost never wired up in series, and there is no reason to do this for a project of this kind. We recommend wiring them in parallel, similar to the diagram below [Fig. 6].
Figure 6: Parallel wiring diagram for lasers
Appendix	73
Step 8: Attaching the line lens and focusing
The instructions in this part may differ from lasers other then those supplied by AixiZ, but the general idea should be the same. There should be three parts to your lasers, the Line Lens, Line Lens Mount, and the laser itself (Fig. 7).
Figure 7: Parts of a laser setup (line lens, mount, laser)
Start by taking the line lens and placing it in the mount. The ridged side should be pointing towards the laser diode. If it is backwards this will become apparent when focused, the line will be very wavy and choppy. Next unscrew the black part from the laser completely, and screw it into the Line Lens mount until its snug (Fig. 8).
Figure 8: Laser alignment procedure - screwing in the Line Lens mount
74	Appendix
Take the line lens mount/black piece, and screw it into the laser about half way. Put on IR goggles, grab a camera (the laser beam is very hard to see without one), power up the laser, and point it at a sheet of paper or other flat surface. Twist the line lens mount (so its screwing in and out of the laser, not the line lens mount) until the line appears very crisp. It should look something like figure 9 when done.
Figure 9: Assembled laser with line lens and mount
Step 9: Mounting and aligning lasers
There is no recommended or specific way that lasers should be mounted, its very setup-specific. People have done everything from duct tape to custom designed and fabricated mounts that allow for millimeter accuracy when adjusting. Mount- ing options very much depend on the setup, and are not within the scope of this tutorial.
Aligning the lasers is a different issue. The Line-Lens mount can be twisted about 180 degrees in either direction without loosing focus, so that makes it very easy to line up. Once they are attached, fold a piece of paper in half so there is a very flat edge protruding from your surface to compare with. Power your lasers on, and go through each of them holding a digital camera up so the beam is visible, and twist the line lens until its flat. You also want to get the laser plane as close to the touch surface as possible, so it doesn’t sense a blob before the object comes in contact with the surface. This can be achieved in most cases by lifting up the back of the laser with small pieces of paper under it, or you may search for a more permanent solution.
Appendix	75
AIppendix C: Building a Rear-DI Setup
nfrared light is shined at the screen from below the touch surface. A diffuser is placed on the bottom of the touch surface so that when an object touches the surface it reflects more light than the diffuser or objects in the back- ground. This extra light is sensed by an infrared camera and (using tracking
software) is filtered into ‘blobs’ which are then converted into (x,y) coordinates Below is a picture illustrating the effect of diffused illumination.
Figure 1: DI - Rear Illumination - schematic
Step 1: Building the Box
The first step in making a Diffused Illumination (DI) Multi-touch table is to build the enclosure. DI setups require a mostly closed box so that no infrared light es- capes. Were light to escape, there would be no uniform infrared brightness within the box, effectively rendering the effect useless. Below is a basic 3D model created using Google SketchUp of the ORION Multi-touch Table.
76
Appendix
Figure 2: Sample box 3D model (above)
The box [Fig. 2] is built out of 12mm craft wood/medium-den- sity fiberboard (MDF). There is a section on the top of the box so that a small mirror could be mounted to increase projec- tion throw. This section is also designed to house some small speakers and possibly an external keyboard. The glass sheet pictured here [Fig. 3] was later replaced with a sheet of 4mm thick glass, 71cmx56cm frosted (sandblasted) on one side for the diffuser/pro- jection surface.
Figure 3: Semi-built box based on 3D model above.
Step 2: Projector
For the projection a SHARP DLP projector Model: PG-M15S is being used.
Step 3: Webcam
The camera used in this setup is an Xbox Vision 360 USB camera that can be used as a webcam on a regular PC. The resolution on the tracking software is 320x240 @ 60fps.. This camera was fairly easy to modify (removal of the IR filter), and was a cheap and easy option at the time for a reasonably good multi-touch camera.
Appendix	77
Step 4: Infrared Illuminators
In the ORION mt there are three illuminators. A diagram illustrating the layout of the illuminators (1x 140 IR LEDs, 1x 80 IR LEDs and 1x 45 LEDs) is below.. The border represents the physical screen projection size in relation to the illumi- nators and the box.
The 140 IR LED Illuminator (Number 1 below) was originally bought by a friend from eBay.
Because of the narrow beam of this IR illuminator, there was a large hotspot on the glass. To get around this, the 140 IR Il- luminator is angled so that the light is emitted and reflected off the side wall of the box.
The placement of the illumina- tors can be seen in this picture:
78	Appendix
In the illuminator placement diagram above, the overall result is that each illumi- nator’s viewing range is overlapped to ensure that the entire viewable projection area is flooded with IR light. [Fig. 5].
Below are some specifications of the 140 IR LED illuminator:
Figure 5: Image of DI setup - projector, illuminators, mirror
•	Built-in light sensor (taped over)
•	Illuminating range: Detecting range is 80m, viewing range is 60m (outdoor)
•	Definition Consumed power: 18W
•	850nm
•	Structure: All weather aluminum and reinforced glass
•	Power: DC 12V 1000mA
The 80IR Led illuminator (Number 2), and the 45IR Led illuminator (Number 3), were designed and built using the LED calculator software available on the
internet: http://led.linear1.org/led.wiz
Originally only the 140IR LED illuminator was used in the setup. The downside to this was that during the day or close to a IR light source (overhead lights or the ambient sunlight), the IR generated was not enough to receive clear blobs. This meant that more IR light was required and therefore more IR illuminators
Appendix	79
(Numbers 2 &amp; 3) were used.
Step 5: Webcam &amp; Projector Placement
80	Appendix
Camera, mirror, and projector placements are important issues to watch for to ensure that the webcam can view the entire touch surface and the projector can project on the entire surface. Originally, there was a camera angle issue in the Ori- onMT where the camera could not capture the entire screen when placed in the
middle of the floor. To overcome this, the cameras was moved to reflect of the 2nd mirror. That way, the entire screen can be captured (with overscan).
Step 6: Touch Surface
You need a thick enough surface to not flex when pressed (to avoid short-term hotspots becoming blobs) and some kind of surface for projection and diffusion. People have used vellum, tracing paper, fabric interfacing, and even cheap plastic tablecloths or shower curtains (in my case, what worked best) on top of the sur- face, in addition to frosted glass or acrylic. Before purchasing a frosted material and/or diffuser it’s wise to test the surface’s suitability both as a projection surface and as an IR diffuser.
Figure 6: Touchlib (blob tracker) screenshot - configuration mode
Step 7: The Finished Product
The next step was to test the setup using the tracking software. If you look closely at the screenshot above you can just make out the 3 glowing sections where the 3 illuminators were placed from the raw camera feed (First 2 windows dsvlcapture0 &amp; mono1). [Fig. 6].
Appendix	81
ATppendix D: Building an Interactive Floor
his chapter explains how to build an interactive floor. This floor differs mainly in the fact that multiple people can walk over and interact with the same floor space as opposed to single person interaction of regular interactive floors. A typical setup would only cover a part of the floor,
ie. a lift lobby, or right at the entrance door, because covering an entire lobby - al- though possible - is unpractical.
Note that this should not be attempted as your first multitouch project. The reason for this is that when you build the interactive floor, you will then have the insight needed to identify problems before you go to far into the project.
Depending on the setup of the room and people installing equipment in the roof, this project will take 4-8 hours. The money spent is extremely dependent on the size of the room and optional parts required. No fixed figure can be given.
Before beginning with the project, search for possibilities to go into the roof for the setup. This will enable you to hide the setup from its users and only have a small hole where the projector projects through. Also, the light conditions in the room is one of the first things to consider. Right next to a window that has sun most of the day will not work. One can, however, control the light in the room by covering the windows with IR blocking film.
You will notice no IR illuminators are needed. The reason for that is that 95% of lobby areas will have a lighting system already installed. They already emit IR in an evenly spread way. If the lobby does not have a lighting system, you can easily add an illuminator next to the camera.
The shininess of the floor is usually not a problem. It actually helps tracking in some situations. In short - the less shiny the floor, the better the result. But if the floor is semi-shiny – or reflective – don’t break your head about it. White floors are prefered as they give the best projection.
Warning: Because the setup relies on having a projector and camera right above the floor you must ensure that all parts in the roof – or hanging from the roof – is 100% secure. If not, the parts could easily drop from the roof onto a person injur- ing him/her or severely damaging parts.
82	Appendix
Different Techniques:
There are basically two ways that you can build the setup.The first being the ability to go into the roof for the installation, the second not being able to. We will cover the In-Roof technique first, then the Below-Roof technique. [Fig. 1]
In-Roof Technique
Figure 1: Roof comparison schematics
The tools required for every interactive floor differs. This list is only a guide to what tools you would generally need.
•	Projector: Try keeping the projector as light as possible. Also use a wide-angle lens for the projector – or a short-throw projector – to ensure the entire floor gets covered.
•	Camera: Use a camera with a wide-angle lens to ensure the entire projected image gets covered.
•	Hammer and nails or screwdriver and screws: Depending on how you plan to install the camera into the roof.
Step by Step Construction:
Appendix	83
Warning: Use a stable ladder. Have an extra person with you to help as a projector falling onto you will result in injury.
Step 1: Mount the projector
First, you need to make a hole in the roof for the projector lens to fit through. Detailed intructions on this cannot be given because of the vast number of roof types. The most important point for this step is that the projector should be se- cured 100% in the middle of the roof. Also ensure that the required area is filled by the projection.
Step 2: Install the camera
A CCTV-like camera will be most preferred due to the fact that they come stan- dard with roof mounts. If yours does have a roof mount, use nails or screws to secure it to the roof. If your camera does not have a roof mount you will need to buy an extra camera mount and install it as per the mount’s instructions.
Step 3: Ensure the safety of the setup
It is advised to put the setup through a test before proceeding. Have an extra per- son close to help with this part.
1.	Wiggle the camera a bit to see if it is secure. If not, secure it more, otherwise continue.
2.	Wiggle the projector a bit to see if it is secure. If not, secure it more, otherwise continue.
3.	Usually you should leave the setup overnight and check the next morning to inspect for any safety issues.
Step 4: Testing
At this point you should have the installation done and ready for testing. How you plug the projector and camera into the computer is up to you. You will most probably need extension cables for the projector and camera.
Install the software of choice and test the setup, adjust as needed.
84	Appendix
The tools required for every interactive floor differs. This list is only a guide to what tools you would generally need.
•	Projector: Try keeping the projector as light as possible. Also use a wide-angle lens for the projector – or a short-throw projector – to ensure the entire floor gets covered.
•	Camera: Use a camera with a wide-angle lens to ensure the entire projected image gets covered.
•	Hammer and nails or screwdriver and screws: Depending on how you plan to install the camera into the roof.
•	Projector mount: As light as possible, but strong as well. •	Mirror: Any will do. •	Mirror mount: Usually a metal arm that can be attached to any surface.
Step by Step Construction:
Warning: Use a stable ladder. Have an extra person with you to help as a projector falling onto you will result in injury.
Step 1: Mount the devices
Mount these in order (projector, mirror mount and mirror), as per the manufac- turer’s instructions.
Step 2: Install the camera
A CCTV-like camera will be most preferred due to the fact that they come stan- dard with roof mounts. If yours does have a roof mount, use nails or screws to secure it to the roof. If your camera does not have a roof mount you will need to buy an extra camera mount and install it as per the mount’s instructions.
Step 3: Ensure the safety of the setup
It is advised to put the setup through a test before proceeding. Have an extra per- son close to help with this part.
Appendix	85
Wiggle the camera, mirror, and projector a bit to see if they are secure. If not, secure it more, otherwise continue.
Step 4: Testing
At this point you should have the installation done and ready for testing. How you plug the projector and camera into the computer is up to you. You will most probably need extension cables for the projector and camera.
Software Used
In order to calibrate the system, you can use touchlib or Community Core Vision (CCV). Any tracking software that supports the filters needed for diffused illu- mination will also work. You need to set your software to only detect large blobs. People moving on the floor will then be picked up optimally. A midrange rectify and midrange blur is suggested.
When setting up the software, ensure that you setup and test it in varying light conditions. This will greatly affect the setup. The recommended conditions are during the day, around 12:00, not overcast (if it can be avoided). This will give the greatest accuracy in how the floor will work on a day-to-day basis.
The following are the steps on how to calibrate the floor using CCV or Touchlib.
1.	Start CCV or Touchlib
2.	Get a flashlight
3.	Shine the flashlight on the floor and calibrate the filter of CCV or Touchlib to only see the flashlight. In other words, set rectify to a rather high amount.
4.	Turn of the flashlight, recapture background (press ‘b’) in CCV 5.	Press ‘c’ to bring up the calibration, then hit ‘c’ again to startthe calibration. 6.	Aim the flashlight at the green cross with the red circle.
7.	Turn the flashlight on and turn it off again as quick as you can. It should be as a quick flash. CCV or touchlib should have picked it up as if it was a touch to a normal screen.
86	Appendix
8.	Repeat steps 6+7 until calibration is complete for all the crosses.
9.	You should now have a quite accurate calibration.
Things to keep in mind while calibrating:
Try to stand clear of the camera’s view area, this would interfere with the calibra- tion.
Try to get the flashlight as close as possible to the camera. In other words, try to keep the light from the flashlight as close to a circle as you can. If you stand at a very low angle compared to the camera you’ll see the flashlight making an ellipti- cal light shape and thus making the calibration less accurate and more difficult to get right. If you can’t get very close to the camera, calibration is still perfectly possible, it will just take some extra time.
Conclusion
This setup is fairly simple. Literally every setup differs so you need to play around with most of the steps to find what works best for your setup. The basic concepts covered here will always apply.
It is advised to have some multitouch experience before building this. That would help you identify problematic areas on the floor before you start, and will save time and money.
Appendix	87
APPENDIX E: References
Please note that references are organized by section to which they are applicable. Reference citation numbers in the text reset per section, as indicated below.
Actionscript 3 (Flash)
[1] Adobe Flash, Wikipedia entry. http://en.wikipedia.org/wiki/Adobe_Flash
[2] “Flash for surface computing” by Manvesh Vyas http://www.adobe.com/ devnet/edu/articles/manvesh-vyas.html
[3] flosc: Flash Open Sound Control http://www.benchun.net/flosc/
[4] Migrating from ActionScript 2.0 to ActionScript 3.0: Key concepts and changes by Dan Carr http://www.adobe.com/devnet/flash/articles/first_as3_ap- plication.html
[5] AS3: Dictionary Object by Grant Skinner http://www.gskinner.com/blog/ar- chives/2006/07/as3_dictionary.html
Python
[1] Python Programming Language. http://www.python.org
[2] PyMT. A Python module for writing multi-touch applications. http://pymt. txzone.net
[3] Zelle, J. M., “Python as a First Language,” Proceedings of the 13th Annu- al Midwest Computer Conference, March 1999. Also available at: http://mcsp. wartburg.edu/zelle/python/python-first.html
[4] touchPy. http://code.google.com/p/touchpy/
[5] PointIR YouTube demonstration. PyCon 2008. http://www.youtube.com/ watch?v=bEf3nGjOgpU
[6] libAVG. A high-level media development platform. http://www.libavg.de/
[7] pyTUIO. A python module for the TUIO protocol. http://code.google.com/p/ pytuio/
[8] Kaltenbrunner, M., Bovermann, T., Bencina, R., Costanza, E.: “TUIO - A
88	Appendix
Protocol for Table Based Tangible User Interfaces”. Proceedings of the 6th Inter- national Workshop on Gesture in Human-Computer Interaction and Simulation (GW 2005), Vannes, France, 2005. see also: http://www.tuio.org/
[9]Teiche,Alex. http://xelapondsstuff.wordpress.com/ [10] Hansen, Thomas. http://cs.uiowa.edu/~tehansen [11] Pyglet. http://www.pyglet.org [12] OpenGL http://www.opengl.org/
[13] Wikipedia: OpenGL http://en.wikipedia.org/wiki/OpenGL
[14] OpenGL Architecture Review Board, Dave Shreiner, Mason Woo, Jackie Neider, Tom Davis. OpenGL Programming Guide: The Official Guide to Learn- ing OpenGL. (“The Red Book”). (Amazon link: http://www.amazon.com/exec/ obidos/ASIN/0321481003/)
[15] http://nehe.gamedev.net/
[16] Wikipedia. GUI Widget. http://en.wikipedia.org/wiki/GUI_widget
[17] PyMT API Documentation. http://pymt.txzone.net/docs/api/
[18]Wikipedia.TransformationMatrix. http://en.wikipedia.org/wiki/Transfor- mation_matrix
[19] Affine transformations. http://www.leptonica.com/affine.html [20] Wikipedia. Angle http://en.wikipedia.org/wiki/Angle
[21] Wikipedia. Euclidean Vector. http://en.wikipedia.org/wiki/Euclidean_vec- tor
Gesture recognition
[1] Grafiti : Alessandro De Nardi
[2] Real-time gesture recognition by means of hybrid recognizers : Corradini An- drea
[3]The Biological Foundations of Gestures: J. Nespoulous, P. Perron, A. R. Lecours.
Appendix	89
[4] A Hidden Markov Model-Based Isolated and Meaningful Hand Gesture Recognition : Mahmoud Elmezain, Ayoub Al-Hamadi, J ̈org Appenrodt, and Bernd Michaelis
Miscellaneous
[1] Han, Jerfferson Y. “Low Cost Multi-Touch Sensing through Frustrated To- tal Internal Reflection.” Symposium on User Interface Software and Technology: Proceedings of the 18th annual ACM symposium on User interface software and technology. Seattle,WA, USA, 2005. 115-118.
[2] Gettys, Edward W, Frederick J Keller, and Malcolm J Skove. Classical and Modern Physics. New York: McGraw-Hill, 1989.
[3] Real-time gesture recognition by means of hybrid recognizers : Corradini An- drea
[4] The Biological Foundations of Gestures: J. Nespoulous, P. Perron, A. R. Lecours.
[5] A Hidden Markov Model-Based Isolated and Meaningful Hand Gesture Recognition: Mahmoud Elmezain, Ayoub Al-Hamadi, J ̈org Appenrodt, and Bernd Michaelis
With contributions from 18 professionals and academics with over 50 figures and 100 pages, the “Multi-touch Technologies” book provides a wealth of practical and theoretical information and a fresh perspective in the field of multi-touch. Being the first publication in its domain, authors have ensured that this revison contains new information, insights, latest research and practice in an accessible manner.
This book covers areas including but not limited to: •	Different methods and tools to build a multitouch system •	How multi-touch systems work •	Challenges in user interaction, design and experience •	Open source applications, frameworks and libraries •	Companies and organizations working in this field
© 2009 NUI Group</Text>
        </Document>
        <Document ID="213">
            <Title>5.4.10 Inter-textuality:  media forms and tie-in-toys</Title>
        </Document>
        <Document ID="102">
            <Title>6.2.2 Digital Input and Output Devices</Title>
        </Document>
        <Document ID="79">
            <Title>Scope, Approach, Methodologies</Title>
            <Text>Approach, Methodologies and Originality</Text>
        </Document>
        <Document ID="435">
            <Title>Reception Theory</Title>
        </Document>
        <Document ID="324">
            <Title>Quotations and Notes - Manovich</Title>
            <Text>

Manovich (nodate), http://www.manovich.net/TEXT/assembling.html

: "As this article has tried to demonstrate, the differences between cinematic and synthetic realism begin on the level of ontology. New realism is partial and uneven, rather than analog and uniform. The artificial reality which can be simulated with 3D computer graphics is fundamentally incomplete, full of gaps and white spots." [][#Manovich:2010vn]


Manovich (nodate), http://www.manovich.net/TEXT/assembling.html

: "The second goal, the simulation of real scenes, turned out to be more complex. Digital recreation of any object involves solving three separate problems: the representation of an object's shape, the effects of light on its surface, and the pattern of movement. To have a general solution for each problem requires the exact simulation of underlying physical properties and processes. This is impossible because of the extreme mathematical complexity. For instance, to fully simulate the shape of a tree would involve mathematically growing every leaf, every brunch, every piece of bark; and to fully simulate the color of a tree's surface a programmer has to consider every other object in the scene, from grass to clouds to other trees. In practice, computer graphics researchers have resorted to solving particular local cases, developing a number of unrelated techniques for simulation of some kinds of shapes, materials and movements." [][#Manovich:2010vn]

Manovich (nodate), http://www.manovich.net/TEXT/assembling.html
</Text>
        </Document>
        <Document ID="291">
            <Title>fulltext</Title>
            <Text>Department of Computer &amp; Information Science Technical Reports (CIS)
University of Pennsylvania	Year 
Real Time Control of a Robot Tacticle Sensor
Jeffrey A. Wolfeld University of Pennsylvania
University of Pennsylvania Department of Computer and Information Science Techni- cal Report No. MS-CIS-81-04.
This paper is posted at ScholarlyCommons. http://repository.upenn.edu/cis reports/678
U N I V E R S I r I	OF	PENNSYLVPu~IA THE MOORE SCHOOL OF ELECTRICAL ENGINEERING SCHOOL OF ENGINEERING AND APPLIED SCIENCE
REAL TIME CONTROL OF A ROBOT TACTILE SENSOR Jeffrey	A.	W olfeld
Philadelphia, Pennsylvania August, 1981
A thesis presented Science in partial fulfillment of the requirements for the degree of Master of Science in Engineering for graduate work in Computer and In£~rmation Science.
Ruzena Bajcsy
Aravind K. Joshi
The work reported he~e was supported in part by NSF grant number MCS-78-07466.
~o the Faculty of Engineering and Applied
Ma~te~s·Thes~s
REAL TIME CONTROL OF A ROBOT TACTILE SENSOR Jeffrey A. Wolfeld
Phil~delphia, Pennsylvania August 1981
Abstract
The goal of the Experimental Sensory Processor project is to build a system which employs both visual and tactile senses, and then explore their interaction in a robotic environment. Here we describe the software involved in the low level control of the tactile branch of this system, and present results of some simple experiments performed with a prototype tactile sensor.
ii
Jeffrey A. Wolfeld
--
Acknowledgments
I would like to thank the following people:
Jim Korein, my office mate, with whom I had many fascinating discussions between 9:00 and 5:00 on weekdays;
Gerry Radack, who occasionally dragged me away from my terminal in order to play music;
Clayton Dane, who helped keep my feet on the ground:
Jeff Shrager, without whom I. might never have gotten pas·t the Abstract;
Taylor Adair, who kept the computer running when it really wanted to crash;
Ira Winston, who seTve~ as the local oracleJ Jack Rebman of the Lord Corporation, without whom this
thesis would have been entirely speculation; David Brown, who got me into this mess in the first place; and my advisor, Ruzena Bajcsy, mother to us all.
iii
--
.-. --~
Table of Contents 1. Introduction •	• • • • • • .. • •
1.1Motivation. • • • • • • • • • • 1.2ProjectOverview. • • • • • • • •
PAGE
2 2 4
••	9 • • 10
•• 11 •	13
•	17 22
• • • • • • • • •
2. Proposed Microprocessor Software	• 2.1Processors.	• • • • • • •
••	• .' • •
2.1.1	T actile	Sensing	Processor 2.1 •.2	Motor	Control	Processor	•
2.2 Cross-Sectional Scan Command. 3. The Implemented Software ·
•	•..• •••
• ••••••••
3.1 Environmental De.t~il~ .. 3.2 Command Format and Interpretation
22
23
26 33· 35 35
• •••• •••••••••
•••••••
• •••••••
3.3 Motor Control. 3.4 Tactile- Data Acquisition ·
4. Experiments and Results
4.1 Calibration
•••••••••••••
4.2 Static Tactile Image Analysis	•	•	•	•	•
•	37 • 37 •	39 •	40
41 •	42
4.2.1SingleImage. 4.2.2 Spatial Resolution.
• • • • • • • • •	•	•	•	•	•	•
4.3 Dynamic Texture Analysis ·	•	•	•	•	•	•	•
4.2.3 Multiple Images. 4.2.4LargeObjects •••••••• 4.2.5 Small Angle Measurement •	•	•	•	•	•
•	•	•	•	•	•
4.4 Conclusions s. Further Work .
••••••••••••• ••••••••••••
44
48
50
•	••• •	•	•••
iv
Copyright 1981 by Jeffrey Wolfeld
. 1.1 Motivation
Chapter 1: Introduction
A rtificial	Intelligence	researchers	have	worked extensively with vision systems in an attempt to give computers, and eventually robots, a sense of sight. A great deal of this research has been directed toward overcoming certain basic inadequ~~ies in our current technology. For example, imperfect light sensors dictate that noise must be eliminated or tolerated. Insufficient spatial resolution requires routines which will interpolate below the pixel level.
One of the most important problems is that a camera produces a two-dimensional image ·of a three-dimensional scene. This invalidates an assumption which one would like to rely upon -- that two adjacent points in the image are adjacent in the scene. Therefore, substantial effort has been devoted to reproducing 3-D data from one or several visual	images.	T actile	sensors	can	.be	used	to	aid	the process.
An imaging tactile sensor, by its very nature, does not have the problem. Since it produces a two-dimensional image
2
of a two-dimensional scene, it does not provide as much information, but it yields useful information clearly,
without the need for complicated heuristics.
We can take this one step further. Suppose a tactile sensor is mounted on some kind of computer controlled 3-D positioning device. Then, by moving the sensor to different points on a target object, the computer can actually obtain
3-D data directly, and much more selectively. If this information is used to supplement and augment visual data, a
great deal of processing may be avoided.
One can come up with many other uses for varying kinds of tactile sensors. Briot [BRIOT-7~ demonstrated that tactile sensors mounted on the fingers of a robot hand can be used to determine the position, orientation, and perhaps even the identity of an obje~t which it has grasped. He also showed that a grid of pressure sensitive sites on a table can tell a robot the location, orientation, and again, the identity of a part. It should be possible with multi-valued pressure sensors, as opposed to binary sensors, to determine the mass of the object. When the angle is small, a tactile sensor can be used to compute the angle between it and the object being grasped, possibly with a view toward improving the grip. Also, if the device is sensit"ive enough, it can be an invaluable aid to a robot attempting to grasp a fragile object without breaking it. Finally, a-tactile sensor makes
it possible to incorporate the properties of surface texture 3
and resilience into the object recognition process.
1.2 Project Overview
The design and development of the tactile system has proceeded with two different sensors in mind. Unfortunately, there are so many disparities between the two that we had difficulty keeping the system general enough to handle both. Let this serve as a demonstration of the variety of characteristics that must be considered for a given application.
The first sensor is about five inches long, with an octagonal cross section about 3/4 inches in diameter. Each of the eight rectangular faces is connected "to a tapered piece, which is in turn connected to a common tip piece. There are a total of 133 sensitive sites -- 16 on each main
face, one on each alternate taper, and one on the tip. Because of the the vague resemblance, we will refer to this sensor as the Finger.
The second sensor, the Pad, is a flat rubber square about two and one half inches on a side. An 8 x 8 grid of conical protrusions identify the 64 pressure sensitive sites. The pad is mounted on a square metal pi~ce, about three and one half inches on a side, which is in turn connected. to another similar piece by four metal posts. These posts have strain gauges on them which measure the force parallel to the object's surface.
4
Initially, we only considered the finger. Because of its shape and organization, o .	the sensor is best suited to applications involving probing and' tracing. This includes testing for resiliency, e~amining surface texture, and tracing cross-sections of an object. In our view, texture would be thought of as a kind of microscopic contour, while the	cross-section	tracings	would	yield	a	macroscopic contour. Taken together, we would be able to acquire an extremely detailed description of very selective parts of the object in question.
Unfortunately, this rather vague idea has not been developed. We have instead dealt with the two descriptions independently with the assumption that they can both be
incorporated into a general object recognition system.
For his Master's Thesis, David Brown [BROWN-SO] developed a three-dimensional positioning device for the finger. Basically,· it is a square horizontal metal frame mounted on four legs. Moving forward and backward on this
is a second, vertical square frame. A vertical track rides left to right on that, and a rod moves up and down in the track. The finger would be mounted with its tip downward at the bottom of the rod.
Thus, we have three degrees of freedom -- the X, Y and Z axes -- each positioned by a stepper motor driving a lead screw. This gives us the capability of examining, from the top, any object or objects placed on a table below the
5
horizontal frame, in a total working volume of about 18 cubic inches. Since the degrees of freedom are strictly -positional, as opposed to rotational, we are not c'apable of reaching under an overhanging lip, or sideways below a -"covering section.	This places certain restrictions on the kind of object we can examine. If we think of the horizontal axes as X and y~ then the object must be describable as a strict function of those two variables. Needless to say, this is not a robot arm, but we felt it
would suffice, temporarily at least, for our research.
The positioning device and tactile sensor are directly controlled by a pair of zao microprocessors, which are in turn under the command of a PDP-ll/60 minicomputer •. Of the
Z80's, one (the Motor Control Processor, MCP) is responsible for driying and p/)sitioning the stepper motors, and the other (the Tactile Sensing Processor, TSP) .is dedicated to tactile data acquisition and compression. The MCP and TSP -communicate with each other via a 14-bit wide parallel data path. The PDP-ll/60 issues high level commands, and receives positional information, through a serial connection -to the MCP. Finally, tactile data is passed to the 11/~O through a DMA link from the TSP.O
One of the aforementioned high le-vel commands would request the microprocessors to trace the cross-section of ~n -object	in	any	arbi trary	plane	in	space, ··passing	the	sequence
of 3-D coordinates back to the host computer. A great deal 6
of thought went into the implementation of this command, and it is, to some extent, responsible for the architecture described above. The procedure will be described in detail in a later section. It is a good example of how tactile sensory feedback can be used in a real time, closed loop fashion.
The finger was designed and fabricated at L.A.A.S., the major robotics establishment of the French government. Because of a severe lack of communication, many of the finger's details were not known to us when the software was being designed. This had a positive affect in that we were
forced to be as general as possible. However, due to a number of unexpected delays, we still do not have the finger in our possession.
We arranged to borrow the pad sL&lt;nsor from Lord Corporation in Erie, Penna.* They traditionally deal with blending rubbers and bonding rubber to metal. This sensor, still in the prototype stage, is an attempt to expand their business.
At any rate, we had the pad sensor in our possession for three very long days. In preparation for that ordeal, we planned a number of different experiments. The Lord people were very helpful in this, and they provided us with the appropriate wooden test objects.
* Lord has since moved to Cary, South Carolina. 7
The characteristics of the pad sensor are very
different than those of the finger. In particular, there is only one sensitive face. This makes the pad much less
suited to contour tracing. We therefore decided to concentrate on some of the other aspects of tactile
dynamic texture analysis, static pattern recognition, and measurement of small angles between the object and sensor surfaces.
The ensuing sections will describe in detail the work performed.
sensing --
8
Chapter 2: The Proposed Microprocessor Software
In anticipation of the arrival of the finger, a great deal of software was planned.	Then, when the delays became apparent, work on those aspects not directly applicable to the pad sensor screeched to a halt. As a result, some of the design described here has not yet been implemented. In a later section we will discuss in detail exactly what the existing software does.
One of the important features of the Experimental Sensor Processor is its delegation of low level tasks to other processors. ~his helps to diminish the computational load on the host pdp-ll/60.	The tactile branch, in keeping with this principle, would have a set of commands which could be invoked by the host to perform various I/O and timing intensive operations, or functions involving real time feedback. Following are some of the c.ommands that were considered:
1. Reset the machine.
2. Move to absolute coordinates (x, y, z), stop on collision with an object. This can be used as a
nfind something in this direction" command. 9
3.
4.
5.
Scan Cross-section -- Trace the contour of an object in an arbitrary plane in 3-space. Returns to the host a list of step vectors describing	the	finge·r' spath.
Local Texture -- Trace around a small circle on the	surface	of	an	object	and	produce	a description of the texture. This could be in terms	of	degree	of	roughness,	degree	of compliance, or something as crude as a list of pressure values for each point in the path.
Search (in an as yet unspecified manner) for either a concave or a convex edge. It is assumed that the finger is already in contact with a surface.
6. Follow the contour of a concave or convex edge. Passes a list of step vectors to the host describing the finger's path.
The first command, Reset, is trivial. It simply involves the reinitialization of variables. The move command, due to its fundamental nature, has been implemented for use with the pad sensor. The cros~-sectional scan
command has rec'eived a great deal of atter-.:ion·, but has not been completely implemented because of its incompatibility with a single-face sensor. The final three commands, Local Texture, Find Edge, and Follow Edge, have to date received very little serious consideration. They are quite tentative, and may never be implemented.
2.1 Processors
As described in other sections of this thesis, the tactile branch consists of two microprocessors, the Tactile Sensing Processor (TSP) , and the Motor Control Processor
(MCP). A different program runs in the firmware of each 10
processor. Both are entirely interrupt driven using the Z-80 vectored interrupt system. From the host computer's point of view, the TSP provides data for texture analysis, and the MCP provides data for contour analysis.
2.1.1	T actile	Sensing	Processor
The TSP program consists of a single loop in which each of the sensors is interrogated for its a-bit pressure value. Each value is thrown into one of three categories with respect to a low and a high threshold. the category
indicates whether the sensor is not touching anything, is in contact with an object, or is pressing the object too hard.* The sensors are then grouped by finger face, and a face status is computed for each face using the following rules:
J~S over range, the face is over range; If there were any face status changes since the last pass,
If any sensor If all sensors are below range, the face is below range~ Otherwise, the face is within range.
the Motor Control Processor is informed.
It is worth noticing that this condensation algorithm is independent of the particular organization of the finger. The number of faces, the faces' orientations, and even the
* We hope that the sensors have enough compliance of their own so we can arrange the thresholds successfully. We would like to guarantee that for any movement toward an object, there is at least one position in which the leading sensor is "in contact" before it exceeds the upper
threshold.
11
mapping of sensor number to face number are stored in tabular form, and may be altered according to the parameters of a different sensor. It will be obvious later that the more faces we have, the easier it is to k~ep in contact with an object. In the ideal case, we would like a hemispherical finger with many sensors, each on its own face.	Such an organization can be accommodated just as well as the current finger.
In addition to providing this condensed status information for the sister processor, the TSP must send some data to the host, for the texture analysis.	How much· data does the host need? If we send it all we can -- 133 a-bit bytes per step, 125 steps per second -- we would need the
equivalent of 20 9600 baud serial commu~ication lines to handle the load! The bottleneck is removed by using a Direct Memory Access (DMA) interface. But even so, we cannot expect the PDP-ll(60 to analyze data arriving at such an incredible rate, and still be able to keep up with the other sensory branches, and perform the higher level recognition tasks at the same time. It simply does not have the computational power.
The answer, of course, is to filter or condense the data before sending it. We have several possibilities in mind. FirstJ a sensor is only considered -valid if its pressure value is "within range".	This filter is always in effect. Other possibilities include averaging sensor
12
readings over time and only reporting after a fixed number of steps, or combining somehow the readings from all sensors on each face which is "within range" to produce a single face pressure value. A final possibility is to arrive at some kind of measure of roughness for the surface under consideration, and only pass that number back to the host com~uter.	This decision has not been made.
2.1.2 Motor Control Processor
The Motor Control Processor's basic job is to control and coordinate the three stepping motors which position the finger. When it is necessary that the host computer know the path that the finger follows during the execution of a command, the MCP provides it.
Steps are taken in a synchronous fashion. That is, if the step rate is set to 125 steps per second (the default case), the processor is interrupted every eight milliseconds to determine which motors are to be stepped, and in which
direction.
So, after each interval, the MCP may pulse any combination of the three motors, and each can be in one of two directions. This leads to 26 possible directions in which a single step can move (ignoring the case where no step is taken at all). We represent. this direction as a
13
6-bit "step vector", organized as follows:
bit5 4 3 2 1 o 1Z1Z1Y1Y!X1X!
1 dir.ection ! step ! direction ! step 1 direction ! .step !
Since this fits easily in an a-bit byte, it is very convenient now for the MCP to g-ive a path to the host computer. It simply sends a one-byte step vector over the serial line for each step taken. The host collects the sequence of step vectors in a buffer, and the exact path can be reconstructed very quickly at any time.
There are, of course, situations in which it is necessary to give an absolute coordinate. For example, when the absolute move command is aborted due to collision with an object, it is necessary to inform the bost what the new position is. A mechanism is provided for t,his, too.
Notice that tne MCP returns (effectively) a sequence of points. It does not try to fit them to curves, surface patches, generalized cylinders, etc. This is left to the host computer.• It is unreasonable to _expect an 8-bit microprocessor which lacks even a multiply instruction to do these in real time.
When moving from one position to another in 3-space, it is desirable to do so in a straight line. This requires varying the speeds of the individual motors so that they all arrive at their destinations simultaneously. The following
14
example shows how we would like to arrange the steps in a sample situation.
AB
x	17
y
21 Z	5
steps
desired time between steps
===~~====~================
9.88 milliseconds
8.00 milliseconds 33.6 milliseconds
===~=~=
The values in column B were arrived at by dividing the column A values into the greatest column A value, and multiplying the result by 8 millisecs. (8 millisecs is the speed at which we would like the fastest motor to operate) •
This is a lot of work for an a-bit microprocessor to perform. Also, if the precision of these calculations is not great enough, it becomes virtually impossible to predict exactly where the finger will be at any given point in time.
Fortunately, the synchronous stepping scheme makes matters much simpl~r.	The overall line of motion is a line in 3-space. This is described and stored in terms of three direction components. There are also two accumulating
counters, one for the mid direction, and one for the min direction. (The mid direction is the dimension which has the second-largest number of steps to take. Min direction is defined similarly.) Both are preset to zero.
After each a-millisecond interval, a step vector is created, and the motors are stepped accordingly. The max direction is always stepped. For each of the other two
15
directions, the accumulating counter is incremented by the corresponding direction component value, and the result .is taken modulo the max direction component. If an overflow occured, a step is-taken.
Applying the algorithm to the above example results in the following sequence of steps.
Step XYZ 1 Step XYZ
==================1================== 1 * 1 11 * 2 ** 1 12 ** 3 ** 1 13 *** 4 ** ! 14 **
5 ***1 15 ** 6 * 1 16 * 7 ** 1 17 *** 8 ** 1 18 ** 9 ***1 19 **
10 ** 1 20 ** 1 21 ***
When a step is taken, two corolla:~.y actions occur. First, if the MCP is providing path information, the step vector is sent to the host. Second, a termination test is made. For the absolute move command, termination occurs when the finger reaches its destination.
This command also terminates if the Tactile Sensing Processor indicates that the finger has come in contact with an object. Primarily, this is to protect the finger from damage. However, it also makes it possible for the host to say, "look in this direction for an object." In that sense, this command can be used as an object finder.
16
2.2 Cross-Sectional Scan Command
This command is invoked by the host to trace the contour of an object's cross-section in any arbitrary plane in 3-space.* The arguments include the coefficients a, band cintheequationofthe planeax+by+cz=0,andapair of special 3-D points which define the search volume. The
finger must already be touching an object, and the plane is assumed to pass through the finger's current position.
Consider a conical object and a slicing plane parallel to the x-y plane. The MCP will drive the finger in the plane such that it remains in contact with the surface of the cone. All the while, it passes its path back to the host. Later, the host will analyze the path, and discover that it describes a circle.
The search volume is included to limit the finger's range of motion. Suppose, for example, the host wanted to construct a 3-D bicubic surface patch.	It could do this by requesting four cross-sectional scans using vertical planes whose y-z projection is a rectangle. Then it could fit curves to each of the four point sequences, and perhaps fit a patch to these four curves.
* My terms will be very confusing unless I define them at the outset. "Plane" generally refers to the arbitrary cross-sectional plane given by the host. "Surface" is the
(possibly curved) surface of the object. "Face" refers to one of the faces of the finger on which sensors are mounted. "Search volume" means the physical volume in which the finger is allowed to move.
17
Unless we provide somemechanism for limiting the search space, there is no way to prevent the finger from doing a complete scan of the o·bject's cross-section, when only a small portion of that scan is needed.
The search volume is a rectangular parallelepiped with diagonally opposed corners defined by two arbitrary points in 3-space. The arbitrary points are chosen by the host computer and passed to the MCP as arguments to this command. Very often, the points may contain special coordinate values of 0 or 'max'. These may be used to effectively leave one or more dimensions completely unconstrained.
In the surface patch example, we would like to constrain the x and y position to the projection of the four slicing planes onto the x-y plane. The z position should not be constrained at all. Thus, the two arbitrary points might be (Xl, Yl,O) and (X2, Y2, max)'.*
The scan will terminate when the finger either exceeds one of the bounds, or returns to its initial position. This second termination condition is useful if the host is
interested in producing a contour map of the object. It could do this._ by requesting a series of scansI	using cross-secti9n planes parallel to the x-y plane, but at varying z values. In this case we would like the finger to
* In addition to this constraint, there is an implicit maximum search volume given by the dimensions of the device.
18
completely circumscribe the object, continuing until it returns to its starting point.
A problem which has not yet been mentioned is that of keeping in contact with the surface of an object. It turns out that in most situations, this is relatively simple. The
method requires three kinds of information.
As described earlier, the finger has a number of distinct faces. The present structure of the positioning device does not allow for rotation or re-orientation of any
kind. Hence, .except for possible translation, these faces are fixed. Their equations, as well as those of the planes perpendicular to them, are predefined as constants in the MCP program.
Second, we have the equation of the ~=ross-sectioning plane. All motion of the finger is to be rt:stricted to that plane. By intersecting this plane with either the plane of a face of the plane perpendicular to a face, we can calculate a line of motion. This can then be fed to the absolute move routine to effect the movement.
Finally, there is the data from the Tactile Sensing Processor. This indicates whether each face is below range, within range, or above range. Typically, there will be only one face which is within range. This is labelled the "active face," because it is the one which is in contact with the surface. There are exceptions, and we will see
19
shortly how we can account for them.
The objective in keeping in contact with a surface is to keep the active face within range. Recalling that by definition of the command, the active face is initially within range, we have the following cases:
(1) Active face is within range; (2) Active face is below range; (3) Active face is above range; and (4) A second face comes within or above range.
In case (1), the finger is in contact with the surface. Our best estimate of the shape of the object at this point is a plane parallel to the active face. Calculate the line of motion (if it has not been calculated already) as the intersection between the active face and the cross-sectioning plane. Send the current position to the
host, and take a st~p.
In cases (2) and (3), the finger either has lost contact, or is pressing the surface too hard. Calculate a line of motion as the intersection between the cross-sectioning plane and the plane perpendicular to the active face. Then take a step along it away -from or toward
the finger's cente~, respectively. Do not send this step vector to the host, because it is not part of the surface contour.
Case (4) could result from several different situations. Take the scenario in which the finger hit a
20
~ -.	..	..~	.. -... concave corner. In this case, the appropriate action is to
make the new face the active face, and then act according to its status.
Another scenario in which case (4) could occur involves reaching either a convex corner, or a point at which the surface curves away from the curr~ntly active face. Again, the appropriate action is to declare ·the new face as the active face, and act according to its status.
There are a number of other situations in which a second face could come within or above range. The appropriate action is not always the same as above. In fact, one could imagine situations ~n whicp ~ third and perhaps a fourth face must be considered. Though these cases have not yet been adequately resolved, we do not
expect them to be o~'erly troublesome.
21
Chapter 3: The Implemented Software
We noted earlier that although the software was designed for the finger, it was eventually implemanted for the pad sensor. The most notable difference between design and implementation was the fact that in the end, we only used one microprocessor. All those commands which required multiple face sensing -- trace contour, follow edge, etc. -- were eliminated because the pad sensor in fact has only one face. It happened that these commands coincided with
the ones which required real time feedback. Therefore, the requirements of the tactile data acquisition software became almost trivial, and.could be handled easily and much more simply by the Motor Control Processor.
3.1 Environmental Details
The microprocessor software is written in Z80 assembly
language. It resides on the PDP-ll/60, which runs under the RSX-IlM operating system. We use a primitive Z80 assembler, written in C, which produces Intel hex-format object code. This we download to the microprocessor' via the 1200 baud serial line which connects the two systems. As it turned
22
out, 1200 baud was as fast as the 11/60 could reliably receive and store data.
The microprocessor system is.made up of a California Computer Systems 5-100 bus and mainframe, 8K of RAM; and a Cromemco Single Card Computer (SeC) with lK RAM and room for
8K of PROM, 1K of which is taken ~p by a modified form of Cromemco's power-on monitor. The sec has five timers, three parallel ports (input/output), and a serial port. Since the AID converter built into the pad sensor produced CMOS output
levels, we decided to temporarily add our own converter, a Cromemco D+7A board.
In	the	following	sections	we	give	a	complete description of the software as it currently stands.
3.2 Command Format and Interpretation
The command language was to be a permanent part of the software. It would"be used initially by a 'humari user to control the pad sensor's movement and data acquisition. Eventually, however, it would become the Experimental Sensory Processor's way of driving its tactile branch.
Thus we had three goals in· mind. First, the command language should be versatile. It should be able to handle the commands described in the previous chapter as well as the simple placement and data acquisition commands we needed for the pad sensor experiments. Second, it should be
23
concise enough, and easy enough to interpret, to be used for interprocessor communication. Finally, it had to be legible, so that the user could issue commands from his keyboard.
We settled on a syntax with mnemonic, single character commands, optionally preceded by an ascii-coded positive or negative integer which defaults to +1 if omitted, and optionally followed by any special arguments required by the command. The preceding integer is decoded by the parser.
I t	generally	refers	to	the	m u ltip lic ity ,	though	its interpretation is up to the individual command routines. The trailing arguments are parsed and interpreted completely by the individual command routines.
Commands may be strung together to form a command sequence. Execution will not begin until a carriage return is received. The sequence is, of course, stered in a buffer until execution is complete. A key advantage to this is that it makes loops possible.	In the syntax, a subsequence may be grouped by parentheses, which in turn may optionally be preceded by a mUltiplicity M.	The entire subsequence will be repeated M times. Subsequences may be nested to any
reasonable depth.
There is one more rather important feature. While the command sequence is incomplete, the Motor Control Processor completely disables interrupts. Since the motors are driven by periodic timer interrupts, all movement must stop.
24
--, Similarly, characters coming from the serial line during
command execution are ignored. This generally does not matter, because execution will have terminated before a new command sequence arrives. However, should it become necessary for the host computer (or user) to abort exec'ution, it (h'e) may send an ESC'ape ch·aracter.. . This causes a non-local subroutine return to the command sequence
input routine, which immediately disables interrupts. The following is a list of the commands currently available.
B nX
Home -- return to inner, upper left corner, and reset the current position to (0,0,0). Move n steps in the X direction (n may be positive or negative, and defaults to +1 if omitted) •
Move n steps in the Y direction. Move n steps in the Z direction.
nY nZ @x,y,z Move to absolute position (x,y,z). n(
=)
Q
18 -is
as G
space
Begin nest. Endnest. Return current position as x,y,z coordinates,	ascii-coded	decimal	values separated by commas. Quit the program	return to power-on monitor •. Take a snapshot of the sensor, store data in memory, increment frame count. Take as many snapshots as possible until the completion of the current motor step. Clear the frame memory.
Send the contents of the frame memory to the host, beginning with the frame count. All data is in ascii-coded hexadecimal. Then clear the ·frame memory.
Null operation.
These commands are obviously very simple. they can be very powerful when grouped together. For example, the sequence
@lOO,lOO,lOO SO( 3( 20X 20Z S -2QZ) 20Y -60X) G 25
'However,
takes 150 snapshots, in a 50 by 3 grid, beginning at &lt;,100,100,100), then sends all the collected data to the host computer. Since optical limit switches prevent the motors from moving past the ends of travel, one could find the
maximum limits in all directions by issuing @lOOOO,lOOOO,lOOOO =
(the actual range is roughly 1200 steps per axis). This would move the sensor to the corner opposite the home position and report the actual coordinates.
This list will eventually be enhanced to include the commands described in the previous chapter. We expect to be able to continue to denote each command with one mnemonic character.
3.3 Motor Control
It is not surprising that the most complicated task performed by the Motor Control Processor is, in fact, motor control. The complexity arises for two reasons. First, it
is intended to be a permanent part of the MCP software, and is therefore very general in design. Second and most important,	the	step	service	routines	effectively	and completely insulate the higher· level command execution processes from the hardware.
At the top level, an individual command routine uses
26
the step services in the following fashion:
Set the direction components in LINE Call SCFILL to fill the step control table Do until termination-condition:
End
Call STEP to initiate a step when ready Call NEWPOS to update current position Call NEXTPO to prepare the next step
Note that it does not concern itself with timing in any way, nor does it have to take into account the physical
limits of the device. The STEP routine guarantees a minimum pulse width (maximum step rate), and even modifies the step request if such an action would drive a motor past its end of travel.
Also note that the routine must actively request that a step be taken.	If, for some reason, the evaluation of the termination conditicn is very time consuming, the motors will simply run slow~r. This has another advantage. Should
the program Qe damaged by an unusually high incidence of cosmic rays, the motors will not go out of control. They will simply stop, because nothing is calling the STEP routine.
Before we take a closer look at these routines, we must discuss the data structures involved. The first one that was mentioned is LINE. It takes three numbers to define the direction of a line in 3-space: delta-x, delta-y, and delta-z. These are the line's direction components. Simply put, when we take delta-x steps in the x direction, we must
27
also take delta-y steps in the y direction, and delta-z steps in the z direction. Within the MCP, these values are stored and manipulated as unrestricted 16-bit integers. However, should it later become necessary to compare line directions, these may have to be restricted to relatively prime integers. LINE is a three word array which defines
the desired path to the step routines.*
A commonly accepted canonical form for these values is a list of direction cosines.	This requires that the values be real numbers, and that the sum of their squares equal unity. Fortunately, we have not found this form necessary.
The second data structure is the Step Control Table (SCTAB). This lS-byte table is basic to the operation of the step service routines. Following is a layout of its
contents.
SCTAB+ 0: (byte) Next port image 1: (byte) Port image skeleton (direction bits) 2: (word) Max direction component 4: (word) Mid direction component 6: (word) Min direction component 8: (word) Mid accumulating counter
10: (word) Min accumulating counter 12: (byte) Max direction's motor pulse and power bits 13: (byte) Mid direction's motor pulse and power bits 14: (byte) Min direction's motor pulse and power bits
Let us digress a moment before we explain SCTAB. Instructions are passed to the stepper motors via an a-bit
* The zeo, of course, does not really have any distinct concept of a .lword.·1 However, being an "old PDP-ll man, I always have and always will refer to a 2-byte quantity as a word.
28
output port, which looks like this:
bit7 6 5 4 3 2 1 a 1 Z 1 Z ! Y-Z 1 Y 1 Y 1 X ! X ! X 1
1 dir lstep Ipower! dir !step !power! dir lstep 1
The three direction bits indicate which direction the corresponding motor is to move. One implies the negative direction, zero implies the positive. The step bits, when pulsed, cause their corresponding motors to take a step in
the indicated direction. Due to a low-pass filter which is applied to these bits for noise immunization purposes, there is a minimum pulse width. The MCP uses a separate timer for this, as will be described later.
Finally, the power bits, when on, cause drive power to be applied to the corresponding motors. For now, the reader need only understand that a motor must have power in order to .operate.
Now we should be able to make sense out of the Step Control Table. The first item, the "next port imagen is exactly that -- the 8-bit quantity that is to be sent by the
STEP subroutine to the motor drive output port at the next opportunity. It is very important to note that this value is, in general, calculated concurrently-with the previous
step, by a call to NEXTPO. The second item, the "port image ~keleton," contains
the three direction bits. These bits are applied with every
29
step. The SCFILL routine sets them according to the signs of the three direction components in LINE, and they do not change again until a new line is chosen.
The next three items, the Max, Mid and Min direction components, are actually the magnitudes of the numbers that appeared in the LINE array, but in sorted order. These are used in conjunction with the Mid and Min accumulating counters to determine which motors to step at the next
timing interval.
Finally, the mapping from the sorted order to the x-y-z order is given by the last three items. Each of these bytes has exactly two bits set, corresponding to the appropriate motor's step and power bits.
The NEXTPO routine first decides which motors are to be stepped, and then adJs together the corresponding mapping bytes, along with the direction bits from the skeleton. The resulting value is the next motor port image.
Let us now return to the . high level control loop given at the beginning of this section. First of all, note that the values passed in the LINE array indicate a direction only. They do not completely describe a line segment in 3-space. It is assumed that the line of motion will begin at the current position, and the control loop is responsible
for knowing when to stop.
Once the LINE table is set, SCFILL is called to fill 30
the Step Control Table. All values are calculated independent of the previous contents. The NEXTPO routine is then called automatically to use the new table- to compute the first port image and place it in the zeroth location.
Since a step is never taken unless specifically requested by the control loop, it is·perfectly reasonable to completely change direction at any time by simply changing LINE and calling SCFILL, before calling STEP again. One need not be concerned with the timing considerations.
Within the control loop itself, the first action is a call to the STEP routine. This routine waits, if necessary, for the previous step to complete. Then it calls CHECK to check the .optical end-of-travel limit switches and, if necessary, modify the candidate port image.	Finally, the
routine outputs the image to the motor port and returns to the calling control loop.
Internally, one of the five on-board timers is also set to cause an interrupt after a time equal to half the minimum step pulse width'has elapsed. The routine which handles that interrupt will clear the motor step bits and set the timer to interrupt again after another equal interval. At that point, an entire step has completed. The STEP routine, if it is waiting, is allowed to proceed with another step.
In this way, something like an open ended squa-re wave is generated on the motor. pulse bits.
31
This brings us to the other subroutine calls in the main control loop. During the timing delays, the CPU is free to do quite a substantial amount of processing. Recall that the STEP routine has the power to modify the candidate port image. This modified image is returned to the control loop, where it is passed again to the NEWPOS routine.
NEWPOS, based on the direction and step bits which were actually sent, updates the current coordinate counters.
The calculation of the next port image is then accomplished by a call to NEXTPO, which proceeds as follows.
1. Begin with the motor port skeleton, which defines the direction bits.
2. Add in the Max direction's pulse and power bits. That motor is to move at the maximum rate, and will therefore always take a step.
3. Add the Mid direction component to the Mid accumulating counter, and take the result modulo the Max direction component.	If there was an overflow, we want to step the Mid motor. Add in its pulse and power bits.
4. Repeat step 3 for the Min direction.
The resulting value is placed in the first byte. of the Step Control Table. An example of this algorithm in operation was given in chapter 2.
There is one final item to discuss. Conceptually, a stepper motor has a series of magnetic coils arranged in a circle around an iron core. As steps are taken, each coil
in succession is energized, drawing the core around the circle. During normal operation, a given coil is only
32
energized for a brief period before its successor takes
over. However, when the motor is standing still, one coil is energized continuously for a long period of time. It can
generate quite a bit of heat -- enough, perhaps, to burn itself out.*
To solve this problem we imp~emented the following scheme. Every time a motor is stepped, its power is automatically turned on. At the same time, its corresponding usage counter is reset to some constant. Periodically, another of the on-board timers interrupts the processor to decrement all the usage counters. When anyone
reaches zero, the corresponding power bit is turned off.
The effect of this is to power down any motor that has not been stepped in the last two seconds. The action is so completely transparent to the higher level control software that we refer to it a~, the "burnout protection demon."
3.4 Tactile Data Acquisition
Due to its temporary . status, the tactile data acquisition is perhaps the least important p~rt of the software. As soon a~ the finger arrives, these routines will be removed from the Motor Control Processor and rewritten completely for the Tactile Sensing Processor,
* I don't know whether motors would actually burn out, but when I found I could fry eggs on them, I did not want to take chances.
33
according to the plans given in chapter 2. Therefore, as might be expected, the current code is far from general. It is entirely driven by the Sand G commands described
earlier. Nothing happens asynchronously.
The entire unused portion of the MCP's memory board is used as a buffer for tactile data. Upon MCP initialization, the frame count is reset to zero. Then, each time a snapshot is requested, the data record is placed in the next position in the buffer, and .the frame count is incremented. When the readout is requested (via the G command), the program simply types it all out, one line per record, beginning with a line consisting solely of the frame count. The information is transmitted in ascii coded hexadecimal,
as an optimization of both transmission time and coding time.
34
Chapter 4: Experiments and Results
In this chapter w~ will discuss the experiments w~.ich were actually performed using the pad sensor. We will consider the methods, the goals, the problems, and the
results. When possible and appropriate, we will refer to figures which illustrate the results.
The pad sensor consists of an 8 x 8 array of sensitive sites whose analog output values are fed into an analog multiplexer, and finally into an analog to digital converter. All this circuitry is part of the sensing device. Unfortunat~ly, since the AID converter emits CMOS voltage levels, and our parallel ports use TTL inputs, we had to bypass the internal AID and use our own. This
resolved the incompatibility, but gave vent to another problem. The pressure signals coming out of the multiplexer ranged roughly from +2.0 to +2.5 volts, and our AID converter expected a range of -2.5 to +2.5. As a result, the digital pressure readings never went below about 235, out of a maximum 255.
In other words, the fact that we can exhibit only a 35
little over four bits of precision is not a reflection on the device, but on the interface. With the right interface, we would estimate upwards of six bits of valid data.
Each of the 64 pressure sensitive sites puts out a slightly different range of voltage levels. They therefore required individual calibration. The most straightforward way of doing this is to press the sensor down hard on a flat surface, take a snapshot, release the sensor entirely, and take another snapshot. This yields a matrix of minimum and maximum pressure values, to which all subsequent data would be scaled in a linear transformation.
Of course, nothing is ever so simple. Each pressure sensitive site requires roughly 1.3 pouOds of pressure to completely depress it. Multiplying that by 64 sites, we find that we need over 80 pounds of pressure t.o acquire the maximum readings. Our Z-axis motor is not capable of this.
The solution was to depress each site individually, and then combine the data into a single matrix of maximum pressure values. 'Fortunately, the Motor Control Processor's command language was flexible and powerful enough to do this painlessly in one command sequence, with two loops for X and Y positioning.
Once the minima and maxima were obtained, it was a simple matter to map all input data into ~ uniform range of o - 255. It is worth mentioning here that throughout the
36
entire testing period, these ranges never changed more than one unit. In addition, we never had any problem with spurious data being generated where there was no contact. Those points always mapped to zero. We were quite impressed with the robustness of the pad sensor.
4.2 Static Tactile Image Analysis
4.2.1 Single Image
The obvious first step in analyzing tactile images is to lay the sensor down on a" known object, take a snapshot, and·:see whether it is re~o9·nizable. This we aia, and the results are depicted in fig. 1.
In fig. 1£ we uEed a one inch square, set off-center, but oriented orthogonally with the sensor's grid axes. There is no question as to the identity of that object. A simple threshold operation would clearly distinguish it from
the background.
Fig. Ie and fig. Id show the same square rotated counterclockwise 30 degrees and 45 degrees, respectively. Fig. ole shows an equi-lateraI triangle, point downward, and fig- Ib depicts the same triangle rotated clockwise about 75 degrees. Notice how some pixels are much lighter than others	in	the	images	with	non-orthogonal	edges.	This
phenomenon arises when the object covers less than half the area of a site. Since the site is conical in shape, the
37
edge must be pressing on the wall of the cone.	It cannot depress the cone as far as it could if it were pressing on
the apex.
In theory, it should be possible in some cases to determine exactly how much of the cone is actually covered by the object. However, we must assume the following:
1) that the object surface, particularly the edge in question, is smooth, 2) that the object surface is in a plane parallel to that of the pad sensor, 3) that the individual sites on the sensor are in fact conical, with
bases that meet the pases of-their n~ighbors, and 4) that we know how to calcul"at'e the actual"" depression as a f"unction of' output pressure value.
Unfortunately, neither of the last two assumptions are valid in our case. The cones are actually cut off before they reach the apex,* and we do not have the data to perform the" depression calculation.
Finally, fig. la shows a one inch diameter circle. Notice that it appears to be · identical to the square in fig. lc. This is a question of resolution. Clearly, if the spatial resolution were doubled or quadrupled, the distinction would be obvious.
----------------------------------------------------------------------- * My offi"ce-mate tells me that the technical term for this
shape is "frustum."
38
'.
4.2.2 Spatial Resolution
How do variations in sensor resolution effect the image? The simplest way to tackle this question is to vary the size of the features on the test objects. We used a set of disks with rai~ed concentric cir~les projec~ing from them in relief. The variations consisted of two amplitudes and three frequencies, totalling six disks.
Fig. 2 shows the images obtained. As might be expected, those disks in which the spacing between the circles approach the spacing between the sensitive sites
(figs.• 2a and 2d). a.~e c.lear.. . As th~e ...f~equency.·incr.eases, the shape becomes less obvious, until it is completely unintelligible at the highest frequency.
The effect of amplitude is also fairly pr~dictable.	At low amplitude, the circles are wider, and tl·erefore more sites are in contact with the surface. This can be seen most clearly (again) in figs. 2a and 2d. Also, the inner circle is more distinct in fig. 2e than in fig. 2b. This is because at the lower amplitude, the depth of a trough is considerably less than the height of a conical site, and
therefore some trough sites come in contact with the surface.
Theoretically, it should be possible to compare pressure values and determine where the troughs and crests occur. However, here we run into the limitation in our 3-D
39
positioning device which we alluded to in the Calibration section. The Z-axis motor, which supplies the normal force, is a bit too weak for this pad sensor. Each sensitive site requires a certain amount of force to depress it, and the motor must be able to exert the sum of these forces in order to obtain a reliable reading. Therefore, as more sites contact the surface, each one receives less pressure. Furthermore, if the surface is not uniform, neither are the reductions in pressure.
4.2.3 Multiple Images
... -:-' How can we improve the spatial resolution with the equipment available to us? One simple way to double the number of data points on each dimension is ~o take a reading at each of the four corners of a small s~lare, whose sides are half the length of the distance between sites. This we did, using the same six disks, and the results are visible
in fig. 3.
The images are slightly clearer, but not as much as we had hoped.	Again, the disappointment is indirectly caused b~ the defic_i~nt Z-axis motor. When taking a snapshot, we try to depress the sensitive cones as much as possible, since we are not capable of depressing any of them completely. To do this, we simply instruct the Motor Control Processor to lower the Z-axis motor until it won't go any further.
40
This works quite well in general. However, consider the following hypothetical cas·e. Suppose the test object is a single sine wave and the sensor is a single cone. First, we lower the cone onto the crest of the wave as far as it will go, and take a snapshot. Then we move the cone to the trough and repeat ·;the ·ope"ration. The two images look identical! In both cases, the cone was depressed as far as it would go, and it is in fact the cone depression which determines the image. This, we believe, is the root of the mUltiple image problem.*
The solution, of course, is to strengthen the Z-axis motor. Then, instead of simply lowering the sensor until it stops, we would lower it to a consistent Z-coordinate. The
resulting set of images would be much clearer.
4.2.4 Large Objects
Can we examine· objects which are much larger than the sensor? For this experiment we used a flat surface about 12 inches long an~ three inches wide -- slightly wider than the sensor pad itself. A set of eight grooves were cut into this surface in order to form a pattern of diverging lines
By taking a s~ri~s of snapshots at successive lengthwise positions, we should be able to reconstruct the entire image, in spite of the fact that it
* Or, "Aye, there's the rub!" 41
-is much longer than the sensor.
The Motor Control Processor's command language again made this a simple task. We took fifty images, stepping about five millimeters between each. The reconstruction, shown in fig. 4b, was accomplished by superimposing the
images in the appropriate positions relative to each other. As before, when the distance between features approaches the distance between sensitive sites, the pattern becomes clearer.
Can we use our multiple image trick to improve the resolution? We repeated the same procedure, except that this time we took three snapshots, four millimeters apart widthwise, for each of the fifty steps lengthwise. The reconstruction, fig. 4c, shows the angled edges much more clearly at lower frequencies than does fig. 4b. At higher frequencies,	however,	both	reconstructions	are	equally unintelligible. Once again, we blame the failure on the Z-axis motor, and our method of maximizing pressure.
4.2.5 Small Angle Measurement
When a robot hand grasps an object, does it have a good grip? Very often, a "good grip" i~ one in which the flat surfaces of the object are wholly in contact with the flat faces of the fingers. The question can then be answered very simply by measuring, for each finger, the angle between
these two planes.
42
This experiment proved'to be extremely successful. Using the one inch square as our test object, we took four snapshots. In the first image we layed the pad sensor flat on the square, as usual, giving us a zero degree standard. For the three subsequent images, we lowered the left end of
the table by 1.0, 1.25, and 1.5 inches respectively, producing angles of 3.3, 4.1, and 4.9 degrees.
The results are shown in table 1. For each image we arrived at a single number describing the slant. The number was calculated simply by averaging all the pressure differences between horizontally adjacent sites. In theory the ratio of the third slant value to the second should be 1.25,* and the fourth to the the second should be 1.5. This was not the case.
However, the first image, whose slant should have been zero, did exhibit a small slant value. If we take this as an error, we can produce a correction factor by dividing it by the slant value for the second frame. When that percentage is subtracted from each of the two ratios arrived at earlier, we get remarkable results. The corrected ratios differ from the expected values by less than two percent!
* Proof is obvious from the geometry, as long as we assume a linear relationship between depression distance and output value.
43
4.3 Dynamic Texture Analysis
We believe that until tactile sensors can be fabricated with extremely fine resolution, information about' the texture of a surface would best be obtairted by moving the sensor along the surface, and examining the changes in pressure readings, as opposed to the pressure readings
themselves.
Toward this end, we tried several times to make the positioning device -drag the pad sensor along different surfaces, but failed each time. The sensitive cones, because they ·were designed to .grasp an object without allowing it to slip, were made out of-high friction rubber. This, of course, directly hindered the experiment. The stepper motors werF not powerful enough to pull the sensor and still maintain enough contact pressure to yield a significant reading.
In the end we performed a singularly unscientific experiment. We dismounted the pad sensor from the positioning device and dragged it by hand along a flat wooden surface, taking 100 snapshots over a period of about five seconds. This may not have been so bad, except that we neglected to measure the exact distance traversed, or anything that could directly or indirectly give us the velocity.
The analysis is interesting, though quite inconclusive. 44
The sensor is made up of an 8 by 8 grid of sensitive cones. Let us define a column as the series of cones lined up in
the X-dir,ection, and a row as the cones lined up in the Y-direction. Given that the sensor was dragged in the positive X~direction, we contend that there should be some aspect of the data which is consistent down a column, but" different across a row. Furthermore, there should be a
small but constant time delay between the features exhibited by one site and those exhibited by the next site down the column.
The motivation for this hypothesis is as follows. Picture a textured surface as a terrain of bumps and ridges. As the sensor grid passes over this terrain, the cones across a row will collect entIrely unrelated data. However, those down a columr will encounter the exact same bumps and ridges that were ~ncountered by their predecessors, but a little bit later. Thus we have eight instances of
eight-fold redundant data. We should be able to find some consistency somewhere.
Initially, we plotted the raw pressure data from each of the 64 cones as a function of time. Fig. 5 is a reproduction of this, with each plot placed in the same grid position as the corresponding cone. We expect to be able to look down a column and see some consistency that does not occur across a row. Unfortunately, no such consistencies were immediately obvious.
45
The next step was to try to home in on the changes in pressure, as opposed to the pressures themselves. However, a simple pairwise difference derivative (see fig. 6) was no more enlightening than the raw data.
Well, what about the Fourier transform? Surely the frequency domain is closer to our goal than the time domain. Unfortunately, applying this transform meant giving up our time delay information, which we needed for comparing curves.
What we really needed was some smooth measure of frequency as a function of time. A colleague* suggested the following procedure. First, tak~ the pairwise difference derivative. Then, pass a window along the time axis. For each point in timf, count the number of zero crossings in the window, and di~ide by the width of the window. A window n units wide would have a maximum of n zero crossings, and thus the ratio would be unity. No crossings would produce a ratio of zero. Note that the operator is valid, and produces the same range of values, independent of the window size. The only difference is in the precision.
We ~sed a win~ow wi~h. a~ ~dd number of points, so it
could be symmetric about the point under consideration. If
the distance to one margin or the other was smaller than half the window size, the window was shrunk accordingly, so
* Thank you, Gerry Radack. 46
that symmetry was maintained. We tried various window sizes in order to obtain the smoothest curve possible without ~osing too many features. The optimal size was about 25 units (out of 100), shown in fig. 7a. A 15 unit window is
shown in fig. 7b for comparison.
There are (finally) some definitely visible similarities among the resultant curves of fig. 7a. Examine, for example, the troughs in rows 6, 7 and 8 of column 1. Notice how similar they are, and how a small, constant time delay occurs between each curve and its successor. The same phenomenon is visible in rows 1, 3, 5
and 6 of the third column, and in rows 1 and 3 of column 7.
As one looks up and down a column, there seems to be some kind of topological similarity. This is exactly what -we want to find. However, identifying it mathematically is no simple task. The obvious operator to apply would be the cross correlation. ·This compares two graphs and produces a -number describing the closeness of the match, then shifts one graph relative to the other and repeats the calculation.
One correlation value is generated for each possible shift. The resulting curve shows not only how well the two graphs
match, but at what time delay value the match is optimal.
Unfortunately, the results were very disappointing. No matter which pair of graphs we compared, the cross correlation never went substantially higher than zero, and
the best match always occured at zero shift. Needless to 47
say, at least one more level of processing is called for. 4.4 Conclusions
First, it is clear that an 8 by 8 grid of pressure sensitive sites i-s generally not enough for pattern recognition of single static images. In most real applications, either the objects will be larger than the pad, or the features will be below the pad's resolution.
With reasonably good positioning equipment, the resolution can be significantly improved, and the size of the area under consideration considerably increased, by taking multiple images. However, this is often too time consuming, and therefore infeasible.
The straightforward solution 4is ·to inc~ease the spatial resolution, the number of sites, or both. We have shown that when feature dimensions are comparable to resolution, shape recognition can be quite simple. This has also been demonstrated by Hillis (HILLIS-al], using a sensor recently developed at the MIT A.I. Laboratory, and of course by Briot [BRIOT-79], who used an array of binary sensors. One typical application for this might be the table sensor which was described in the introduction.
A more novel approach might be to build multijointed fingers for the robot gripper, such as the three fingered hand developed by Ken Salisbury (SALISBURY-81] at the
48
Stanford A.I. Laboratory. This would enable the robot to manipulate the object while transporting it, in such a way -that it becomes riot only feasible, but a matter of course to
take multiple tactile images.
In the experiment concerning measurement of small -angles, we obtained impressive results. The computed values were even more accurate than we had hoped. From this we conclude that a tactile sensor with properties similar to those of the pad sensor is eminently suited to applications involving small angle measurement, such as grip improvement.
As far as texture analysis is concerned, we believe our approach is a good one. Visually, it is apparent that we are on the right track. However, the experiment must be repeated in a mucl more controlled fashion, and different surfaces must be examined and compared. Then, we hope we will eventually be able to manipulate the data in such a way
that we can use it to iden~~fy the surface.
49
Chapter 5: Further Work
As was mentioned earlier, the pad sensor was in our possession for only a short time, by no means long enough for exhaustive experimentation. In fact, many of the more
interesting ideas occured to us after the sensor was returned, when we began to analyze the data.
It should be possible to calculate the coefficient of friction between various surfaces and the rubber face of the sensor. First, one must know the force as a function of digital output for each sensitive site, as well as for the strain gauges on tl.e metal posts.	Then, one would drag the sensor along the surface in question, and take force measurements. The normal force N is simply the sum of the forces on all the sites, and the frictional force F is derived from the horizontal forces given by the strain gauges.	By	plugging	these	numbers	into	the	equation
F =uN one can calculate u, the coefficient of friction. This might be usable as a distinguishing characteristic between surfaces.
It might also be useful to measure granularity. This could be done simply by placing the sensor onto the surface
50
and counting the number of sensitive sites which exhibit
significant pressure. Of course, the grains in the test surfaces must be comparable in size to the resolution of the sensor.
Certainly the dynamic texture analysis tests should be repeated and extended. Once that data has been hashed out, it should be possible to identify surfaces based on pressure response to friction.
Finally, there are two aspects of tactile sensing which we have not experimented with because they are better suited to the -finger than the pad sensor. First, the finger should be capable of poking a surface and comparing predicted pressure with actual pressure in order to measure of surface
resilience. Second, there is the whole qu~stion of tracing cross sections and producing, essentially, ~l	3-D description of the contour of an object.
Thus we have shape based on both static images and contour descriptions, granularity, coefficient of friction, and surface resilience and texture. These features, when they are better understood, should be incorporated as distinguishing characteristics into the Experimental Sensory Processor.
51
a) 1- diameter circle
Fig- 1. Single Image Shape Recognition c)	1.5"	triangle	rotated	75 0
r
-• . _ - -
e)	1" square rotated 300
52
.
_ Ll~[_lD_ tt=8::--..I~
·
~3 f i i 1 ~~ DDL1DODDD
DDD~~DDD'
DDCJDDDDCl ~.-	_.. ~--i•-•'.01
. DDtJ
j·
·. i:~~
•
OO~.----- - ~
LD ~
t":""..=.:.J r::--....: L J	.
D
_
~ ~
E::-:d:.-
:- .
~-:.-=	~:
.:
_
t._
_
=.=-~
-
.
.t
.
.
:.-; [1-;! •	• -	::'::!	__1
._
~ "'n"'--' ~...
·.
O
~.:.
.
:-
~::;::It:l 'O ~.	f:~:
rl ~
[.·,. ~w.·.-_'
=:
. ._. - -
.. _...
-.-tL.- • t
... J l~::d
0 11 .-lLJ~ L.	l.~
~:: ::'~':"';:-': O lliTI ...-- ~- O·~· D~'
O
..-	L..:.-	• •.,
~~-~-j
fB] c=:-:-.d	I ;~;jt:=.:~
~.@l*-3 · ~~DD·~~D
EE~ ,r-l0
gf=l ~ O·
DC I8:3 --I bL___ •--l	~-=..
~f
. t . :-'::
:.
...
[l
a
.:{
:.:..
~	:~	: r;'::J8==.. :==t. .-E
r-- c=--t
::
._. --	....
.t. f:.:~.~1
.
.
~ ~1 ~~DDD
.. a......_
:;.:
.,;
~
~ ~ ; ; 2 :  J' . ' ~	. _ ~ t : •.;.:,-
~
~~nD~~O D ~~:;:::Jl	~ ~ ~ 3	•
~-::-
::~-~ .	~=.:.== .
•	__t:::=::	-----' __
.l t
. .. . nr~-=-tr-~=- nDD.
:=
.--
:
-
--
~..3l~
'
D
D
Dp:~.ia==D_~
~D~[lDDD~ L...J.~
DD.DLDDL'I~D ~DO~~DD
DnU~ .JL L@Jl DDD~DODD
b) Two circles, low amplitude
~Five CirCl[~ low amolitude LJD[]D ODD
e) Two circles, high amplitude
·f) Five circles, high amplitude
DOITJ[JOnDD
- OD~LJD5:~D~ Dg§fJ=~~§8DD~
rl~~D'ln~n~
.•. ~ . L LJP-_=3L..~~l.
~_-:-; t":- ~ ~ ~~
I r:-....:::...:	t-:=..::...: 0 ~ lr-llr--J[~l
t;t3 P.::;;; ~3 t:;=~ DD~r-1EPODO
~~10~~...:..J
l-:.':_.~.~_
~DDDD~DD DD rlDr,~..1t~~
l--1 _ L_
DDO-,r--:lO·~ D D ~:a	t~:-3
S3
I.
0
O
t
]
[
DO [~~j'l ..	i _ _ .	_ _ ..
D~ODD~~f@ . D ~=;:qDDDDD	~DnODn[l
L..D~
t:±.~.....:-: ~l.. O
DDD~~OOO
DOOOOODO a) One circle, low amplitude
Fig. 2.	Single Image Recognition Varying Frequency and Amplitude
I -'D
[J~Ir-JD~9n
L __
=. __
~
'
~~
•
--
J
'
D
~.=t
t1-~L.....j -	~---J.	L.	L-....~
L.-.L ._l.
t-=. -=:,j --~ ....=.:.J ~.---'..... ___._ L .	P _ t : -_	. _ '
DD F:'.•:'_:'"-:-::1:J ~
'·-lOC-.r-lO DD- D~UL_ -lL-.. I
=:r ~~LJDLJDDD
·DC1DDOfALJM Db8l[JDDLJDLJ DC-1Df.@D[JL]D
~
~ ~..:J~.3god b"3.~	I
d) One circle,
-t.
..
::
high amplitude
l~:.
~
..
l
~
I2Sl t8..I8l
·m
-mm
a) One circle, low amplitude
Fig. 3. Multiple Image Recognition Varying Frequency and Amplitude
b) Two circles, low amplitude
e) Two circles, high amplitude
Fig. 4a. Drawing of the Large Test Object
0.
55
n Lmrrrr
d
til
~.
:3 \Q
U1 o
••0-:. """J I t !Lj ! i 1 T j . l ! J 1 1	' " '	u . "
i 111I11I11I1111I111111I111I11I t1l1l11ll11rTlfllf °1
I '
n	"	"	"
"	"	"
' I
I '
- •
nnrrn '.	'	lliUl· [.. tLll.!.!UI~IIPI'P	t'i.·:r ·1
tnl's3 c: .....
.	~w..uu.:. [CiillETIBntIIlllIBIIJlIllIIIIIIIIil
[-J .[_[_]
n •
-iT-ir-l, ._u. tt, -'
....
1..~lJLjJ r"1ml . 'i~-",I IilJ.IWJLI
l. UlUl~;
r Ill l1llllllll'llll
'
41·
UI
0\
[
&lt; (1)
H\
t'1
m(1) In
den ....
r-1'1 1.. L.. J
Il11lTIIfOOillillllilllilOtJ!	.
'-L..u. ......
•
~~-I
• , ..,
m -'	,..-
°1
--.11 l' " II " II l' " l' " " " "
~o
LJ	l._~Llllaa.illJ. '	l r;rm
I I
"	"
. .
[]
•• ••
c . ['[-1	I' ' [ -rr'[~II!! 1111111!1II111"III jUllJLllJW
='
(' !.r: fi C n::,c; =~.= t! ~='.:-.~Sl-:n 0 II n ,[.ll:'If.r ..ll-tCtI:n'":"a,tI:r~tI~;:::::L;:::;£~!t:rcti
=
C :: !; !: :- ,.
=:.!, :~i~'
:4O f1" fI !;:.,.I,f':.!!!,IJtl::[1
f• C I i I I t J ' : ' : t l : : : : 0 : J : ; ! j f i t i ! ; : ; : 4 t ; ! i :~ : : : ; ( j I~
It:. ~ t" =:.:1'=...
:::~:;tJ00tJDDIJ0a.nDa:aaDODD11
=
~
I.'
O...a
It.~JI1IIlllnUHnHfi.lm[nrIfib¥[[[[[[IIf[i.iiilllllnIUIIIJIIIIIWI(llIIWllllllllllllilll1111 lUI . J
""I'""'" II'"'I,..
mtJ)~ mer
n
:r;:t. :: :: ==.tt::. :.
~.
: ' ! ' : t i	r J · , : i ( 1 I I	f J
Ii .; fi !,~;
~ ri fi !~ " ~; !. tJ C: :: "
w
)(
:;:~ :, !. " t1 f,.tt"~."r:.:nitl~...tt.t::ttl!:4~tlnl!t;ltJl"rf;:1,~J,:, f. I. ::
•I:;~ ~,'ItfI'
or, " Itl,l , J:? tt ·f, 10 ':.tL. ',.'i·. :'.!' '., '.
I ,f til ,.: :';' tit,
,
~ • t ~.. :- ~:·!.~t::'I,:·:.'::r,:·.:·.'''·:·
••
•
.~
: •.	=••	"	t
I~
,.
'I II
'. :"
;:	t.:.
..... EJ\Q
•.
I.
!. fl
f, ~
f : t : . r t " C · t t : t 1 1 t : = l ~ t O : : % . 1 a . 1 ~ : t . : ~ r 1 : : J1 1 1 : ( r , C l l l j r , , t t l f J n , a ~ t l : : : J J t t . J t I : D J t ] : : C 1 1 : r . t r . J n : : t : r t t : t c t : r . : l t t t : t . t : t t : t I : ~ t e t ! J t : n l t t J c : : l l : t t J : : : t n : t J !t J : 1 l ! t . . t , ,. _ ~ !\-~.;. ~.!:: ~~':''''' :. " !" II I.n II II:J III' n 'III n tJ 0 n [1 U COO D C [J D U U CIiU II C LCrr,c rClI'tl llj~lJ: I,
aJ • rt t 1 ~ ....· 0
..• ~:t-.:.~r:I;
I,
r; := t: 0 cell C 0 [J C 11 C
I I ~ ~.. ~~
:: t~ t1 : l~·t~ te::lto,r·~....:::-.··::t~:~tllt ~t:tIIIttl'tnt01f1:1)11"111Jrli:utc:CJ:11tC'l;:.~::t]::-~ ~
=
~ :. ! e :'7..-': :-: ....-= ~ :~,:=!~~,·1Cn;::1;1r:1l"J::t.1r::=~nr::!I::::1)C!JIIJXCI:rnC:1I:0CDtDCCtDtDCIJCC:DC:CDroa:a:U.DtYl':O::rJ=:J (1 0 11 !i t:	o
~. t1~n,~:.:·rf'~ ~,~:::.'=~t"~~~:-t.fI.1f1~114!';~:
:~
:-
•.
~: r:
~ :- :-:: ~':"" ! . =- ~o : . :
,.. nn
f; ~:: :: ::. :. :::::;:C:..C"'~c::..c:=c=..:~c::c:::t:C=:;t::::tlJwtDt1J::JJrt.ilJJcraX]";Lrc.;	"
•.:~:"!;!l~
~ ::.:!~. tJ fJ:~
= :: ti:1." 'i :. ~.fIJl:i~· "'111:-11 ~J It
'i.:'=- tit "::.r C	, j	.,-.
=
:: :- =
=
=
: .	.."	: - ~.. ft~l·.::"•.'c:..lr.:::'f.jlllllll~llJll~II.Jl'lJn)rrJII.laUCa.jt'Cn::aj=~~:::J~1J:..""'Ct
t1 t1
~..
~
X ;: ri.~:l~=~~t!~C:=:~C":c:'c::c::ccc::tt:rn=a::m[!:ja:lI::r.JlI:CnXtO:O=C::~=::i	I:	H\
I... t111n~ 11 t. I, "	'. r, ••	f'
U1 otIJ
~...	~: : ...~::r-= ::'" =:"=:~·:11:1:1t. :.. ;~:.
r't"-:~::L':-·~~i :.=::r;:--:..:"~"::
t-h
t1
nl
EJ
(1)
UI
~
-----------...- - ~ . _ . _ - - - - . . . . . -	..~ t
TABLE 1 -- Measurement of Small Angles ==~=======~~=======~=~=~~=====~=~=====
Table	Horiz. Avg. Slant Data Difference Diff. Ratio*
=====
on
- - - -
45 64	80 42 48 64 48 60 75 34 56 51
====::::::=::=	=====	=====
19	16	12.625 6 16
12 15 22 -5
49 96	78	1.00 52 112 59 120 68 68
112	114	1.23 144 120
80
11·2	142	1.53 192 165
99
1" 15 28 16 17
1.25"
1.5"
64 160 80 192 7S 195 85 153
48 160 48 192 6.0 180 56 136
48 160 48 240 60 225 71 170
* Ratio is calculated as the vertical averag~ divided by the vertical average at 1" slant, multiplied by one minus the
ratio of the 1" s~·ant value is to the table slant, the better the results. As the reader can see, the results are exceedingly good.
to the 0" slant. The closer this
57
•
L+~
Fig. 5. Raw pressure data as a function of time. Each plot is in the same grid position as the corresponding cone. Sensor was dragged toward +X, with Row 1 in the lead•.
58
~h	_L':
-,
~'f:~ ~-.:..-.-
4
..~~~ . --==-... - .
:E -t L
- t . - -
~.-
=- .....~ -=---=
~_.--
==-f~ ~
~-
'=
::=:-~~ ~- ==- ;
,
UU..__
:;::=.=
=:~
~
~:-:..__-.i ~:"!-I---- =.,~~
-.-.:- === .J:::::iI=
•L
..1.
I
_~--
~;r;::r'l. --;--.~:~;~ ~-«J-i ~-- -oa.;;
---1:
. __
--.-.i.·f. ~
F
.-
~
=c
_L :t.-.--
:±:
... ,.
Ii
~~t= =~_-:4.~
I~
---:::~
-----=.:.L- ::-:...::{==--.
--===~:~
=J.=.
f
~
-----...-- - - - :.-.....
-~ ~a": -.- a-..,
Fig. 6. Pairwise Difference Derivative
~ -~
.J~ _.~~-~--
--r.-:.:
~
' - _ . . ~. . . ­
~L~
~.
-==2!
--
_-
....
59
.~	. ~. . ~ . / .	. .
•
Fig- 7a_ Frequency as a Function of Time Window = 25 Units
60
[
•
..
Fig. 7b. Frequency as a Function of Time Window = 15 Units
60a
References
[BOYKIN-SO] Boykin, W. H., and Diaz, Gary., "The
Application of Robotic Sensors -- a Survey and
Assessment," ASME Century 2 Conference, August 12-15, 1980.	-
[BRIOT-79] Briot, Maurice, "Utilization of an 'Artificial Skin' Sensor for the Identification of Solid Objects," Proc. of 9th International Symposium on Industrial Rohots,1WaSfiIngton, D.C., March 12-1s,-r979.
[BROWN-SO] Brown, David J., "Computer Architecture for Object Recognition and Sensing," Master's Thesis, Department of Computer and Information Science, University of Pennsylvania, December, 1980.
[DANE-al] Dane, Clayton, Forthcoming PhD. Dissertation, Department of Computer and Information Science, University of Pennsylvania, 1981.
[HILL-73] Hill, John W., and Sword, Antony J., "Touch Sensors and Control," in Remotely Manned Systems -- Exploration and Operation in Space, ed. by Ewald Heer, California Institute of Technology Press, Pasadena, California, 1973.
(HILLIs-a1) Hillis, William Daniel, "Active Touch Sensing," Massachusetts	Institute	of	Technology,	A rtificial
Intelligence	Laboratory,	Memo	629,	A pril,	1981.
[IVANCEVIC-74] Ivancevic, Nebojsa S., "Stereometric Pattern Recognition	by	A rtificial	Touch,"	Pattern	Recognition,
V ol.	6	pp.	77-83,	1974.
[KINOSHITA-7S] Kinoshita, Gen-ichiro, "A Pattern Classification by Dynamic Tactiie Sense Info. Processing," Pattern Recognition, Vol. 7 pp. 243-251,
1975.
(NITZAN-80']	Nitzan,	David ,	"Assessment	of	Robotic	Sensors, I' Workshop on the Research Needed to Advance the State of
Knowledge-rn~otics, April	lS-I7	1980. 61
(OKADA-77] Okada, T., and Tsuchiya, S., "Object Recognition by Grasping," Pattern Recognition, Vol. 9 pp. 111-119, 1977.
(PURBRICK-8l] Purbrick, John A., "A Force Transducer Employing Conductive Silicone Rubber," Prec. 1st
International Conference on Robot Vision and Sensory Controls,	Stratford-upon-A von,	UK.,	IFS	(Publications> Ltd., April 1-3, 1981.
(SALISBURy-a1]	Salisbury,	Ken,	Stanford	A rtificial Intelligence Laboratory, Personal Communication, May, 1981, and, Proc. of 1981 Joint Automatic Control Conference, Charlotsville, Virginia, June, 1981.
62</Text>
        </Document>
        <Document ID="180">
            <Title>Introduction to the Annotated Bibliography</Title>
            <Text>To scope the research and map the many domains that can possibly relate to the current study , I intend to produce a review of literature. For practical purposes (in order to produce a bibliography, I include all the research literature currently under consideration as references. They will not all be considered in review.
 
</Text>
        </Document>
        <Document ID="214">
            <Title>Acknowledgements</Title>
            <Text>A big thank you to Sher Doruff, Leslie Hill, Esther MacCallum-Stewart and all my fellow students of the SmartLab, UEL, for their close reading of, feedback and comments on this work. Without their continued help, this thesis would not take shape.

I would like to thank the participants of the symposium "Raining Catz and Dogz: Virtual Pets" at the Articial Intelligence and Simulation of Behaviour (AISB) Conference at the University of Newcastle and the organisers of Digital Arts Week and ETH, Zurich for their responses to an early, early versions of this work.
</Text>
        </Document>
        <Document ID="103">
            <Title>6.2.3 Haptic Capture</Title>
        </Document>
        <Document ID="436">
            <Title>Torque</Title>
            <Text>Notes on Incorporations ed. Crary and Kwinter

Georges Canguilhem Machine and Organism pg44


Hillel Schwartz  Torque: The New Kinaesthetic of the Twentieth Century pg70

Manuel DeLanda Nonorganic Life pg128

foreword 
Embodiment "those strategies through which human life combines with, and assimilates, the minute, shifting, often invisible patterns and rhythms of the concrete historical milieus within which it unfolds." pp12 Crary and Kwinter

"The flux and energy of human life became  increasingly reduced to finite quantities of forces and sensation. ...

Guattari - The Age of Planetary Computerization

the concept of DOUBLE

puppetry finds a double in computational media.

What is this DOUBLE?

Artaudian? Guattarian



Georges Canguilhem Machine and Organism pg44

mechanistic 

the machine is data

Julien Pacotte notes that the movements of joints and the eyeball are 'mechanisms' 46

Definitions: "A machine can be defined as a man-made artificial construction, which essentially functions by virtue of mechanical operations. A mechanism is made of a group of mobile solid parts that work together in such a way that their movement does not threaten the integrity of the unit as a whole. A mechanism therefore consists of movable parts that work together and periodically return to a set relation with respect to each other." 46 


varying degrees of freedom of movement - quantifiable

setting limits on the amount of movement

"In every machine, then, movement is a function, first, of the way the parts interact and, second, of the mechanical operations of the overall unit" 46

"every movement is geometric and measurable" 46

"In mechanics, movements are simple propagated, not created." 46


elementary principles of kinematics

receive and transform energy -- 47

energy sources

body as machine 48
aristotle
descartes

Espinas  Quaestiones mechanicae

Plato in Timaeus - compared movement of vertebrae to hinges, pivots 
Descartes - spring operated


Interesting discussion of techne, skill and art - and

Marx and the 'tool' - human powered, the machine, powered by a natural force

Hillel Schwartz  Torque: The New Kinaesthetic of the Twentieth Century pg70

François Delsarte 1840 Cours d'esthétique appliqué -- he taught a system relating gesture to expression, expression to the soul. Orders and Laws of Movement

Steele MacKaye 'harmonic gymnastics' a series of Delsartean exercises "to so train and discipline the body that it would become a responsible and expressive instrument through which fluid movement could pass without the obstacles of stiff and unyielding joints and muscles." Ted Shawn cited in Schwartz 72
Isadora Duncan Ruth St Denis -- mothers connect them to Delsarte's disciples / son
absolute integrity of gesture
his attention to the expressive power of the torso and his desire for movements liberated from highly mannered codes of motion.


isodora's centre

what is the 'centre of a rigid body'?

1918 Helen Moller 
"All true physical expression has its generative centre in the region of the heart. ... Movements flowing from any other source are aesthetically futile." 73 cited in Schwartz
a new kinaesthetic  sincerity, a loving accommodation of the forces of gravity, fluid movement flowing out of the body centre, freedom of invention, natural transitions through many fully expressive positions.

Jaques Dalcroze - rhythm spiritual or corporeal? Assuredly, both.


images of motion

gesture in writing
graphology  pg78
expressive of

expressive systems
codification

rhythm direction density scale speed transitions


rhythm = vitality
fluidity = sincerity
freedom of expression = spontaneity and honesty
naturalness of transitions ... etc

OPERATIVE rather than EXPRESSIVE movement

repetitive ...
the savage
the rhythmic 

learning child development and movement

the precision of gestures

finger painting...ideal form of recorded expression for the child 87


EXPERIMENT 
Make the figure only controllable from a central 'body'
the centroid

streamlined gestures 91
clean fluid curvilinear gestures


new naturalistic acting manuals 


true expression to a private impulse 91

Dale Carnegie - gestures 'that are born on the spur of an instant."


"Gestures, then, were expressive *releases* rather than practiced achievements --or so they must always seem to be, in casual conversation as in the most defiant of harangues or meditative of dances. If they were true releases, gestures were also true confessions and would, it was presumed, be socially intelligible. The difference between this theory of gesture and that on which, for example, much nineteenth-century melodrama operated, was subtle but remarkable: one read not merely the attitude but the entire stretch of the outlooming motion of which the last position struck was just the finishing touch." 92



Frued and gestures of hysteria: twitches, seizures, paralysis, tics choreas, convulsions, aphasias, "hysterical attacks were theatrically intelligible." 92 

emotional expression

freud on pantomime "Freud proposed in 1895 that his patients' "pantomimes" were giving true expression to private anguish otherwise and harmfully smothered." 92


frauds theory of the hysterical conversion of psychic energy to somatic innervation

autonomic nervous system, emotions and mental states - curvilinear motion, kinaesthetic sets, flowing movement - therapeutic



Writing and Paragraphs

Similarity, Contiguity and Compound Association

Unity, Coherence and Mass (Emphasis)


Attitudes page 97
Diderot: critique of self-concious gesturing -- led to well-composed tableaux vivants..
pastime- imitating compositions of well known paintings.


stock attitudes: whole body, half body gestures 98


an art of the extremities (hands face feet) and static postures

Charles Aubert in 1901 rails: "It is not enough to make gestures and grimaces,. For the registration of an emotion to be complete, the body and all its members must cooperate. The grace definiteness, and power of an actor depend on the harmonious participation of the whole organism." cited in Schwartz 98


"Acting should consist
	Always in attitutudes.	
		Often in facial expressions.
		Rarely in gesticulations. " cited in Schwartz 98

Copeau (1879) 

kinestructs and kinecepts 104

kinaesthetic ideals (kinestruct)
kinaesthetic experiences (kinecepts)

"Current wisdom maintains that modern life, with its essentially industrial momentum has processed our world and our bodies into dissociated, fetishized, ultimately empty and machinable elements; most perversely [Schwartz has] been positing the emergence of a new kinaesthetic that insists upon rhythm, wholeness, fullness, fluidity and a durable connection between the bodiless of the inner core and the outer expressions of the physical self." 104

"The West entered the twentieth century in the company of assembly lines, time clocks, scientific management, time-and-motion studies, flashbulb photography, silent films, ragtime music and Cubism. These conspired not only to make people intensely aware of isolated moments but also to isolate their own movements, to fractionate them into multiple perspectives, infinite exposures. Like snapshots and headline journalism, the rhythm of life in the new century has been as staccato, syncopated or jerky as most popular dances. At the factory, in the designer kitchen, in commercial elevators and business offices, on the escalators of department stores and the moving sidewalks of international airports, people have begun to carry themselves like "robots" (from the Czech word for drudgery and slavery, in Karel Capek's 1921 play RUR). No wonder that such theatre visionaries as Gordon Craig, the Futurists, the Dadaists and later performance artists would call for a marionetted or mechanised (nonhuman) stage. 105

Elizabeth Seldon (1930) on Elements of Free Dance (opposed to pre 1900 ballet):
the action for speed moved from Kick to Swing
Thrust to Winding / Unwinding
Beat to Phase (legato)
Disjointed highly articulated motions to integrated motions of pull and release
changes in direction to working along the path of motion  106

[Toc and Fondue]


we do not move like machines...

"If women, men and children these days experience themselves as off-balance, gawky, clumsy, stiff, they also share a vision and experience of flowing movement spiralling outward from a soulful centre. That vision and experience may be soon transformed by a powerful literary and cinematic mythology of androids, cyborgs and free-fall, or by an art of amazing puppets and marionettes. There may soon enough be a very different notion of what it is to move or move well. In the meantime, we may say grace." 108


Notes: Excellent reference list

The solar plexus, as "a kind of common centre of action and sympathy, to the whole system of organic nerves." (Sylvester Graham, A lecture on Epidemic Diseases [1833; new ed. Boston 1838] p10)


dancer biogs
duncan
martial arts 

anthropology and dance
Groos The Play of Man 1908 [1898]
Charles Bell Anatomy of Expression  (1806)
Darwin the expression of emotions in man and animals

"By the start of the twentieth century, a person's repertoire of gestures was generally considered by anthropologists, physiologists and psychologists to be a complex mix of autonomic reflexes, racial inheritance, cultural conditioning and idiosyncratic experience. Exactly where one lay the burden of the expressive or the operative aspects of the new kinaesthetic depended (I suspect) upon one's political position, an issue which begs for study." 114


Piano playing movement

Drawing By Oskar Schlemmer
</Text>
        </Document>
        <Document ID="325">
            <Title>Preservation and Heritage</Title>
            <Text>Digitisation is central to contemporary processes of archival storage and information retrieval. For books, images and video, digital optical recognition and scanning is commonplace, the act of curating geometry and surface qualities and textures of 3D objects through digitisation is emerging MEMO:EXAMPLES. The use of portable 3D scanners to digitise heritage artefacts offers interesting models for the archiving of fragile material. 

In PROJECT X, PROJECT Y and PROJECT Z, I explore the practicalities of digitising heritage shadow puppets. The projects go beyond simple interactive 3D models and present an approach where game technologies, physical simulations and tactile interactive strategies (multitouch surfaces) permit the user/performer to animate in real-time articulated screen based figures.

Puppets and performing objects 'live' in expressive contexts. The 'Shadow Engine' software and setup allows kinetic objects to be chosen,  operated, the interactive selection of scenographic elements 
Puppets should move and form the basis of play. The kinetic and experiential are a major component in the 'recontextualisation' of performing objects.

[MEMO] I wish to explore the Theatre and Performance Collections at the V&amp;A: digitising sets, model theatres, puppets.
utility of interaction in museums and heritage:

The heritage application of the techniques are secondary to an exploration of the 'expressivity' of virtual objects. This chapter: 

- discusses and differentiates the author's approach from similar work;
- documents the practical projects and creative process;
- deconstructs ideas of materiality and physical simulation (e.g. material properties transparency, translucency, gravity, spring, mass, collision)
- examines the act of 'configuration' and calibration as principles of puppet expressivity
- appraises glitch as evidence of digital spontaneity
- evaluates user-testing of multiple variations of the  system

REVIEW OF PREVIOUS WORK
on spectacle, the ensemble of production and production design and relating contemporary digital art works, in formal terms, with precedents.

: "... forms of visual digital culture ... share something of the same cultural space with an earlier tradition of popular entertainments. What they are doing in terms of aesthetic practice bears direct comparison with prior forms of popular entertainment that operated with the same overall principles." [p.56][#Darley:2000bh]

marvels
NOTES practically engaging with, enabling play with vulnerable artefacts.

</Text>
        </Document>
        <Document ID="292">
            <Title>Reading Note Template</Title>
            <Text>

Course/Topic/Date

Cues

main ideas
questions that connect points
diagrams
prompts to help studies
memos

When: after reading/session, during review
Notes

concise sentences
shorthand symbols
abbr.
lists
make space
do a mind-map

When: during reading / session


Summary

Top level main ideas for quick reference
When: after reading/session, during review

	
How to use

Copy the table above to the clipboard, 
Choose Command-N to save the clipboard contents as a new rich note in DEVONthink.

Note that this document is locked to prevent inadvertent changes.

Column widths can be adjusted to the user’s tastes.</Text>
        </Document>
        <Document ID="181">
            <Title>Summary of findings</Title>
            <Text>10.2 Summary of findings
</Text>
        </Document>
        <Document ID="215">
            <Title>Definition: the Evocative Object</Title>
            <Text>Evocative Objects</Text>
        </Document>
        <Document ID="104">
            <Title>6.2.4 Motion Capture</Title>
        </Document>
        <Document ID="437">
            <Title>Milestones</Title>
            <Synopsis>What do you want to do?
What do you have to do?

How will you do them?
Who will check up on you?
Who will give you feedback?</Synopsis>
        </Document>
        <Document ID="326">
            <Title>Quotations and Notes - Currell</Title>
            <Text>:"A shadow is an image cast by an object intercepting or impeding the light or the comparative darkness formed when such an object causes a difference in the intensity of light on any surface. A shadow, however, does not have a separate existence by depends  for its existence  but depends for its existence, its nature and its form upon the source of light that creates it and the surface upon which it is cast." [p.7][#currell07]

Do justice to the outline from David Currell</Text>
        </Document>
        <Document ID="293">
            <Title>main</Title>
            <Text>devrese Rsthgi Rll A
namretse Wenya W 􏰣􏰣􏰣􏰈 c􏰐
􏰣􏰣􏰣􏰈gnirpS
gnireenignElacirtcel E
ni yhposolihPforotcoDfoeergedehtrofstnemeriuqerehtfotnemll􏰏luflaitrap
nierawaleDfoytisrevinUehtfoytlucaFehtotdettimbusnoitatressid A
namretse Wenya W
yb
ECAFRUSHCUOTCARTDNAH
gninnalPdnasmargorPcimedacAroftsovorPeciV
􏰇D􏰇hP􏰅hguanavaC􏰇CnhoJ
gnireenignEfoegelloCehtfonaeDmiretnI
􏰇 D􏰇hP􏰅irezS􏰇ZsardnA
gnireenignElacirtcelEfotnemtrapeDehtforiahC
􏰇D􏰇hP􏰅rehgallaGlaeN
namretse Wenya W
yb
ECAFRUSHCUOT􏰆ITLUMANO
NOITALUPI NAMCIDROHCDNA
isrevinUehtybderiuqerdradnatslanoisseforpdnacimedacaeht
steemti noinipoymnitahtdnanoitatressidsihtdaerevahItahtyfitrecI
􏰤dengiS
eettimmocnoitatressidforebmeM
deriuqerdradnatslanoisseforpdnacimedacaeht
steemti noinipoymnitahtdnanoitatressidsihtdaerevahItahtyfitrecI
eettimmocnoitatressidforebmeM
asaytisrevinUehtybderiuqerdradnatslanoisseforpdnacimedacaeht
steemti noinipoymnitahtdnanoitatressidsihtdaerevahItahtyfitrecI
􏰤dengiS
eettimmocnoitatressidforebmeM
􏰇D􏰇hP􏰅telecnoBselrahC
􏰇yhposolihPforotcoDfoeergedehtrofnoitatressid
asaytisrevinUehtybderiuqerdradnatslanoisseforpdnacimedacaeht
steemti noinipoymnitahtdnanoitatressidsihtdaerevahItahtyfitrecI
noitatressidfoegrahcnirosseforP
􏰇D􏰇hP􏰅sailEnhoJ
􏰇yhposolihPforotcoDfoeergedehtrofnoitatressid
asaytisrevinUehtybderiuqerdradnatslanoisseforpdnacimedacaeht
􏰤dengiS
􏰤dengiS
steemti noinipoymnitahtdnanoitatressidsihtdaerevahItahtyfitrecI
vi
􏰇yrtneatadrofsdohtem
gniugitafsseldetcefrepIsasraeyruoftsapehtrofhsilbupdnamargorpoteunitnoc
otemgniplehrof􏰅eisseBrehtomymdna􏰅nellEretsis ym􏰅gotse WarabraB􏰅nomeL
esineD􏰅aisraPkraM􏰅niveLaraS􏰅dduBhtuRharaS􏰅stnatsissagnipyt yM
􏰇tnemurtsni evisnopserylreporpanoodnacsdnahehttahwhtiw
emgniripsniroftnusfoorplacitamehtamdnatsrednudnaetirwe mgnikamrofJdsiwsihyrracoteunitnocehyaM􏰇enodebtondluocdnadluoctahwfo
snoitamalcorphtiwemgnignellahcdnagnElacirtcelEeht
ni	hcraeser ymgnirusne􏰅erawaleDotemgnitivni rof􏰅rehgallaGlaeN􏰇rD
􏰇noitatressidaesopmocot hcihw
nopunoitadnuofeuqinuaemevagwohDEL WONKCA
v
􏰇emasehtodotemthguatdna
􏰅syawrevelcdnasuoremunniniapcinorhcthg􏰏otflesrehthguatohw
􏰅eisse B􏰅rehto my M
􏰤otdetacidedsitpircsunamsihT
􏰇namretseWenyaWrofpihswolleF
etaudarGnoitadnuoFecneicSlanoitaNaybdednufyllaitrapsawkrowsihT
􏰇gninnigebehtylnosawtaht􏰥noitacudeedarghtf􏰏aylnohtiwnwot
emohruootyticirtcelegnicudortniroftlaWrehtafdnargymknahtI􏰇ainrofilaC
otdetnalpsnarttogtsomlaylimafrehwohevoba morfrepsihwsyawlaehsyam
􏰥sremmusymotsseneltnegdna􏰅yrotsih􏰅yrteopgnignirbrofylimafrehdnaandE
rehtomdnargymknahtI􏰇gnimmiwsdnasdoof nworg􏰆emohemoselohwhtiwteid
noitacavymgnicipsrofrehtomymknahtI􏰇reitnorfehtfoscihtednamsitamgarp
ehtgnicrofnerofosladna􏰅snoitacavymnorehtonaretfatcejorpyzarcenopoleved
otemitemgnivigteyroballaunamgnihserferrofmrafehtotkcabemgniwardrof
rehtaf ymknahtI􏰇yawarafosdevomIhguohtneveraeyaeciwtemohememoclew
syawlaohw􏰅ylimafelbatsdna􏰅tneitap􏰅gnivolahcusevahotykculosmaI
􏰇eslegnihtynarofyraewootsawdnimym
nehwnoisiveletehtgnirahsrofnivas􏰆ekortsyekagniddarof􏰅􏰇cnI􏰅egdeorci MtallaHnairB
􏰇erawtfosgnillorcs
suounitnocylnos􏰛􏰉􏰟SO􏰅llorcStoHgnitirwylsuorenegrofsahcussdnammoCuneMitalupinaMcidrohCii v
􏰡􏰇􏰈
􏰡􏰉􏰤􏰤􏰤􏰤􏰤􏰤􏰧ecnotatemebsevitcejbocimonogreynamosnaCniMlaitraPnoigeRnoitatnemgeStcirtSemgeSaHfoseitreporPaleDretliFdnaecned􏰋􏰈􏰤􏰤􏰤􏰤􏰤noitidnoCgnippawSehtfosnoitaterpretnIcirtemoeG􏰤􏰤􏰤􏰤􏰤modeerFfoseergeDowTnahteromninoitalupinaMlacinahceMniseciveDgnitnioPgniddebmEytimixorPognitsersdnahhtobfoegamiytimixorPetta􏰐dnahthgirfoegamiytimixorpdetcerrocobdnapotnosnoitcesssorcdehctahfooitarecnis􏰝d􏰆􏰝ani
mrofinusiyarraedortcelemargolellarapehtnonoitalopretnilacitreValopretni
lacitreV􏰇yarraedortceleralugnatcerfouoiravrofseitiugibmarosnesnoitcejorPowtehTivx
es
yppolsgnisu􏰝brosleehmlapdnasregnaenotsniagadezeeuqssregn􏰏ehthtiw
dnahdenettahcae
tadeilppaerahcihwstsetegdetcatnocehtgnizirammustrahcwolFrper􏰅egde
tcatnocehtsdrawotsworevisseccusgnolagnideecorpdna􏰝elcric
dell􏰏􏰜mumixamlacols􏰛puorgehttagnitratssnrettaphcraeslacipyTesyppolsthgirdnatfelehtfosnoitisopehT􏰊􏰇􏰊
􏰌􏰍􏰤􏰤􏰇ssecorpnoitatnemgesegamiytimixorpehtfomargaidwochgirdnanoitisoptluafednidnahtfelfoegamiytimixorPgirdnanoitisoptluafednidnahtfelfoegamiytimixorPdnahtfelfoegamiytimixorPotdrawtuodetator
dnaecafrusgnisnesfothgirraftadnahthgirfoegamiytimixorPnesfotfelraftadnahthgirfoegamiytimixorPeporpdnaegamiytimixorpdehtoomsnUregn􏰏
xedni eht dnihebretemitnecatuobagnissapbmuhtafopam
noitatnemgesdengilaylreporpdnaegamiytimixorpdehtoomsnUixorplacolehtserucsbosedortcele
margolellarapdevaelretniyllacitrevybgniraemslacitrevehtesuaceb
spitregnpuorgpitregnmixorpgestcirtsdna
sleehmlapehtdnuoraxobehtniselurnoitatnemgesyppolsgniylppa
ybdeniatbodnahdenettaotcarttanaotsregn􏰏ev􏰏fo
tnemngissaninoitaivedriapregn
evreserptonseodcirtem􏰈Leht􏰅cirtem􏰉Lehtekilnutahtsetartsulli
raenillocerariaptcatnocdnariaprotcarttaehtnehwesaclaicepScirtemecnatsid
tnere􏰎idrofroivahebtnemngissanisecnere􏰎idsetartsullielgnairt
thgiramrofriaptcatnocdnariaprotcarttaehtnehwesaclaicepSttadna
riaptcatnocaivstnemngissaderauqs
elbissopowtehtfostsocehtgnirapmocrofnoitcurtsnoccirtemoeGta
owtnehwstnemngissaelbissopgniwohsnoitcurtsnoccirtemoeG
􏰇stniop
rotcarttatrapdnahfognirdnuoradetcurtsnocmargaidllecionoroVanoitarapesrennisusrevslgnarotcevehtsusrevsmertI􏰇devomersiregn􏰏yknipehtnehw
rotcarttaregnvomersitcatnocregn􏰏yknipehtretfasruccohcihwspawsytitnedIarapeslacitrevehtsusrevrotcafssendednaHfoegarevaehtsusrevrotcafnoitceridgnihctulcdnaHesuacnacsleehmlaphtobfognigremroecnesbAnoitaivedlaidar
fostimilehtotesiwkcolc􏰆retnuocyllufdetatordnahanisregniFesiwkcolcyllufdetatordnahanisregniFguoranidegnarraspitregnitneditcerrocylsuoenatnatsnI􏰌􏰉􏰇􏰋
ixx
enoititrap
eraecafrusehtfoflahtfelehtnoedisybedisdecalpsregn􏰏
elddimdna􏰅xedni􏰅bmuhtthgirehtsulpsregn􏰏dnahtfelevcerroc
denoititraptoneraecafrusehtfoflahtfelehtnoedisybedisdecalp
regnedis
decalpregn􏰏elddimdnabmuhtthgirehtsulpdnahtfeleritneehTcerrocdenoititrap
eraecafrusehtfoflahtfelehtnoedisybedisdecalpsdnahllufowTtdewollasiecafrusfoedistfelotnollewnwodgnihcuotdnahthgiRctulcfotceedisiecafrusehtfoelddimtfelehtnidecalpbmuhtthgirAtitnedidnahfoyromemmretiddetamitseehtsusrevrotcafnoitarapesdnah􏰆retnI􏰠􏰋􏰇􏰋
􏰉􏰈􏰉􏰤􏰤􏰤􏰤􏰤􏰤􏰤􏰤􏰇smlapsade􏰏itnedistcatnoctsomretuodnatsomrenni
ehtneewtebnoitarapeslatnozirohehtsusrevrotcafnoisehocmlaP􏰤 􏰤 􏰤 􏰤 􏰤􏰇ralucricyltcefrepton
erahcihwsgnirrotcarttarofseruliafecnegrevnocgniwohssmargaiD􏰈􏰇C
llarap
neewtebyawflaherastcatnocdezis􏰆muidem􏰆ot􏰆llamsnehw
esiranachcihwsesaibnoitalopretnilacitrevehtgnitartsullimargaiDwhtf􏰏
ehtnignitratsrohtuaehtybdetpodatuoyalyekkarovDdenartdnah􏰆elohwmorfdetcartxestnenopmocyticoleVuoenatlumismorfdetcartxestnenopmocyticoleVrutsegdnammocdnahtfelrofsgnippaMoitomlaretaldnasnoitalupinamelpmisehTvesehTregniF􏰈􏰇􏰋
􏰋􏰈􏰤􏰤􏰤􏰤􏰤􏰤􏰤􏰤􏰤􏰇slennahcerutsegdnammocdnahthgirrofsgnippaMofdnegeLdecudorpslangisrofmretlarenegA􏰝nuonocralucitrapagnihcuotdnagnisoohCnoitceleslennahc
􏰇ecafrus assorcasregntresuehtrofreisaeerasnoitanibmocemoS􏰇ylsuoenatlumis
ecafrusatcatnochcihwdnahenonosregnrtsevititeper
deraefylediwtsomehtfoenohguohT􏰇detaertnufilortnoc
rotomdnahfonoitadargedlautneve dna􏰅thgintasniaptsirw
prahs􏰅gnilgnit􏰅ssenbmunsesuaC􏰇tsirwehtfoedisrednueht
talennutlapracehthguorhtssaphcihwsnodnetehtfonoitam
􏰆ma􏰐niybdesuacevrennaidemehtfonoisserpmoCemordnyslennutlaprac

oitatoRnoitanorpmraerof
􏰇ecafrusehtgnihcuotyllautcatonsitubgnisuacsisdnahthgirrotfelehtfohcihwgninimreteDnoitac􏰏itnedidnah
􏰇etargninnacs
yarraehtsanwonkoslA􏰇yarragnisnes ytimixorpelohweht
snacserawdrahgninnacsedortceleehthcihwhtiwycneuqerfehTetaremarf
atnocecafrusralucitrapagnisuacsi dnahnevig
anobmuhtroleeh mlap􏰅pitregn􏰏hcihwgninimreteDnoitac􏰏itnediregn􏰏
􏰇ykniprognir􏰅elddim􏰅xedni􏰅bmuhtehtfoynAregn􏰏
􏰇stne m
􏰆erusaemedortcelegnirobhgiengnitalopretni dnagnipuorgyb
deniatboerasnoitacol tcatnocesicerP􏰇hse􏰐pitregn􏰏sa hcus
tcejboevitcudnocrehtonafohcaorppaybdesuacecnaticapac
edortceleniegnahcehtsiretemarapdesnesehT􏰇STMehtfoyar
􏰆rarosneseht mrof hcihwfosdnasuoht􏰅etalpevitcudnocniht A
vdayramirpstIyalyekkarovD
􏰇sllabkcartdetarepo􏰆bmuhtnosdrawkcabbmuhtgnillup
rognipyt nehwhgihootsbmuhtgnidlohybdesuacebnaC
􏰇tsirwehttashtaehsnodnethguorhtssapyehterehwbmuhteht
esiardnadnetxehcihwsnodnetehtfotnempartnEemordnySsinoitatorrof
eerht dnailosasnoitceridtnednepedniforebmunehTdnyslennutlapracsaemaSemordnyslennutlatibuc
i	vxx
􏰇dnahnevigehtfoyknipehtsdrawoTretuo
􏰇ecafrus
ehtssorcaedils dnahrepecnodeussi ylnoeroferehterasdnam
nachcihwecneuqesyekrodnammocAtohs􏰆eno
􏰇stcatnocregn􏰏elpitlum
fosnoitisopehtgnirusaemylsuougibmanufoelbapachtaenred
ahrosrucsiht
􏰅neercsehtssorcagnivomretniop worranaybdetonedyllausUrosrucesuom
􏰇laide m
sayllamroferomnwonk􏰅dnahnevigafobmuhtehtsdrawoTrenni
􏰇enalpaniyllanogaid
gnivom􏰇g􏰇e􏰅ylsuoenatlumismodeerffoseergedllanirosexalla
gnolaevomotelbissopsitihcihwnisksatrosecivedlortnoClargetni
􏰇secneuqes yek hcaer
􏰆ot􏰆drahrorailimafnurofyllacidaropsdohtemsihtesustsipyt
deniarT􏰇yekhcaerofdraobyekehtgnihcraesyllausiv􏰅ylnosreg
􏰆n􏰏xedniehthti wsyekekirtsyllacipytstsipytecivoNgnipyt	kcepdnatnuh
􏰇dnahehtfotrapelbahsiugnitsidemaseht ybdetcelretniopesuomehtnoylivaehylersIUG
􏰇draobyekaderiuqerylnohcihwsecafretnienildnammocredlo
otdesopposa􏰅esuomehtybyllapicnirpdetareposexobeugolaid
dna􏰅snottub􏰅swodni w􏰅sunem􏰅snocisahhcihwpotksedybdetacidniyllausuerasegnahcetatsesohw
sretemaraperawtfossuounitnocfolortnoctceriDnoitalupinamlacihparg
􏰇sregn􏰏ehtotnisehcnarbmlaplatsidehterehw
􏰅segnalahplamixorpdnaslapracatemehtneewtebtniojehtfo
edisrednuehtgnitcetorphseehtfonoitatoRnoitanipusmraerof
iivxx
stelbatgniwarddnailytS􏰇seirettabyvaehrosdrochtiwdereb
􏰆mucneerewsnoisrevredlohguohtna􏰅erusserp􏰅noitomesohwneplaicepsAsulyts
􏰇noito mreg
􏰆n􏰏laretaltsafgnidulcniecafrusehthtiwtcatnocdrohcfeirb A
􏰇ecaf
patgnidils
􏰆rusehtssorcadrohcanisregn􏰏llafonoitomlaretaldelpuoCedils
􏰇smargorp
DACniedomrosruclanogohtrognisuro􏰅yrtemoegnattahnaM
nignivird􏰇g􏰇e􏰅emitatamodeerffoeergedrosixaenognola
elbissopylnositnemevomhcihwnisksatrosecivedlortnoCelbarapes
nocfognipyt
esuacebdoogylriafsideepstubsahhcihwtuoyalyektebahplaehTtuoyalyekYTREWQ
mdesu
erastelbatgniwarddnaskcuPernackcupehttahteraesuom
lanoitnevnoceht morfsecnerenetfoecivedekil􏰆esuomA	kcup
􏰇nepagnidlohfisarehtegotdehcniperaregn􏰏xedni
dnabmuhteht dnasmlapehtrednudelrucerasregnenmlapehtfoesabehttasdnuomyhseedortcelenahtiwdetaicossaebtonnactifI
􏰇segamiytimixorpevisseccusssorcadekcartyltnetsisrepsihcihw
tcatnocecafrusafoyrotcejarteht􏰆tuostniopyknipeht hcihwnitsirwehtfoerutsopdetatoRnoitaivedranlu
􏰇stcatnocregn􏰏lla
revodegarevanoitisoplabolgatroperylnonactubsregn􏰏eerht
roowttcetedyamyeht􏰅syarraedortcelenahtrehtarsedortcele
nmulocdna worgnolsniatnocsdaphcuotesuaceBidercAdaphcuot
devomebnaCp nod
􏰆netehterehwnodnetasdnuorrus hcihwhtaehsehtfognillewSsitivinysonet
iwecafrusehtotregnrwdnahhtiwdesuebosla
nactubsrengisedscihparg dnastsitraybnetfotsomdesuera
xi x x
niegnahcelpmisahtiwslennahcnoitalupinamlacihpargFOD􏰆􏰋larevesdnagnipyt
neewtebylsuoenatnatsnihctiwsnehtnacrotarepoehTSTMehtnosregn􏰏elpitlumfosnoitom
dezinorhcnyssevreserhcihweuqinhcetnoitargetnitupnilevonasecudortnikrow
sihT􏰇secivedyrtnetxethtiwnoitargetnidrawkwaoteudmaertsniamehtretneot
deliafevahnoitalupinamreveotniesuomehtfonoisnecsaehtetipseD
􏰇erutcurtsgnirehtottcepserhtiwseititneditcatnocehtstros
ylevitce􏰎ecirtemtsocecnatsid􏰆derauqsagnisustnioprotcarttatrapdnahfognir
aotstcatnocfotnemngissAdnastniartsnoclacinahc
nahemasehtotdnopserrochcihwspuorgesoht
segami evisseccusssorcasknil gnikcart􏰆htaP􏰇tcatnocecafruselbahsiugnitsidhcae
otgnidnopserrocspuorglexipseziretemarapdnastcurtsnocegami ytimixorphcae
fonoitatnemgestnedneped􏰆txetnoC􏰇smlapotspitregn􏰏gniknilserutcurtsfoytilib
􏰆isivniehtemocrevootyrassecenerasetamitsenoitisop􏰆dnahmorfgnippartstoob
sahcusseuqinhcet􏰅rettulcdnuorgkcabfoecnesbasahcusscitsiretcarahclacigol
􏰆opotlaicepstibihxesegami ytimixorpSTMhguohT􏰇􏰝STM􏰜ecafrushcuot􏰆itlum
gnisnes􏰆ytimixorpassorcaedilsdna􏰅hcuot􏰅hcaorppasdnahsastcatnocmlapdna
regn􏰏elpitlumgniyfitnedidnagnikcartrofsdohtemsecudortnihcraesersihT
TCARTSBA
xxx
􏰇noitanibmocdraobyek􏰆esuom
lacipyteht nahtgniugitafssel	hcumdna􏰅tneic􏰑eeromhcum􏰅elbailersaylraensi
elohwasametsysSTMehttahtdnuofevahI􏰅tnemucodsihteraperpotepytotorp
afoesuyliadymnopudesaB􏰇sdnahhtobniselcsumrevoylnevesksatgnitubirtsid
dna􏰅sdnahgnitserroftroppusgnivomertuohtiwecrofnoitavitcaecivedgniziminim
􏰅snoitomgnimohdnagnitnioptnadnudergnitanimileybscimonogresezimitpoSTM
ehToitator
dnasihT􏰇noitarug􏰏nocdnah
􏰈
taht	dedulcnoc evahot	meessrerutcafunamretupmoC􏰇scihpargfonoitalupinam
esicerprofetairporppaniylraelcsihceepstubdraobyekcimonogredna􏰅sevolgatad􏰅gnitupmocnepecnisdellulosla
sahsecivedtupnievitanretlaaivnoitingocererutsegfonoitagitsevnicitsaisuhtnE
xeni
htiwsmetsysgnippiuqeniytlucraeppaotteyevah
secivedtupnietisiuqerehttub􏰅suotiuqibuemoceboslaevah􏰝DAC􏰜ngised􏰆dedia
􏰆retupmoc dnagnisworbbewsahcussksatgnidnamedyllaunaM􏰇slortnoc􏰝FOD􏰜
modeerf􏰆fo􏰆eergedhgihdnanoitalupinamdednah􏰆owtsuoenatlumisfossenevitiut
􏰆nidnaycneicwnoitatnem
􏰆irepxeotsevlesmehtdnelylisaesmetsysretupmoc wefyrev􏰅etadoT
􏰤tnemal􏰪􏰊􏰉􏰨sreyMdnanotxuB
􏰅rehtoehthtiwgnitniopelihwdnahenohtiwgnillorcsnigniyonnaehtsnrutsemagretup
􏰆mocgniyalpdna􏰅liam􏰆egnidnes􏰅srepapgnitirwacfoksirehtetabrecaxenoitanorpmraerofdnanoitaivedranlu
ehtkeht
ot kcabsdnahgnivommorfsresuegaruocsidoslayamesuomdnadraobyekneewt
talupinamylismulcfoksatelpmisehtnevE􏰇syawrehtonithguohtfoniartehtskaerbdnadnaheht
nosdnamedseubsadeyalpsidylraelcera
snoitcaelbissopllaesuacebnraelotysaeyreverasecafretniesehT􏰇secneuqesyek
􏰆tohlacisnesrucnI
􏰇esuretupmoclarenegnisdraobyekdnaeci mecalperothguonelacitcarpsitey
􏰅noitcaretnilarutsegdednah􏰆owtfosniagecnamrofrepdetapicitnaehtseveihcahcihw
ecivedtupnilaunam􏰅lacimonoce􏰅cimonogrenahtiwouqsutatsehtegnellahclliw
noitatressidsihT􏰇ytilacitcarpllarevofosmretnienodtuoebtonnac􏰅daphcuotro
esuom􏰇g􏰇e􏰅ecivedgnitnioplanoisnemid􏰆owt adnadraobyekafonoitanibmoceht
􏰊
secivedelbapacerutsegtey􏰅detanimileebtonnacgnipythcihwni􏰉􏰉hctaCayb
deimytsebotsraeppassergorP􏰇srekrow􏰆ocyonnadnaeciovehtniartsnactupni
dnammocdnatxetrofhceepsnoecnailerlatottub􏰅snoitautisemosnidraobyek
ehtnoecnednepedsecudernoitingocerhceepS􏰇yrasseceneradraobyekehtotkcab
stnemevomtneuqerfsagnolsasdohtemnoitalupinamlaunamibrehtoro􏰪􏰠􏰣􏰨 kcup
rosulytshtiwtelbatgniwardatpodatonlliwsrotarepotsoM􏰇noitalupinamdnah
elohwfoytilitasrevehthctamtonnacdnaregnlaerew
nehwkrowtneserpehtnagebIdna􏰅sailEnhoJ􏰇forP􏰅rosivdacimedacayM
􏰇ecafretniehtfoyretsamsuoicsnocbusniatsustonnac
Iesuaceb􏰅retupmocehthtiwreventub􏰅onaipehthtiwssenenohcussecneirepxe
ylisaeI 􏰇ylsuoicsnocbus etaludomnacsdnaheht hcihw􏰅gnisarhplacisumeht gni
􏰆tcefrepnoecnamrofrepgnirudsetartnecnocehsroeh􏰅eceipaderetsamyllacinhcet
sahtsinaipaecnO􏰇ylsseltro􏰎etsitraehtmorfwo􏰐otecnaunwollahcihwlortnoc
fossenhcirdnayteltbusaedivorpyehttey􏰅tsitraehtfosnoitnetniehtetapicitna
otsecnegilletnilaic􏰏itra niatnoctonodsehsurbtniapdnasonai P􏰇noisivcitsitraeht
ninahtrehtarretupmocehtfosgnikrowehtnidessorgnesniamertsitraehterus
􏰆neoslasecafretniysmulc􏰅stce􏰎elausivdnaoiduawengnizamaelbanesretupmoc
tahtgnitnarG􏰇lootyramirpehterasretupmochcihwnistcejorpcitsitrafoyti
􏰆lauqehtrednihoslayamecafretnidraobyek􏰆esuomehtfoseicauqedaniehT
􏰖􏰇niapeht	morfrevocer otseidob
ruoyekatlliwtiregnoleht􏰅ygolonhcetwensuoirolgnokrowuoyredraheht􏰩􏰅syas
hcihw􏰅noitareneGtenretnIehtfosdnimtsebehtnonoitatimilegnartsagnisopmi
sierutaNtupmocfoemulov
􏰋
tnere􏰎idehtot desutegtsumsrevirdsatsuj􏰅ecafrus htoomsahtiwgnitcaretnifo
􏰖leefevitalerybdehsiugnitsidebnacytivitcallaylraenesuaceB
􏰇epytotsulytsehtnwodtupdnapukcipyltnatsnocotevahtonseodrotarepoeht
os􏰅sulytstuohtiwrohtiwpirgnepagnimrofybdetacidniebnacedomgnitirwdnaH
seccaunemseriuqerregnolonsmargorpgniwardninoitatordna
gnizistcejbOnottubehtotpirtaeriuqerregnol on
drawrofdnakcabresworB􏰇egapaforenrocehtgnippi􏰐otnikadnahtnanimodg􏰏snartyllacitamardebnacnoit
􏰆caretniretupmoc􏰆dnah􏰅noitatressidsihtnidebircsedsasnoitomdnahgnizingocer
dna􏰝STM􏰜ecafrusevitisnes􏰆hcuot􏰆itlumahtiwdraobyekehtgnicalperyB
􏰇ecafrusehtssorcaylhtoomsedils yehtsasregnussidna􏰅hsilgnEfoycnadnuderehtgnisutuoyalyekehtrevotfirddnah
gnikcart􏰅yekhcaeforetnecehttatoddesiaragnimroftoybdetasnepmocebnacsyeklacinahcemmorfecnerefer
elitcatfokcal eht􏰅gnidraobyeklacisumfoyteltbusehtyrractonoddnacitsillab
yllaitnesseerastnemevomgnipytecnistahtsisehtopyhehtnopusegnihesimorpmoc
ehTmdnahelohw
dnagnipyt hcuotgnipolevedybesimorpmocastpmettanoitatressidsihT
􏰇draobyekehtsaecapslacisyhpemasehtni evirhttonnac
􏰌
esuacebdehprom􏰆erpsituoyalehTht􏰅etelpmocteytonsierawtfosnoitingocergnipytfotnempolevedhguohT
gnipyT􏰈􏰇􏰉􏰇􏰈
􏰇emitnevigatadelbaneeblliwlevelllikss􏰛resueht
htiwrapnodnanoitacilppanarofyrassecenytilanoitcnufehtylnotahtgniwollof
ehtnidemussasitieroferehT􏰇traps􏰛resuehtnonoitpadarogninraellaitnatsbus
seriuqer􏰅troppustonnacsecivedgnitnioplanoitnevnoc hcihw􏰅serutsegdnammoc
dnanoitalupinamrosructxetsahcusytilanoitcnufdecnavdaeromehtylnOonahtreisae
ebdluohsSTMeht	morf ytilanoitcnufesuomdnadraobyekcisabgniniatbO
noitarepOeciveDlaniFfoyrammuSsresu􏰅lanoitpecxeecafrusehtfoscimonogreehtsekamecrofnoitavitcayek
fonoitanimileehtelihwdnA􏰇emehcsgnipytdraobyekdrohcagninraelsatluciattaoT􏰇sworyektoohs
􏰆revoylssorgottonyrttsumoslayehT􏰇gnipytnisllulgnirudecafrusehtnomeht
tserdnatuoyal yekehtrevoydaetsylriafsdnahehtdlohotnraeltsumsresuwehtnokcep
dnatnuhnacsecivoN􏰇ecafrusehtotsdaphcuot dnasdraobyeklanoitnevnoc	morf
refsnartslliksgnitniopdnagnipytcisab􏰅racwenanoslortnocehtfossenevisnopser
snoitalupinamcidrohcfoyteirav
asezingocerSTMehtscihpargfonoitalupinamdnasdnammocgnitideroF
snoitalupinaMcidrohC􏰉􏰇􏰉􏰇􏰈
􏰇setarhgihtatoohsrevotneverpoterusserpregn􏰏ybdellortnocebnacetar
taeperehT􏰇detfil erasregnepyt
rotaeper􏰆otuanA􏰇noitcaynagnikovnituohtiw􏰎otfil dnaecafrusehtnotserot
dnahelohwehtswollasihT􏰇detarenegebtonlliwlangissserpyekehtneht􏰅hcumoot
ecafrusehtnodnuorasedilsrodnahemasehtmorfsregnafrusehtsehcuotregn􏰏anehwylnodetavitcasiyekasuhT􏰇regn􏰏ehtyb
spatthgilhT
noitavitcAyeKnacsyekegapdnaworrA􏰇skci􏰐regn􏰏delgnaylreporphtiwdekovnidna
ylisaedehcaerebnacyehterehwdraobehtforetnecehtta􏰪hgirehtgnidnetxeybdesseccaeraeteleddnaretnE􏰇tsujdaotdeen
lliwbmuhtrehtiehtiwgnicapsotdesueraohwelpoepos􏰅tfelehtrednuecapskcab
dnabmuhtthgirehtrednudecalpsiecapS􏰇snoisrucxednahecnatsid􏰆gnolecuder
ot􏰝spotpal nienodnetfosisa􏰜degnarraererasyekgnitidednanoitcnuf􏰅revewoH
􏰇snoitacolevitalerdradnatsriehtnillaerasyektfihsdnarebmuncitebahplaehT
􏰇syekrehtofosretnecehttastoddesiar dnasyek woremohehtrofsnoisserpedro
snoitatnedniedulcni yamSTMehtfosnoisreverutuF􏰇cranagnolallafyllarutan
lliwspitregn􏰏ehtev􏰏llaecnO􏰇seunitnocgninoitisoprosruc
elihwecafrusehtotpordnactserehtrodetfilebnacregnesuomehtgnivoM
gnitnioPhThsofsnocisebircsed􏰉􏰇􏰈elbaT
􏰇trapa
mehtskci􏰐rorehtegotspitregneepsemasehttaecafruseht
ssorcanoitceridemasehtnisregnocpitregn􏰏􏰆bmuhtehT􏰇ecafrusehtssorca
sregn􏰏esehtgnidilsnehtdnaemitemasehttaecafrusehtnosregn􏰏fonoitanibmoc
􏰣
􏰇spitreg
dulc

irro
tfelnoitalsnartelbisreveR
􏰇nwodro
punoitalsnartelbisreveR
􏰇noitcerid
ynani􏰝edils􏰜noitalsnarT
􏰇􏰝tohs
􏰆eno􏰜ecafrusnopatfeirB
noitoMdrohCfoepyTnocInoitoM
gn􏰏owtdnahthgireht
􏰅elpmaxeroF􏰇noitareporosructxetgnidnopserrocehtslortnocdrohcdnahtfeleht
elihwnoitareporosrucesuomaslortnocdrohcdnahthgirehttaht hcussdnahgnoma
tilpserastnemngissadrohcehtneeht	moozdnanapot desuebdluoclennahcpitregn􏰏􏰆eerht􏰞bmuht
eht􏰅potksedlanoisnemid􏰆eerhtahtiwdrawrof	dna kcabresworB􏰇gnitarelecedtuohti w􏰇dnuorgkcabwodniwehtgnitatorrognimoozotnidednapxe
ebnacgnillorcs􏰅ecafrusehtot bmuhtehtgnippordyb􏰅niagA􏰇dnahgnitniopeht
etisoppodnahehtnoylbareferpdnaskcilcnottub
yradnoces􏰅nottubesuomyradnocesaezilituhcihwsretupmocnO􏰇dnahehtgnitator
rognitcartnocdnaecafrusehtotregn􏰏dnabmuhtgniniamerehtgnippordybsgard
gniruddetatorrodezisereboslanacstcejbO􏰇sdaphcuotfopatgnidecerpdrawkwa
ehttuohtiwdrohcregndyramirpnO
gniggarD􏰉􏰇􏰉􏰇􏰉􏰇􏰈
􏰇 kcil c
esuomyramirpasecudorpecafruseht noylsuoenatlumissregn􏰏owtesehtgnippaT
􏰉􏰈
􏰇 er ut
􏰆sopdnahgnitsernistfihs
etarelototgnippamoN
􏰇gnimohtuoyalyeK
􏰇 wodni wtnerruc
fogninnap􏰟gnillorcs
suounitno C
􏰇stnedi c
􏰆cadiovaotgnippamoN
􏰇nottubesuomyramirp
aivnoitceleSnnahC
IUG drohCdnaH
thgi R
gnisnes
evitcaehT􏰇elbatrofmocsignitserdnahelohwgnirusnednanoitanorpmraerof
gnicuder􏰅􏰌􏰈tuobasyawedissdnahehtstlitecafrusehtfoelddimehtssorcahcra
tisuhT􏰇draobyekTACPMBI
decnahnenasatnirptoof emasehtyletamixorppasahepytotorpSTMehT
erawdraHgnisneSnocepytotorpSTManodetcudnoceranoitatressidsihtnistnemirepxellA
yrammuSerawdraH􏰊􏰇􏰈
􏰇serutseggnitirwdnahhtiwsmetsyserutufnodekovniebdluocsdnammoc
unemlanoitiddArrucehtsevaswercsagninrutfisanoitatoresiwkcolcAtfopatsuoenatlumis􏰅elpmis asemocebypoC􏰇tucsekovni	ylevitiutnirehtegot
mehtgnihcnipnehtdnatrapanwodregn􏰏erofdnabmuhtehtgnitteS􏰇sdnammoc
unemnommocrofecapsdrohcehtnisniamer mooremos􏰅sihtllaretfanevE
etsaPdnaypoC􏰅tuCsahcussdnammoCuneM􏰌􏰇􏰉􏰇􏰉􏰇􏰈
􏰇syekegapehtdetalumedrohcregn􏰏ruofthgireht
elihwrabllorcsehtlortnocdluowdrohcregn􏰏ruoftfelehT􏰇syekworra 􏰶tfihsmocehtot dehctam􏰆llewsi	PSDsiht􏰅ecnamrofreptniopgni
􏰆tao􏰐kaepSPOLFM􏰠􏰍hti W􏰇smhtiroglanoitacinummocdna􏰅noitingocerenihcametatsgninnacs􏰅yrome mHSALFdna
MARcitats􏰅􏰝PSD􏰜rossecorplangislatigidasniatnocsdraobrossecorpehT
erawdraHgnissecorPlangiSebosladluowygolonhcetrosnesehtmocebdluocsSTM
siygolonhcetrosnesehT􏰇oitaresion􏰆ot􏰆langiseht
gnidargedtuohtiwsnoisnemidegralyrevotdelacseberoferehtnacdnasecnaticapac
citisarapotenummiyleuqinusiygolonhcetrosneslevons􏰛STMehttahtetoN
􏰇elbatcetednu
emocebstcejbollamsecnehwtdiortneceht􏰅􏰊ret
􏰆pahCni debircsedsanoitalopretni dnanoitatnemgesyarraelbatiushti W
􏰇segamiytimixorpfomaertsonaotedort
􏰆celeevirdamorfselpuocycneuqerfsuonorhcnysawohgnirusaemyb􏰅ecnaticapac
morf
ecnaticapacro􏰅ecnaticapac􏰆flesedortceleehterusaemsrosnesytimixorpehT
nlairesCtalumedraobyekdnaesuom
􏰅􏰪􏰣􏰉􏰨􏰇proCsiseniKmorfelbaliavasexobretrevnoc􏰉􏰟SPhti	W􏰇eci mdnasdraobyek
CP	MBI gnitalumerof elbaliavaerastrop􏰉􏰟SPspbk􏰠􏰈owT􏰇draobnosnoitarugaeraspetsesehT
􏰇snoitanibmocregn􏰏ralucitrapfosnoito m
otesnopserniretupmoctsohehtrofstneveesuomdnadraobyekgnitareneg􏰒
􏰇sregn􏰏sade􏰏itnedistcatnocehtmorfsnoitomdnahgnitcartxedi􏰒
􏰇segamifomaertsehtssorcatcatnochsees ersiPSD
ehTING HARDWARE
CALIBRATION AND PROXIMITY IMAGE FORMATION
CONTACT TRACKING AND IDENTIFICATION
HAND MOTION EXTRACTION
CHORD MOTION RECOGNIZER
HOST COMMUNICATION INTERFACE
TYPING RECOGNIZER
FINGER SYNCHRONIZATION COMPONENT
PENGRIP DETECTOR
DETECTOR
􏰡􏰈
􏰇seludomerawtfosdnaerawdrahSTMehtfomargaidkcolbllarevOuht􏰅spitreg
􏰆n􏰏fosdeennoitatnemgesgnitci􏰐nocehT􏰇stcatnoctnecajdaneewtebseiradnuob
serucsbosaremacoedivotderapmocyarragnisnesytimixorpehtfonoituloser wol
eht􏰅oslA􏰇ecafrusehtevobatao􏰐hcihwstrapdnahfoytilibisivniehtsiesehtfo
tluc􏰑idtsomehT􏰇hcraesergnissecorpegamisuoiverpnidesserddaneebtonevah
hcihwsegnellahclaicepsstneserposlasegami ytimixorpfoygolopoteht􏰅segami
lacitpoeugalphcihwsnoitairavgnithgildnarettulcdnuorgkcabehtkcalsegamiyti

aderetnuocneevah
I􏰇metsysahcusfolaitnetopnoitargetnieuqinuehtpolevedyllufottsrlpmitsr􏰏ehttonsisihtelih W
􏰇nwodhcuotregn􏰏suonorhcnysadnasuoenatlumis neewteb
noitcnitsidehtaivecafrusemasehtnognitniopdnagnipytfonoitargetni onoitcartxe􏰒
􏰇sretsulcdnahthgirdnatfelotnistcatnocfogninoititrap􏰒
􏰇etalpmettnioprotcarttanafoselgna
rotcartta􏰆retniehtottcepserhtiwstnioptcatnocfognitrostnairavni􏰆noitalsnart􏰒
􏰇segami
evisseccusssorcadekcartstcatnocmlapdnalofehtnidnuorgskaerbnoitatressidsihtnidebircsedkrowehT
snoitubirtnoCfoyrammuSiwnoitatorriapregnrehtehwfosseldrager􏰅 􏰠􏰣naht
􏰑
eromybstnioprotcarttaforiapehtneewtebelgnaehtmorfsre􏰎idstcatnocowteht
neewtebelgnarotcevehtsselnudeppawsylsuoenorreebtonlliwstcatnocforiap
ynafoseititnediehtfo musehtfodesopmocsitsoctnemngissaehtfIthcihwstrap
dnahfosnoitac􏰏itnediezilibatssplehetamitsenoitisopdnahevitavresnocaybgnir
rotcarttasregnnir
aotstcatnocecafrusfotnemngissaeno􏰆ot􏰆enotsocmuminimehtgnidniF
􏰇sretemarapnoitomdnahfonoitcartxesdiaoslastcatnocregn􏰏rehtorofredro
tnetsisnocagniniatniamdnabmuhtehtgniyfitnedI􏰇ecafruseht hcuot hcihwsregnnihsiugnitsidylbailer
􏰅elbisaefyleritneroyrassecensyawlatonsirehtonaenomorfsregn􏰏gnihsiugnitsid
elihW􏰇ecafrusehtnotserotsmlapwolladnasnoitommlaperongi otsmhtirogla
noitingocernoitomehtrofevitarepmisiSTMegralanostcatnocregn􏰏morfstcatnoc
mlapgnihsiugnitsida􏰅erofebdetpmetta neebtonsah
enolanoitamrofnignisnesyti mixorpmorfstrapdnahfonoitac􏰏itneditcerroC
􏰇seitiugibmaegami	yti	mixorpemocrevooterehde􏰏itnedi stniartsnoc
laci motanaehtezilituotdeenlliwmetsysnoitatnemgesyna􏰅cohdatahwemosera
STMehtrofdepolevedselurnoitatnemgesehthguohT􏰇setamitsenoitisopdnah
􏰠􏰉
ottpmettasnoitaredisnocrehtodnasiht nodesabylluferacsgnithgiewyticolevreg
bolacinahcemoiB􏰇tnednepedniedameb wohemostsumsretemarapnoit
reporplecnactonodhcihwnoitatordna
􏰅noitalsnart􏰅gnilacsdnahgnirudsnoitomregnlanoitisoplan􏰏ehtsehcaorpparesuehtsA􏰇􏰪􏰉􏰌􏰈􏰨sresurof
tluc􏰑idebnacmodeerffoseergedowtnahteromgnisutcejbonafotnemecalpsuo
􏰆enatlumissnoitactdetimilylluferacsideredisnocebotsesehtopyhnoititrap
dnahforebmunehtecniS􏰇dnahhcaerofsgnirrotcarttatnednepedninoseiler
hcihwdeyolpmesihcaorppanoitazimitpolairotanibmocasdnahtneverptonodhcihwsecafrus
noegnellahcaoslasitcatnocecafrus hcaesesuacdnahhcihwgninimreteD
􏰇krowemarfrotcarttaehtotnidetaroprocniylisaetonerahcihwseiticolev
dna􏰅snoitarapes􏰅selgnatcatnoc􏰆retnifostsethtiwecneserpbmuhtse􏰏irevenituor
noitac􏰏issalcbmuhtyzzufA􏰇dnahehthtiwdengilasignirrotcarttaehtrehtehwfo
sseldragerniaga􏰅sgniredrodnaselgnagnirrotcarttaehtottcepserhtiwstcatnoceht
trosotmhtiroglatnemngissaehtsesuacytreporpsihtibehtezis dnanoitalsnart dnahfotnednepednirennamaniserutpacoslati
nagnisnesytimixorpehtfosliatedrehtruF􏰇yar
􏰆ragnisnesehtgnisopmocsedortcelemargolellarapdevaelretniehtfonoitanalpxe
lanoitiddasedulcnilEybdengised􏰜krowsihtni desuepytotorpSTMehtrofsnoitac􏰏i
􏰆cepsemos􏰅revewoH􏰇noitatressidsihtnidezylanaebtonlliwdnasailEnhoJ􏰇forP
ybdetnevnisawSTMehtfoerawdrahgninnacsdnagnisnesytimixorpehT
derevoCtoNsitah Wcuotdnagnisiarylsuonorhcnysybdetceleseb
oslanacslennahcdrohc weNmgnirudemit ynataecafrusehtotnopordotsregn􏰏ev􏰏lla
swollaenihcametatsnoitcelesdrohceht􏰅elbissopsahcumsaecafrusehtnosdnah
riehtfothgiewehttserotsrotarepoegaruocneoT􏰇ecafrusehtfosaeradezilaiceps
neewtebgnivomronottubhctiwsenylno
esuomdnadraobyekneewtebgnihctiws􏰅t􏰏enebegnirfsuomronenasatuB􏰇rosruc
esuomehtgnigdunylsuoirups morfsdnahgnitserrosekortsyekgnicnalgtneverpot
yrassecensisihTafrusanognipythtiwtsixeocotytilanoitcnufesuom
rof yawelbissoptsebeht ylbaugrasi tahwnopusegrevnoc hcraesersihT
􏰇stnenopmocnoitomneewteb
ecnerefretnigniniamerehtsevomersdeepstnenopmocnoitomfonoitubirtsidehtno
tnednepedshtdiwenozdaedhtiwgniretlihtnidedulcnitoneradnadetcefrepneebteytonevahsmhtir
t
dn􏰏otsmhtiroglanoitingocergnipytdetacitsihpossezilituSTMehthguohT
􏰇noitalupinamcidrohcgnirudsnrettapnoitomdnahniseisarcnysoidietarelot
otuowyriuqni
foenilluftiurferomAlot yrev
ngisedybsimhtiroglanoitac􏰏itnedidesab􏰆rotcarttaehtecniS􏰇sresuSTMfonoital
􏰆upopediwarevonoitazimitpolacitsitatsroferutcetihcragninraelenihcamaotni
mehttt
de􏰏itnedisahhcraesersihttahtwoNarolacinahcemoiblanoitiddafonoitaroprocnisehttey􏰅snrettapderolpxenuylsuoiverpevlovnisegamiytimixorp
morfnoitac􏰏itnedidnagnikcartdnahtahttcafehtybde􏰏itsujsisihT􏰇smetsysnoit
􏰆ingocernrettapnigninraelenihcamdrawotdnertehtskcubhcraesersihT
􏰇rehtieerehdetagitsevnitonsirezingocergnitirwdnahaotmehtdeefotyrassecen
segami pirgnepehtfognissecorperpehteroferehT􏰇ylbailerdetnemgesgnieb morf
noitarug􏰏nocsihtfosregn􏰏dehcnipehtstneverpyarraedortceleepytotorpeht yb
gniraemslacitrev􏰅decudortniy􏰐eirbsinoitarug􏰏nocdnahpirgnepehthguohT
􏰇krowsihtnishtapxelpmocezingocerotedameblliwtpmettaonsuhT􏰇􏰪􏰉􏰋􏰈􏰨􏰉􏰟SO
rofnePs􏰛MBIsahcusenignenoitingocergnitirwdnahgnitsixenaottuptuoeb
ylisaedluocslennahcpirgneprodrohcregncfonoitingocerdetagitsevnisahhcraesersuo
lsraenilelpmis
fotsisnockrowsihtnidezingocerebotserutsegnoitalupinamcidrohcehT
llabrellorrofdlehsahpihsnoitalersihT􏰇secivedecrof
􏰆hgihromuidemnahtregnolsemitlarevessecivedecrofnoitavitcalaminimetarelot
llitsdluocI􏰅detcerrocerewstibahdabymretfatahtdnuofevahIcruosrojamenO
eenigneyna
nisA􏰇senildaedteemotylsuounitnocgnikrowdnamdnagnihctertsreporphtiwskeewwefaniderevocerebyllausunac
htgnertS􏰇􏰪􏰡􏰈􏰈􏰨ecnarudnefossollufniapsesuactiyllasrevinueromtub􏰅sesacereves
nihtgnertselcsumdnanodnetfossolsesuac􏰝ISR􏰜yrujniniartsevititepeR
seciveDtupnIcimonogrEfongiseDehtnO􏰍􏰇􏰈
􏰇ISR
morfsre􏰎usydaerlaohwenoybgnitidenoitatressidfosdnamednoitcaretniesnetni
dnaesrevidehtllateemotecivedtupniwenlacidarahcusrofelbakramersititaht
etaicerppalliwnoitatressidanettirwsahohwenoyna􏰅noitaulavec􏰏itneicslamrof
asadnatstonyamsihtelih W􏰇tnemucodsihtfonoitaraperpgnirudecivedyrtne
dnammocdnatxet􏰅scihpargyramirpymsaSTMehtgnisu morfsnoitavresbodeliat
􏰆eddnalainomitsetymedulcniodI􏰅revewoH􏰇seidutsresulamroftcudnocotemit
neebtonsahereht􏰅yltnecerdetelpmocylnosawepytotorpSTMehtecniS
􏰇derevocsisnoitalupinam
cidrohcmorfsnoitomgnipytsehsiugnitsidhcihwrotcetednoitazinorhcnysregnnutrofnU
􏰇ecivedcimonogreeromaesudnatpoda
otmehtgnittegylpmisoteuqinhcetreporpylppadnanraelylsuoicsnocotelpoep
gnittegmorfstfihsmelborpehTspatdrahfognirrajlufniapehtelpoepevitabmoc wollatub
tcapmipitregn􏰏netfosthgimecafrusdaphcuottnailpmocacerrocelpoeptsom􏰅elpmaxeroF􏰇stibahegasudnaserutsopreporp drawot
etativargsresutnarticlacertsomehtnevetahtosdengisedebnacseciveD
􏰇􏰪􏰡􏰈􏰈􏰨enilpicsid􏰆fles
fotolasekatsihttub􏰅ecimdnasdraobyeknialpnoyrujnifoksirehttimilhcihw
seuqinhcetreporpnraelnacsresusuoitneicsnoC􏰇skcilcnottubehttahterusneot
ecrofyrassecenehtsemitlarevesylppadnaesuomehthcnelclliwelpoepirwrieht
gniniarts􏰅ydobehtmorfyawaswobleriehtdlohdnadrawtuosdnahriehtetatornetfo
elpoep􏰅draobyekdradnatsafo woremohnosdnahriehtdlohot􏰅ecnatsniroF􏰇tro􏰎e
ssecxeroserutsopdrawkwaaivetasnepmocotelpoepsegaruocneseciveddradnats
fongisedcimonogreroopehthtnoitasuacISRehthtiwtnetsisnocylniatrecsiti􏰅sresullat􏰏eneb
ylralimissecivedecroflaminimtahtevorptonseodecnedivelatodcenaymelihW
􏰇espalerlufniaptuohtiwyletin􏰏ednieunitnocotkrowwollasecivedecrofnetemesuacotmeesyllautca
hcihwlaitnesse
niamerot dnahymwollayehttahtsi nommocni evahsecivedecroflamini meseht
esailaedulc
ahI
􏰇emitdnaydobssorcadaolkrowgnitubirtsidfotraehtsanwonkesiwrehtO
selcsumemasehtfonoitcaevititepereziminiMecudersehctiwsyek
dnasnottubesuomlacinahcemecrofirotcafrojamaebotdeveilebsitroerasevitcejboesehtgniteemrofsdohtemlacipythguohTd
morfderehtagsnoitadnemmocer􏰅AxidneppAni deweiverhcraeserscimonogreeht
nopudesabdetalumrofneebevahsevitcejbongisedcimonogregniwollofehT
sevitcejbOngiseDcimonogrEeicetehT
htrevo
lortnoctsomehtevahohwesohteratsebepocohwstneitapISRehthtryamsresUpmi yrev
ere􏰎idotsdaoltfihsdna woN
erutsopfonoitairavwollAuaceblartuentsomehtyllaretilera
secafrustalF􏰇sresudednah􏰆tfeltsniagaetanimircsidtondluohsseciveDaderolpxeneebsah
sepahsllabkcartdna􏰅esuom􏰅draobyekelbatrofmocrofecapsngisedehT
serutsoplartuenegaruocnE􏰅spuorgelcsumyratnemelpmocyolpme
hcihwsecivedwollefhtiwelbitapmocylhgihrehtieebtsumeciv
xetrofsdroclacovehtasnoitanibmocregn􏰏rosregn􏰏suoirav
gniyolpmesliatnesihtlevel dnahehttAegasubmilroelcsumetanretla
􏰇elbaliavaslennahcnoitalupinamevitanretla
sahecivedafinoitomsuounitnocenonidesserpxeebnetfonac
snoitcaecivedlevel
eromebnacsedoM􏰇ksatarofecivedetairporppatsomehtot
gnihctiwsmorfresuehtegaruocsiddnagniyonnaemocebsnoig
􏰆erecafrusrosecivedneewtebsnoitom􏰅evissecxefIsnoitomgnimohecuder
􏰇secneuqesesuom
evititeperfokcabyalpdnagnidrocerorcamdnaarts
􏰆nomedotsmianoitatressidsiht􏰅serutsegtroppusdnatro􏰎eetubirtsidotsdrohc
regn􏰏tnadnubadnanoitadnuofehttaecafrusecrof􏰆noitavitca􏰆orezahtiW
􏰇esudraobyeksuoenatlumishtiwelbitapmoc
tarepoesuomlevel􏰆wol􏰅tneic􏰑enihtiw
sregnsopwollaroecrofnoitavitcayeketani
􏰆miletonseodtitub􏰅htgnertsregn􏰇laemeceipylnoairetirceseht sserddasecivedgnitsixetseihtlaehehT
􏰧ecnotatemebsevitcejbocimonogreynamosnaC􏰊􏰇􏰍􏰇􏰈
􏰇snoitatskrowriehtgnivael
ylralugermorfelpoepegaruocsidthgimhcihwydobehtotdehcattasevolgroseriw
diovasi	odnacrengisedecivedehtll A􏰇sekortsyekforebmunniatrec aroemitfo
tnuomaniatrecaretfakaerbaekatotelpoepdnimerotelbaliavasierawtfoS
skaerbtseregaruocsidtonoDhtekamserutseglortnoclevelllamssakcabdeeflausivnisgaL􏰇retupmocehtmorfesnopseragnitcep
􏰆xeelihwesnetniameryamsresu􏰅skaerb􏰆orci myhtlaehgnikatnahtrehtaR
noitapicitnaresueziminiMsuonorhcnysasehsiugnitsidrotcetednoitazinorhcnys
regn􏰏 A􏰇STMeht nodeveihcaerayeht	wohsnialpxednanoitalupinamFODspleh
mhtiroglanoitamitsenoitisopdnahevitavresnoc	A􏰇tcatnocecafrus hcaeesuachcihw
regn􏰏dnadnahehtyfitnediyllufsseccushcihwsmhtiroglanoitazimitpolairotanib
􏰆mocsecudortninehtretpahcehT􏰇seuqinhcetnoitingocererutsegdnahfoweivera
hti wsnigeb􏰅noitamitsEnoitisoPdnaHdnanoitac􏰏itnedIregniF􏰅􏰋retpahC
􏰇􏰎otfildnanwodhcuotregn􏰏tcetedylbailerotseuqinhcetgnidulcni􏰅dessucsidera
segamiytimixorpevisseccusssorcastcatnocgnikcartrofsdohtemtfA􏰇stcatnocdetnemges
rofnoitcartxeerutaefsrevocoslanoissucsidehT􏰇segamiytimixorpnidnahehtfo
straptnere􏰎idetarapesotyrassecenselurnoitatnemgesdnastniartsnoclacimotana
ehtstneserp􏰅gnikcarThtaPdnanoitatnemgeStcatnoCdnaH􏰅􏰊retpahC
􏰇segami yti mixorphcusoteuqinuscitsiretcarahclacigolopot
ehttuostniopdnayarrarosnesepytotorpehtybderutpacsnoitarugtfonoitarbilacsebircsednehttI􏰇sdaphcuot
dna􏰅saremacoediv􏰅sevolgatadsahcusseigolonhcetgnisnesnoitomdnahdetalerfo
weiverahtiwsnigebitatnemelpmilliwron􏰅llufniderevocebtonlliwrotceted
pirgnepdnarezingocergnipytehttahtnoitpecxeehthtiw􏰅􏰡􏰈egaPno􏰉􏰇􏰈erugiF
fomargaidkcolbSTMehtswollofylhguornoitatressidehtfonoitazinagrO
eniltuO􏰡􏰇􏰈
􏰣􏰉
􏰇melborptnemngissagnirrotcarttaehtottroselbbub
gniylppanehwderetnuocnesmelborpecnegrevnocehtsnialpxe CxidneppA
imidrofseuqinhcetnoitalopretniraenilthsiwohwngisedecivedtupnihtiwdecaf
sreenignerofdednetnisi dna􏰅sdraobyekcimonogrefongisedehtninesiraylsuoiv
􏰆erpevahhcihwseussisedulcnitIl
􏰆oimedipednalacigoloisyhpsweiver􏰅sreenignErofscimonogrE􏰅AxidneppA
􏰇sedivorpSTMehtsa
hcustupniFOD􏰆􏰋􏰅launamibfoegatnavdallufekatotde􏰏idomebdluohssmetsys
gnitarepo dnasnoitacilppaerawtfos	wohfoeniltuo nahtiwsedulcnoc 􏰍retpahC
􏰇stnemecnahneSTMerutufrofsnoitseggussre􏰎odnaseidutsresuybnoitaulave
lamrof evresedhcihwseussiserocsrednulainomitsetsihT􏰇tnemucodsihteraperpot
shtnomeerhtrofepytotorpSTMgninoitcnuf ylluftsrdnaserutsegdnahecaps􏰆eerfehtgnirutpacrofetairporppaylraelcerasmetsys
hcuS􏰇nosaersihtrofstnemirepxeytilaerlautrivniralupopneebevahsmetsysnois
􏰆ivretupmocdnasevolgataD􏰇tcejbognicafretninaforotcafmroflacisyhpehtyb
deniartsnocnuorivne
s􏰛resuehtnitcejbonanodetnuomsrosneserusserproytimixorphtiwro􏰅srosnes
lacitsuocarolacitpoetomer htiw􏰅dnahehtot dehcattasrosnescitengamortcele
rolacinahcemhtiwdetcetedebylbaviecnoc nacnoitomdnanoitisopdnaH
gnisneSnoitoMdnaHrofsdohteMdetaleRylralucitrapsinoitcesygolopotdnahsihT􏰇stcatnoc
dnahfostnemegnarradnaserutaeflacipytehtetartsulli hcihwsegamiytimixorpfo
gnilpmasahtiwsedulcnocretpahcehTejbodnuorgkcabsahcusgnissecorp􏰆erpegamiytimixorp
sessucsidretpahcehtnehT􏰇gnikcartdnahfosnoitacilppayadyreverofdetius􏰆llew
ylralucitraperasyarragnisnes yti	mixorpyhwsnialpxednaseigolonhcetgnisneseseht
foweiverahtiwsnigebretpahcsihTOLOPOTDNANOITAMROFEGAMIYTIMIXORP
􏰉retpahC
􏰈􏰊
htiwtnetsisnocdna􏰅tsocsecudersrosnesnoixe
􏰆noctcetedhcihwsmetsysytilaerlautrivrofsevolgdrohcrohcnipstekramimdna
skaerbtsergnikatmorfsresusegaruocsidtiesuacebegatnavdasidcimonogrena
dnaegatnavdasidlacitcarpahtobsi sihT􏰇sksatretupmoc􏰆nonsemuserresueht
nehwdevomer ebnetfotsumsevolg􏰅tne mhcattaylidobasa􏰅eromrehtruFpacyllaitnetopnac􏰪􏰢􏰋􏰈􏰨sevolGataD
hguohT􏰇llewsasnoitatimilevahseigolonhcetgnisnesnoitomecapsgninimreted􏰅nwonktonsirednesehtfoezagfonoitceridehtfI􏰇llataenoon
rosudnihebenoemosrofdednetniyllautcasierutsegehtnehwsutagnirutsegsi
enoemosknihtylnekatsimsnamuhewnetfowohredisnockrow􏰆ocot
serutsegrostnemtsujdalarutsopmorfretupmocehtrofsnoitcurtsniebotdednetni
snoitomhsiugnitsidotretupmocaroftluc􏰑idyrevsiti􏰅oslA􏰇tcejbomr􏰏atsniaga
stsermrarodnahehtfotrapemosnehwnahtelbissopsinoisicerpsselylbaredisnoc
ososdnahdetroppusnuehtgnidloh
tubeilmelborptsram
srotcaflarevestub􏰅snamuhneewtebnoitacinummoc ni raeppayehtsaegaugnal
􏰉􏰊
􏰇modeerftnemevom
ninoitcuderthgilsehtrofpuekamnahteromlliwstrapmitcatnocecafrustaht
kcabdeefelitcatdnatnetniresufoytiralcdevorpmiehtsnoitacilppaynamroF􏰇ecaf
􏰆rusanosregn􏰏elpitlumfosnoitomgnilacsdnalanoitatormorfdetcartxeebnac
modeerffoseergedartxetahtetartsnomedlliw􏰌retpahC􏰅snoisnemidflah􏰆a􏰆dna
􏰆owtotdeniartsnocsiecafrusanoytivitcaregn􏰏laudividnihguohT􏰇erongidluohs
retupmocehttahtecafrusehtmorfyawasnoitomdnaezingocerotdednetnisiretup
merab
ehtta􏰅sedivorpecafruslacisyhpahtiwtcatnocfonoitcetedcasilootehtsakcabdeefcitpahhciredivorphcihw
stnemurtsnilacisumdnaslootdnahgnitalupinamfoyrotsihgnolehtserongiecaps
eerflanoisnemid􏰆eerhtni gnikcartdnahnosisahpmeehttluc􏰑idyrevtiekamoslanoituloseraremacdetimildnanoisulccO
lanoitatupmochtiwtuoderetltfo
hcumeehtdiovaseigolonhcetnoisivretupmoC
noitingoceRerutseGoediVdnaelbailereromebot
tuosnruttcatnocpitregn􏰏lacisyhphcus􏰅noitatressidsihtfoyhposolihpngisedeht
􏰊􏰊
rebmunlatotehtelihWaera
􏰇yrtiucricgnissecorplangisotsrosnesehttcennocotyarrarosnes
ehthtaenrednunurebtsumxirtamnoitcennocatubargetni
ylno􏰝bnisrosneserauqsehT􏰇smlaprobmuht ybdetcurtsbonu wor
latnozirohylhguoranieilspitregn􏰏ehtgni mussaslangisnmuloceht
niamixamehtgnitnuocybdetnuocebnacspitregn􏰏gnihcuoT􏰇xirtam
ehtfosegdeehttasnoitcennocdeenylnodnaedortcelenmulocdna
worhcaessorcaetargetnisrosnesgninnapsnmulocdnaworiF
􏰝b􏰝a
evitcaehtfoegdeehttanmulocdna worhcaeotdetacollasitnemelerosneseno
􏰅􏰝a􏰈􏰇􏰉erugiF􏰜xirtamevitcejorpanI􏰇secirtamrosnes􏰖evitcejorp􏰩sllaceeLtahwno
yleryehtesuacebregnfelehtotstolprablatnozirohehtniatbootyarraD􏰉eht
fo wor hcaerevognitargetni ybdetalumisera􏰪􏰌􏰈􏰨ressaKdnatessiBfosedortcele
gninnapsnmulocdnaworehtmorfderusaemebdluowhcihwslangisevitcejorp
ehTetaredomhtiwyarragnisnes
eviticapacaniruccodluowsahcusslavretnimmravfosnaissuaGlanoisnemid􏰆owthtiw
detalumiserastcatnocecafrusleeh mlapdnamid􏰆owtehtotderapmoc
hcaorppanoitcejorpsihtfosnoitatimilehtetartsnomedaeragnisnesehthcuot
smlapdnasregn􏰏htobnehwsahcus􏰅rehtonasanmulocemasehttcesretninactrap
dnahenohcihwnisecafrus hcuotregralrofsuougibmasemocebdohtemgnitnuoc
amixamnoitcejorpsihtorebmunehtgnitnuocdna woranieilsregn􏰏eht
gnimussaybenodsisihttahtsnialpxe􏰪􏰌􏰈􏰨ressaKdnatessiBottnetapehT􏰇spitreg
􏰆n􏰏eerhtot pufoecneserpehttcetedotelbaerasretupmocpotpalrofitcejorpfoseitiugib
􏰆maehtetipsedsregn􏰏elpitlumezilituyllaitraptekramehtnosecivedemoS
􏰇noitatimilsihtmorfre􏰎us
llitstekramehtnosdaphcuoteviticapacsallewsasneercshcuotderarfnidnaevaw
citsuocaecafrusehtotoorerauqsehtylnootdetalersidedeensrosnesfo
a)	b)
c)	d)
􏰌􏰊
􏰇dirgrosneserauqseht
nielbanrecsidllitseraaminimlanogaidehthguoht􏰅gnitnuocpitreg
􏰆n􏰏gnitneverp􏰅raeppasidspitregn􏰏neewtebamini mnoitcejorpeht
tahtrehtegotesolcoseraspitregn􏰏eht􏰝dnI􏰇amixampitregn􏰏eht
tnuocllitsnactihguoht􏰅noitatordnahehtgninimretedmorfdohtem
noitcejorpeht gnitneverp􏰅􏰝stolprablacitrevdnalatnoziroh􏰜snoitcej
􏰆orpnmulocdnaworemasehtecudorplla􏰝c􏰆􏰝anidirgrosneserauqs
ehtnonwohsstnemegnarratcatnocpitregntcejorPhnacslanoga
tniteg
snoitcejorpnmuloc mlapehtesuacebgnihcuoteraspitregnluocenonoitcejorpworehtmorferoferehT􏰇owttsujotniegremotamixam
noitcejorpnmulocehtgnisuactubnoitcejorpworehtniamixameerhtgnivaelht􏰥ytiugibmasihtevloserylbailertonnacsedutilpma
noitcejorpniegnahcehtesuaceB􏰇tahwemosegnahcsedutilpmaehthguoht􏰅egnahc
tonseodamixamnoitcejorpforebmunehth􏰜
noitcejorpworehtnimumixamlanoitiddanagnisuac􏰅pitregn􏰏xedniehtsanmuloc
emasehtylraenni bmuhtehtsedulcni b􏰊􏰇􏰉erugiF􏰇noitcejorpworehtnienodna
snoitcejorpnmulocehtniamixamruofgnicudorpserpehtyb
dedulccosisrosnesnoitcejorpmorfgnitnuocpitregnidybdetarapesylraelcllitseraamixamlacolhguohtemasehT􏰇re􏰎id􏰝dirg􏰜
stnemegnarrapitregnsnoitcej
􏰆orpnmulocdna worehthtobniraeppaamixamruofc􏰆a􏰉􏰇􏰉erugiFnI􏰇snoitator
a)	b)
c)	d)
􏰡􏰊
􏰇smlapehtfoesohthtiwdegremteghcihw􏰅amixam
noitcejorp worpitregn􏰏ehtserucsborehtruf􏰝dnismlapehtgniddA
􏰇bmuhtehtfoecneserpehtni ylbailerdetnuocebtonnacspitregn􏰏
gninaemesretni	bmuhtehtesuaceb	mumixamnmulocatontub
mumixamworasdda􏰝bnitcatnocbmuhtagniddA􏰇srablatnoziroh
ehtni	mumixamnoitcejorp worenodnailsasniatnocylpmis􏰝a􏰇spitregn􏰏sasnmulocemaseht nismlap
dnabmuhtehtfoecneserpybdesuacgnisnesevitcejorpniseitiugibmAetalplellarapneewtebecnaticapacehtecniS
􏰇􏰪􏰢􏰢􏰨􏰝Fp􏰠􏰠􏰈tuoba􏰜egralylevitalersidnuorghtraeottcepserhtiwydobnamuheht
foecnaticapacehttubFpwef ayllacipytsietalpedortcelenadnahse􏰐pitregn􏰏
evitcudnocehtneewtebecnaticapacehtecnisdnuorgotecnaticapacedortceleeht
sesaercniylevitce􏰎eregn􏰏afoecneserpehT􏰇sedortcelelatemfoyarradetalusnina
dnasregn􏰏ehtneewtebecnaticapacehterusaemotsinoitpogniniamerehT
syarrAedortcelEgnisneSi mixorpregnoyarranahtiwgnigamilacitpoevitcA􏰇􏰪􏰢􏰢􏰨erusserpregn􏰏
etaitnere􏰎idtonnacsmetsyshcusdna􏰅moorgel dnaytilibatroptimillliwelbateht
rednuscitpoyklubehtyletanutrofnUcalpotsi hcaorpparehtonA􏰇gnisnesytimi
􏰆xorpecrof􏰆noitavitca􏰆orezotroirefniyllacimonogresiecrofnoitavitcalaitnatsbus
riehttub􏰅noitarug􏰏nocsihtniyllacimonocedetcurtsnocebnacsrosneserusserp
enarbmemelitcaTotsihtsezilitu􏰅snoitceridruofmorfsmaeb
derarfni htiwsregn􏰏fonoitcesretnisesneshcihwrotinomretupmocafoneercseht
ottnemhcattanaebmunehtnahtsselenoyllarenegsistcatnocelbatacolylsuougibma
niTaerutsegrognipytrehtierof	wols oot hcum
sihcihw􏰅spf􏰌􏰆􏰈eveihcaotelbaneebylnoevahdluowtisregn􏰏netrof􏰅stcatnoc
ecafrusfosnoitisopdnaforebmunehtnoylralugerridednepedetargninnacssti
hguohTpesuomen􏰏ehtecudorperotesraocootylbaborpsawgnicapsedortceleeht
wollasecnaticapacsaibesreveredoid
nmulocehttahtsawngiseds􏰛eeLfoegatnavdasidlapicnirpehT􏰇rotsiseregrahcsida
hguorhtdnuorgotsnmulocdetcelesfoegrahcsidehtgnimitnehtdna􏰅sworesohtni
sedortceleehtgnigrahcylevitceles􏰅senil	woreromroenonoegatlovehtgnisiar yb
spuorgralugnatcerniroylgnisderusaemerewsegnahcecnaticapacedortcelE􏰇enil
gnigrahcsidnmulocaotdetcennocedoiddnocesadnaenilgnigrahcworaotdetcen
􏰆nocedoidenodahedortcelehcaE􏰡 htiw􏰋􏰢􏰣􏰈 ni	yarra hcustsr􏰏ehttliubeeL
􏰇ecafrusehttsniagadenetta􏰐
si dnahelohwehtsselnusmlapdnastniojregn􏰏ehtfonoitcetedstneverposlati
􏰅derettulcnudnaraelcsinoitamrofniytimixorppitregn􏰏serusnesyarraedortcele
foegnargnisnesytimixorpdetimilehtelihW􏰇􏰪􏰋􏰊􏰈􏰨ecafrusehttsniagasnetta􏰐
pluppitregnniatboebylisaenac	mm􏰉􏰇fonoisicerP􏰇sedortceleeht hcaorppasregn􏰏
ehtsayllacitamardsesaercninoituloserlaitapS􏰇sedortceleehtfosretemillimwef
􏰠􏰋
esiraodsesaibnoitalopretnilacitrevimoceb􏰅spitregn􏰏
rehtoehtfosnmuloctcesretnidnadnihebssapykniprobmuhtehtelihwaniecno
gnicaps worenonahtssel
nmulocedortceleemasehtniraeppahcihwstcejbohsiugnitsidottlucocnoitalopretnilacitrevfoytimrofinusevorpmisihthguohT
iravnmulocani
sedortceleneewtebsnoitces􏰆ssorclatnozirohehtfooitarehT􏰇yrtemoeglacisyhprieht
aivetalopretnisedortcelemargolellarapdevaelretniyllacitreveht􏰅tsartnocnI
􏰇noitalopretnielbailerategotlangishguoneretsigertonodwolebdnaevoba
sedortceleehttahtllatossiedortceleeht􏰅elddimehtnisiregn􏰏afI􏰇edortcele
nafoelddimehtninaht􏰅sedortceleowtneewtebetalopretniotelbissopsitierehw
􏰅sedortceleehtfomottobdnapotehtraensegnahcnoitisoplacitrevotevitisnes
eromeranebebylirassecentondluowtub
􏰅tnuoctrapetercsidehtgnirewolybsyarraepytotorphcraeserfonoitacirbafsdeeps
tnuocedortceleninoitcudersihT􏰇noitalopretninoitisoplacitrevniseitimrofinu􏰆non
suoiresgnisuactuohtiweerhtforotcaf aybyrassecensworforebmunehtecuder
otyrtemoegedortceleegdewlaicepsasyolpmetIiwecnaticapacrosnesderusaemtiylbissoP−15 −10 −5 0 5 10 15
Horizontal Position on Surface (X axis cm)
􏰇ediwmc􏰌􏰉􏰇􏰠ylnoerasedortceletub􏰅 mc􏰋􏰇􏰠si hctipnmulocdna
mctical Position on Surface (Y axis cm)
a) b) c) d) e)
a) b) c) d)
􏰉􏰋
􏰇yllaudargsegnahcsedort
itreV􏰤􏰍􏰇􏰉erugiF
􏰇gnicaps worrellamsadnasedortceleerauqslanoitidartrofemehcssihtfosgnivas
tnuocedortceleehtnodnabaotevahdluowlocneewtebronoderetnectonerahcihwstcatnocllamsrof
etnilacitreVisaehcumwohdezilaerlavomeRdnuorgkcaBdnanoitamroFegamIelitcaTegnahgninnacsehtotsegnahcroni M􏰇ecafruseht
gnihcuotylerabsiregn􏰏eht nehwelcycpatehtfodnerogninnigebehtraenrucco
thgimpatehtgnitcetednacselgnisehtesuacebyletaruccaderusaemebtonnacpat
ehtfoelddimehtniecafrusehtotnotuosmottobpitregnroycneuqerfnacsyarraeht􏰅noitatressidsihtrofdetcudnocstnemirepxeeht
gniruD􏰇noitcetedelbailerrofsiht nahtrellamstahwemosebtsumdoirepnacseht
tubdnaedortceletahtfostnemerus
􏰆aemneewtebyleritnerucconacedortcele narevospatregn􏰏kciuq􏰅hguonetsaf
tonsietargninnacsehtfi􏰅revewoH􏰇rulbnoitomtibihxetonodsegami ytimixorp
STMtahtsiemitnoitargetnitrohsylevitalers􏰛STMehtfoegatnavdanA􏰇stuodaer
neewtebdoirepehtfotsomrevolexiphcaetasnotohpgnimocnietargetni hcihw
saremacoedivnidesuyllacipytsyarraDCCehthtiwstsartnocsihT􏰇sdnocesillim
ytnewtotnetfodoirepgninnacsyarralatotehtotderapmocsdnocesorci mderdnuh
wef afodoireptrohs ylevitaler arevoderusaemsi edortcele hcaefoecnaticapac
ehteroferehTcitsiretcarahctnatropmirehtonA
STMnorulBnoitoMoN􏰣􏰇􏰈􏰇􏰉
􏰋􏰋
regn􏰏wenehT􏰇egamitxenehthtiwsnoitalerrocrofetalpmetecnereferasadesu
sipitregn rofsruotnocevitcadnastnenopmoclapicnirpcilcesuometalumesuht
dnaspatregn􏰏􏰖raeh􏰩otksedehtrednudecalpsienohporcima􏰅ecafrusksedeht
hcuotyllautcasregnsaremac
ehT􏰇kseds􏰛resuehtevobahgihdetnuomsaremacelbamoozdnarotcejorpneercs
retupmocahtobsniatnocmetsysehT􏰇stnemucodlatigiddnarepaphtiwnoitcaretni
gninibmocrofxoreXtadereenoipmetsysasiafnoitcetedpitregnevninagninrael
dnaspitregn􏰏gnidn􏰏yb􏰅yltceridevigsevolgatad hcihwnoitamrofni􏰅selgnatnioj
regn􏰏revocerotseirtosladamhA􏰇aremacdnadnahneewtebecnatsidehtsetacidni
ylhguor naissuaGderetnecahtiwdethgiewaerahctapdnahlatotehT􏰇sehctap
dnahdetnemgesehtotsespillegnitt􏰏ybdeniatboerasnoitisopregniF􏰇egamieht
elpmasbusylevitpadadnanoiger hcraes niksehtti miltsumti􏰝dnocesrepsemarf
􏰠􏰊􏰜setaremarfhtiwpupeekottub􏰅smargotsihrolocniksnwonkotsehctapegami
gnihctamybdnuorgkcabehtstnemgessmetsysgnigami
dnahelitcatdnadesabllautcaewtaht
seitreporpeht catagnizylanaapuorgedortcelehcaerofnoitazilaitiniecivedgnirud
detarbilacerasdlohserhTrtceleralugnatcerotnidedividbusylevisrucersiyarraehT􏰇stnemerus
􏰆aemecnaticapacedortcelefonoitammusdnagnipuorggolanayberawdrahnignidlo
􏰆hserhtdnagniretl􏰏esionsenibmoc􏰪􏰢􏰢􏰨mhtiroglagninnacseertyranibs􏰛eeL
gninnacSeerTyraniB􏰈􏰇􏰉􏰇􏰉􏰇􏰉
􏰇sedortcelenmulocdna worllafostnemerusaemmorf
diortneclabolgaetupmocylpmisyeht􏰥snoigertcatnocyhse􏰐otniegamiehttnemges
otevahtonodtubnoitatpadates􏰎oezilituodsdaphcuotevitcejorpregn􏰏􏰆elgnis
tahtetoN􏰇tcatnocyhse􏰐fosnoigeryfitnediotdedlohserhtebylpmisnacegami
ytimixorpeht􏰅deretl􏰏siesionlacirtcelednatnuoccaotninekaterastesol A􏰇emitrevoegnahcotdetcepxetoneraseitimrofinu
􏰆nondnuorgkcabhcusnastnenopmocetercsidfosecnaticapaccitisarapehtniseitimrofinu􏰆nonlaitaps
􏰅revewoHcitsalprorepaP
􏰇dnuorgkcabehtnielbisivebotdetcepxetonerastcejbosuoenartxeesuacebreisae
hcumsisyarraedortcelemorfsegamiytimixorpfonoitatnemgesdnuorgkcaB
noitamroFegamIytimixorProfsdohteMttimilnoitalerrocehtfostsoc
lanoitatupmocehthwtfihsegamietalpmetfotnuomaehtybdetacidnisinoitisop
ac O
stes􏰎olacolehtnehT􏰇nelcycnacsgnirudjnmulocortcelehcaesetadpuylsuounitnocSTMeht􏰅tnesbaebot
nwonkerasregn􏰏ehtnehwemitaebtonyamerehtecniS􏰇draobehtnoydaerlaera
sdnahs􏰛resuehtfinoitazilaitiniecivedgnirudliaflliwnoitarbilactesbegamiytimixorpetelpmoca mrofotnacsedortceleecrofeturbasyolpme
STMehtnoituloser􏰆wolaneveotderapmocllamsllitssi􏰖slexip􏰩
forebmuneht􏰅egral meestiekamyamyarraedortcelenarofyrassecenstnenopmoc
etercsidforebmunehthguohttahtdnimnipeeKegn􏰏lanoitiddaehtfothgilniyllaicepse
􏰅elbigilgenemocebevahsnrecnocdaehrevogninnacsehttahttliubsawepytotorp
s􏰛eeLecnishguonedesaercnievahsdeepsgnissecorpgolanadnalatigidhtoB
gninnacSyarrAeturBeleegralehtgnomatsolebnacstcat
nipuorgdnacapacedortceledesrucerehtfodiortnec
ehtsadetupmocsinoitisopregnhcaernoisrucerehtecnOoepahslavo
ehtt􏰏ylroophcihwspuorgralugnatcerotdeniartsnocebnahtrehtaredortceleyb
edortceleepahstcatnocregnohserhterofebesiontabmocotsedortcelefospuorg
ralugnatcereritnefosecnaticapacehtdegarevayllacirtcelesadeledomebnacstes􏰎oehtevobasgnidaer
ynaosecnaticapacenilesab muminimehtottpadaylkciuqstesrof
desaercedrehtrufsi 􏰏􏰅sihttneverpoT􏰇edafotraeppalliwsetunimwefadraobeht
nognitsertfelerahcihwsdnahitagenelgnisot ytinummisedivorpnoitarepoxamehterehw
jijijijiji
r wolsyrevgniwollaybdnaylevitucesnocdaer
eraseitimixorpwoleerhttsaeltanehwylnostesitagentahtsi dohtemsihtforegnad
ehT􏰇detfilerasregn􏰏sanoossagnisaercedybsevlesmehttcerroclliwstesworjijiji
	E
􏰤nehtsiEegamidetcerroctceridehtdna􏰅dnahnevigafobmuhtehtdrawotsnaem􏰖renni􏰩noitceridehT
􏰇ecafrusehtotlamronsixazanierusserpsadeterpretninehterastnemerusaem
ytimixorP􏰇enalpecafrusehtnihtiwsexalanoitceridydnaxotrefer􏰖lacitrev􏰩
dna􏰖latnoziroH􏰩􏰇sregn􏰏tnecajdaneewtebecnatsidehtotecnereferniton􏰅ecafrus
ehtdnatrapdnahaneewteberusserproecnatsidehtotecnerefernidesuebylno
lliwahtniderutpacegamiytimixorpeht
fo􏰖lexip􏰩enosetutitsnocelcycnacsralucitrapagnirudedortceleenotaderusaem
atadytimixorpeht􏰅swolloftahtnoissucsidehtnI􏰇segamielpmasesehtniezis
edortceleehtybdetacidninahtsyarraedortceleren􏰏eriuqerylraelclliwnoitingocer
gnitirwdnahsahcussnoitacilppaniatreC􏰇pamytisnetnielacsyargnevigehthtiw
elbisivebtonotsawoloseraslevelesionlacirtcelednuorgkcabdna􏰅egami hcae
morfdevomerneebydaerlaevahstes􏰎orosnesdnuorgkcaB􏰇llatapuwohsecafrus
ehtfosretemilli melpuocanihtiwstcejboevitcudnocylno􏰅segamilacitpoekilnu
􏰥stcejbodnuorgkcabybderettulcnuyllatoterasegami ytimixorpehttahtecitoN
􏰇yletelpmocerommargolellarapehtspalrevodna􏰅erusserpdnahoteudecafruseht
tsniagasesserpmocnidahSahfoseitreporplacipytetartsullioT
segamIytimixorPdnaHfoygolopoThttcetedotlexipdesu􏰎idhcaeotstsetmumixamlacoldnadlohserhtecnacirtnecce
E􏰇sleeh
mlapretuodnarenniehtsetarapesylraelcyellavyti mixorproesaercegrala􏰅ecafrus
ehttsniagadehsupyllanoitnetnisimlapehtforetnecehtsselnU􏰇yllanogaiddetneiro
dna􏰅gnolboyldlim􏰅egraletiuqebotdnetsleehmlapesehT􏰇egamiehtfomottobeht
ssorcastcatnocegralyrevforiapehtesuacsleeh mlapretuodnarenniehT
sulcllamssaelbisivoslasi
sesullacmlaperofehtmorfhsenoitaludnuesnetniylralucitrapeht
sasraeppa
tce􏰎esihtserucsbosedortcelemargolellarapehtybgniraemshguoht􏰅nmulochcae
gnolaytimixorpnisnoitaludnuthgilsesuacstniojregnspitregn􏰏ehtdna􏰅ecafrusehtgnihcuoteblliwstrap
esehtfowefaylnosecnatsniynamnitub􏰅dnahenofomottobehtmorfecafruseht
hcuotnachcihwstrapdnahehtfollasedulcniegamidnahdenettaf dna􏰅egami ytimixorptnerrucehtnisraeppadnaecafrusehtsehcuot
14
12
10
8
6
4
2
0
−2
−4
−6
Index Fingertip
Middle Fingertip	Ring
Fingertip
Proximal Phalanges
Forepalms
Thumb
Inner Palm Heel
Pinky Fingertip
Outer Palm Heel
0 2 4 6 8 10 12 14 16 18
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
otnodenettatal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰈􏰌
tnatsnocylriafniamershtdiwregn􏰏eht􏰅revewoH􏰇sedortcelemargolellarapehtyb
gniraemslacitrevroftonfiralucricraeppadluowdna􏰅􏰡􏰇􏰉erugiFni nahtretrohs
hcumraeppasregn􏰏ehtgilserasregn􏰏
ehtesuaceB􏰇elbisivtondnaecafrusehtevobaraferasmlaperofdnasegnalahplam
ehtotlamroneraslianregnoglanoitatnemges ynAstniopelddaseht􏰅gnicapsedortcelelatnozirohnevigeht
tA􏰇mehtneewtebstniopelddasrosyellavytimixorpthgilsybylnoelbahsiugnitsid
ebotsarehtegotesolcoseraspitregnsasaraeppaotregn􏰏xednidnabmuhtesuac
yamsedortceleegdewehtybgniraemslacitrevtublsuoiverpehterakrowsihtnideredisnocebotsemertxe
owtehT􏰇tluafedehtmorfyravhcihwsnoitisopatxujdnasepahstcatnocetarelot
tsumti􏰅serutsegdnahfoegnarediwatroppusotmetsysgnikcartaroF
seitreporPegamIdnaHdesolCyllaitraP􏰊􏰇􏰊􏰇􏰉
􏰇􏰋retpahCfotcejbusehtsiegnellahcsihT􏰇noitatressidsiht
nidebircsedkrowehtfomelborpgnignellahctsomehtsesopsegamiytimixorpeht
morfgnissimsierutcurtsdnahgninevretninehwytitnedidnahneveroregn􏰏gnihsil
􏰆batseylbaileR􏰇noitingocererutsegdnahfosmretnignisnesytimixorpeviticapac
fognimoctrohsniamehtoslasitidemretnifokcal hcuselihW􏰇secneuqesnocrehtrufsahegami
ytimixorpgnirruccoylnommocsihtniraeppatonodhttcafehT
􏰇degnahcnusisleehmlapehtneewteb
noitarapesehttubitregn􏰏eht􏰅esacsihtni
􏰅esuacebsedortceleleehmlapdnabmuhtlartnecehtsakradsaraeppatonodpit
􏰆regn􏰏hcaeforetnecehttasedortceleeht􏰅oslA􏰇noitagnoletcatnocfosseldrager
14
12
10
8
6
4
2
0
−2
−4
−6
0 2 4 6 8 10 12 14 16 18
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰊􏰌
􏰇rehtegot
dehsiuqsspitregnecalpylralimissidnadraobyek
retupmocdradnatsaforotcafmrofehtsahepytotorpSTMehttahtneviG
noitoMdnaHfosegnaRelbatrofmoC􏰌􏰇􏰊􏰇􏰉
􏰇sregn􏰏dehcnipehtnrecsid
yletaruccaotdedeensisedortcelemargolellarapgniraemsyllacitrevtuohtiwyarra
rosnesnoituloserrehgiha􏰅noitac􏰏idomlaminimhtiwnoitarug􏰏nocpirgnepehtot
dnetxenoitatressidsihtnidetneserpsdohtemnoitac􏰏itnedidnanoitatnemgestcat
ratnemegnarradnaezisnisecnereahtregralraeppaoslaselkcunkehTdnugnilrucehT􏰇spitregn􏰏ehtfodaetsniecafruseht hcuotyllautcasregn􏰏eht
fopoteht	morfselkcunkeht os􏰅ts􏰏agnikamfisarednudelrucerasregntsniecafrusehtgnihcuoterayehtesacsihtnitub􏰅nep
agnidloherewyehtfisarehtegotdehcniperapitregnsi hcihw
􏰅noitarug􏰏nocpirgnepani dnahthgirafoegamiytimixorpasisstcatnocmlapnihtiwaminimlaitrapllA􏰇saeraegralriehtssorca
ytimixorphse􏰐roniagrosnesnisnoitairavronimoteudaminimlaitrapsuoirups
niatnocyamhcihw􏰅smlapfosdeennoitatnemgesehthtiwstci􏰐nocsihT􏰇detceted
eboslatsumsnoitceridlanogaidniaminimlaitrap􏰅detatorsiworpitregn􏰏ehtesac
14
12
10
8
6
4
2
0
−2
−4
−6
0 2 4 6 8 10 12 14 16 18
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰌􏰌
􏰇nepagnippirgfisasleehmlapehtsdrawotrednudelrucsregn
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
F􏰇noitisoptluafedstinisidnahtfeleht
nehwdnahthgirehtfonoitisopdrawtfel mumixamehtswohs􏰊􏰈􏰇􏰉erugiF􏰇rehtona
enopalrevororevossorcsdnahehttelotdetcepxetonerasrotarepotahttcafeht
ybdeti milrehtrufnevesinoitisopdnah􏰅ecafrusehtnoerasdnahhtobnehW
􏰇sutarappaehtgnicaftonsiosrots􏰛rotarepo
ehtfiroydobelohwehtfosnoitrotnochguorhtelbissopylnoerasnoitatorrehtruF
􏰇􏰉􏰈􏰇􏰉erugiFnidnahthgirehtrofnwohssa􏰅sixaecafruslacitrevehtotlellarap
mraerofhtiwnoitisopdnahtluafedeht morfsrucconoitatordrawtuoroesiwkcolc
􏰇timillacinahcemoibstiotesiwkcolc􏰆retnuocdetator
dnaecafrusgnisnesfotfelraftadnahthgirfoegami ytimixorPawniehtsezimixamnoitautissihTinehwrucconacnoitatordrawnimumixam
sti􏰅ecafrusehtnosi dnahenoylnonehW􏰇detimilylriaferanoitarepolamron
14 12 10
8 6 4 2 0
−2 −4 −6
14 12 10
8 6 4 2 0
−2 −4 −6
−15 −10 −5 0 5 10 15
Horizontal Position on Surface (X axis cm)
−15 −10 −5 0 5 10 15
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)	Vertical Position on Surface (Y axis cm)
􏰡􏰌
􏰇titsniaga
pudnahthgirdnanoitisoptluafednidnahtfelfoegamiytimixorP􏰤􏰊􏰈􏰇􏰉erugiF
􏰇timillacinahcemoibstiotdrawtuodetator
dnaecafrusgnisnesfothgirraftadnahthgirfoegami ytimixorP􏰤􏰉􏰈􏰇􏰉erugiF
14 12 10
8 6 4 2 0
−2 −4 −6
−15 −10 −5 0 5 10 15
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰢􏰌
􏰇snisuocdaphcuotynitstisahgihsasoitaresion􏰆ot􏰆langisevahnac
STMegral a􏰅devomerebtsumhcihwtes􏰎orosnestnatsnocasahedortcelehcae
hguohT􏰇sdnahhtobfosedisrednuehtmorfstcatnocforebmunynafosnoitisop
ehtenimretedyleuqinudnatcetedottneic􏰑ussisedortcelednasuohtwefafoyar
􏰆ranA􏰇rettulcenecsdnuorgkcabfoecnesbadna􏰅noisulccopitregn􏰏fonoitneverp
tnavdaesehT􏰇seuqinhcetgnisnesnoitom
dnahrehtorevosegatnavdaynamsahgnisnes yti mixorpdesabsnesevitcaehtfopotehtrevoeiloslanacspitregnoosnwod
devomdnahthgirdnanoitisoptluafednidnahtfelfoegamiytimixorPhtfoelddimmottobeht
14 12 10
8 6 4 2 0
−2 −4 −6
−15 −10 −5 0 5 10 15
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰣􏰌
􏰇smetsysdesab􏰆oedivnahtnoitingocererutsegetaruccaeromdnaretsaf
hcumtroppusnacmetsysgnigamiytimixorpehtdnarotarepoehtfosnoitasnes
ehtneewtebecnednopserrocesolcorpdnahhguohtnevE􏰇ecafrusehtssorca
dnahehtgnidilsdnagnigagnefosnoitasnesnwos􏰛rotarepoehtotyltcaxetsomla
dnopserrocSTMehtybderusaemslangisytimixorpehT􏰇serutsegdnahecapseerf
fodiovkcabdeefehtdnakcitsyojrodraobyeklacinahcemafokcabdeefecrofdna
elitcathcirehtneewtebesimorpmocderolpxenuylsuoiverpasrenahthgirdnanoitisoptluafednidnahtfelfoegamiytimixorPospuorgotniega
􏰆mi ytimixorphcaepugnikaerbrofelbisnopsersieludomnoitatnemgesehT
􏰇sretpahchtobfoyduts yb
dootsrednuebylnonacelohwasametsysehtfogninoitcnufeht􏰅seludomgnikcart
htapdnanoitatnemgestcatnocehtnoetartnecnoclliwretpahcsihtelihwsuhT􏰋noitceS
nidebircsedsetamitsenoitisopdnaheht􏰅etacidni􏰈􏰇􏰊erugiFnishtapkcabdeefeht
sA􏰇seludomrehtomorfkcabdeeffoplehehthtiwdeppartstoobebnetfonaceludom
anihtiwdevloserebtonnachcihwseitiugibmadna􏰅snoitatnemgesden􏰏edylluferac
􏰅tnetsisnocnopusegnihnoitac􏰏itnedidnagnikcarthtapfoycaruccaeht􏰤tnedneped
􏰆retniylhgiheraseludomehTdetneserpsmhtiroglaeht􏰅revewoH􏰇noitac􏰏itneditcatnocesacsihtni
ro􏰅noitingocertcejbodnactsomfotaht
selbmesererutcetihcrametsysllarevoeht􏰅swohs􏰈􏰇􏰊erugiFnimargaidmetsyseht
sA􏰇detacitsihposetiuqebtsummetsys gnikcarteht􏰅sdnahhtobybnoitalupinam
cidrohcsuoenatlumisrognitsertroppusotredronIhT
GNIKCART
HTAPDNANOITATNEMGESTCATNOCDNAH
􏰊retpahC
IMAGE SEGMENTATION
PATHS FROM PREVIOUS IMAGES
CURRENT PROXIMITY IMAGE
PARAMETERIZED ELECTRODE GROUPS
NEW PATHS &amp; UPDATED PATH PARAMETERS
IDENTIFIED CONTACT PATHS
ESTIMATED HAND &amp; FINGER OFFSETS
CONTACT PATH TRACKING
HAND IDENTIFICATION
FINGER &amp; PALM IDENTIFICATION
HAND POSITION ESTIMATION
z-1
􏰈􏰍
􏰇seludo m
noitacraperutaefdnaorez
ottessretemaraptnemerusaemllahtiwnoitaitnatsnitcejbotluafedrollunarof
devresersimoc􏰪n􏰨 􏰉P􏰵􏰪n􏰨 􏰈P􏰅􏰇g􏰇e􏰥rettellatipacehtgniwollofyletaidemmixed
􏰆nilaciremunaybdehsiugnitsiderasnoitaitnatsnitcejbohtapropuorgralucitraP
yx
detadpuerahcihwsretemaraPrapralucitraP􏰇elohwasa dnaharof
Hdna􏰅smlapro􏰅sregn􏰏
􏰅bmuhtsahcus dnahehtfostraprof	F􏰅seirotcejartroshtaptcatnocrof	P􏰅sedort
􏰆celefospuorgrofGybdetonedsepytatadtcejbolevel􏰆hgihruoferaerehT􏰇Kfo
stpircsbusnide􏰏icepserasdlohserhtdnastnatsnocgnilacslabolGcehtdnagnissecorpegamiytimixorplevel􏰆wolniesirahcihwsyarralanoitnev
􏰆nocehthtobebircsedyltnetsisnocotdengisedsinoitatonlacitamehtamehT
sepyTelbairaVrojaMdnanoitatoN􏰈􏰇􏰊
􏰇􏰢􏰇􏰉erugiFfosnoitisopregn􏰏tluafedeht otevitalerstes􏰎o
noitisopregn􏰏dnadnahllarevofoetamitseevitavresnocasniatniamrotamitsenoitis
􏰆opdnaheht􏰅yllaniF􏰇dnahanihtiwtcatnochcaeotytitnedileehmlapro􏰅pitregn􏰏
􏰅bmuhteuqinuasngissaeludomnoitac􏰏itnediregn􏰏eht dna􏰅tcatnocecafrus hcae
sesuacdnahhcihwsedicedeludomnoitacytilibisnopsertnatropmiehtsah
eludomgnikcart htapehtj
􏰆arttcatnocdnahhcaegnolayticolevehtsetupmocnehttI􏰇tcatnocdnahemas
ehtotdnopserrochcihwsegamiytimixorpevisseccusmorfspuorgehtseirotcejart
otnisknileludomgnikcart htapehT􏰇puorghcae	morfserutaefcirtemoegstcartxe
􏰊􏰍
􏰇segami tneuqesbusrof
snoitatnemgestcerrocgnicudorpruccoodtaht
seruliafehToitatnemgesfogninuttneicTtanibmoceht
nostniartsnocehtnekaewdluowspuorgfognittilpsrognigremsuoenorrefotnare
􏰆lotylhgihebotssecorpnoitac􏰏itnediehtgningiseD􏰇gnorweblliwsnoitac􏰏itnedi
tcatnocehtfoemostsaeltadna􏰅gnippameno􏰆ot􏰆enoaebregnolonlliwgnippam
ytitneditcerroceht􏰅spuorgelpitlumotnitrapdnahenomorftcatnocehtstilpsro
strapdnahelbahsiugnitsidowtfostcatnocehtsegremylsuoenorressecorpnoitat
􏰆nemgesehtfIewtebgnip
􏰆pamtcerrocehttahtemussassecorpnoitac􏰏itnediregn􏰏ehtfosmhtiroglanoitazi m
􏰆itpolairotanibmocehT􏰇mrofrepotdetcepxeerasrotarepohcihwsnoitarug􏰏noc
dnahfoegnareritneehtrofsselwa􏰐ebsnoitatnemgestahtevitarepmisitiStcatnoCehtotnoitcudortnI􏰈􏰇􏰉􏰇􏰊
􏰇gnignellahcsniamerrehtonaenomorfdetarapesdnadepahsylsuoiraverahcihw
stcatnocyhse􏰐gnitnemgesyletarucca􏰅detcepxeeraenonesuaceblaivirtsisegami
ytimixorp morfstcejbodnuorgkcabgnivomerhguohT􏰇noitaziretemaraptcapmoca
aivmetsysgnikcartehtfoslevelrehgihotpuorghcaetneserperotnehtdna􏰅leeh
mlaproregn􏰏emaseht􏰅􏰇e􏰇i􏰅trapdnahelbahsiugnitsidemasehtylrednuhcihw
slexiprosedortceleesohtpuorgotsinoitatnemgestcatnocfoevitcejboehT
noitatnemgeStcatnoCatnemges
egamiehtfoweivrevonasedivorp􏰉􏰇􏰊erugiFnimargaidwo􏰐atadehT
ssecorPnoitatnemgeSehtfoweivrevOof sevitaviredlaropmetoitapssahcusseititnauqdetacilpmoc
etupmocsdohtemroirpesehT􏰇sdnahfosegamilacitporofsdohtemnoitatnemges
roirpsaevisnepxeyllanoitatupmocsaylraentonsinoitatnemgesegamiytimixorp
􏰅􏰡􏰌􏰅􏰉􏰋􏰨sdohtemgnigrem
dehsretawotdetalererasdohtemnoitatnemgesSTMehtelihwsuhT􏰇snoitcerid
llaniton􏰅retnectcatnocehtotevitaler hcraesfonoitceridehtgnoladetupmoc
ebdeenylnosevitaviredlaitapslaitrap􏰅tcatnocehtforetnecehtotlexipegdena
morfgnissaprotcevehtotylralucidneprepdetneiroebotdnetstcatnocxevnocfo
segdeecnis􏰅eromrehtruF􏰇tcatnocrepslexipnezodatuobagniniatnocegamiehtfo
snoitropllamsylevitalerotsnoitatupmocnoitcetedegdeehtsen􏰏nocamixamlacol
morfdrawtuognihcraesesuacebdaehrevolanoitatupmocelttilylgnisirprussrucni
ssecorpgniworgnoigersiht􏰅xelpmocerasnoisicednoitcetedegdefi nevE
􏰇detnemgesgniebstcatnocfosepytehttuobakcabdeeflautxetnoc
fotrosemosnodnepedtsumnoisicedsiht􏰅snoitarug􏰏nocdnahfoegnarediwafo
noitatnemgeselbailerroF􏰇stcatnocneewtebegdenasase􏰏ilauqtahwyltcaxeediced
otnehtsinoitatnemgesegamiytimixorpfoegnellahcyramirpehT􏰇slexipdeesrof
setadidnacsuoivboehteraamixamlacoL􏰇􏰪􏰉􏰡􏰈􏰅􏰠􏰡􏰅􏰢􏰍􏰅􏰊􏰈􏰨sdohtemnoitatnemges
gniworgnoigersanoisivretupmocninwonkyllarenegeraslexipralimisrofdrawtuo
hcraesdnalexipdeesatatratshcihwsmhtiroglA􏰇slexipyradnuobpuorgsasegde
ehtgnikram􏰅segdetcatnocrofsedortcelemumixamlacolmorfdrawtuohcraesotsi
noitatnemgesegamiytimixorpothcaorppatneicoretnecehttasedortceleehtecniS
CURRENT PROXIMITY IMAGE
SMOOTHED PROXIMITY IMAGE
LOCAL MAXIMUM PIXELS
FLATTENED FINGERTIP FEEDBACK
ESTIMATED HAND POSITION OFFSETS
DIFFUSE CURRENT IMAGE
SEARCH FOR SIGNIFICANT LOCAL MAXIMA
DEFINE SEGMENTATION STRICTNESS REGIONS
CONSTRUCT ELECTRODE GROUPS AROUND LOCAL MAXIMUM PIXELS
COMBINE OVERLAPPING GROUPS
FIT ELLIPSES TO COMBINED GROUPS
PARAMETERIZED ELECTRODE GROUPS
􏰌􏰍
􏰇ssecorpnoitatnemgesegamiytimixorpehtfomargaidwofaybslevelmetsys
rehgihtadetneserperyllufgninaemebnacpuorghcaeos􏰅slavoelbmeserylhguorlli w
spuorgtsom􏰅detnemgesylreporpfI􏰇puorgedortcelehcaemorfsretemarapnoitisop
dna􏰅ezis􏰅epahstcartxeotsissecorpnoitatnemgesehtfoegatstsalehT
􏰇spuorgrehtofoamixamlacol ehtniatnochcihwspuorg
gnippalrevosenibmocegatsgnigremaacnI􏰇spuorgelpitlumotninekorb
tonsileehmlaphcaetahtdnapuorgelgnisaotnideniojtoneraspitregn􏰏tnecajda
tahtserusnesihT􏰇raeppaotdetcepxeerasmlaperehwegamiehtfosnoigerniselur
noitcetedegdeyppolsseilppatituboitatnemges
eht􏰅etamitsenoitisopdnahaneviG􏰇stnemerusaemthgiehtcatnocregn􏰏dnasetam
􏰆itsenoitisopdnahfo mrofehtnideilppuserasesylanalevel􏰆hgihesehT􏰇segami
suoiverpfosesylanalevel􏰆hgihfo􏰎ospartstoobmetsysnoitatnemgeseht􏰅stcatnoc
dnahfosepyttnere􏰎idfosdeennoitcetedegdegnitci􏰐nocehtevloser oT
􏰇egamielohwehtssorcaselurnoitcetedegde
emasehtevahottneicfinu􏰆nonronimoteudaminimlaitraptibihxeyam􏰅dnah
rehtoehtno􏰅stcatnocleehmlapegraL􏰇egdenaderedisnocoslasisregn􏰏tnecajda
neewtebtniopelddasro muminimlaitrapa􏰅yellavytimixorpthgilsaybdetarapes
ylnoeraspitregn􏰏esacnitub􏰥sleveldnuorgkcabotsllafytimixorpdesu􏰎idnueht
erehwslexiptadetacidnisyawlaerasegdE􏰇tcatnocyhse􏰐tinuaotsdnopserroc
E
hcihw Gsedortcelegnirobhgienfopuorgxevnoc􏰆imesamumixamlacoltnac􏰏ingis
hcaednuorastcurtsnocmetsysehT􏰇lexipmumixamlacoladeredisnocsislexip
robhgientseraenthgiellafoseitimixorpdehtoomsehtdnadlohserhtecnac􏰏ingisa
sdeecxeytimixorpdehtoomsesohwlexipynA􏰇sevitaviredlaitapsfonoitatupmocni
dnaslexip mumixamlacolrof hcraesehtnidezilitusiegamiytimixorptnerruceht
􏰡􏰍
sa􏰅yrassecensignihtoomsthgilsylno􏰅srosnesytimixorpehtfooitaresionsiondetalosinatahtecnahcehtecuderot
􏰤st􏰏enebowtsahsuhtgnihtoomsehTamixam
􏰇slexiprobhgientseraenthgiellafoseitimixorpehtdna􏰅 K􏰅dlo
􏰆hserhtecnacwteb
tolaraemsydaerlasedortceleegdewehtdnamcoitubirtnoceht nahteromrevensisrobhgienruofehtfonoitubirtnocehttahtos
yx yx
setar
yx
ehtatnemgesehtgamIytimixorP􏰊􏰇􏰉􏰇􏰊
􏰒􏰷z
􏰇Gyticirtneccedna􏰅Gnoitatneiro􏰅Glangisytimixorp
􏰢􏰍
ebotsahenilgnidividsihtfotnemecalpehtos􏰅sregn􏰏ehterofebnwodhcuotyam
smlapehtsesacerar niro􏰅draobehtgnihcuotebtonyamsmlapehtecitcarpnI
􏰇ylppadluownoitatnemgesmlapyppolshcihwwolebdnaylppadluowselurnoitat
gnidivid
latnoziroh AmetsysehtfI􏰇bmuhteht
evobaraeppayllausudnasmlapehtevobaeilsyawlaspitregn􏰇noitisopdnahdetcepxedna
stniartsnocnoitisopatxujtcatnocezilituotsi hcaorppaevitce􏰎etubrelpmis A
􏰇noitatnemgesylekiltsomeht kcipoterudecorpnoitaulavenatcurtsnoc
otyrtdnasesehtopyhnoitatnemgeselpitlummrofotebdluowhcaorppaevisnepxe
yllanoitatupmocdnaemosrebmuc A􏰧detelpmocsahnoitatnemgesretfaderusaem
ebylnonac noitisopdnaepahstcatnocevitalersahcusserutaeflautxetnoc nehw
yppolssusrevtcirtsebdluohsnoitatnemgeserehwedicedmetsysehtnacwoH􏰇segde
tcatnocmlapsaretsigerdluohsytimixorplevel􏰆dnuorgkcabhtiwsedortcelerosleeh
mlapneewtebsesaercegralylno􏰅sdrowrehtoni􏰥ylippolsdetnemgesebdluohssmlaP
batseieritne
ehtssorcamrofinutonerasdeennoitatnemges􏰅􏰊􏰇􏰊􏰇􏰉noitceSnidetnihsA
snoigeRssentcirtSnoitatnemgeSsehtfosegatsrehtollanahttsoclanoitatup
􏰆moceromsrucniti􏰅egamielohwehtotdeilppasi hcihwamixamlacolrofhcraeseht
sedisebnoitarepoylnoehtsi gnihtoomsesuaceb􏰅sselehtreveN􏰇spitregn􏰏tnecajda
neewtebsyellavytimixorpehtserucsbognihtoomshcumooT􏰇􏰍􏰇􏰊erugiFnidetacidni
􏰣􏰍
fosseldragergeryppolsehtfoseiradnuob
rewoldnaretuoehtmrofecafrusehtfosegdeedistuoehT􏰇sbmuhtevitcepserrieht
fosnoitisoplacitrevdetamitseehtraendecalperasenilgnidividlatnozirohehtdna
􏰅eilotdetcepxeerasregn􏰏xedniehterehwsnmulocyarrarosnesehtfoedisnitsuj
ebotyllaciripmednuof neebsahsenil gnidividlacitrevehtfosnoitisoptsebehT
􏰇segdetcatnocsaevrestsumstniopelddasytimixorperehw􏰅noigernoitatnemges
tcirtsehtsigniniamernoigerdepahs􏰆TehT􏰇derongiylegraleraaminimlaitrap
erehw􏰅snoigernoitatnemgesyppolsthgir dnatfelehttneserpersrenrocrewol eht
niseniltuoralugnatceR􏰇dnahhcae nileeh	mlapdnaregn􏰏hcaefonoitisopdetam
􏰆itseehtetacidnimargaidehtnisngissulP􏰇sdnahhtobrofstes􏰎odetamitseeht
gniorez􏰅snoitisoptluafedrieht ni erasdnahehta􏰊􏰇􏰊erugiFnI􏰇stes􏰎odnahdetam
􏰆itseehthtiwetalsnartsnoigernoitatnemgeseht wohsetartsulli􏰊􏰇􏰊erugiF
􏰇ecafruseht osnoitisopegarevaehtskcart􏰅noitisop
dnahtluafedehtotdezilaitinisietamitsenoitisopdnahhcaeyllacisaB􏰇segami
suoiverpmorfseititnedidnasnoitisoptcatnocgnisusetamitseevitavresnocsevired
eludomnoitamitsenoitisopdnaheht wohliatednisebircsedludomnoitamitse
noitisopdnahehtmorfkcabdeeflevel􏰆potsaelbaliavaerasetamitsenoitisoP
􏰇dnahhcaerofyltnednepednidecalpeb
dluohssredividnoigeryppolseht􏰅yllacirtemmysdenoitisoptonerasdnahehtesacnI
􏰇detcepxeerasbmuhterehwsnoigeredulcxeotsredividlacitrevybden􏰏nocrehtruf
ebdluohssnoigeryppolseht􏰅smlaprospitregn􏰏htiwgnigremmorfstcatnocbmuht
tneverpotsbmuhtehtotylppaosladluohsnoitatnemgestcirtsecniS􏰇tadesseug
15 10 5 0
−5 −20 −15 −10 −5 0 5 10 15 20
a)
15 10 5 0
−5 −20 −15 −10 −5 0 5 10 15 20
Strict Segmentation Region
Left Sloppy
Right Sloppy
Strict Segmentation Region
Right Sloppy
Left Sloppy
b)
15
10
5
Left Sloppy	0
Strict Segmentation Region
Right Sloppy
−5 −20 −15 −10 −5 0 5 10 15 20
c) Horizontal Surface Position (cm)
Vertical Surface Position (cm)
􏰠􏰡
􏰇draobehtyppolsthgirdnatfelehtfosnoitisopehT􏰤􏰊􏰇􏰊erugiF
􏰈􏰡
yeht􏰅draobehtfomottobehttanoigernoitatnemgesyppolsehtninwodhcuot
spitregn􏰏rehtegotdehsiuqsfi􏰅esiwekiL􏰇etamitsenoitisopdnahehttcerrocyllaut
􏰆nevednasmlapsamehtyfitnedierlliweludomnoitac􏰏itnediregn􏰏eht􏰅spitregn􏰏
sanoitac􏰏itnediriehthtiwtnetsisnocniebotsaegralossemocebspuorgesehtfo
epahs dnaezisehtfi􏰅revewoH􏰇spitregn􏰏gnippalrevofo worasadeh mlapaylnofI􏰇snoitisopdetamitse
riehtwolebroevobarafootnwodhcuotyldetcepxenudnahehtfostrapfielbissop
erasrorre􏰅draobehtdnuorasevomsdnahehtsasmlapehtrednusnoigeryppols
ehtgnipeektadoogyrevyllarenegsirotamitsenoitisopdnahehthguohT
􏰇stcatnocrewefhtiwdnahehtfonoitisop
detamitseehtedirrevostcatnocecafrustsomehthtiwdnahehtgnittel􏰅rehtona
enootesolcoottegsnoigerthgir dnatfel eht dluohs􏰅dnasetamitsenoitisopdnah
ehtneewtebnoitarapeslatnozirohmuminimagnisopmiybdetnemelpmisitce􏰎e
sihT􏰇dnahetisoppoehtfonoigernoitatnemgesyppolsehtgniretnemorfdnah
enofosregnniamerdnahtfelehtnehwyleritneecafruseht􏰎oevom
etamitsenoitisopdnahdnanoiger yppolstfel ehttahtswohsclseht nillafsmlapehtylnodnasmlapehttaht
serusnesihT􏰇tfelrewolehtdrawotsevomdnahtfelehtdnatfelreppuehtdrawot
sevomdnahthgirehtsasnoitisopdnahdetamitseehtwollofsnoigeryppolsehtwoh
swohsboe
detalsnarteranoiger yppolsthgirehtfosenil gnidivideht dnal eht
fosenilgnidivideht􏰅snoitisoptluafedehtmorfyawaevomsdnahehtsA
􏰇􏰎otfil tanoitisopdnah
worxa mlacol
􏰉􏰡
worxa mlacol Gwolebdnaevobayltceridhcraesnehwtub􏰅 Glauqedluohs GesacsihtnI sujsnmulocehtfo
t xe n	verp
secidniehT􏰇􏰈􏰯j􏰦 jdna􏰈􏰞j􏰦 j􏰅tfelehtdrawotgnihcraesnehW􏰇stset
muminimlatnozirohnisedortcelegnirobhgienotgnirreferesurofdeniatniamera
txen verp
s􏰛mumixamlacolehtotdeddasiyradnuobehtgnihcaererofebderetnuocne
edortcelehcaE
Gehtgnolasdeecorpyllait
􏰆inidna􏰝􏰋􏰇􏰊erugiFniselcricdell􏰏􏰜secidniesehttastratsseiradnuobtcatnocrof
loc
x a ml a c o l
gnihcraeS􏰇mumixamlacols􏰛Gpuorgfosecidni nmulocdnaworehteb
G
worxa mlacol dna Gtel􏰅GpuorgnevigaroF􏰇yradnuobtcatnocehtrofgnitsetsnoitcerid
ruofnideecorpdnalexipmumixamlacols􏰛puorgamorftratshcihwsnrettaphcraes
drawtuolacipytstcipedsssecorpnoitatnemgeseht􏰅den􏰏edsnoiger
noitatnemgesyppolsehtdnadnuofneebevahslexipamixamlacolehtecnO
nrettaPhcraeSnoitatnemgeSorrenoitatnemgesehtenimaxerehtruf
lliwwortnerrucehtniytimixorpmumixamhtiw
lexipehtmorfwortxenehtotniecnavdaylnonachcraesehtesuaceB
􏰇yradnuobehtedistuodeecorptonseodhcraesehttahtecitoN􏰇ti
sretnuocne hcraesehtsapuorgeht ot deddasi	hcihwlexipastneserper
daehworrahcaEeht􏰅􏰇e􏰇i􏰅detcennocebtsum
wors􏰛puorganihtiwsedortceleehtfollaesuacebxevnococneneebevahseiradnuobnehW􏰇exedni
i
nmulocdna exedni	worhtiwEtesedortceleehtfoedortcelenaebeteL
lnahtrehtonaotedortceleenomorftfihsaminimlaitrapehtsa
sdiortnectcatnocniseitilibatsnieromesuacotdnuofsawtcatnochcaemorfsnoitub
ocstcatnoctnecajda
owtehtnoitroporptahwninwonktonsitiesuacebpuorgynanidedulcnireven
erasyellavytimixorpfomottobehttasedortceleaminimlaitraptahtetoN
wortobworpot
demmisworehT􏰇wolebroevobayltceridsedortceleot Gnmuloceht
nideilppaylnoerastsetesehT􏰇evitagenerastsetyradnuoblacitrevsagnolsa
ylevisrucersruccosworlanoitiddaothcraesehtfotnemecnavdA􏰇tcatnocehtfoegdir
lanogaidrolacitrevehtgnoladeretneebotdnetswortneuqesbussuhT􏰇mumixam
lacolehtfo worehtrofseodtisasnoitceridhtobni drawtuoyllatnozirohsdeecorp

worxa mlacol
swortneuqesbusnihtiwhcraeslatnozirohehTsebotsworerutuf
worxa mlacol
verp
i􏰅mumixamlacolehtfoworeht wolebroevoba
ver p
foxedniehT􏰇
G􏰦
yltceridsworesehtroF􏰇stsetmuminimlaitraplanogaiddnalacitrevnidiaot i
sadedrocerebtsumwordehcraesylsuoiverpehtfoxedniehtsworgnicnavdaerofeB
􏰇noitalopretnidiortneclacitrevrofpuorghcaenielbaliavaeblliwsworeerhttsael
􏰌􏰡
ecnac􏰏ingisamixamlacolehtfoflahtuobaro􏰅esiondnuorgkcabderusaemehtfo
snoitaiveddradnatsruofroeerhtottessidlohserhtecnacdlohserhtecnaclarutaN􏰇􏰌􏰇􏰊erugi Fni
trahcwo􏰐ehtybdetacidnisa􏰅noitceridrehtonaniemuserothcraesehtdnayra
􏰆dnuobpuorgasadekramebotlexipaesuactsetelbacilppaynanostluserevitisoP
􏰇snoigernoitatnemgesyppolsdnatcirtshtobotemasehtdeilppasihcihwtsetecnac
􏰆􏰏ingisytimixorpehtdnasnoigernoitatnemgestcirtssusrevyppolsrofre􏰎idhcihw
stsetmuminimlanoitcerid􏰤sessalcelurowtfotsisnocstsetegdetcatnocehT
stseTyradnuoBnoitatnemgeSohtiwstcatnochse􏰐foepahslavo
lacipytehtt􏰏ylesolcnacyeht􏰅ralugnatcerebtondeenspuorgedortceleecnistaht
sitniopyekehT􏰇puorgenootnidenibmocgniebmorf􏰝smlapsahcus􏰜sretsulcedort
􏰆celeedi wfo􏰝sregn􏰏sa hcus􏰜stoohs􏰎olacitrevelpitlumsegaruocsidoslatniartsnoc
ytixevnoc􏰆imesehT􏰇htdiwregn􏰏nisnoitaludnugnitarelotsahcussecneuqesnoc
worpot
ronimsahnrettaphcraesehtnotniartsnocytixevnoc􏰆imesehT􏰇ralacsera G
wortob 􏰪􏰨loctsal
dna Gsaerehw􏰅sworssorcare􏰎idnacstnemeleesohwsyarraera Gdna
􏰪􏰨l oct sri f
Gecnis􏰝a􏰋􏰇􏰊erugiF􏰜puorganihtiwdewollaerastnemgesnmulocdetcennoc
XIMITY > BACKGROUND LEVEL ?
A
GET NEXT ELECTRODE IN DIRECTION OF SEARCH
RETURN &amp; SWITCH DIRECTION OF SEARCH
MARK BACKGROUND LEVEL EDGE AS GROUP BOUNDARY
N
YB
N
IN STRICT SEGMENTATION REGION?
Y
SEARCHING	N HORIZONTAL ?
VERTICAL MINIMUM ?
NY
Y
MARK EDGE BETWEEN FINGERTIP AND THUMB OR PALM
SEARCHING HORIZONTAL ?
Y
N
HORIZ.	N DIST. TO LOCAL MAX
HORIZ.	Y OR DIAGONAL
MINIMUM ?
B
B
> 2cm ? Y
TALL HORIZONTAL MINIMUM ?
Y
N
MARK EDGE BETWEEN FINGERS
N
A
B
MARK EDGE BETWEEN PALM HEELS
􏰍􏰡
􏰇nrettaphcraesnoitatnemgess􏰛puorgaybderetnuocnelexiphcae
tadeilppaerahcihwstsetegdetcatnocehtgnizirammustrahc wolF􏰤􏰌􏰇􏰊erugiF
􏰡􏰡
􏰪verpi􏰨locxa m ruccootdetcepxesilexipmumixams􏰛wortnerruceht􏰅 Glexipmumixam
s􏰛worsuoiverpehtfonmulocehtta	wor ani snigebhcraeslatnozirohasA
􏰇sedortceleerauqsfoyarranoituloserrehgiha
rofde􏰏ilpmisebdluocyehttbuodon􏰥cohdatahwemosraeppayamyehteroferehT
􏰇yarraedortcelemargolellarapehtnospitregn􏰏tnecajdayllanogaidtnemgesdna
amini mlanogaidtcetedotyllaciripmedevloveevahstsetehT􏰇hcraesfonoitcerid
ehtnignisaercnistratsytimixorprevenehwdetacidniyllarenegsitniopelddasro
muminimlaitrapa􏰅􏰝􏰋􏰇􏰊erugiF􏰜mumixamlacolamorfyawasdeecorpsyawlahcraes
ehtecniS 􏰇hcraesfonoitceridtnerruceht nognidnepedsrobhgienlanogaiddna
lacitrev􏰅latnozirohotylppastsetaminim􏰆laitrapnoiger􏰆noitatnemges􏰆tcirtS
stseTaminiMlaitraPnoigeRnoitatnemgeStcirtS􏰉􏰇􏰍􏰇􏰉􏰇􏰊
􏰇spitregn􏰏ehtmorfyawasnoigernoitatnemgesyppols
ehtevomotsetamitsenoitisopdnaheht nisnoitcerrocesuacyllautnevelliwmeht
fonoitac􏰏itnedireporp􏰅segamiwefarofspuorgetarapesotnidetnemgestpekeb
nacspitregn􏰏tnecajdaehtfI􏰇detarapesylbailersnoigernoitatnemgesyppolsotni
rednawhcihwspitregnlavhcusfoamini mlaitraplatnozirohehterongistsetnoigeryppols
ecniS􏰇dlohserhtecnac􏰏ingis wolylbanosaerynaevobaniamerotsedortcelehcus
foytimixorpdehtoomsehtsesuacsretnectcatnocehtmorfnoisuyellavytimixorpehtni
levelytimixorpdnuorgkcabtaedortceleenotsaeltaevahllitsteynoigernoitatnem
􏰆gesyppolsani nwodhcuotyldetcepxenuhcihw􏰝􏰣􏰇􏰉erugiF􏰜spitregn􏰏tnecajdafo
gnigremstneverposlatsetecnac􏰏ingisehtniytimixorpdehtoomsnuehtgnisU
􏰇puorgeht
nidedulcniebotegdetcatnocehtedistuoslexipesuactonseod􏰝b􏰍􏰇􏰊erugiFees􏰜
aeratcatnoceurtehtfoedistuoytimixorpfonoisu􏰎idtahtserusnetsetsihtniyti
ecnacifingisamixam
􏰆mixorpdehtoomsnahtrehtardehtoomsnuehtgnisUtisop
􏰅tnenopmoclanogaidaevahoslastsetesehT􏰇ytimixorpmumixamehtevahotdnuof
􏰪i􏰨locxa m
wortnerrucehtnilexipehtfo G􏰦jnmulocehttadeilppaerastsetmuminim
txen
laitraplacitrev􏰅 iwortxenehtotecnavdaotrehtehwgnidicednehW
􏰇sregn􏰏ehtotesolcoottegsnoigernoitatnemgesyppolsehtfisregn􏰏
denetta􏰐rosbmuhtotdeniojgniebmorfsmlaptneverpsplehsihT􏰇derongisawdna
noigeryppolsehtniderruccotniopelddaslautcaehtfineve􏰅hcraesfonoitcerideht
nignisaercnieranoigertcirtsehtniseitimixorpfipotsotdecrofeblliwnoigertcirts
aotnisrednaemdnanoigeryppolsanistratshcihwhcraesa􏰅lexiptnerrucehtot
lauqeronahtretaergoslasilexipsuoiverpehttahtgnitsettuohtiwlexiptnerruceht
nahtretaergsilexiptxenehtfoytimixorpehttahtgnitsetylnoyB􏰇wortnerruceht
rofyradnuoblatnozirohagnihsilbatse􏰅noitceridtnerrucehtnihcraeslatnoziroheht
potsnactsetrehtiE􏰇dehcraesgniebtcatnocehtottnecajdayllanogaidtcatnocayb
desuacyellavytimixorplanogaidafoecneserpsetacidnidehcraesebotwortxeneht
j􏰥txenij􏰥itxenj􏰥ijridlacitrevdna􏰅lanogaid
􏰅latnozirohehtniytimixorpnisesaercnilarenegedulcniotdenekcalssitsetlatnozi
􏰆roheht􏰅lexip	mumixams􏰛 worsuoiverpeht	morf	yawarehtrafsnmulocroF
􏰇detcejerylsuoenorre
ebdluowsegdirlanogaid􏰅wortnerruceht ni ytimixorp mumixamevahtondid
v e r p j􏰥 i hcihwnmulocamorfdetratshcraesehtdnadekcehctonsaw􏰪n􏰨 SfI􏰇lacitrev
yltcefreptonerahcihwsegdirytimixorpevaheroferehtdnatnalsatadetneiroera
txenj􏰥ij􏰥iverpj􏰥i hcihwstcatnocllatsetarelotsihT􏰇􏰪n􏰨 S􏰵􏰪n􏰨 S􏰶􏰪n􏰨 Sfidetacidniylnosi
yradnuoblatnoziroha􏰅􏰇e􏰇i􏰅hcraesfonoitceridehtnilexiptxenehtniesaercninarof
gnikcehctsujfodaetsnideilppasislexipsuoiverpdnatxenhtobfotsetmuminim
􏰪verpi􏰨locxa m laitraplluf a􏰅􏰈 􏰞
􏰪verpi􏰨locxa m G 􏰦􏰵j	􏰦􏰵􏰈 􏰯	Gtaht hcus jsnmuloc
􏰪verpi􏰨locxa m esehtroF􏰇 Gottnecajdayletaidemminmulocaronmulocemasehtni
􏰣􏰡
latsidehtforetnecehtmorfecnatsidlacimotanamumixamehtotsdnopserrochcihw
􏰅mumixamlacols􏰛puorgtnerruceht woleb mc􏰌tuobanahteromebtontsum
derongigniebmuminimlaitrapehtfonoitacollacitreveht􏰅yllanoitiddA􏰇mlaperof
roegnalahplamixorpehtfomumixamlacolehtton􏰅pitregn􏰏amorfgnimocsihcraes
ehttahterusneotsplehsihT􏰇mumixamlacols􏰛puorgtnerrucehtmorfdrawnwod
ebtsumhcraesfonoitcerideht􏰅tsriF􏰇llewsagninetta􏰐ebdluochcihwsleehmlap
rosmlaperof ehttondnasegnalahplamixorpeht ylnohtiwdegremeraspitregn􏰏
ehterusneotedamerastnemerusaemeromlareves􏰅netta􏰐otgninnigebtsaelta
erasregn􏰏ehttahtsegamisuoiverpmorfdehsilbatsesahmetsysehtecnO
􏰇mehtfoenohtiwegrem
dluowspitregn􏰏delrucfoworadnihebsretemitnecwefanihtiwgnilevarttcatnoc
bmuhtyna􏰅tnemeriuqerezisregn􏰏elpitlumsihttuohtiwdelbaneeboterewgnigrem
egnalahpfI􏰇tcatnocbmuhtgnol􏰅de􏰏itnedisimaybdelbanetonsignigremegnal
􏰆ahptahtserusneregn􏰏tsomrenniehtfonoisulcxednasezisregn􏰏elpitlumfonaem
cirtemoegehT􏰇sregn􏰏gnolrehtoehtgninetta􏰐tuohtiwregn􏰏gnolenonetta􏰐ot
tluc􏰑idyllacinahcemoibsititahttcafehtgnisu􏰝􏰣􏰇􏰉erugiF􏰜pitregn􏰏delrucadnih
􏰆ebbmuhtamorf􏰝􏰡􏰇􏰉erugiF􏰜sregn􏰏denetta􏰐fosegnalahplamixorpsehsiugnitsid
kcabdeefsihT􏰇dlohserhtasessaprustsomrenniehttubsregn􏰏llafosthgiehtcatnoc
ehtfonaemcirtemoegehtegamisuoiverpehtni nehwtessi hcihwga􏰐afo mrof
ehtsekatkcabdeefsihT􏰇ecafrusehtotnogninetta􏰐erasregn􏰏sade􏰏itnedistcatnoc
ehtfotsomtahtgnitacidni􏰝􏰉􏰇􏰊erugiF􏰜kcabdeefmetsysnoitac􏰏itnediybdelbane
ebtsumgnigremhcushtiwegrem
otsleehmlaproemotspuorgpitregnarapesdex􏰏ylevitalerehttahtseetnaraugdnaezisleehmlap
sezimixamsihT􏰇sleeh mlapowtylesicerpotnielbissopsasmlapehtybdecneu􏰐ni
sedortceleehtfoynamsategotsinoitatnemgesnoigeryppolsfolaogehT
tseTesaerCleeHmlaPnoigeRnoitatnemgeSyppolS􏰌􏰇􏰍􏰇􏰉􏰇􏰊
􏰇asrev eciv
dnapitregn􏰏dehctertstuonagnihcaerrevemorfmumixamlacolleehmlapamorf
hcraesgnitneverp􏰅stsetyradnuobtcatnoclacitrevehtforomraehtnisknihcehtsll􏰏
worxa mlacol
yletelpmocsihT􏰇detratshcraeserehw􏰅
G􏰅mumixamlacolehtfo woreht
morfmc􏰢tuobanahtrehtrafsidehcraesebotwortxenehtrevenehwdehsilbatse
ebyradnuoblacitrevadnapotsdluohs hcraes􏰅noigernoitatnemgesyppolsrotcirts
fosseldragereroferehT􏰇mc􏰢tuobanahtregnolsidnahtludaegarevaehtnomlap
robmuht􏰅regn􏰏􏰅trapdnahontahtevresbootsisihttneverpot yawtselpmisehT
􏰇puorgenootnileehmlapdnapitregn􏰏agnigrem􏰅derongieblliwtiesacrehtie
nI􏰇egnalahplamixorpdnapitregn􏰏ehtneewtebronoigernoitatnemgesyppolseht
nirehtiesileeh mlapdnapitregn􏰏aneewteb mumini mlacitrevylnoeht hcihwni
esirayllanoisaccosegami􏰅drahyrevdraobehttsniagadenetta􏰐si dnaheht nehw
􏰅smlapdnasregn􏰏fognigremtneverpotsnoituacerpevobaehtfollaetipseD
tseTnoitatimiLthgieHtcatnoC􏰋􏰇􏰍􏰇􏰉􏰇􏰊
􏰇egnalahplamixorpaflugneotmuminimlaitrap
lacitrevatsapeunitnoclliwpitregn􏰏amorfhcraes􏰅temerasnoitidnocesehtfolla
fI􏰇mumixamlacolpitregn􏰏ehtottnecajdanmulocaronmulocemasehtnieb
dluohsaminimlaitraplacitreveht􏰅yllaniF􏰇smlaperofehtmorfsegnalahplamixorp
ehtetaitnere􏰎idllasnoitidnocesehT􏰇derongieblliwderetnuocnemuminimlaitrap
lacitrevtsr􏰏ehtylnodna􏰅noigernoitatnemgesyppolss􏰛dnahehtfoenilgnidivid
latnoziroheht evobasretemitnecelpuoc aeboslatsummumini mlaitrapehtfo
noitacollacitrevehT􏰇egnalahplamixorpehtforetneceht ot􏰝pitregn􏰏􏰜egnalahp
􏰈􏰢
hcihwsworllanidehsilbatseebotsleehmlapneewtebseiradnuoblatnozirohwolla
tsapdnetxetsummuminimlaitrapehtos􏰅ediw
ylriafebtsumyellavytimixorpeht􏰅yllaniF􏰇peedylriafebyellavytimixorpehttaht
seriuqerhcihw􏰅mumixamlacolehtfotahtflahtuobanahtsselebdluohsedortcele
muminimlaitrapehtfoytimixorpeht􏰅dnoceS􏰇detanigirohcraesehterehwedortcele
mumixamlacolehtfonmulocehtmorfmc􏰉tsaeltajnmulocataebtsumaminim
laitrapgniyfilauqos􏰅leehmlaprehtieforetnecehtmorf mc􏰉tsaeltaesaercehtecalp
stniartsnoclacimotana􏰅tsriF􏰇aminimlaitraprehtoynatondnaesaercsihttcetedot
dedeenerastsettnegnirtsylriaF􏰇dlohserhtecnac􏰏ingisehtevobasedortceleamini m
laitraphtiwyellavytimixorpllataybylnodetarapeseblliwyehtdna􏰅levelytimixorp
dnuorgkcabehttaniamerlli wmehtneewtebedortceleon􏰅deilppasierusserperomsa
tuB􏰇etarapesspuorgedortceleleehmlapowtehtpeekdnaesaercsihttcetedlliwtset
ecnac􏰏ingisytimixorpeht􏰅ylthgilecafrusehthcuotylnosleehmlapehtnehW
􏰇sleehmlapehtneewtebesaercroyellav
ytimixorpegralehttpecxeezisleehmlapezimixamotderongierasnoigernoitatnem
􏰆gesyppolsniaminimlaitraplla􏰅eroferehT􏰇sleehmlapehtfoenohtiwdegremeb
dluohsyehtelbissopnehwtub􏰅yrassecenfismlaperofetarapesgniyfitnedirofsnaem
sahmetsysnoitac􏰏itnediehT􏰇detcepxeoslaerasmlaperehwecafrusehtfosnoiger
rewolehtnieraspitregn􏰏ehtnehwsleehmlapforiapamorfdrohcregn􏰏agnitrats
spitregn􏰏tnecajdaforiapagnihsiugnitsidnilacitircylralucitrapsinoitarapesleeh
mlapehT􏰇strapowtotnidetnemgesyltnetsisnocsimlapehtfiregnortserastniarts
eititnedileeh
mlapehtfoenongissaylsuoenorretonnactisleehmlapowtfonoitacidniraelcsah
metsysnoitac􏰏itnediehtsagnolsA􏰇smlapmorfsregn􏰏hsiugnitsidylbailermet
􏰆sysnoitac􏰏itnediehtplehsretnecleehmlapneewtebsnoitarapesegralylevitaler
ehtdnasezisleehmlapegralylevitalerehT􏰇derusaemebnacsleehmlapneewteb
􏰉􏰢
􏰇deteledsi gNG􏰤􏰤􏰈Gfspuorgfotesdetcennoclanigiro
eht􏰅dehsin􏰏sipuorgrepusehtotninoitadilosnocretfA􏰇􏰝a􏰣􏰈􏰇􏰊erugiFnisruccosa
􏰅spuorgdetcennocehtfoowtneewtebseilytivacnoclatnozirohafispuorgdetcen
􏰆nocehtfoynafotraptonerahcihwsedortceleedulcninaclluhxevnoc􏰆imesehT
K
worpot wortob 􏰝􏰡􏰇􏰊􏰜SG􏰦􏰵i􏰦􏰵SG􏰤i􏰢KGnim􏰦SG
K
wortob wortob
􏰝􏰍􏰇􏰊􏰜KGnim􏰦SG
K
wo r p o t
wo r p o t
􏰝􏰌􏰇􏰊􏰜KGxam􏰦SG
􏰤spuorgdetcennocehtfolluhxevnoc􏰆imesehtgnimrof ybSGpuorgrepus
aotnidenibmocsigNGidetcennoc
eraspuorgowT􏰇puorgrehtoehtfotnemelenaoslasipuorgenofoedortcelemumi
􏰆xamlacolgnitanigirohcraesehtfignippalrevoebotden􏰏ederaspuorgowT
􏰇noitcartxeretemaraperofeb
spuorgrepuselgnisotnispuorggnippalrevosenibmoc􏰉􏰇􏰊erugiFfoegatsnoitanibmoc
puorgeht􏰅metsysehtfotserehtottcatnocyhse􏰐elbahsiugnitsidreppuorgenoylno
gnitneserpfotseretniehtnIuorgetarapesehtfodirtegotyrassecenosla
siti􏰅revewoH􏰇amixamlacolelpitlumsahac􏰏ingispalrevootspuorgrofelbissopsitisnoigernoitatnemgesyppolsnI
spuorGgnippalrevOgninibmoC􏰡􏰇􏰉􏰇􏰊
􏰇tnatropminusitnemerusaemnoitarapesleehmlapehttahtegralosera
stcatnocmlapehtenodsisihtnehwtub􏰅ecafrusehtotnodrahdehsupsimlapehtfo
retneceritneehtnehwsidetcetedtonsiesaercehtemitylnoehT􏰇sessorcesaerceht
􏰊􏰢
strapdnahegralecnis􏰅trapdnahafoezisehthtobnopusdnepedti􏰅puorgehtni
z
lexiphcaerevoytimixorpsetargetni GytimixorppuorglatotehtecnistahtetoN
z
E
E
E
G􏰉e
X
G􏰉e
X
G􏰉e
G
􏰝􏰈􏰈􏰇􏰊􏰜
􏰝􏰠􏰈􏰇􏰊􏰜
􏰝etnecdethgiew􏰆ytimixorpeht􏰅noitisoppuorgforotacidni
cisabaevigoT􏰇sretemitnecniretnecedortceleehtfoecafrusehtnosetanidrooc
yx
ehtebednaeteldna􏰅elexiproedortcelenafoytimixorpdehtoomsnueht
j e􏰥 i e
z	E
E􏰦 etel􏰅Gpuorgnisedortcelefotesehtsi
eb􏰪n􏰨
GtahtneviG
noitatupmoCdiortneCesu􏰎idnuehtsezilitussecorp
noitaziretemarapehtesaercnioslanacnoisu􏰎iD􏰇dnuorg
􏰆kcabegamiehtrostcatnoctnecajdaotniyradnuobpuorgafoedistuolangiss􏰛tcat
􏰆nocehtfoemosesu􏰎idnacti􏰅egamielohwehtrevoderusaemsalangisytimixorp
ssiugnitsidplehotnoitatneirodna􏰅yticirtnecce􏰅ytimixorppuorglatot
sahcusstcatnocfoserutaefcirtemoegsezilitumetsysnoitac􏰏itnediehT􏰇yticolev
regn􏰏tce􏰐ersegamievisseccusneewtebnoitisoptcatnocnisegnahcdna􏰅noitisop
tcatnocdnahstce􏰐erdiortnecpuorG􏰇puorgedortcelehcaemorfsretemarapnoit
􏰆isopdna􏰅ezis􏰅epahsstcartxessecorpnoitatnemgesehtfoegatstsalehT
sretemaraPpuorGgnitcartxE􏰢􏰇􏰉􏰇􏰊
􏰋􏰢
􏰇􏰝􏰠􏰢􏰈􏰥􏰠􏰌
􏰋􏰦 G
􏰝􏰉􏰈􏰇􏰊􏰜
􏰡􏰍
yx	xx
GG
􏰊􏰉
yyyxxxvoc
􏰤 G􏰥 G􏰥 Gstnemomdnocesfo Gxirtam
ecnairavocpuorgehtfonoitamrofsnartyratinuaseriuqererudecorpgnitt􏰏espille
ehT􏰇sretemarapespilleybdetamixorppallewsiepahsrieht􏰅xevnoceraspuorgtsom
ecniS􏰇􏰋retpahCninoitac􏰏itnedidnahdnaregn􏰏tsissalliwsretemaraphcus􏰅ylla
􏰆noitnetni noitatneiroroepahstcatnocyravtonlliwyllacipytresuehtelihW
gnittiFespillE􏰉􏰇􏰢􏰇􏰉􏰇􏰊
􏰇sedortcelemargolellarapehtybdesuacsnoitallicsodnasesaibnoitalopretnilacitrev
etaroilemaotdesusdohtemnoitalopretniraenilnonsniatnoc BxidneppA
peromhtiwspuorgesuacotdnet
􏰌􏰢
􏰇sliafnoitatnemges
hcihwnisesacerarehtdnaselurnoitatnemgesxelpmoceromehtfoecnatropmieht
etartsulli hcihwsnoitarug􏰏noc dnahfognilpmas astneserpnoitcessiht􏰅noitaulave
evitatitnauqtnedneped􏰆rotarepodnanoitacilppanatpmettanahtrehtaR􏰇deiduts
gniebnoitacilppaehtniesirahcihwsnoitarug􏰏nocdnahtnemgesottluc􏰑idsusrev
detnemgesylisaefoycneuqerfevitalerehtnoylhgihdnepeddluowecnamrofrepnoit
􏰆atnemgesfonoitaulaveevitatitnauqynA􏰇rotarepodelliksaybdemrofrepsatsaelta
􏰅snoitarug􏰏nocdnahdezilituylnommoctsomehtrofsselwa􏰐erasnoitatnemgestaht
de􏰏irevsahrohtuaehtybSTMehtnonoitalupinamcidrohcdnagnipytyliaD
sdohteMnoitatnemgeSehtfoecnamrofreP􏰣􏰇􏰉􏰇􏰊
􏰇shtgnelsixaroni mdnarojamfodaetsni
z
ezistcatnocfoerusaemyramirpehtsadesusi Gytimixorppuorglatotdna􏰅seulav
derusaemriehtnahtrehtarseulavtluafedotteserastcatnocllamsfoyticirtnecce
dnanoitatneiroeht􏰅noituloser wol evahsegami yti mixorpfi􏰅eroferehT􏰇sretemarap
espilledett􏰏ehtnahterusserpregn􏰏sallewsaezistcatnocforotacidnielbailererom
z
asiGytimixorppuorglatoteht􏰅syarraedortcelemargolellarapnoituloserwolnO
􏰝􏰠􏰉􏰇􏰊􏰜
pitregniFegareva z z
Z􏰦G􏰦􏰤 G
􏰤enodnuoraytimixorplatotaevahlliw
pitregn􏰏delruclacipytehttahtosdezilamroneryllaciripmesiytimixorppuorglatot
eht􏰅yllaniF􏰇enootlauqeronahtretaergebsyawlalli wyticirtnecceeht􏰅htgnelsixa
ronimehtotlauqeronahtretaergsyawlasihtgnelsixarojamehtecnistahtetoN
􏰝􏰣􏰈􏰇􏰊􏰜
roni m
roja m
G
G
􏰒
􏰦G
􏰒
􏰤 Gyticirt
􏰆neccenaotnioitarriehtaivdetrevnocerashtgnelsixaronimdnarojameht􏰅slevel
metsysrehgihtasmlapmorfspitregn􏰏gnihsiugnitsidelihwecneinevnocroF
􏰍􏰢
􏰇ecafrusehtevobadednepsus
niamersmlaperofehtdna mlapehtforetnecehtylnO􏰇gnihtoomsybdegremton
erahcihw􏰝tcatnocfothgirreppudnatfel rewol tasedortceletsekrad􏰜amixamlacol
owtsesuacleehmlapretuoehtdna􏰅ecafrusehtgnihcuoterasegnalahplamixorpeht
yllaicepsesisa􏰅segde
tcatnocdnuoradrawtuodelboslasahyti mixorptubneveeromeratcat
􏰆nochcaeforetnecehtraenslexipfoseitimixorpdehtoomstahtetoN
􏰇tuodaerpsnahtrehtarrehtonaenotsniagadezeeuqssregn􏰏ehthtiw
dnahdenetta􏰐afosegamiytimixorp􏰝bdesu􏰎iddna􏰝adehtoomsnU􏰤􏰍􏰇􏰊erugiF
􏰝b 􏰝a
desu􏰎iddnadehtoomsnuehthtobsedulcni􏰍􏰇􏰊erugiF􏰇noitatnemgestcerrocrof
snoigernoitatnemgesyppolsehtfotnemngilareporperiuqer hcihwsnoitisopatxuj
tcatnocehtfotsomsedulcnirehtegotdezeeuqssregn􏰏htiwdnahdenetta􏰐A
􏰡􏰢
owtotni nekorbylsuoenorresniamertcatnocleehmlapretuoeht􏰅rehtoehtgnippal
􏰆revomorfpuorgs􏰛mumixamlacolhcaegnitneverp􏰅amixamlacolleehmlapretuo
laudehtneewtebyellavytimixorpehtgnolaseiradnuobhsilbatseselurnoitatnemges
tcirtsecniS􏰇egamiehtforenrocthgirrewolehtotnwodsmlapeht􏰎odevomsi
noigernoitatnemgesyppolsehtnehw􏰅􏰇e􏰇i􏰅selurnoitatnemgestcirtshtiwdessecorp
si dnahelohwehtnehwdeniatbopamnoitatnemgesehtswohsa􏰡􏰇􏰊erugiF
􏰇denibmocerayehterofebspuorg
gnippalrevolaudividnitonrartibrasisrebmunehtfogniredroeht
􏰥puorgnoitatnemgesemasehtfosrebmemeraemasehtderebmunsedortcelehcihw
nispamnoitatnemgessyalpsid􏰡􏰇􏰊erugiF􏰇rotamitsenoitisopdnaheht ybdeniater
noitisopdnahnwonktsalehtgnitcidartnoc􏰅noigernoitatnemgesyppolsehtninwod
hcuotyldetcepxenuspitregneppah
nactahwwohslliwesacremrofehT􏰇egamielohwehtgnirevocnoigernoitatnemges
yppolsehthtiwniagadnaegamielohwehtgnirevocnoigernoitatnemgestcirtseht
hti wecnodetnemgessawtatnemgesfosepytehtetartsnomedoT
􏰇spitregn􏰏tnecajdafonoitatnemgesezidrapoejdluochcihwgnihtooms
lanoitiddasdiovadnadnahrepspuorgleeh mlapowtnahteromhtiwmetsysnoitac
􏰆􏰏itnediehtgnisufnocsdiovasihT􏰇elbissoprevenehwspuorggnippalrevoenibmoc
otelihwhtrowsititahthguonenetforaeppaamixamelpitlum􏰅sselehtreveN􏰇􏰍􏰇􏰊eru
􏰆giFfoamixamlaudehtdecudorphcihwdnuofsawerutsopdnahaerofebdedracsid
erewleeh mlaprep mumixamlacolenodahylnohcihwsegamilareveS􏰇erusserp
dnahfosnoitubirtsidddooteudyllacidaropsraeppaylnonoisu􏰎idgnirudetarapes
niamerothguonetcnitsiderahcihwamixamelpitluM􏰇egamidehtoomsehtniyllaic
􏰆epse􏰅mumixamlacolenoesuacylnolliwleehmlaphcaeyllausutahtetoN
000 22 000 111
22220001111 2222 000 111 44 22235444 222 33 555 4444
22 33 555 444 6625554
6666 6666 7777
66 777777 88888888 7777777 88888888888	999 7777 88888888888	999 7777
88888888888 9999997777 888888888 99997777
SLOPPY
STRICT
STRICT SEGMENTATION REGION 000
0000000000 00000000000
2222 333 000 44 222 33 000 444 222 33 000 4444
22 33 000 444 6620004
6666 6666 7777
SLOPPY SEGMENTATION REGION
66 777777 88888888 7777777 88888888888 77777777 88888888888 77777777777
88888888888 7777777777 888888888 77777777
aleht􏰝bnI
􏰇sregn􏰏gnirdnaelddimehtfosnoitroplatsiddnalamixorpneewteb
noitatnemgesehtnistilpsgnisuac􏰅delbasidoslasiamini mlacitrevreg
􏰆n􏰏denetta􏰐foecnareloT􏰇spuorgllamorfdedulcxeerastcatnocleeh
mlapehtfoegdeehttasedortceleehtfoemosdna􏰅amixamlacollaud
ehtotgnidnopserrocspuorgetarapesowtsniatniamylsuoenorretcat
􏰆nocleehmlapretuoeht􏰝anI􏰇dnahelohwehtrofselurnoitatnemges
yppolsgnisu􏰝brosleeh mlapdnasregn􏰏htobrofselurnoitatnem
􏰆gestcirts􏰝arehtiegnisudnahdenetta􏰐ehtrofstlusernoitatnemgeS􏰤􏰡􏰇􏰊erugiF
􏰝b􏰝a
􏰣􏰢
rotnemngilanoigernoitatnemgessihT􏰇stcatnocmlapehtnodengilaylreporpsi
noigernoitatnemgesyppolsehtnehwpamnoitatnemgesehtswohs􏰢􏰇􏰊erugiF
􏰇regn􏰏gnirehtfoegnalahplamixorp
ehtdesrevartsyawlahcraesehtesacsihtnidna􏰅worhcaemorftoohs􏰎olacitrev
enowollofylnonacnrettaphcraesxevnoc􏰆imesehtesuacebtubsegnalahplamixorp
dnaspitregn􏰏neewtebaminimlacitrevehtfoesuacebtonpuorgpitregn􏰏degrem
ehtmorfetarapesniamersregn􏰏elddimdnaxedniehtfosegnalahplamixorpehT
􏰇noigernoitatnemgesyppolsanirehtegot􏰆dezeeuqsnwodhcuotspitregn􏰏ehtemit
ynarucconacgnigrempitregn􏰏tcerrocnihcuS􏰇egremspitregn􏰏gnirdna􏰅elddim
􏰅xednitnecajdayllatnoziroheht􏰅stsetaminimlaitraplatnozirohedulcnitonodselur
noitatnemgesyppolsecnis􏰅revewoH􏰇lluhxevnoc􏰆imesriehtaivdenibmocebsuht
dnapalrevootamixamlacollaudleehmlapretuoehtfohcaemorfspuorgeht
wollaselurnoitatnemgesyppolsesuacebsruccosihT􏰇sedortceletcatnocmlaplla
niatnochcihwspuorgleehegralowtotnidetnemgesylreporpsimlapehtesacsiht
nI􏰇selurnoitatnemgesyppolshtiwdetnemgessidnanoigernoitatnemgesyppolseht
nihtiwseil dnahelohwehthcihwniemertxeetisoppoehtswohsb􏰡􏰇􏰊erugiF
􏰇spuorgpitregn􏰏evitcepserriehthtiw
egremotliafot􏰝esacsihtnisregn􏰏gnirdnaelddimehtmorfesoht􏰜amixamlacol
tcnitsidhtiwsegnalahplamixorpsesuacnrutnisihT􏰇aminimlacitrevregn􏰏denet
􏰆ta􏰐foecnarelotselbasidoslanoigernoitatnemgesyppolsehtfotnemngilareporpmi
ehT􏰇noigernoitatnemgesyppolsehtepacseyldetcepxenusmlapehtnehwsmlap
morfspitregn􏰏hsiugnitsidotmetsysnoitac􏰏itnediehtrofredrahtignikam􏰅wol
ylsuoenorreebotsretemarapytimixorplatotdnaezispuorgdetcartxeehtesuac
nacsihtsemitemos􏰅egamielpmaxesihtrofdedulcxeneebevahsedortceleynamton
hguohT􏰇spuorgmlapehtllafotuotfelyleritneebotsedortceletcatnocmlapwef
adesuacevahselurnoitatnemgestcirtsehttahtstsegguspamnoitatnemgesehtot
spuorg
111 33 111 222
33331112222 3333 111 222 55 33312555 333 11 222 5555
33 11 222 555 7732225
STRICT SEGMENTATION REGION
7777 7777 8888
77 888888 SLOPPY SEGMENTATION REGION
99999999 8888888 99999999999 88888888 99999999999 88888888888 99999999999 8888888888
999999999 88888888
􏰠􏰣
􏰇sregn􏰏tnecajdaehtmorfetarapes
puorgenootnisenibmoc􏰅egnalahplamixorpdnapitlatsideht htob
􏰅􏰇e􏰇i􏰅regn􏰏hcaefoelohwehT􏰇sleehmlapehtneewtebyawflahesaerc
ehtybtilpseraspuorgmlapehttey􏰅spuorgriehtnidedulcnierasleeh
mlapehtotlamixorpsedortcelell A􏰇sregn􏰏ehtrofselurnoitatnemges
tcirtsdnasleehmlapehtdnuoraxobehtniselurnoitatnemgesyppols
gniylppaybdeniatbodnahdenettaheht􏰉􏰈􏰇􏰊erugiFnI
􏰑
􏰇noit
􏰆atnemgestcirtsetipsedegremotspitregn􏰏sesuacnoitatorrehtruf yna􏰅􏰉􏰈􏰇􏰊erugiF
ninwohseblliwsA􏰇spuorgetarapesruofnimehtspeekdnamehtneewtebaminim
laitraplanogaidehtstcetedmebotmehtsesuacspitregnhtsetarts
esirpmocti nopu
snoitairavdnaerutsopdnahdetnemgesylisaesihttahtoslaetoNngisytimixorpehtsuhT􏰇slevelytimixorpdnuorgkcabtasedortceleyb
rehtonaenomorfdetarapesllaerastcatnocehtdnamumixamlacolenoylnosah
tcatnocelbahsiugnitsidhcaeesuacebtnemngilanoigernoitatnemgesfosseldrager
yltcerrocdetnemgesebnacegamiehT􏰇spuorgleehmlapehtmorfdedulcxeebnac
stcatnocmlapehtfoyrehpirepehtnosedortceleehtfowefasleehmlapehtfonoit
􏰆atnemgestcirtshtiwtahtnoitpecxeehthtiw􏰅erehwyrevenoitatnemgestcirtshtiw
roylnosmlaprevonoigeryppolsehthtiwdeniatbosipamemasehttub􏰅􏰣􏰇􏰊erugiF
ni nwohssinoigeryppolsehtnidnahelohwehtrofpamnoitatnemgesehT􏰇deilppa
eraseluryppolsrotcirtserehwfosseldrageryltcerrocdetnemgessi 􏰢􏰇􏰉erugiFfo
erutsopdnahlartuenrotluafedeht􏰅noitatnemgestcerrocrofnoigeryppolsehtfo
tnemngilareporpseriuqer hcihwdnahdenetta􏰐􏰅dezeeuqsehtottsartnocnI
􏰇snoitidnocgnitarepolautcarednu
noitatnemgestcerrocsihtgnicudorpylbairavni􏰅seulavtcerrocriehtotdezilibats
evahyleruslliwetamitsenoitisopdnahehtsuht dnasnoitac􏰏itnediregn􏰏􏰅ecafrus
ehtotnodenetta􏰐sahdnahelohwehtemitehtyB􏰇sleehmlapdnasregn􏰏htobrof
noitatnemgestcerrocehtseveihcatifosretemitnecelpuocanihtiwtnemngilayna
1223 111	222 333	44 111 22 333 444
444
5555 5555
5555
STRICT
66666666 66666666666 7777
66666666 777777777 SLOPPY
STRICT SEGMENTATION REGION
1223 111	222 333	44 111 22 333 444
444
SLOPPY SEGMENTATION REGION
5555 55555 5555
6666666666 777777 66666666666 77777777 66666666666 777777777
􏰉􏰣
􏰇detarapes􏰆llewdnallamsylevitalererastcatnocecnisdeilppaera
selurnoitatnemgesartuenehtfonoitatnemgestcerrocsihTorp􏰝bdesu􏰎iddna􏰝adehtoomsnU􏰤􏰠􏰈􏰇􏰊erugiF
􏰝b􏰝a
1 1122
333 112244
333 224455
STRICT SEGMENTATION REGION 333 44555
555
66 6666666 6666666 6666666 666666 777777777
777777777777 77777777777
SLOPPY SEGMENTATION REGION
STRICT SEGMENTATION REGION
GION
1 1111
333 111111 3333 111111 333 11111
111 SLOPPY SEGMENTATION RE
66 6666666 6666666 6666666 666666 777777777
777777777777 77777777777
􏰋􏰣
􏰇 􏰌􏰋sahcumsadetnalssispitregn􏰏fo woreht
􏰑
nehwnevedetarapesylreporpspuorgpitregn􏰏peek􏰝bnoitatnemges
tcirtsfostsetaminimlanogaideht􏰅revewoH􏰇degremebotmehtfo
emossesuac wordetnalsanispitregnitrapafosegami yti mixorp􏰝bdesu􏰎iddna􏰝adehtoomsnU􏰤􏰉􏰈􏰇􏰊erugiF
􏰝b􏰝a
111 111	22 111	22
3 22
STRICT SEGMENTATION REGION 333333	22
3333333	222 3333333	222 33333	222
6666666666	2 6666666666666
66666666666666 6666666666666
SLOPPY SEGMENTATION REGION
sapbmuhteht
nehwsrorrenoitatnemgessesuacoslagniraemsedortcelemargolellarapehT
􏰇sedisllamorfdehcaorppa
ebdluoctioslatsedepanodetnuomsawSTMehtfiylralugerderetnuocneebylno
dluownoitarug􏰏noceht os􏰅ydobehtfosnoitrotnoc drawkwaseriuqererutsopgnittis
􏰇pitregn􏰏 hcae
ybdesuacyllamronamixamyti mixorplacolehtserucsbosedortcele
margolellarapdevaelretniyllacitrevybgniraemslacitrevehtesuac
􏰆ebspitregn􏰏fonmulocehttnemgesotliafselurnoitatnemgesllA􏰤􏰊􏰈􏰇􏰊erugiF
lamronehtmorfnoitarug􏰏nocdnahsyawedissihtgnimrofreptahtetoN􏰇selurnoit
􏰆atnemgestcirtshtiwnevegnigremelbadiovanugnisuac􏰅spitregn􏰏ehtneewtebsyel
􏰆lavytimixorpehtsedihsedortcelemargolellarapehtybgniraemslacitrev􏰅setacidni
􏰊􏰈􏰇􏰊erugiFnisnoigeryppolsdengilaylreporphtiwpamnoitatnemgesehtsA􏰇erut
􏰡􏰣
ebsyawlalliwsegdirytimixorptahtnoitpmussaehtfoecneuqesnocasieruliafsihT
􏰇ylreporpdengilasinoigernoitatnemgesyppolsehtnehwnevespuorgegnalahplam
􏰆ixorpmorfdetarapesspuorgpitregnidfonoitcet
trevraendetneirotonerasregn􏰏eht
nehwnoitatnemgesregn􏰏denetta􏰐fosgnimoctrohsehtsesopxe􏰢􏰈􏰇􏰊erugiF
􏰇etarapesniamer
smlapdnasregn􏰏rofspuorgnoitatnemgeseht􏰅leehmlapretuoehtotdetcennoc
siregn􏰏yknipehtdnaleehmlaprenniehtotdetcennocsiregn􏰏xedniehttaht
wohssegami ytimixorpehthguohtnevetahtoslaetoN􏰇ecafrusehtotnodenetta􏰐
sidnahehtfotserehtnehwsmlaperofyfitnedielbailernacmetsysnoitac􏰏itnedi
ehtesuacebrorrenaderedisnoctonsisihT􏰇palrevolautumhguorhtsleehmlapeht
htiwdenibmocebtonnacdnamumixamlacoletarapesasahtifipuorgnwostisteg
tcatnocmlaperofatahtsetartsullipamsihTaitraplacitreveht􏰅a􏰋􏰈􏰇􏰊erugiFfonoitatnemgesgnirud
delbaneylsuoenorresaw􏰝􏰊􏰇􏰍􏰇􏰉􏰇􏰊noitceS􏰜noitatnemgesregn􏰏denetta􏰐rofgnigrem
egnalahplamixorpfitahtetoN􏰇rehtiegnicapsworedortcelemargolellaraptnerruc
ehthtiwylreporpdetnemgesebtonnacti􏰅pitregn􏰏xednidnabmuhtehtfonoitis
􏰆opatxujemassihtsniatnoctebaminim
laitraplacitrevehtserucsbogniraemsedortcelemargolellaraptahtesolcoserapit
udehsup
yllufpitbmuhtehthtiwdnahdesolcafoegaminasia􏰌􏰈􏰇􏰊erugiF􏰇􏰝b􏰋􏰈􏰇􏰊erugiF􏰜
selurtcirtshtiwyltcerrocdetnemgesebllitsdnapitregn􏰏xedniehtottegnacti
12
112233344 55223344 5534
555 555
STRICT SEGMENTATION REGION
SLOPPY SEGMENTATION REGION
6666 777777 66666 77777777 6666666 777777777
66666 777777777
gesdengilaylreporpdnaegami ytimixorpdehtoomsnU SEGMENTATION REGION
SLOPPY SEGMENTATION REGION
55555 666 555555 666666 555555 66666666 66666666
􏰣􏰣
􏰇pitregn􏰏xedni ehtfokcabeht gnihcuot bmuht afopam
noitatnemgesdengilaylreporpdnaegami ytimixorpdehtoomsnU􏰤􏰌􏰈􏰇􏰊erugiF
􏰝b􏰝a
􏰠􏰠􏰈
􏰇gnihcuoterasmlaperofehttaht drahosecafrusehtotnodenettaEGION
aaaaa aaaaa aaa
22 111 2222 33
1111 2222 333 111 22 333 44 111 22 333 444 111 222 333 444 1111 22 333 44 111111 2222 33
11111 22222 ccc SLOPPY SEGMENTATION REGION
11111 222 cccccc 111111 cccccccc 111111111 cccccccc
111111111111 cccccccc 111111111111 ccccccccc 11111111111 cccccccc
22 111 2222 33
1111 2222 333 111 22 333 44 111 22 33 444
STRICT SEGMENTATION REGION 1 222 333 444
111 22 333 44
1188829933 aaaaa	88888 99999 ccc
aaaa	88888 999 cccccc SLOPPY SEGMENTATION REGION
aaa 888888 cccccccc 888888888 cccccccc
888888888888 cccccccc 888888888888 ccccccccc 88888888888 cccccccc
􏰈􏰠􏰈
􏰇􏰝b
snoigeryppolsdengilaylreporpgnisunoitatnemgestcerrocehtdna
ah
􏰑
thgirdenetta222222222 4
222222222222222222 444 222222222222222222 444
2222222222222222222 4444 22222222222222222 444 2222222222222222 44
2222222222222222222222 SLOPPY SEGMENTATION REGION
222222222222222222222 22222222222222222222
111111 22222222222222222222 1111111 22222222222222222222 1111 222222222222222222222
222222222222222222222 2222222222222222222
111 222 33 1111 222 4
3333 1111 222 444 3333 111 222 444
333 15552664444
777 555 666 444
777758644 STRICT SEGMENTATION REGION 7888888888888888888888
888888888888888888888
111111 11111
1111
88888888888888888888
88888888888888888888 SLOPPY SEGMENTATION REGION
88888888888888888888 888888888888888888888
888888888888888888888 8888888888888888888
oitatnemgestcirtsfonoitcetedamini mlaitraplanogaideht
􏰅􏰝bsmlapehtrevoylnonoitatnemgesyppolshti W􏰇dnahelohweht
sesolcnednaspuorgesehtdnuoralluhxevnoc􏰆imesehtmorfsmrof
puorgrepuselgnis	A􏰇palrevootspuorgllasesuac􏰝aerehwyreve noit
􏰆atnemgesyppolS􏰇dnahdenettaboneebevahseitilibatsnihcushguohT􏰇ygolopottcatnocmlapnisegnahc
eltbusoteudtxenehtni pukaerbdnaegamienorofrehtegotegremylbatsnu
nacsleehmlapehtro􏰐ylluferasmlapnehwtub
􏰅tnetsisnocyltcefrepsiserutsopdnahdenetta􏰐nutsomfonoitatnemgesehT
􏰇sesserpyeksameht
terpretniyleslafnacmetsysehtiwsnwodhcuotsuoirupshcussleehmlaprosmlaperofsade􏰏itnedierastrapdnah
elbatsnuesehtsagnolsA􏰇nwoddehcuotylwenro􏰎odetfilsahtrapdnahasedulcnoc
rekcarthtapehttaht hcumosevomyamsdiortnecehtegami	wenanitrapakaerb
roegremylneddusstcatnocegralfI􏰇eludomgnikcarthtapehtybdetupmocyticolev
tcatnocehtnirettijesuacdnasegamineewtebtfihsylsuoenorrenacdiortnectcatnoc
dnahderusaemehtsnoc
noitatne mgeS􏰇dnahyranoitats afosegami	yti	mixorpevisseccusssorcapihsrebme m
puorgfoycnetsisnocehtsiecnamrofrepnoitatnemgesfoerusaemrehtonA
􏰇segamiytimixorpevisseccusssorcaelbatssniamergnigremehtsagnolsarorre
naderedisnoctonsidnametsysnoitacnoc􏰆imesaivnoitanibmoC
􏰇􏰌􏰋otsrobhgienlanogaidtseraenneewtebelgnaehtsesaercnihcihwgnicapswor
􏰑
rellamsahti wylekilsselebdluoweruliafsihT􏰇spuorgowtotniecafrusehtotnodrah
desserpbmuhtdetneiro􏰆yllanogaidastilpseruliaffoepytsihtyllanoisaccO􏰇stset
amini mlanogaidybpunekorberanmulocrehtoyreveniamixamesuachcihweseht
sahcussegdireuqilboyltneicrgnikcarts􏰛htaphcae􏰅sdeepsregn􏰏hgihtashtapfognikaerbreporpmi
tneverpoT􏰇suidargnikcartstinihtiwpuorgatuohtiwhtapevitcanaevaellliw
􏰎otfildna􏰅shtapgnitsixefoiidargnikcartehtedistuoraeppaotpuorgwenasesuac
yllausunwodhcuottahttcafehtybdetcetedylisaeera􏰎otfildnanwodhcuotregn􏰏
􏰅setarnacsegamielbanosaerrofsuidargnikcartehtnahtretaergsisregn􏰏neewteb
noitarapeslaretallacipytehtecniS􏰇ecafruseht􏰎ostfilronwodsehcuotylwen
tcatnocdnahlacisyhpanehwenimretedoslatsumssecorpgnikcarthtapehT
􏰇noitacol tcatnocecafrustnerrucfonoitciderpehtotnisegamisuoiverpnishtap
gnitsixegnoladerusaemseiticolevgnitaroprocni ybdevorpmi ebnacecnamrofrep
gnikcarT􏰇rehtonaenofosuidar gnikcartehtsanwonkecnatsidanihtiwerayeht
sselnudehctamebtondluohs	htapdna puorgaeroferehTacinahcemoib􏰅oslA􏰇shtapdnaspuorgrehtootnahtrehtonaenootresolceb
lliwtcatnocemaseht	morf gnisirahtapdnapuorgauorgtnerruchcihwedicedtsum
mhtiroglagnikcarteht􏰅egami ytimixorptsalehtecnisdevomsahtrapdnahhcae
erehwenimretedoT􏰇tcatnocdnahlacisyhpemasehtotdnopserrochcihwsegami
ytimixorpevisseccusmorfspuorgesohtrehtegotniahcotssecorpgnikcarthtapehtfo
ytilibisnopserehtnehtsitI􏰇egamiytimixorphcaerofhctarcsmorfmehtstcurtsnocer
mhtiroglanoitatnemgesehttahtesnesehtniyrotisnarteraspuorgedortcelE
melborPgnikcarThtaPehtotnoitcudortnI􏰈􏰇􏰊􏰇􏰊
gnikcarThtaPtnetsisreP􏰊􏰇􏰊
􏰇yleritneraeppasidti ekamlliwsyarrarosnesnoituloser
rehgih􏰅esionrewolrehtehwnwonksitilitnurehtrufdetagitsevniebtonlliwmelborp
siht􏰅eruliafnoitatnemgesfosesacerarrehtoehtekilhcumosniam
hgityrevrofsgniriaptnere􏰎id
ylthgilsecudorpyamnoitaziminimmustnemngissadnaelurgniretsulcrobhgien
tseraenehT􏰇rehtonaenoottsesolceratahtshtapdnaspuorgneewtebstnemngis
􏰆sastpeccaylnoelursihT􏰇secnatsidgniriapfomusehtgniziminimylticilpxefo
daetsni􏰪􏰉􏰡􏰨 kcirtaPdnasivraJybdecudortnieuqinhcetgniretsulcsrobhgienraen
derahsehtotdetalerelurasezilituerehdetneserpdohtemgniriapehT􏰇ybraen
shtaprospuorgrehtofosecnatsidgniriapehtgniredisnoctuohtiwmehtneewteb
ecnatsidehtnopuylelosdesabedamebtonnachtapdnapuorgariapotnoisiced
ehtsuhT􏰇puorgehtotdengissaebdluohsretsulcehtnihtaptsesolcehtylno
nierehw􏰅puorgelgnisadnuoraderetsulcerashtaplarevestahtesacehtdna􏰅htap
ehtotdengissaebdluohsretsulcehtnipuorgtsesolcehtylnonierehw􏰅htapelgnis
adnuoraderetsulceraspuorglarevestahtesacehthtobeldnahtsumtitahtesnes
ehtninoitazimitpolabolgamrofreptsumdohtemgniriaphtapismhtiroglanoitazimitpo
tnemngissaemasehttubMehtylraelC􏰇STMehtnoemitenoynatatneserpebnacspuorg
􏰠􏰉otpu􏰅sdnahhtobmorfstcatnoc mlaperofdnaleehmlapdna􏰅sbmuht􏰅sregn􏰏
llagnitnuoc􏰅revewoH􏰇puorgdengissasti dnahtaphcaeneewtebsecnatsideht
fomusehtdeziminimhcihwenoehtdekcipdnasgniriapelbissoplladetaulave
ylpmisehn􏰏􏰊
htiwtlaedylnoenibuRecniS􏰇noitisopdetciderpehtsanoitisophtapnwonktsaleht
desuylpmiseH􏰇noitareleccaroyticolevhtaptsapmorfsnoitacoltcatnoctnerruc
tciderpotyrassecentidngnikcarthtapehtdesserddaxv
htapdetciderp􏰯n􏰨PteL􏰇noitareleccaregn􏰏
gnirudsnoitciderpdabyllaitnetopehthgiewtuonoitciderpdesab􏰆yticolevfost􏰏eneb
eht􏰅stnevenoitareleccaoreznahtnetfosselruccostnevenoitareleccahgihhcusecniS
vderusaemylsuoiverpgnidulcnI
􏰇seiticolevregn􏰏tsaphti w
detadpusinoitacolnwonktsalsihtsselnupitregn􏰏tnecajdanafonoitacolnwonk
tsalehtrevothgirraeppasuhtnacpitregn􏰏eno􏰅yllatnozirohsedilsylkciuqsregn􏰏
foworafI􏰇noitarapespitregn􏰏lanimonehtotelbarapmoceulava􏰅semarfneewteb
mccsellevartlliwregn􏰏asdeepsnoitalupinamesuomlacipyttA􏰇sedilsfodne
dnagninnigebehttatpecxe woletiuqsinoitarelecca􏰅htoomsebotdnetsnoitom
regn􏰏dnadnahesuaceB􏰇spf􏰠􏰠􏰈otdesiarebylisaenactubsfI􏰇etaremarfronacsyarraehtdnaacoltcatnochtapfonoitciderpehtrehtehW
noitacoLtcatnoCfonoitciderP􏰉􏰇􏰊􏰇􏰊
􏰇 wolebdebircsed
ssecorpgnikcarthtapehtnispetsehtfoyrammusarofSTART
END
PREDICT CURRENT POSITIONS OF EXISTING PATHS
FOR EACH GROUP FIND CLOSEST PATH
FOR EACH PATH, FIND CLOSEST GROUP WITHIN TRACKING RADIUS
FORM GROUP-PATH PAIRS IF GROUP &amp; ACTIVE PATH ARE CLOSEST TO ONE ANOTHER
ATTEMPT TO PAIR REMAINING GROUPS WITH RECENTLY DEACTIVATED PATHS
ALLOCATE NEW PATHS FOR ANY REMAINING UNPAIRED GROUPS
DEACTIVATE ANY REMAINING UNPAIRED PATHS
UPDATE PATH PARAMETERS
􏰢􏰠􏰈
􏰇 mhtiroglagnikcarthtaptcatnocehtgnizirammustrahc wolF
Ptsesol c
􏰝􏰢􏰉􏰇􏰊􏰜 lP􏰲kG
􏰤dlohtsumsnoitidnocgniwollofehtfollA􏰇mc􏰈
tuobasisuidargnikcartlanimonehT􏰇suidargnikcartehtnahtsselsimehtneewteb
GtsesolcPtsesolc
ecnatsidehtdna􏰅rehtonaenootrefer lPdna kG􏰅􏰇e􏰇i􏰅rehtonaenoot
tsesolcerayehtfirehtonaenohtiwderiapylnoeralPhtapdnakGpuorg A
Gk G
􏰝lP􏰥kG􏰜dnimgra 􏰦
􏰉
l P
Gtsesol c
lP
􏰤tiotecnatsid
ehtdrocerdnapuorgevitcatsesolcehtdn􏰏􏰅lPhtapevitcahcaerofnehT
y de r p
y
xde r p	x
l P􏰯kG􏰜 􏰦􏰝l P􏰥kG􏰜 d
􏰝􏰌􏰉􏰇􏰊􏰜
􏰝	l P􏰯kG􏰜 􏰞􏰝
􏰉􏰉􏰉
􏰤cirtemecnatsiddetupmocylisaenasiecnatsidnaedilcuEderauqsehterehw
APG􏰜dnimgra􏰦kG
􏰉
􏰤ti otecnatsidehtdrocerdnahtapevitcatsesolcehtdn􏰏
􏰅kGpuorgevitcahcaeroF􏰇Gebegamitnerrucehtnidetcurtsnocspuorgedortcele
fotesehttel dna􏰅APebegamisuoiverpehtnievitcashtapfotesehtteL
eluRgniriaPtsesolCyllautuMarevessdeeps
hcaerylisaeesehT􏰇mraerofdnadnahelohwehtfosedilslaretalrofhguoneegral
siSTMehT􏰇aerallamsarevonoitisopdex􏰏ylevitaleranidnahehthtiwnoitom
regn􏰏gnikcart ylnosawehtahtroetaremarfrehgihadahemarFrosneSehttaht
sawyrassecennusnoitciderpdesab􏰆yticolevdnuofenibuRnosaerehtylbissoP
yvyyderp

xsser p G􏰦 P
t sserp t􏰦P
􏰤swollofsadezilaitini
erasretemarapsti􏰅nwoddehcuottsujsahtrapdnaha􏰅􏰇e􏰇i􏰅npetsemittaGpuorg
ybdetratsneebtsujsahPhtapafI􏰇sretemarappuorggnidnopserrocmorfsrete
wolebnwohssnoitauqeretl􏰏evissergerotuaelpmisehtseilppa
STMehT􏰇seuqinhcetgniretl􏰏dradnatsaivhtapdengissastiotnipuorghcaefo
sretemarapdetcartxeehtetaroprocniotsi gnikcarthtapfopetslan􏰏ehT
sretemaraPhtaP􏰋􏰇􏰊􏰇􏰊
􏰇detratssihtap
wenyllatota􏰅ybraentsixeshtapdetavitcaedyltneceronfI􏰇gnimitkcilc􏰆elbuodfo
esael ersuoiverpt
noitcetednidiaot Pniderotssiemitesaelers􏰛tI􏰇detavitcaersadekram
yllaicepsdna􏰅puorgehtotdengissa􏰅detavitcaersiosrodnocestsalehtnihtiw
detavitcaedneebsahhcihwhtaptsesolcehT􏰇puorgehtfosuidargnikcartehtnihti w
shtapdetavitcaedyltnecerynaotnevigsiytiroirp􏰅htapevitcaynaotdengissaeb
tonnacpuorgeht􏰅􏰇e􏰇i􏰅puorgdetalosinarofdetacollaebotsdeenhtapwenanehW
􏰇noitacolemasehtrevospatneewtebtnemngissahtapfoytiunitnocevreserpotlufesu
siti􏰅spatregn􏰏evititeperfonoitceteddnagnicnuobedpatregn􏰏diaoT
􏰇ecafrusehtmorf􏰎otfiltrapdnahgnitneserper
􏰅detavitcaedsipuorgevitcanahtiwderiaposebtonnachcihwhtapevitcaynA
riapebtonnac	hcihwpuorgevitcaynA
otnuomaeht
xv	xv
erehwevissergerotuaredrodnocesrotsr􏰏erasretl􏰏ehT􏰇􏰝􏰪n􏰨 P􏰥􏰪n􏰨 P􏰜rotcevyti
ri d
deeps
􏰆coleveht morf􏰪n􏰜􏰞GG􏰦􏰪n􏰨P
􏰝􏰠􏰌􏰇􏰊􏰜
z􏰎zp 􏰎 􏰝􏰉􏰋􏰇􏰊􏰜􏰝􏰪􏰈􏰯n􏰨P􏰜􏰝G􏰯􏰈􏰜􏰞GG􏰦􏰪n􏰨P
x 􏰎	x
􏰤npetsemitot􏰪􏰈 􏰯n􏰨 Phtapevitcafonoitaunitnocasi
zzv
t 􏰋􏰦 G 􏰦 nahemaseht
ybdesuacspuorgniahclliwtI􏰇spf􏰠􏰌foetaremarf adnasdeepsdnahgnitarepo
lamronrofylsselwa􏰐smrofrepevobadebircsedssecorpgnikcarthtapehT
stluseRgnikcarThtaP􏰌􏰇􏰊􏰇􏰊
􏰇shtapde􏰒uhseryltnetrevdanimhtiroglagnikcarthtapytluafafitnetsisnocniemoc
􏰆ebdluowsrekramytivitcaregn􏰏tnatropmiesehT􏰇uaetalpytimixorpafokaepeht
ylirassecentontubgninnigebehtgnitacidni􏰅dlohserhtevitisopllamsyreva woleb
zv
sllaf􏰪n􏰨Pytimixorpfoegnahcfoetarehtnehwderutpacyllautcaerasretemarap
sti􏰅nwodhcuotretfanoosylriafelbaliavasirekramytimixorpkaepehterusneoT􏰇􏰎o
stfilregn􏰏ehtlitnusuaetalpnehtdnasesirylkciuqytimixorp􏰅ecafrusehtspatreg
yesael er 􏰆n􏰏asA􏰇􏰝 P􏰥 P􏰥 P􏰜􏰎otfilregn􏰏dna􏰅􏰝 P􏰥 P􏰥 P􏰥 P􏰜
y s s e r	p	xs s e r	p	t	s s e r	p ytimixorpregnmorfsretemarapezisdnanoitisopregn􏰏htiw
srekramlaropmetniaterseodSTMeht􏰅spatdnasdrohcregn􏰏fonoitceteddiaot
􏰅revewoH􏰇yrotcejartregn􏰏hcaefostnioptsapllaerotsotdeentonseodti􏰅gni
􏰆tirwdnahroserutseghtapxelpmocezingocerteytonseodSTMehtecniS
􏰇tnemerusaemdiortnecpuorgtnerrucehtnoyleritneseilermetsyseht􏰅langis
elbailer􏰅gnortsagnisuacecafrusehtsehcuotylmr􏰏regn􏰏ehtnehwtub􏰅yticolevhtap
dehsilbatseylsuoiverpehtnoylivaehseilermetsyseht􏰅kaeweraslangisnehwsuhT
xesael er
tesael er
zkaep	ykaep	xkaep	t kaep
zz
􏰈 􏰦􏰵 Pfi esle
P􏰋􏰤 􏰞􏰋􏰤
􏰤
􏰶
􏰎
􏰝􏰍􏰌􏰇􏰊􏰜 􏰳G
z
􏰈 􏰶 Pfi	􏰢􏰤
􏰵
􏰶
􏰢
􏰤lamronnahtrewolseitimixorplatothtiwspuorg
􏰎z
rofdesaercedsi Gelopretl􏰏ssap􏰆woleht􏰅Pytimixorplatothtiwylbaredisnoc
sesaercnistnemerusaemnoitisopfoytilibailerehtecniS􏰇snoitarelecedgnirudtoohs
􏰆revoylthgilsrosnoitareleccagnirudnoitisopregn􏰏lautcadnihebgalnactuptuo
retl􏰏noitisoptub􏰅noitomyticolevtnatsnocgnirudyaledgnikcartorezsahretl􏰏
12 10 8 6 4 2
−2 −4 −6
9 63 79
6 6
9
7 033
963
6
9
3 39
6
−15 −10 −5 0 5 10 15 Horizontal Position on Surface (X axis cm)
􏰇spetsemitniatrectastcatnoctnerrucllatcennocsenildettod􏰅seirotcejartehtfognimit
evitaleretacidnioT􏰇yrartibrasiredroxednI􏰇mhtiroglagnikcartehtfoytiunitnocevorpot􏰎otfil dna
yx
nwodhcuottadetnirperaisecidnihtapruofehTA􏰇thgirrewolehtta􏰎otfilotcrananignidilsdna
ecafrusehtfotfelrewolehtnoylsuonorhcnysanwodgnihcuotspitregn􏰏dnahtfelruoffoseirotcejarT􏰤􏰈􏰉􏰇􏰊erugiF
􏰊􏰈􏰈
Vertical Position on Surface (Y axis cm)
􏰋􏰈􏰈
noitaunitnoc􏰅tcatnocaotytitnedinadengissayltcerrocsahmetsysnoitac􏰏itnedi
dnahdnaregn􏰏ehtecnO􏰇􏰌retpahCnimetsysnoitazinorhcnysehtdnaretpahc
gniwollofehtnimetsysnoitac􏰏itnediehtfosbojehtse􏰏ilpmisoslassensselwa􏰐sti
􏰅sselehtreveN􏰇dezilitusinoitciderpdesab􏰆yticolevfiyllaicepse􏰅spf􏰠􏰌evobasetar
emarfdnasgnicapsregnemargolellaraptuoesahphcihwytisnedrosneslacitrevni
stnemevorpmiybdesserddatsebebnacseruliafesehtos􏰅seruliafnoitatnemgeserar
ehtfotsomsesuacsedortcelemargolellarapdevaelretniybgniraemslacitreV􏰇tsubor
teyelpmisylevitalerebotretpahcgniwollofehtfometsysnoitacapdnahtcnitsidyllanoitcnufot
dnopserrocylbailerspuorgedortceletahtossnoigernoitatnemgesdnaselurnoitceted
egdeehtfogninutluferaCyrassecensisutatsregn􏰏denetta􏰐dnanoitisopdnahtsapfo
kcabdeeflautxetnoC􏰇amini mlaitrapsuougibmaybylnodetarapesstcatnocniatnoc
snoitarug􏰏noctnatropmirehto􏰅tnemgesotysaednadetarapes􏰆llewerastcatnoc
snoitarug􏰏nocdnahdesuylnommoctsomehtnihguohT􏰇stcatnocgnirobhgien
morfsedortcelegnidulcnituohtiwsedortceles􏰛tcatnochcaeetaluspacneyltneic􏰑e
nacserutcurtsataddnasnrettaphcraespuorgxevnoc􏰆imes􏰅xevnocerastcatnoc
tsomecniS􏰇amixamlacoltnac􏰏ingistahtworgpuorgnoitatnemgesgnitrats yb
tneic􏰑eyrevebnacsmhtiroglanoitatnemgesegamiytimixorpos􏰅segamiytimixorp
dehtoomsniamixamlacolesuacylbairavnistcatnocdnahfosretnecehT
yrammuS􏰋􏰇􏰊
􏰇elbatsnuemocebsdnahdenetta􏰐yllufrofsnoitatnemges
tcatnocmlaperofnehwsinekorbylsuoenorrerode􏰒uhsertegotnwonkerashtap
hcihwni esacylnoehT􏰇sregn􏰏gnidilsruofrof gnikcartfoelpmaxeevitatneserper
asevig􏰈􏰉􏰇􏰊erugiF􏰇nwodhcuotregn􏰏ronoitarelecedemertxe ot eudylnoshtap
􏰌􏰈􏰈
􏰇sregn􏰏
elpitlumfos􏰎otfilrosnwodhcuotsuonorhcnystceteddnasesserpyekecnuobedot
desusrekramelcycefil htapehtfoytidilavehtserusneoslagnikcarttcatnocetar
􏰆uccA􏰇egamiytimixorpevisseccusyreveninoitac􏰏itnediergnizilibatsedyllaitnetop
dnaevisnepxeyllanoitatupmocgnidiovasuht􏰅􏰎ostfiltcatnocehtlitnuytitnedi
ehtniaterottneic􏰑useblliwmetsysgnikcart htapehtybhtaps􏰛tcatnoctahtfo
ofsezilituSTMeht hcihwstniartsnoclaci
􏰆nahcemoibdnasmhtiroglaehtstneserpretpahcehtnehT􏰇smetsysgnisneslacitpo
ybderutpacserutsegdnahfonoitingocernohcraesersuoiverpotsetalerSTMeht
rofmelborpnoitac􏰏itnediehtwohfonoissucsidahtiwsnigebretpahcsihT
􏰇noitalupinamcidrohcrognipytsadeterpretnisi m
gniebnoitommlaptuohtiwecafrusehtnoerehwynadnaheritneehttserylefasnac
srotarepotahtosdeitnedi bmuht dnagniredroregn􏰏elbailersallewsadnahgnitanigirorieht
htiwstcatnocecafrusfogniretsulcelbailersdnamed􏰌retpahCnidetartsnomedsnoit
idebdluohsspatyekesuacebelihwhtrow
ebtonthgimtcatnocecafrushcaeyfitnediotgniyrt􏰅gnipytrofdesuebotylno
sawSTMehtfI 􏰇tnuoctcatnocehtezilituylnosregnmmochtiwsnoitcaretnirofyrassecenneebtonsahnoitacsomnehwstcatnocecafrusrofseititnedileeh mlapdnapitregn􏰏gnihsilbatse􏰅gni
􏰆snesytimixorpeviticapacfognimoctrohsniamehtelkcatlliwretpahcsihT
NOITAMITSE
NOITISOPDNAHDNANOITACIFITNEDIREGNIF
􏰋retpahC
􏰡􏰈􏰈
ehtlennahcnoitalupinamlacihparghcihwgnizingocer􏰤strapeerhtotnidetarapes
ebnehtnacksatnoitingocerehT􏰇snoisnemideerhtroowtnistcejbolacihparg
gnitalupinamrofserutseglortnocelpmisfoyteiravaezingocerotsiSTMehtfoevit
􏰆cejboyramirpeht􏰅srezingocererutsegevitacinummocesehtottsartnocnI
serutseGevitalupinaMsusrevserutseGevitacinummoC􏰈􏰇􏰈􏰇􏰋
􏰇􏰪􏰢􏰋􏰈􏰨sevolGataDybdesnesserutsegsuounitnocvitatneserperezingocerotdengisedneebylnoevahsmetsys
ehtfotsom􏰅serutsegevitacinummocfotebahplanaezingocerot neebsahsmet
􏰆sysesehtfoevitcejboehtesuaceB􏰇segaugnalngissuoiravfoserutsopdnahecaps
astnuocrevenbmuhtehtsuhT􏰇bmuhtehttpecxeregn􏰏ynaot
refernac􏰖pitregn􏰏􏰩mretehttahtsnoissucsidgniwollofehtgnirudrebmemeR
􏰇snoitomdnahfoegnarelbatrofmocehtrofsnoitac􏰏itneditcerroc
nopuecnegrevnoc kciuqsesuacselcycgninnacs evisseccusrevoseludommetsys gni
􏰆kcartneewtebkcabdeefetacirtniwohetartsullilliwstluseR􏰇retsulctcatnocehtno
deretnecllatasignirrotcarttaehtrehtehwfosseldragerrehtdnuorastcatnocdnahelpitlumstros
ylevitce􏰎emhtiroglatnemngissaeht􏰅cirtemtsocderauqs􏰆ecnatsidrotcartta􏰆tcatnoc
ahtiwtahtnwohssitI􏰇stnioprotcarttaehtybdemrofmargaidionoroVdethgiew
ehtmorfdenimretedylelossistcatnocdetalosihcihwmelborp
􏰢􏰈􏰈
noitalupinamlacihpargrofdedeentonsnoitarug􏰏nocdnahgniniamerynA􏰇sloottne
􏰆re􏰎idtcelesotettelaplootgniwardehtotsnoisrucxetneuqerfgnidiova􏰅smargorp
gniwardnislootresarero􏰅sulyts􏰅hsurbtniapralucitraptcelesotdezilituebthgi m
snoitarug􏰏nocdnaheromwefA􏰇slennahcev􏰏dnamedylnognillorcswodniwdna
􏰅noitcelesrosructxet􏰅gnitnioprosructxet􏰅gniggardrosrucesuom􏰅gnitnioprosruc
esuomsahcussnoitalupinamlatnemadnuF􏰇slennahc nezodanahteromrof sesu
enigami neverorofsgnippameziromemotdrahebyamtidna􏰅slennahcnoitalu
snezodgniniatnocegaugnalerut
􏰆segcilobmysatroppusotyrassecenerasanoitalupinamlacihpargdevorpmirof
yrassecenerasnoitarugsnacrota
􏰆repoehthcihwslennahcnoitalupinamlarevesgnivahybylsuodnemertesaercninac
stnemnorivnelacihparghcirniycneicstnemeriuqertnere􏰎idyrevsecalpevitcejbonoitalupinamlacihpargehT
􏰇􏰌retpahClitnudesserddaebtonlliwnoitcartxeFOD􏰆itluM􏰇retpahc
suoiverpehtnidetcurtsnocshtaptcatnocehtotseititnedidnahdnaregnorpgnikcartdnanoit
􏰆celeslennahcehtelkcatlliwretpahcsihT􏰇tnednepedniydaerlaerasrosnesnoitator
llablanogohtromorfstnemerusaemecniseugolanaonsahstnenopmocnoitomtned
􏰆nepednignitcartxedna􏰅llabesuomehtfonoitatorehtgnirusaemotsdnopserroc
snoitomdnahgnikcart􏰅nwoddlehgnieberasnottubhcihwgnikcehcotsdnopser
􏰆rocnoitceleslennahcehtgnizingocer􏰅esuomaotderapmoCadnahevitalupinamehtgnikcart􏰅gnitcelessirotarepo
􏰣􏰈􏰈
spitregn􏰏ehtfodnuorgkcabehttahtossmlapehtgnihcuotnidelrucerasregn􏰏
ehtnehwgnidulcni􏰅snoitarug􏰏nocdnahfoyteiravediwanispitregn􏰏ev􏰏llaetacol
otkrowtengnippamraenillacol aniartyllufsseccus􏰪􏰌􏰈􏰈􏰨rettiRdnarekloN
􏰇aremacehtsecafyletamixorppahcihwdnahdehctertstuona
rofskrowylnohcaorppasihtylraelC􏰇snoitacolpitregn􏰏ehttneserperotdemussa
eraselgnaesehtgnolamlapehtmorftsehtrafspuorglexipehtdna􏰅selgnaregn􏰏
ehttneserpernehtmargotsihmrofsnarthguoHehtfoskaepehT􏰇elcricmlapeht
edistuospuorglexipforetnectsirwottcepserhtiwselgnaehtot􏰪􏰣􏰨mrofsnarthguoH
aseilppadna􏰅elcricgnisolcnehtsiretnecmlapehtgnimussaybmlapehtsetacoltsreneewtebnoitalerrocoCdnayelworC􏰇pitregn􏰏
aforetnecehtkcart dnaserutseggnitniopregn􏰏xedniezingocerotsmetsysrieht
detpadaevahsrehcraeserlareves􏰅gnisneslacitpoevissapfotxetnocehtnI􏰇spitregn􏰏
ehtsahcus dnahehtnostniopc􏰏icepskcartdnaetacoltsumsrezingocernoitalu
􏰆pinam􏰅sihtodoT􏰇snoisnemidlarevesninoitomdnahtcartxeylesicerpotelba
eboslatsumsrezingocernoitalupinamelihw􏰅elohwasanoitarug􏰏nocdnahhcae
fonrettapehtnrecsidot deenylnosrezingocerlobmystahtsiserutsegcilobmys
susrevnoitalupinamfonoitingocerneewtebecnere􏰎idtnatropmirehtonA
segamIlacitpOetomeRnihtiwsregniFgnitacoL􏰉􏰇􏰈􏰇􏰋
􏰇sdnammocroslobmys
tnere􏰎idotsFODlanoitalsnartowtdna􏰅gnilacs􏰅lanoitatorehtnisnoitomgnisoppo
gnippamyblennahcrepserutsegdnammocrolobmystcnitsidthgieotpuezingo
􏰆cernacSTMehT􏰇serutsegdnammocrolobmysrof	devreserebllits nacslennahc
􏰠􏰉􏰈
sawerehdetpodanoitulosehT􏰇gnitroshtapehtotnoitulosiroirpaonsi
erehtsuhT􏰇noosdna􏰅regn􏰏erofehtsi hcihw􏰅bmuhtehtsisregn􏰏ehtfo
hcihwllettonnac􏰝stelbatregn􏰏itlumdna􏰜emarFrosneSeht􏰅revewoH
􏰇esiratonseod
gnitros htapfonoitseuqehttaht hcuseraecivedehtfoscitsiretcarahc
ehTbotelbissopebdluowti􏰅suhT􏰇regn􏰏hcihwotsdnopserrochtap
hcihwgninimretedmelborponsiereht􏰅evolGataDehtsahcusnoitac􏰏issalchtapfo
noitanibmocgnitluserehtybserutsegehtgniyfitnedidnare􏰏issalchtapsihotnihtap
s􏰛regn􏰏hcaegnideefybsregn􏰏eerhtroowtgnivlovniserutsegxelpmocezingocer
otsawevitcejbos􏰛enibuR􏰇smetsys gnisnesecnaticapacregn􏰏saerutcurtsregn􏰏
etaidemretnignitcetednisnoitatimilemasehtdere􏰎us􏰪􏰣􏰉􏰈􏰅􏰢􏰠􏰈􏰅􏰡􏰠􏰈􏰨􏰝􏰢􏰊egaPno
noitpircsedrehtrufees􏰜emarFrosneSeht􏰅smaebthgilfonoitcurtsbodesneshcihw
metsyslacitpoevitcanasAgnisnesytimixorprof mel
􏰆borpnoitac􏰏itnediregn􏰏ehtderetnuocneevahotnwonkrehcraeserylnoehT
noitachcihw
erutcurtsregn􏰏etaidemretniehtfoytilibisivniehtybdetacilpmocyltaergsipitreg
􏰆n􏰏hcihwmorfsemoctcatnochcihwgninimreted􏰅revewoH􏰇detnemgesylreporp
erastcatnocehtecnoysaeylevitalererasegamiytimixorpssorcastcatnocgnikcart
dnasdiortnectcatnocgnirusaemyletarucca􏰅retpahcsuoiverpeht ni debircsedsA
􏰇segamilacitpoetomermorfnoitacolpitregn􏰏nahttnere􏰎idyllaitnatsbussinoitam
􏰆rofniegamiyti mixorpmorfspitregn􏰏gniyfitnedidnagnitacolfomelborpehT
segamIytimixorPmorfnoitac􏰏itnedIfoytilibisaeFehT􏰊􏰇􏰈􏰇􏰋
􏰇regn􏰏xedninafonoitceridgnitniop
ehttcartxeyllufsseccusoslarettiRdnarekloN􏰇hse􏰐mlaptsartnocwolfostsisnoc
􏰈􏰉􏰈
􏰇ylsuoenatnatsniregral emocebtonodyeht􏰅ecafrusehtotlamronspitregn􏰏naht
regralyllausuerasleeh mlapdnabmuhthguohT􏰇egamiytimixorpehtniraeppa
tonoderoferehtdnallataecafrusehtgnihcuottonerasleehmlapdnabmuhteht
semitemoSastcatnocmlapdnabmuhtfosnoitatneirodnaseziseht􏰅elpmaxeroF
􏰇denibmoceralareves nehw
erutcurtsdnahfoytilibisivniehtrof puekamylnonacyeht􏰅suougibmadnakaew
erastniartsnocemosecniS􏰇etatsgnikcartmetsysroegamiytimixorptnerrucehtni
elbaliavaerastniartsnoc	wef	yrevsemitemostaht eblli wegnellahcehT􏰇ygolopot
egamiytimixorpnonahcemoibdnalaci
􏰆motanawefaetiuqebottuonrutyllautcaereht􏰅noitanimaxeresolcnopU
ytitnedItcatnoCnostniartsnoCfoyrammuSsisylanaretsulclacihcrareih
􏰅re􏰏issalcehtfogniniartgniruDap
htapfosnoitanibmocllafosecnere􏰎iddnasmusehtmorfserutaeflabolgdetupmoc
euqinhcetgniretsulcsihT􏰇􏰪􏰠􏰊􏰈􏰨gnitroshtapdediovahcihwnoitingocererutseghtap
􏰆itlumxelpmocrofeuqinhcetgniretsulchtapapolevedot notnewenibuR
􏰇gniredroemasehtevahsyawlatondidstniopgnitratshtapesohwserutsegralimis
ybdesufnocebdluoctihguoht􏰅ezingocerotgniyrtsawehserutseghtapcashtapfognitroselpmisotdetroserenibuR
rrocfogniredroeht􏰤noitalergniredrosihtfoderiuqersi
ytreporpycnetsisnocehT􏰇shtapneewtebnoitalergniredronaesopmiot
􏰉􏰉􏰈
othguoneeraenolastniartsnoctcatnocoitalertcatnoc􏰆retnierom􏰅ecafrusehthcuotstrapdnaheromsA􏰇suidar
detimilahtiwretsulcralucricanidegnarraebotdnetdnahenomorfsleehmlap
dna􏰅spitregn􏰏􏰅bmuhteht􏰅rehtegotlladeredisnocnehW􏰇gniredrolatnozirohsiht
nierehwynaylraendesrepsretniebnacsleehmlapdnabmuhteht􏰅revewoH
􏰇setanidrooclatnoziroh
riehtfogniredroehthtiwedicniocdluohscrariehtnihtiwseititnedipitregnossorclaitrapfoytilibissopylnoehT􏰇dexe􏰐yllaitrapsregn􏰏ehthtiw
demrofrepyllamroneraseitivitcaesehtesuacebnoitalupinamcidrohcdnagnipyt
gnirudruccotondluohsrevossorcregn
sanoixe􏰐mrofinu􏰅tnerrucnoc􏰅dednetxeniamersrehtoehtelihwxe􏰐sregn􏰏eht
foemosekamotelbissopsitihguohtlA􏰇􏰝􏰠􏰈􏰇􏰉􏰬􏰡􏰇􏰉serugiF􏰜dednetxednadexe􏰐
erasregn􏰏saseiravsuidaresohwcralatnozirohaotnielttesot dnetspitregnfrusadnascitameniktniojregn􏰏fo
noitcaretniehT􏰇stcatnoclaudividnifoserutaefcirtemoegmorfderusaemebylpmis
tonnacdnaspihsnoitalertcatnoce􏰎idsyawlatonlliwsnoitatneiroesehtdna􏰅pitregn􏰏
lamronanahtreggibrogibsasitcatnocalitnuylbailerderusaemebtonnacitca􏰅ylthgilylnoecafrusehtnoleehmlaprobmuht
ehtsehcuotyllanoitnetnirotarepoehtfiofspitregnrusehttcapmiyehtsselnU
elba􏰏itnedi hcae 􏰒
􏰇strapdnahdevlovnifo
ytitnedinodnepedstcatnocelpitlumneewtebsnoitarapesdnaselgnadetcepxemuhtdenetta􏰐foezisetaredom􏰒
􏰇leehmlaprenni dnabmuhtfonoitatneirolanogaid􏰒
􏰤noit
􏰆ac􏰏itnedirofelbaliavasemitemoserastniartsnocgniwollofeht􏰅yrammusnI
􏰇erutsegehtfo dneehttarucco hcihw
stnemegnarraregn􏰏gnisufnoc􏰅emertxeehtyfitnedi otgnideentuohtiwybtegyam
erutsoplartuen􏰅laitiniehtfosnoitac􏰏itneditcerrocehterutsegatuohguorhtdnetxe
nachcihwmetsys A􏰇dneehtsdrawotnahtnoitalupinamafotratsehttasselebot
sdnetlartuenmorfnoitaivedroerutsopninoitairavehtsuhT􏰇dexe􏰐ylthgilssregn􏰏
dnathgiartsstsirwhti wserutsoplartuenotnruterotdnetsdnahhtob􏰅noitalupinam
lacihpargnidegagneylevitcatonnehwtahtsinoitavresbotnatropmilanecafruseht
nihcrathgilsadnatuoyalyektilps A􏰇ecafrusehtfoedisrehtoehtotedilsyam
yehthguoht􏰅rehtonaenorevossorcotdetcepxetonerasdnaheroferehT􏰇serutseg
STMniesufoebdluowti wohenigamiotdrahsiti􏰅ecnamrofreponaipdecnav
􏰆dagnirudsruccorevossorcdnahhguohtlA􏰇deniartsnocllewylriafoslaeraelohw
asadnahhcaefonoitatordnanoitisopeht􏰉􏰈
lartuenehtmorftratsnetfolliwyehthguoht􏰅noitisopdnahenoynamorftratsot
demussaebtonnacsnoitalupinamcidrohc􏰅ecafruseritneehtrevosedilsevlovnilli w
snoitalupinamcidrohcecniS􏰇dezisahpmerehtrufebdluohsmetsysnoitac􏰏itnedi
nanosecalphcaesdnamedgnitsartnoceht􏰅gnipytdrohcton􏰅noitalupinamcidrohc
troppusotsi STMehtnonoitac􏰏itnediregn􏰏foesopruplautcaehtecniS
􏰇STMehtnognipyt drohctroppus
othguoneetaruccaeraretpahcsihtnidetneserpsdohtemnoitac􏰏itnediregn􏰏eht
􏰅gnipytdrohcgniruddex􏰏ylriaferasnoitisopdnahllarevotahtnoitpmussaelbanos
􏰆aerehtneviG􏰇syekworemohehtrevonoitisopcitatsylevitaleranisniamerdnah
ehtsemehcshcusnitub kcurts
rotarepoehtfiylnosyeketavitcaotevitcirtserylirassecennuebdluowti􏰅deednI
􏰇ytitnediregn􏰏ybton􏰅tuoyal ehtnisnoigeryekehtotevitalerpatregn􏰏ehtfo
noitisopehtybdetacidniebdluohslobmysyekdednetniehT􏰇ytitnediregn􏰏eht
wonkotdeenlaeronsiereht􏰅tuoyal yekdetubirtsidyllanoitnevnocanosekortsyek
sadeterpretni	ylnoeraspatregnhtoonnehwecafrusehtnopatdetalosinadesuacpitregnroFspitregn􏰏lletotelbaebtonlliwmetsyseht hcihwnisecnatsni eb
llitslliwereht􏰅revewoH􏰇evobadetonstniartsnocehtfoplehehthtiwtontsaelta
􏰅elbatnuomrusnitonsimelborpnoitac􏰏itnedieht􏰅wohslliwretpahcsihtsA
sesaCdeniartsnocrednU􏰊􏰇􏰊􏰇􏰈􏰇􏰋
􏰇serutsopemertxenahtrehtarserutsoplartuenmorftratsyllaususerutseg􏰒
􏰇detimilylriafsnoitatordnahelbatrofmocfoegnar 􏰒
􏰌􏰉􏰈
yravdnaesoohcotmodeerfehtneviG􏰇STMehtrofsinoitac􏰏itnedisarotarepoeht
rof gnidnamedyllacinahcemoibdnaylevitingocsatsujebyamspitregn􏰏fosrebmun
ralucitrapnahtrehtarspitregn􏰏fosnoitanibmocralucitrapfolortnocdnanoitazir
􏰆omeM􏰇tirofseugraoslasrotcaf namuhfonoitaredisnoc􏰥esimorpmocyppahnuna
deredisnocebtondluohssnoitanibmocpitregn􏰏elbamrofrepfogniloopsihT
􏰇dehsiugnitsiderew
sregn􏰏eromroowtfosnoitanibmocllafi	dnah hcae	noelbaliavaebdluowhcihw
cdnagniredroreporpdnaecneserpbmuhtfonoitceted
tcerrocnopudnepedylnolliwrezingocernoitalupinamcidrohceht􏰅sesacllani
yltcerrocsregn􏰏yfitnediottsebstiodlliwmetsysnoitac􏰏itnediehtelihW􏰇tebahpla
cilobmyseritnenarevocot nahtslennahcnoitalupinamlacihpargelbaviecnoclla
revocotyrassecenerasdrohcrewefraftanibmocregn􏰏suougibma
yllaitnetoplooplliw􏰌retpahCniteserutsegdrohcehtfongisedluferaC
snoitanibmoCpitregniFfognilooPhcus􏰅mehtfosnoitanibmoctnere􏰎idtubspitregn􏰏foreb
􏰆munemasehtfognitsisnocsnoitalupinamcidrohcesohthsiugnitsidylbailerotelba
ebtonlliwSTMehttahthguonenetforuccosnoitaivedhcuS􏰇etamitsenoitisop
dnaheht ybdetciderpsnoitacollatnoziroheht	morfretemitnecanahterometaiv
􏰆edspitregn􏰏ehtfignihcuoteraspitregn􏰏hcihwyltcaxelletotyawerusonevah
lliwSTMeht􏰅ecafruseht gnihcuoteradnahanospitregn􏰏llasselnu􏰅eroferehT
􏰇spitregn􏰏neewtebre􏰎idottondnetserutaefesehtalerdnaahemasehtot
gnidnopserrocsegamiytimixorpevisseccusmorfspuorgehtsknilniegami ytimixorptnerrucehtstnemges
􏰊retpahCnidebircsedeludomnoitatnemgesegamiehThtroF
metsySnoitac􏰏itnedIdnagnikcarTdnaHehtfoweivrevO􏰉􏰇􏰋
􏰇snoitomelbasoppoeuqinustierutpacot
noitceleslennahclaitiniehtretfasdrohcesehtot deddaebnacbmuhtehthguoht
􏰅slennahcnoitalupinamlacihpargtceleslliwspitregn􏰏foylelosdesopmocyllaitini
sdrohcdna􏰅slennahcerutsegdnammocfonoitcelesrof devresereblliwbmuhteht
gnidulcnisdrohctsidylevitingocebyambmuhtehttahttseggusoslabmuhtehtotdetovedxet
􏰆rocrotom􏰆yrosnesfonoigeregral ylevitaler dnabmuhtehtfonoitomelbasoppo
yleuqinuehT􏰇sregn􏰏fotesralucitrapadnepsusrosserphcihwselcsumregn􏰏eht
foesurevodiovasplehoslagnitniopsahcusslennahcnoitalupinamnommocgnissec
􏰆canehwsnoitanibmocpitregn􏰏egnahcretni otrotarepoSTMehtgniwollA
􏰇spitregn􏰏fopuorg
suougitnocafodesopmocsdrohcsagnolsaeciwt kootllitssdrohcdrawkwatub
􏰅stcejbuslamronsatsafsaeciwttuobasdrohcllamrofrepdluocstsinaipdeniarT
dimehtsahcussregnhcihwnisnoitanibmockcipotreferpyllarutansrotarepo
ION
PATHS FROM PREVIOUS IMAGES
CURRENT PROXIMITY IMAGE
PARAMETERIZED ELECTRODE GROUPS
NEW PATHS &amp; UPDATED PATH PARAMETERS
IDENTIFIED CONTACT PATHS
ESTIMATED HAND &amp; FINGER OFFSETS
CONTACT PATH TRACKING
HAND IDENTIFICATION
FINGER &amp; PALM IDENTIFICATION
HAND POSITION ESTIMATION
z-1
􏰡􏰉􏰈
􏰇seludo m
noitac􏰏itnedidnagnikcartregn􏰏dnadnahrofmargaidlevel􏰆metsyS􏰤􏰈􏰇􏰋erugiF
􏰢􏰉􏰈
nevenoitisopdnahllarevofoetamitseevitavresnocaniatniamotseititneditcatnoc
dengissaehtsallewsasnoitisoptcatnocesulliwrotamitsenoitisopdnahehT
􏰇dnah
thgirehtsetonedHRdnadnahtfelehtsetonedHL􏰅elohwasadnahralucitrapaot
gnirrefer nehW􏰇htapregn􏰏xednithgirehtsetoned􏰉FR􏰅elpmaxerof􏰥dnahthgir
rofRrodnahtfelrofLnahtiwdex􏰏erpebnacnoitatonsihtytitnedidnahralucit
􏰆rapaetonedoT􏰇􏰈􏰇􏰋elbaTfonoitatonehthtiwotderrefereblliwshtaptcatnoc
de􏰏itnediehT􏰇shtaptcatnocllaot dehcattasecidniregn􏰏dnadnahorez􏰆noneblli w
seludomnoitac􏰏itnediehtfotuptuoehT􏰇snoitac􏰏itneditcatnocfotestnetsisnoc
yllacimotanadnayllacinahcemoibtsomehtdntselliweludomnoitac􏰏itnediregn􏰏ehtdna􏰅tcatnochcaesesuacdnahhcihw
enimretedlliweludomnoitac􏰏itnedidnahehT􏰇retpahcsihtfosucofehteraseludom
noitamitsenoitisopdnahdna􏰅noitac􏰏itnedidnah􏰅noitac􏰏itnediregn􏰏ehT
􏰇htap hcae
gnolayticolevlaretalsuoenatnatsniehtsahcusseirotcejarttcatnocehtotdetaler
stcatnoCwo􏰐revO􏰟sesullaCmlaperoF􏰈􏰈F􏰤􏰤􏰤􏰢F
leeHmlaP􏰝laidem􏰜rennI 􏰡F
leeHmlaP􏰝laretal􏰜retuO 􏰍F
regni Fykni P	􏰌F
regni Fgni R	􏰋F
regniFelddi M	􏰊F
regni FxednI	􏰉F
􏰝pitregn􏰏aton􏰜regni FbmuhT	􏰈F
traPdnaHymmuD􏰟lluNehT	􏰠F
emaNtraPdnaHnommoCnoitatoN
􏰇serutcurtsatadhtapdecniyllaudarg􏰅semarf
egamilarevesrevoraeppayllaudargyamstraplaudividnisti􏰅nwodsehcuotdnah
asA􏰇emarftsr􏰏ehtnitcerrocebotderiuqertonerasnoitac􏰏itnedieht􏰅sraep
􏰆paregn􏰏ehthcihwniemarfegamitsr􏰏eht	morf htapregn􏰏hcaefoytiunitnoc
tcefrepniatniamdnaetaercotdetcepxesi hcihw􏰅rekcarthtapehtekilnU
􏰇ecafruseht 􏰎odetfil
ebotdemussaeratcatnocymmudaotdeppampudneseititnediesohwsregniF
􏰇eno􏰆ot􏰆enognippamehtpeekotdetaerceblliwstcatnocymmud􏰅seititnedielbissop
nahtecafrusehtnotneserperastcatnocrewefnehW􏰇seititnediregnalpmettcejbofoyrarbilamorfhctam
tsebehtgnikcipnahtrehtaRastimilreppuesuacebsnoitacilppanoisivretupmoctsomninahtdeniarts
􏰆noceromsiegatsnoitingocertcejboeht􏰅esacsihtnI􏰇egatsnoitingocertcejboehtot
gnidnopserrocseludomnoitac􏰏itnediregn􏰏dnadnahehthtiw􏰅smetsysnoisivretup
􏰆moctsomfotahtselbmeserylssorgerutcetihcrametsysllarevosiht􏰅niagA
􏰇tsr􏰏debircsedeblliwmhtiroglanoitamitsenoitisopdnaheht􏰅smhtirogla
noitac􏰏itnediregn􏰏dnadnahehtfogninoitcnufetaruccaehtotlaitnessesi kcabdeef
noitisopdnahdetamitsesihtesuaceB􏰇snoitac􏰏itnedisuoiverpriehtniagerylekil
eromlliwyehtusaem
tsalehtniateryliraropmetlliwnoitisopdnahdetamitseeht􏰅noitac􏰏itnedidnahdia
oT􏰇erutcurtstnerappaonsahretsulctcatnocs􏰛dnahehttahtegamiehtniraeppa
stcatnocwefosnehwstcatnocyfitnediplehlliwssecorpnoitac􏰏itnediregn􏰏ehtot
snoitisopdnahdetamitseehtfokcabdeeFhcihwsnoitac􏰏itnedireilraenopudesabsetamitsenoitisopdnah
suoiverphtiwstcatnocstifoseititnedidnasdiortnecehtnopudesabstnemerusaem
noitisopdnahtnerrucsenibmocetamitseeht􏰅ecafrusehtsehcuotyllaitrapdnaha
nehW􏰇niaganwodhcuotlliwtierehwfosseugtsebastneserperetamitsenoitisop
sti􏰅ecafrusehtgnihcuottonsidnahaesacnI􏰇segamiytimixorpehtnielbisiv
hse􏰐onhtiwecafrusehtevobagnitao􏰐sidnahanehwgnidulcni􏰅snoitidnoclla
rednunoitisopdnahlaretalfosseugevitavresnocaebotdednetnierasetamitse
noitisopdnahehT􏰇sessecorpnoitatnemgesdnanoitac􏰏itnediehtotkcabdeefgnisaib
tnatropmisedivorprotamitsenoitisopdnahehtseht􏰎odetfilyliraropmeterasregn􏰏elihwsiseretsyhfo
mrofgolana􏰅rekaewaedivorpsetamitsenoitisopdnaheht􏰅tnerappaemoceblliwsA
􏰇niagasetucexemhtiroglanoitac􏰏itnedieht􏰅serutaefreraelcrotcatnoclanoitidda
nasahcusstniartsnocwenhtiwsraeppaegamiytimixorpasanoossAxenevetondeen
mhtiroglanoitac􏰏itnediehT􏰇noitaunitnochtapaivdednetxeylpmiserastcatnoc
gnitsixefoseititnedieht􏰅noitac􏰏itnediottnenitrepnoitamrofniwenonniatnoc
hcihwsegamiroFwoH
􏰇detal
􏰆umuccaevahseulcnoitarug􏰏nocdnaherofebtnemngissanaottimmoctonseod
metsysehtosdewollasierutsegdnahaniylraesnoitac􏰏itnedifogni􏰒uhseR􏰇rehto
􏰆naenoneewtebkcabdeeffosnoitaretiwefarevonoitulostnerehocanoegrevnoc
ot detcepxeebylnonacyeht􏰅meht neewtebkcabdeefehttuohtiwdeniartsnocred
􏰆nuebyamseludomnoitamitsenoitisopdnahdna􏰅noitac􏰏itnedi􏰅noitatnemgeseht
􏰈􏰊􏰈
derusaemehtniatbootstes􏰎otcatnoclaudividniehtsegarevapetstxenehT
􏰇etamitsenoitisopdnahetaruccaylriaf aniatsusottneic􏰑useblliwtcatnocecafrus
de􏰏itnediylreporp􏰅elgnisafonoitisopeht􏰅noitatupmoctes􏰎otnedneped􏰆ytitnedi
sihthtiW􏰇tuoyalyekehtfoworemohnognitsernehwsa􏰅􏰝􏰢􏰇􏰉erugiFees􏰜desolc
yllaitrapsregn􏰏htiwerutsoplartuenanisidnahehtnehwsnoitisopmlapdna
regn􏰏otdnopserrocsnoitisoptluafedesehT􏰇iytitnedistihtiwleehmlaproregn􏰏
yfed xfed
y x
ralucitrapehtfo􏰝 iF􏰥 iF􏰜noitisoptluafedehtdna􏰅􏰝􏰪n􏰨iF􏰥􏰪n􏰨iF􏰜􏰅noitisop
derusaemstineewtebtes􏰎onatcatnochcaerofetupmocotsnoitac􏰏itnedidnah
􏰆nihtiwehtsezilitutnemerusaemnoitisopdnahs􏰛STMeht􏰅egarevaehtgnitupmoc
nehwdnahehtforetneceht morfsemoctcatnochcaegnimussafodaetsnI
retnecdnaheurteht􏰅leehmlapa
morfyllautcasitcatnocehtfiro􏰅thgirehtotmc􏰢􏰆􏰋ebdluowretnecdnaheurteht
􏰅bmuhtthgireht	morf yllautcasitcatnocenolehtfI􏰇ecafrusehtotnodenetta􏰐si
dnahehtfotserehtlitnuecafrusehthcuotnevetonseodyllausuretnecdnaheht
esuacebretnecdnaheht	morf ebotylekilnuyrevsitcatnocdnahenol	A􏰇tcatnoc
enolsihtfonoitisopehttasi retnec dnahehtemussaylgnorwdluowtnemerusaem
ehT􏰇egarevaehtrofelbaliavasitcatnoc dnahenoylnonehwredisnoc􏰅revewoH
􏰇mlapehtforetnecehtdnuoragniramrofyllacipytsleehmlapdnasregn􏰏ehtecnis
mlapehtforetnecehtrednuerehwemosgniyl􏰅etamitsetnecedaebdluowdiortnec
gnitlusereht􏰅􏰡􏰇􏰉erugiFfodnahdenetta􏰐ehtnisa􏰅ecafrusehtgnihcuotsyawla
erewstrapdnahllafI 􏰇ytitnedifosseldragerstcatnocs􏰛dnahehtllafosnoitisop
ehtegarevaotebdluowtnemerusaemnoitisopdnahagniniatbofodohtemtselpmis
ehT􏰇egamiytimixorptnerrucehtnisnoitisoptcatnocehtmorfderusaemebtsum
noitisopdnahllarevona􏰅tsriF􏰇yletarapesdnahhcaerofdetaeperebtsumhcihw
􏰅ssecorpnoitamitsenoitisopdnahehtfospetslaudividniehtswohs􏰉􏰇􏰋erugiF
noitisoPdnaHtnerruCgnirusaeM􏰈􏰇􏰊􏰇􏰋
START
END
GET HAND'S IDENTIFIED PATHS
COMPUTE OFFSETS BETWEEN EACH FINGER'S MEASURED AND DEFAULT POSITIONS
COMPUTE AVERAGE OF OFFSETS WEIGHTED BY CONTACT PROXIMITY
ADJUST FILTER POLE TO CURRENT IDENTIFICATION CONFIDENCE
COMPUTE WEIGHTED AVERAGE OF HAND CONTACT VELOCITIES
AUTOREGRESSIVELY UPDATE HAND OFFSET ESTIMATES FROM MEASURED OFFSETS AND VELOCITIES
UPDATE FINGER OFFSET ESTIMATES
IMPOSE MINIMUM SEPARATION BETWEEN HANDS
CONVERT ESTIMATED OFFSETS TO ABSOLUTE POSITIONS
􏰉􏰊􏰈
􏰇ssecorpnoitamitsenoitisopdnahfotrahc wolF􏰤􏰉􏰇􏰋erugiF
􏰊􏰊􏰈
xo m
ebot􏰪n􏰨 Hesuacylisaenacti􏰅dnahgnorwehtotdetubirttasitcatnocenoleht
fI􏰇mc􏰢sahcumsayb􏰎oebnactnemerusaemnoitisopdnaheht􏰅ytitnedimlap
roregn􏰏gnorwehtdengissasiti dnaecafrusehtgnihcuotsitrapdnahenoylnofi
􏰅elpmaxeroF􏰇revoezimitpootstcatnocfoelpuocasahylnometsysnoitac􏰏itnedi
ehtfiyllaicepse􏰅esacehtsyawlatonsihcihw􏰅tcerrocsyawlaerasnoitac􏰏itnedi
semussatI􏰇noitac􏰏itnedisimtcatnocotelbitpecsussi dohtemsiht􏰅revewoH􏰇sret
􏰆emitnecelpuocanahteromtnemerusaemehtbrutreptonlliwsnoitisoptluafed
riehtmorfsregnsaybdesuacsrorreegralehtsetanimiletnemerusaemnoitisopdnahrofdohtem
tnedneped􏰆noitac􏰏itnedisiht􏰅tcerrocerasnoitac􏰏itneditcatnocehtsagnolsA
yaleDretliFdnaecnedretl􏰏ehtesuaclliwsihT􏰇orezotteserastes􏰎oderusaemeht
􏰅orezeraseitimixorpllanehw􏰅􏰇e􏰇i􏰅ecafrusehtgnihcuottonsidnahanehW
􏰇noitisopdnahllarevofonoitacidnielbailereromaylppus􏰅sregn􏰏
elbixe􏰐ylhgihehtotderapmocretnecdnahehtotevitalerelibommi gnieb􏰅sleeh
mlapehtesuaceblaic􏰏enebsi sihT􏰇egarevaehtetanimodlliwseitimixorplatot
egralrieht􏰅ytimixorplamronnahtrewolhtiw
stcatnoctahtdna􏰅egarevaehtnoecneu􏰐nionevah􏰅orezsiytimixorpesohw􏰅sregn􏰏
z
detfiltahtserusnesihT􏰇􏰪n􏰨iF 􏰳􏰪n􏰨
wo m
iF􏰅􏰇e􏰇i􏰅ytimixorplatotderusaemsti
wo m
yletamixorppasileehmlapdnaregn
i F
xfed
x
tyacedyllaudargstes􏰎odetamitseeht
􏰅ecafrusehtgnihcuottonsi dnahaelihwtahthguonellamsedamsi􏰊􏰇􏰋noitauqE
nimret􏰏eht􏰅eroferehT􏰇tserot􏰅noitisoptluafedro􏰅erutsoplartuenehtotnruter
yllautneveotylekilsi dnahehtkciuqtsum
dnasnoitarepofoecneuqesxelpmocafoelddimehtniecafruseht􏰎ostfildnahafI
􏰇ecafrusehtevobagnitao􏰐sidnahehtelihwnoitisopdnahfoetamitseevitavresnoc
adlohtsumstes􏰎oderetl􏰏eht􏰅ecafruseht􏰎odetfilyliraropmetsahhcihwdnah
arofnwodhcuotnopunoitac􏰏itnedidnanoitatnemgestcerrocegaruocneoT
􏰇snoitac􏰏itnedielbailereromnopudesaberewyehttahtepoh
ehtnisetamitsetesfruseht hcuotylmr􏰏dnahehtfostrapynam
nehw􏰅􏰇e􏰇i􏰅hgihsisnoitac􏰏itneditcatnocniecned􏰏nocnehwsuhT􏰇elbatsnueb
􏰎o
lliwretl
􏰈
X
􏰡 􏰦i
􏰤dnahehtrofseitimixorptcatnoc
􏰎o
fomusehtotlanoitroporpylhguortessi􏰪n􏰨HeroferehT􏰇noitatneirodnaezis
sahcusserutaefgnitaugibmasidgnortsevahyehtesuacebytilibailernoitac􏰏itnedi
evorpmioslaseitimixorplatotegralhtiwstcatnoC􏰇ecafrusehttfeltsaldnaheht
ecnisdnahehtmorfnwoddehcuotevahhcihwsregn􏰏forebmunehtsiecned􏰏noc
noitac􏰏itnedifoerusaemelpmiseno􏰅ecafrusehttcatnocdnahehtfostraperom
saytilibaileretalumuccasnoitacdroccadetsujdaebdluohs􏰪n􏰨 Helopretl􏰏ehT
yoe	xoe
􏰇􏰝􏰪n􏰨 H􏰥tarapeslatnozirohmumini maesopmiotsuoegatnavdasiti􏰅rafosdebircs
􏰆edsatnednepedni	neebevahdnahhcaerofsnoitatupmoctes􏰎oehtelih W
noitarapeSdnaHgnicrofnE􏰋􏰇􏰊􏰇􏰋
􏰇􏰊􏰇􏰋eru
􏰆giFnidetartsullioslasisnoitauqeesehtybderutpacerutcurtsretlmmi neewtebderusaemnoitomtcatnocehtgniddayb
detanimileylefasebnacyaledgnikcartnoitomsihteroferehT􏰇ytitnediregn􏰏fo
y v	xv
tnednepedni􏰝􏰋􏰇􏰊􏰇􏰊􏰜ssecorpgnikcarthtapehtnisrucco􏰝􏰪n􏰨 iF􏰥􏰪n􏰨 iF􏰜seiticolev
tcatnoctnerrucehtfotnemerusae M􏰇noitomtcatnochtoomsotton􏰅ytitneditcatnoc
nisegnahcelbanoitseuqotylwolstcaerotsiretl􏰏ehtfoesoprupehT􏰇sretemitnec
larevesybsregnrdeepstnatsnocataorezdrawotyacedot
edamebnacstes􏰎odnahdetamitseeht􏰅ylevitanretl A􏰇noitisoptluafedotsnruter
1
z
Current Contact Identities
Current Contact Positions
Default Finger Positions
Current	Unit Delay Contact Vlocities
Predicted Offsets
Measured Offsets
Hand’s Current Identification Confidence
Estimated
Filter	Hand Offsets Pole	Separation
Enforcer
Updated Hand &amp; Finger Position Estimates
Segmentation &amp; Identification Modules
􏰍􏰊􏰈
􏰇tluafedmorfstes􏰎oevitalerfomrof
ehtnierasegatsetaidemretnitaslangisehttahtosegatstsalehtta
ni kcabdeddadnaegatstsr􏰏ehtni 􏰎odetcartbuserasnoitisopregn􏰏
tluafedehttahtetoN􏰇rotamitsenoitisopdnahrofmargaidretliF􏰤􏰊􏰇􏰋erugiF
􏰡􏰊􏰈
􏰇snoitac􏰏itnediehthtiw
tnetsisnocemocebotylkciuqegnahcotdewollaebdluohsetamitsenoitisopeht􏰅meht
stcidartnocetamitsenoitisopehttubhgihsisnoitac􏰏itnediniecned􏰏nocnehw􏰅dnah
􏰎o
rehtoehtnO􏰇wolsisnoitac􏰏itnedi niecned􏰏nocehtnehw􏰪n􏰨
Helopretl􏰏ehthtiw
noitisopdnahdetamitseniegnahcfoetarehttimilottnatropmiossitiyhwsihcihw
􏰅snoitac􏰏itneditcerrocniecrofnieroslanactituB􏰇srotcarttadengissariehthtiw
rettebpuenilstcatnocehttahtosgnirrotcarttaehtevomot􏰅mehtezilibatsotsdnet
siht􏰅tcerrocerasnoitac􏰏itneditnerrucehtgnimussA􏰇snoitac􏰏itneditcatnoctnerruc
ehtecrofnierotsaosevomotdnetsetamitsenoitisopdnahdetadpuehT
􏰇tneinevnoceromyllautcasinoitatneserpertesmiytimixorptxenehtfosisylanagnirudsessecorpnoitac􏰏itnedidnanoitatnemges
ehtot kcabdefera􏰝􏰪n􏰨
yoexoe
H􏰥􏰪n􏰨
H􏰜setamitsenoitisopdnahdetadpuehT
seludoMnoitac􏰏itnedIdnanoitatnemgeShtiwsnoitcaretnI􏰌􏰇􏰊􏰇􏰋
􏰇dnahtfelehtfotes􏰎odetamitsetnerrucehtsi
xoe
􏰪n􏰨HLdna􏰅desopmiebotnoitarapeslatnozirohmuminimehtsipesdnahnim
xfe d
xfe d
􏰅sbmuhtthgirdnatfelneewtebnoitarapestluafedehtsi􏰝 􏰈FR􏰯 􏰈FL􏰜erehw
xoe
xfe d
xfe d
xoe	xoe
􏰝􏰢􏰇􏰋􏰜􏰝pesdnahnim􏰞􏰪n􏰨HL􏰞􏰝􏰈FR􏰯􏰈FL􏰜􏰥􏰪n􏰨HR􏰜nim􏰦􏰤􏰪n􏰨HR
􏰤yawehtfotuonoitisopdnahthgirdetamitse
ehtecrofotdeilppaebnacgniwollofehtekilnoitauqena􏰅tonsitfelehtdnadetfil
si dnahthgirehtfI􏰇yletelpmocecafruseht􏰎oylbissop􏰅ecafrusehtfotfelrafeht
ottluafed morfdecrofebdluowdnahtfeldetfilehtfonoitisopdetamitseehtesac
sihtnI􏰇decalpsidsi dnahrehtoehtfonoitisopdetamitseeht􏰅detfilsidnahrehto
ehtelihwdraobehtfoedisetisoppoehtotsedilsdnahthgirehtsahcusdnaha
nehwtahthcusnoitisopdnahthgirdetamitsednanoitisopdnahtfeldetamitseeht
􏰢􏰊􏰈
suidarahtiwgniranieilotsrotcarttaehtsesuacerutsopdnahdesolc􏰆flaheht
morfstnioprotcarttaneewtebselgnadnasecnatsidehtgnitteS􏰇tuoyalyekehtni
syek woremohehtfosretneceht hctamosladluohssnoitisoppitregn􏰏tluafedehT
yfed xfed
􏰇noitamitsenoitisopdnahnideyolpme􏰝iF􏰥iF􏰜snoitacolmlapdnaregn􏰏
tluafedemasehtebdluohsesehT􏰇􏰝􏰢􏰇􏰉erugiF􏰜delrucyllaitrapsregn􏰏htiwerutsop
tluafedehtnisi dnahehtnehwsmlapdnasregn􏰏gnidnopserrocriehtfosnoitisop
etamixorppaehtotteserastnioprotcarttaehtfosnoitacol evitalerehT
􏰇shtapymmudotdengissa􏰅􏰇e􏰇i􏰅dell􏰏nutfeleblliw
srotcarttaemos􏰅ecafrusehtgnihcuottonerastrapdnahemostahtesacehtnitub
􏰅eno􏰆ot􏰆enoebdluohssrotcarttadnashtaptcatnocneewtebgnihctamsihT􏰇leeh
mlaproregn􏰏ralucitrapaotsdnopserrochcihwytitnedinagnivahtnioprotcartta
hcae􏰅stnioprotcarttatrapdnahfoetalpmet aotstcatnoc hcta motstpmettacatnocehtroF
gniRrotcarttAcisaBehT􏰈􏰇􏰋􏰇􏰋
􏰇stcatnocdnahtfelgniyfitnedierofebsretemarapemosotdeilppaeb
tsumyrtemmysrorrim􏰥dnahthgirehtnihtiwnoitac􏰏itnedirofsiwolebnoitpircsed
ehT􏰇tsr􏰏debircsedeblliwti􏰅ecafruseht not􏰏nacsdnahynamwohfosseldrager
dnahhcaerofemasehtskrownoitac􏰏itnediregn􏰏roretsulc􏰆nihti wesuaceB􏰇retsulc
s􏰛dnahrehtoehtfotnednepedni􏰅retsulcs􏰛dnahhcae nihtiwstcatnocfotnemegnar
􏰆raehtsezylanassecorpnoitac􏰏itnediretsulc􏰆nihtiwanehT􏰇sgnolebtcatnoc hcae
retsulchcihwotsedicedtsr􏰏ssecorpnoitac􏰏itnedidnahehT􏰇tilpsyllacihcrareih
simetsysnoitac􏰏itneditcatnoceht􏰅retsulcs􏰛dnahrehtoehtnihtiwtnemegnarra
dnafonoitacolehtfotnednepedniyllaususiretsulcdnahanihtiwsregn􏰏fotnem
􏰆egnarraehtesuaceB􏰇sdnahetisoppofosregn􏰏ehtgnilgnatnediovaotekilsrota
􏰆repoesuacebetarapesniamerotdnetsretsulcehtdna􏰅retsulcralucricamrofot
dnetdnahhcaefostcatnoceht􏰅sdnahelpitlumrofhguoneegralsecafrusnO
noitac􏰏itnedIregniF􏰋􏰇􏰋
START
HAND ASSIGNMENTS TENTATIVE ?
N
# FINGER ATTRACTORS Y ASSIGNED > 1
DEFINE IDENTITY ATTRACTORS AT DEFAULT CONTACT POSITIONS
TRANSLATE ATTRACTOR TEMPLATE BY ESTIMATED HAND OFFSET
COMPUTE MATRIX OF DISTANCES FROM EACH PATH TO EACH ATTRACTOR
COMPUTE ATTRACTOR WEIGHTING FACTORS FROM FEATURES OF EACH PATH
FIND ASSIGNMENT BETWEEN PATHS AND ATTRACTORS WHICH MINIMIZES SUM OF WEIGHTED DISTANCES
OR &lt; 5 ? YN
UPDATE FINGER COUNTS AND SUBSETS
VERIFY THUMB ASSIGNMENT
END
􏰣􏰊􏰈
􏰇mhtir
􏰆oglanoitac􏰏itnedi􏰝dnah􏰆nihtiw􏰜 mlapdnaregn􏰏ehtfotrahc wolF􏰤􏰋􏰇􏰋erugiF
􏰠􏰋􏰈
􏰇snoitisopleehmlaptluafedderusaemehtnahtrewolsretemitnecelpuocayllautca
erasrotcarttamlapeht􏰅􏰌􏰇􏰋erugiFfomargaidionoroVdethgiewnuehtnI
􏰇tnemecalpsidlatnozirohdetcepxenuronoitator
dnahhcumostontubnoisnetxednanoixe􏰐regn􏰏foeergedhgihaetarelotlli wsllec
ionoroVehttahtedulcnocnaceno􏰅worrandnallatrehtareraspitregn􏰏rofsllec
ionoroVehttahtneviG􏰇yltcerrocde􏰏itnediebllitsdnaretnecdnahdetamitseot
tcepserhtiwnwodhcuotnactrapdnahralucitrapahcihwrevoegnarehtsetacidni
llecionoroVhcaefoepahsdnaezisehtsuhT􏰇rotcarttas􏰛llectahtottcatnoceht
ngissadnanihtiwseiltcatnocehtllecionoroVhcihwenimretedylpmisnacmhtirogla
noitac􏰏itnedieht􏰅ecafrusehtgnihcuotsi dnahafotrapllamselgnisaylnonehw
􏰅􏰇e􏰇i􏰅gnihsiugnitsidtoneraserutaefstidnaretsulcdnahehtnitcatnocenoylno
sierehtnehW􏰇􏰪􏰍􏰈􏰈􏰨gnirehtnirotcarttarehtoynanahtrotcarttatahtotresolc
sillecionoroVs􏰛rotcarttananihtiwtniopcirtemoegyrevE􏰇nwohstahtmorfsixa
lacitrevehttuobaderorrimebtsumgnirrotcarttaeht􏰅dnahtfelasidnahnevig
ehtfI􏰇rotcarttahcaednuorallecronogylopionoroVehtdnadnahthgirehtrof
stnioprotcarttaehtybdemroferutcurtsekilfed yoe y
yx
􏰤nehtera􏰝􏰪n􏰨 jA􏰥􏰪n􏰨 jA􏰜snoitisop
rotcarttalan􏰏ehT􏰇tes􏰎onoitisopdetamitses􏰛dnahehtybelohwasadetalsnartsi
dnahnevigarofgnirrotcarttaeht􏰅eroferehT􏰇retsulcdnahehtnoderetnecylhguor
tpekebdluohsgnireht􏰅gnihctamrotcartta􏰆tcatnocfoycaruccalamitporoF
􏰇snoisnetxedna
snoixe􏰐regn􏰏foyteiravediwarofllewmrofrepotmhtiroglagnihctamehtwolla
lliwsihT􏰇ts􏰏adnadnahdenetta􏰐􏰅dehctertstuonafotahtneewtebyawflahtuoba
12
10
8
6
4
2
0
−2
−4
−6
0 2 4 6 8 10 12 14 16 18 20 Horizontal Surface Position (cm)
Vertical Surface Position (cm)
􏰈􏰋􏰈
􏰇snoitisopregnnediregn􏰏htiwdelebals􏰛x􏰜stniop
rotcarttatrapdnahfognirdnuoradetcurtsnocmargaidllecionoroVadetalsnarthcae
􏰉
dnaiPhtaptcatnochcaeneewtebenalpecafruseht nisecnatsidderauqsehtteL
􏰇srotcarttaforebmunehtsaemasehtstcatnocymmudsulpecafrusforebmunlatot
ehtgnikamybdnuofebnacgnippameno􏰆ot􏰆enoaserusnesihT􏰇stcatnocgnissi m
ynafoecalpnitcatnocymmudasastca􏰅rotcarttahcaeotecnatsidorezsahhcihw
􏰅issastidnarotcartta
hcaeneewtebsecnatsidderauqs􏰅dethgiewfomusehtseziminimhcihwstcatnocdna
srotcarttaneewtebtnemngissaeno􏰆ot􏰆enoehtsdn􏰏noitazimitpolabolgsihT
􏰇srotcarttagnirobhgienotseog hcihwdnallecionoroV
deipuccoehtforotcarttaehtotseogstcatnocehtfohcihwenimretedotyrassecen
sinoitazimitpolabolg A􏰇srotcarttagnirobhgiendnarotcarttas􏰛llecehtrofetepmoc
tsumllecemasehtnigniylstcatnoceht􏰅emitenotarotcarttas􏰛llecionoroVyna
ot dengissaebnacstcatnocehtfoenoylnoecniS􏰇llecionoroVemasehtnihtiw
eil yamenonahterom􏰅ecafruseht hcuot drahafostrapelpitlumnehW
slleCionoroVrofetepmoCstcatnoCelpitluM􏰊􏰇􏰋􏰇􏰋
􏰇desueb
otsnoitisopleehmlaptluafedeurtehtgniwolla􏰅sllecionoroVleehmlapehtknirhs
hcihwsgnithgiewleehmlapecudortnilliwregnipyt
esuacyllautcanacsihT􏰇leehmlapasade􏰏itnedisimyllanoisaccosiyknipdexe􏰐ro
bmuhtdetcudbanatahtegraloserasllecionoroVleehmlapehT􏰇ezisllecionoroV
yknipdnabmuhtfotsocehttahcumootsllecionoroVleehmlapehtdegralneevah
dluowmc􏰋􏰆fosnoitisoplacitrevreporpriehtotdrawrofsrotcarttaesehtgnivoM
tcarttaotdengissaderedisnoc
ji
siitcatnocdna􏰅jrotcarttaotitcatnocgningissafotsocyrartibranasi cerehw
pehtrevoeziminimotsi􏰪􏰌􏰊􏰨melborpgnimmargorpraenila
sanoitalumroflarenegeromstI􏰇smelborpgnimmargorpregetnidnasmelborpgnim
􏰆margorpraenilfosessalcehtmorfesaclaicepsasimelborptnemngissaehT
melborPtnemngissAehT􏰋􏰇􏰋􏰇􏰋
􏰇noitomregn􏰏fosegnarredi wrevo
noitac􏰏itnedi tcerrocsniatsussecnatsidrotcartta􏰆tcatnocralucitrapfosgnithgiew
tnednepedilcuEderauqsehtnosaereht􏰅melborptnemngissaeht
rofseuqinhcetnoitulosehtweiverlliwsnoitcesgniwollofehT􏰇seuqinhcetlacitame
􏰆htamnwonk􏰆llewfoyteiravaybdevlosyltneic􏰑eebnac􏰅melborptnemngissanasa
scitamehtamniyllac􏰏icepseromnwonk􏰅melborpnoitazimitpolairotanibmocsihT
i
􏰇j􏰲􏰳nehwrehtonaenootdengissaderedisnocerajrotcarttadnaitcatnocerehw
rehtroFborpnoitazimitpolairotanibmocasA􏰇􏰪􏰌􏰡
􏰅􏰊􏰊􏰅􏰈􏰊􏰅􏰡􏰨dohteMhtaPgnitnemguAtsetrohSehtnopudesabseuqinhcetnoitazimi
devlosebnacmelborptnemngissaehtderauqsnuehtsmelborphcraesersnoitarepo
dlrow􏰆laeresehtnI􏰇pukciptxenot􏰝noitacolgnitrats􏰜􏰎opordtsalmorfecnatsid
ehtotnoitroporpnisesaercniixathcaerofdaehrevoehttahtnoitpmussaehtdna
noitacol
eht nevigsregnessap	Motsixat	MngissaotgniyrtebdluowelpmaxenA􏰇secnatsid
gnihctamfotesa morfxirtamtsocatcurtsnocodsmelborphcraesersnoitarepo
emos􏰅revewoH􏰇enalpanistniopneewtebsecnatsidehtmorfdeviredtonsixirtam
tsocehtsnoitacilppaesehtfotsomnI􏰇sllikstnerehcraesersnoitarepofodle􏰏ehtnidohtemnoitulosderreferp
ehtneebevahti nopustnemevorpmi dnadohteMnairagnuHehTeiravecnamrofrephguoht􏰅􏰝
M􏰜O
􏰊
si ecnamrofrepesactsrowstI􏰇noitanimilenaissuaGotralimissnmulocdnaswor
xirtamfosnoitalupinamediughcihw􏰪􏰠􏰢􏰨gino􏰮Kdna􏰪􏰋􏰊􏰨yra􏰓vregEybderevoc
􏰆sidseitreporpxirtamlaiceps nodesabsawdohtemsihT􏰇melborptnemngissa
ehtgnivlosrofdohteMnairagnuHehtpolevedottsrtcurts
deniartsnocyllaicepsehtneuqes doohrobhgien
egnahcxeehtfogniredroehtdnastnemngissalaitiniehtnosdnepedmuminimlabolg
ehtyllautcasidnuof muminimlacolehtrehtehW􏰇muminimlacoladrawotesaerced
otmustnemngissalatotehtsesuacgnippawslanoitidnochtiwmehtgnizimitpodna
􏰅sriaprotcarttatnecajdaevisseccusufgnisaercni
cinotonomasiotehtevorpmisyawlalliwstnemngissarotcartta􏰆tcatnoc
owtehtgnippawsyllanoitidnocybtesbussihtnihtiwsecnatsidtnemngissafomus
ehtgnizi mini M􏰇srotcarttatnecajdaforiapaebyllausulliwtesbuseht􏰅􏰉si kfI
􏰇gnirrotcarttaehtmorfsrotcarttafotesbusasidoohrobhgienegnahcxe􏰆ka􏰅melborp
noitac􏰏itnediregn􏰏ehtfoesacehtnI􏰇􏰪􏰈􏰨sdoohrobhgienegnahcxe􏰆kgnisudezimitpo
yllatnemercniebnacmelborptnemngissaehtsahcussmelborplairotanibmoC
hcraeSlairotanibmoCdezilacoLtnemngissanehwgnirrotcartta
ehtfosisylanadnangisedehtotnisthgisnitnatropmisre􏰎oti􏰅dnoceS􏰇 mumini m
labolgehtraennoitazilaitininanevigtsafylriafmuminimlabolgehtsdn􏰏ti􏰅llew
sa􏰝M􏰜OebottuonrutlliwecnamrofrepesactsrowstihguohT􏰇mumitpoyllacol
􏰊
tsaeltallitssisnoitac􏰏itnedifotessuoiverpatahtylkciuqyrevyfirevnactiezilacol􏰅revoeroM􏰇seuqinhcet nwonk􏰆llewrehtoehtfoynahtiw
noitatnemelpmietanretlanayrtotdeenonebotdemeeserehterawtfosSTM
ehtni detnemelpmieuqinhcettsr􏰏ehtsawhcraeslairotanibmocdezilacoL
􏰇noitac􏰏itnediregn􏰏emit􏰆laerrofhguonetneic􏰑eebdluohsseuqinhcetnoitulos
􏰍􏰋􏰈
􏰇noitac􏰏itnediregn􏰏niaminimlabolg􏰆nondiovaotgnilaennadetalumisnaht
syawrelpmiseraereht􏰅swohs CxidneppAsA􏰇muminimlabolgehtnikcolotemit
revodesaercedsierutarepmetgnilaennaroecnairavesionehT􏰇amini mlacolfotuo
kcabpmujothcraesnoitazimitpoehtswollasihtsmelborpegralnI􏰇 mustnemngissa
ehtesaercedtonodyeht nehwnevenekateraspawsemostahtos􏰡􏰈􏰇􏰋ytilauqeni
foedisthgirehtotdeddasimretesiona􏰅gnilaennadetalumisfodohtemehtnI
rgnippawsfingissaregnifdengissahtapdengissa
􏰆saesehT􏰇hP􏰝􏰜bAdnagP􏰝􏰜aA
sknil􏰋􏰇􏰋
􏰇 muminimlabolgehtotesolceroferehtdnarotcarttatcerrocstiot
esolc htap wenehtstupyllaususihttub􏰅htapgnitsixe􏰆erpahtiwdell􏰏ydaerlasi
tahtenoebthgimrotcarttatsebehtecnisrotcarttatsebehthtiwtcatnocweneht
stupnoitazilaitiniehttahterusnetonseodsihTidionoroVagnitcurtsnoc􏰅htapwenagnikcipottnelaviuqesinoitarepo
sihT􏰇gnirehtnisrotcarttaelbaliavatsesolcehtotdengissaeranwoddehcuot
ylwenevahhcihwstrapdnahfoshtapeht􏰅nacsyarrarosneshcaeretfA
stnemngissAlaitinIgnisoohC􏰇gnireht nosrot
􏰆carttatnecajdaneewtebylnofodaetsni gnirehtfosedisetisopponosrotcartta
neewtebspawsriaptcatnocedulcniotsdoohrobhgienegnahcxeehtgnidnapxeyb
dehcaermuminimlabolgehtdnadediovaebnacyehTirehtfosedisetisopponeewtebdnetxesyawlatonlli w
noitcestxenehtni debircsedsnoitalergniredroehtfoytivitisnartesuaceB
􏰇gnirehtfolasrevartetelpmoc hcaeretfa
ecneuqesegnahcxeehtfonoitceridehtgnisrevercecnamrofrepesactsroW􏰇yrasseceneragnirehtfo
slasrevarteerhtroowtnahteromtonyllausuos􏰅rotcarttareporpriehtfosrotcartta
elpuocanihtiwstcatnoctupotsdnetlaitinieht􏰅oslA􏰇snoitisoptnerrucriehtrofgnitrostcerroc
ehtllitssisegamiytimixorpsuoiverpnisnoitisopriehtnodesabstcatnocfognitros
ehttahtyfirevnacgnirehtfolasrevartelgnisairedrotroselbbubehtsariaprotcarttatnerrucehtrofnoitidnoc
gnippawsehtgnizilitu􏰅gnirehtnotroselbbubaotstnuomayllacisabsihT
􏰇spawsynagnitpeccatuohtiwedamsilasrevartetelpmocenolitnustaepergnireht
dnuoralevarT􏰇gnirehtdnuorarotcarttatnecajdatxenehtdnarotcarttatnerruceht
neewtebdeilppasitsetpawshcaE􏰇deretnuocnerotcarttahcaetapawslanoitidnoca
rofgnisuap􏰅redroesiwkcolc􏰆retnuocroesiwkcolcrehtienignirrotcarttaehtdnuora
sdeecorpylpmisecneuqesehT􏰇erutcurtsraenil adluowsa􏰅gnirehtdnuoralasrevart
robhgientseraenelpmisastroppusgnirrotcarttaehtfoerutcurtsralucricylhguor
ehtcdezilacol ybdezilitusecneuqesdoohrobhgienegnahcxeehT
ecneuqeSegnahcxeewteb
baenilehtotnogtcatnocehtgnitcejorpyB􏰇stcatnocowtrofetepmocsrotcartta
ehtnehwesacehtotnosirapmocroflufesueblliwnoitaterpretnirehtonA
auqsecnatsidnaedilcuEarofdetcurtsnocsmargaidionoroV
sideht
gnirauqstahtsnaemylpmis􏰣􏰈􏰇􏰋noitauqEottnelaviuqesi􏰢􏰈􏰇􏰋noitauqEtahttcaf
ehT􏰇srotcarttaowtylnofomargaidionoroVafoesaclaicepsehtgnieb􏰍􏰇􏰋erugiF
􏰅srotcarttasuoiravneewtebsrotcesibralucidneprephcusfodesopmocyllautcaera
sllecionoroVfosllawehT􏰇si gPsasrotcarttaeht neewtebrotcesibralucidneprep
ehtfoedisemaseht nosi	hcihwrotcarttaehtetnicirtemoegehtawStcatnoCelgniSfonoitaterpretnIcirtemoeGongisednidiahcihw􏰡􏰈􏰇􏰋noitauqEfo
noitidnocgnippawsriapehtfosnoitaterpretnicirtemoegserolpxenoitcessihT
noitidnoCgnippawSehtfosnoitaterpretnIcirtemoeGb
a)	b)
􏰣􏰋􏰈
􏰇srotcarttaehtfo􏰝sworradettod􏰜rotcesibralucidnep
􏰆repehtssorctonseodkniltnemngissaeht hcihwni􏰝besacsreferp
noitidnocgnippawsehT􏰇rotcarttayreveotecnatsidorezsahhcihw
􏰝D􏰜tcatnocymmudadengissasyawlasirotcarttagnisolehT􏰇􏰝elcric􏰜
tcatnocecafruslaerenorofetepmoc􏰝sessorc􏰜srotcarttaowtnehw
􏰝sworrayvaeh􏰜stnemngissaelbissopgniwohsnoitcurtsnoccirtemoeGneserperytilauqeniehtfoedisthgirehtdnastneserperytilauqeniehtfoedistfelehterehw
idetatserebnacnochcaemorfsralucid
􏰆neprepmrof􏰅woN􏰇enildetneiroyllatnozirohagnolaeilbdnaasrotcarttaemussa
mderauqs􏰆ecnatsidehtfoytreporpgnitseretnisihT
􏰇riaprotcartta eht
gnitcennocenilehtotnodetcejorpsastcatnocehtfogniredroehtnoylnosdneped
noisicedgnippawseht􏰅cirtemderauqs􏰆ecnatsidehthtiW􏰇desusicirtemderauqs
ecnatsidnaedilcuEasselnudrawrofthgiartssatonsi􏰅stcatnocfollufsignirrotcartta
ehtnehwsa􏰅stcatnocowtrofgnitepmocerasrotcarttaowtnehwesaceht􏰅tcatnoc
ecafruslaerenorofgnitepmocylnoerasrotcarttaeromroowtnehwroivahebgnip
􏰆pawsnialpxesmargaidionoroVotnoitazilarenegstidna􏰍􏰇epedtonseodwnoitalergniredroehtotdetsartnocebdluohs􏰠􏰉􏰇􏰋noitauqE
ninoitisopriaprotcarttaetulosbahtiwnoitcejorptcatnocehtfonosirapmocehT
ecehtotevitaler
gnoitcejorpehtfonoitisoplatnoziroh
􏰠
go go
x h' x x h' x a g' b a g' b
oo hh
a)	b)
􏰈􏰌􏰈
􏰇cirte mderauqs
􏰆ecnatsidehtrednu􏰝afoesohtnahttsocrewolevah􏰝bfostnemngissa
eht􏰅nwohssnoitisoptcatnocevitalerehtroF􏰇srotcarttaeht neewteb
enil􏰝dilos􏰜ehtotno􏰝sralucidneprepdettod􏰜snoitcejorp􏰛stcatnoceht
fogniredroehtgnirapmocotsecudernoisicedgnippawseht􏰅cirtem
tsoctnemngissaehtsiecnatsid􏰆derauqsfI􏰇􏰝sessorc􏰜srotcarttaforiap
adna􏰝selcric􏰜stcatnocforiapaneewteb􏰝sworrayvaeh􏰜stnemngissa
elbissopowtehtfostsocehtgnirapmocrofnoitcurtsnoccirtemoeGeewtebenilehtotnodetcejorpsasetanidrooctcatnocehtfonoitaler
xxx xxx gniredronaotsecudernoitidnocpawseht􏰅b􏰯g􏰦bgdna b􏰯h􏰦bhgnitut
􏰠􏰠 􏰠􏰠
x
􏰆itsbusdna􏰅evitagensibatahtosbrotcarttafotfelehtotsiarotcarttagnimussA
gniyfilpmisdna􏰡􏰉􏰇􏰋noitauqEotnisnoisnapxeesehtgnitutitsbus
xxxx bh􏰭
􏰉􏰠􏰉􏰠􏰉
yx
hh􏰞ah 􏰦 jjhajj
ret
nidetatserebnacshtgnelrotcevlanogaidyllaitnetopeseht􏰅meroehtnaerogahtyP
􏰊􏰌􏰈
b a 􏰧b a
􏰇 Brotcesibrotcarttaehtfotfelsihcihwrotcarttaehtotdengissapusdne
baib
rotcarttaehtfothgirehtotrotcarttaehtotcatnocehttahtosdeppawseb
lliwstnemngissaeht􏰅croa􏰢􏰇􏰋erugiFnisarotcesibtcatnocehtssorcsknilrotcartta
􏰆tcatnocehtfitahtserusne􏰝􏰡􏰈􏰇􏰋noitauqE􏰜noiretircgnippawsderauqs􏰆ecnatsideht
􏰅niagA􏰇esachcaenibaotralucidneprepdetcurtsnocsistcatnocehtneewtebhg
ba􏰧hgba􏰧ba
tnemgesehtfo Brotcesiba􏰅bafo Brotcesibralucidneprepehtotnoitidda
nIwtfoselpmaxesniatnochtotnosirapmocroF
􏰇stcatnocregn􏰏elpitlumfonoitac􏰏itnediot
ecnairavnielacsdnanoitalsnartedivorperoferehtnacnoitalergniredrosihT􏰇enil
latnoziroh􏰆nonanogniylsriaprotcarttaottluserehtsdnetxeyllaivirtecapsetan
􏰆idroocehtfonoitatoR􏰇srotcarttaehtneewtebnoitarapesehtroriaprotcarttaeht
fotnemngilalatnozirohehtfotnednepednisinoitalergniredroeht􏰅brotcarttafo
tfelehtotsiarotcarttadnaenillatnozirohanoeilsrotcarttaehttahtsnoitpmus
􏰇􏰋erugiF
fostnemngissagnitsixeehtesuacebnekattonsiahtfosseldragertahtserusnenoitidnocelpmissihT
Bgh⊥ab	Bgh⊥ab
go go
Bab⊥ab
xxxx
aBbab ab⊥ab
h	Bgh⊥ab
oo
hh
a)	b)
h	Bghsocderauqsralucid
􏰆neprephtoberahcihw􏰝sworradettod􏰜srotcesibriaprotcarttadna
riaptcatnocaivstnemngissaderauqs􏰆ecnatsidfonosirapmoclausiV􏰤􏰢􏰇􏰋erugiF
􏰌􏰌􏰈
redisnoC􏰇owtnahtrehtosrewopecnatsidhtiwsesirahcihwroivahebgnippaws
tnereN
􏰇baotnodetcejorpsagniredrotcatnocehtsallewsasecnatsidknil eht
fostcudorpehtniecnerejg􏰭ajj
􏰠􏰠
􏰭􏰭􏰭
􏰤noitac􏰏ilpmisehthguorhttsisrephcihw
􏰧
xx xx
jjgbjjjjhajjps
􏰆errocytilauqenieht􏰅enosirewopecnatsidehtfisidnaedilcuEeht hcihwnisecapscirtemroF
scirteMrehtOhtiwroivaheBgnippawSriaPtcatnoCrttaehtfonoitisopetulosbaehttub􏰅elgnarotcartta􏰆retnieht
ottcepserhtiwderedroerastcatnoceht􏰅stcatnocecafrusgnitepmocowthtiW􏰒
etulosbaehtotgnidroccarotcartta
tsesolcehtotseogylpmistcatnoclaereht􏰅esactcatnocecafrusenoehtnI􏰒
􏰇stcatnocecafrusowtroenorofetepmocsrotcarttaowtehtrehtehwnognidneped
tnere􏰎idetiuqsitsocderauqs􏰆ecnatsidrednuroivahebgnippawss􏰅tniopemasehttabaotno
tcejorpstcatnochtob􏰅sdrowrehtonI􏰇stsoctnemngissalatotemasehtecudorphtob
yehtecnisderreferpsib􏰉
􏰦jjhajjtahtserusnemeroehtnaerogahtyPehttgnelehtfomus
􏰭
ehtnahtretrohssijjhajjesunetopyheht􏰅cirtemecnatsidnaedilcuE􏰆derauqsnueht
􏰭
roftahtserusneytilauqenielgnairtehT􏰇􏰠 􏰦jjgbjjgnikam􏰅renrocralucidneprep
􏰭
ehttabrotcarttadnagtcatnochtobhtiwelgnairtthgiramrofsrotcarttadna
stcatnocehtsuhTdna􏰅brotcarttaenofopotnothgirsigtcatnocenohcihwni
stsoctnemngissaehtemnaedilcuE􏰆derauqsnuehtrednU
􏰇tsocorezsahdnaelbisivnisi􏰝ani	knil	gbehtetoNcirtemrorrenasanesohcsirorrederauqsnosaer
enosistsocroshtgnelfosnoitanibmocmrofinueromrofecnereferpsihT
tebalworramrofinueromehtsrovafshtgnelderauqsfomusehT􏰇srotcartta
ehtottcepserhtiwstcatnocehtfogniredrolatnozirohehtesreverdnaetarapsidera
atemnaedilcuE􏰆derauqsnueht
􏰇riaptcatnocehtfonoitalsnartlatnozirohyrartibra
rednusrotcarttaehtfotahthtiwtnetsisnocebotstcatnocehtfo
gniredrolatnozirohehtsesuacsihT􏰇􏰝bnisashtgnelworramrofinu
eromroftsoclatotrewol asecudorpcirtemderauqs􏰆ecnatsidehT
􏰇srotcarttadengissariehtottcepserhtiwstcatnocehtfogniredro
latnozirohehtesrevertemnaedilcuE􏰆derauqsnuehtrednU􏰇riaptcatnoceht
fonoitalsnartlaretalrednugniredrotcatnocehtevreserptonseod
cirte mdnaatropmirehtonA
eggtcatnocesacsihtnitahtetoN
aseruliafgnitrosfoytilibissopehtsetaercsiht
hguohT
ehtnehwevitisnartebotesaeclliwsnoitalergniredroesiwriapeht􏰅yllacinhceT
􏰇gniradnuoraroscrayrartibragnolagnitrosgnidivorp􏰅riaprotcarttalacolehtfo
elgnaehtt􏰏otsegnahcnoitalergniredroeht􏰅raenilloctonerasrotcarttaeromro
eerhtnehwsuhT􏰇srotcarttanevigowtneewtebelgnaehtsehctamelgnaesohwenil
aotnonoitcejorpriehtotgnidroccaderedroerastcatnoceht􏰅noitidnocgnippaws
riaptcatnocderauqs􏰆ecnatsidehthtiwtahtllaceR􏰇cirtemnaedilcuEderauqsnu
ehtgnisutnemngissarognitrosetanidrooclatnozirohrehtienahtlarenegeromsi
cirtemderauqsfognilacssetarelotoslatnemngissaderauqstylnodeilppa􏰪􏰣􏰡􏰨mhtiroglagnitros
lanoitnevnocasatluseremasehtgnicudorp􏰅gniredrorotcarttaehtottcepserhtiw
stcatnocehttroslliwecneuqes doohrobhgienegnahcxeetairporppanatnocynarofnoitidnocgnippawsderauqsngilasimyllatnozirohsistcatnoc
pitregnemngilasi m
latotgnisuac􏰅gnorwyrevebsemitemoslliwsetamitsenoitisopdnahehtesuaceb
noitac􏰏itnediregn􏰏roflaic􏰏enebylralucitrapsisihT􏰇srotcarttaehtottcepserhtiw
detalsnartsistcatnocforetsulcehtnehwnevegniredrosevreserphcihwnoitatumrep
ehtgnikcipfoecneuqesnoclanoitiddaehtsahshtgnelmrofinurofecnereferpeht
􏰅 melborptnemngissanafotxetnoceht nitaht nwohssahnoitcessihT􏰇smelborp
􏰣􏰌􏰈
sriaptcatnocehtecnisdnattaehtottcepserhtiw
thgirreppuehtdrawotsretemitneceerhttuobadetalsnartsidnahelohwehttpecxe
srotcarttaehtsaselgnaevitaleremasehthtiwdegnarraerasregn􏰇deppawsylsuoenorre
eraseititnediregn􏰏erofebdetareloteblliwtnemegnarraregn􏰏ninoitaivedlacol
hcumwohyltcaxeetacidnisnoitidnocgnippawsriaptcatnocehtfosnoitaterpretni
cirtemoegeht􏰅spitregn􏰏follufsi hcihwgnirrotcarttanaotdeilppanehW
gniRrotcarttAehtnospawSgnizylanAnocecafruslaer
gniniatersrotcarttatnecajdafospuorgnihtiwsdlohllitsnoitalergniredroriaptcat
􏰆nocehttub􏰅tnemngissapitregn􏰏nistfihssuoenorregnivael􏰅stcatnoclaerehtmorf
tsehtraferahcihwsrotcarttaesohtdrawotetagaporpotstcatnocymmudsesuac
rrotcartta
tsniaganoitcetorplaicepsonsre􏰎odnascirtemecnatsideSninwohssawsA􏰇stcatnocymmudpugnivigyllaitnetop
􏰅stcatnoclaerrofetepmoclliwsrotcarttaytpme􏰅stcatnocregn􏰏ruofotowtylnofo
retsulcafosegdeehtnOeylnoeratnemngissa
derauqsgnirrotcarttaehtecnogniredropitregn􏰏reporp
ehtrofecnereferponsahcirtemnaedilcuEtnehwgnirehtdnuora
roscragnolatrososlanaccirtemnaedilcuE􏰆derauqsnuehthguohT􏰇laudargylriaf
F3 o
F4o F5
F2 o
o
xx x A3A4
A2 x
A5
F1 o
A1 x
A7	A6
xx DD
􏰝worradettodgnol􏰜rotcesibralucidneprepehtfoelgnaehtnehw
secnemmocgnippawsytitnedisuoenorrE􏰇detarelotebotdeetnaraug
sielgnatluafedeht morf􏰝􏰋F􏰜regn􏰏gnirehttuoba􏰝􏰌F􏰜yknipeht
fonoitatorfo􏰠􏰣otpu􏰅eromrehtruF􏰇noitalsnartynarofdecudorp
􏰑
eblliwsregnnoitaivedriapregn􏰏dnanoitalsnartdnahfoecnareloT􏰤􏰈􏰈􏰇􏰋erugiF
􏰈􏰍􏰈
tnemngissariaptcatnocfotaht htiwerpretnicirtemoegehtenib
erlliwhcihwsnoitacorerewstcatnoceht nehwdlohylnodluowelursiht􏰅srotcarttariehtneewtebrotcesibralucidneprepehtfoelgnaehtsehcaorp
􏰆pamehtneewtebelgnaehtsaylsuoenorredeppawsebotnigeblliwsregnerpdenimretedebnacrevossorcriap
regnengilaeblliwstcatnocf􏰠􏰣otpufo
􏰑􏰑
noitatorarofyltcerrocdeniatniameblliwytitnedisehtesoppus	woN
􏰇nwohssastnemngissaregn􏰏deredroyltcerrocehtdn􏰏
lliwecneuqesegnahcxeeht􏰅srotcarttaregn􏰏focraehtnihtiwrehtonaenoniartsnoc
􏰉􏰍􏰈
ybdenrevogllitssigniredroehT􏰇gniredroreporpriehtniatniamllitslliwnoitidnoc
gnippawsriaptcatnocderauqs􏰆ecnatsideht􏰅thgirehtotdetfihsylsuoenorreneeb
evahspitregn􏰏gniniamereerhtehtfostnemngissaehthguohttahtecitoN
􏰇erehtsniamertcatnocymmudehtos􏰅llecionoroVregn􏰏xedniehtnieil
stcatnoclaeronhtehtaevirralliweecorpgnitset paws nehWehtfoedisthgirehtnoerutaefeuqinu
ylriafevahsleehmlap􏰅oslA􏰇sretemitnecev􏰏nahteromfotnemngilasimlacitreva
eriuqerdluownoitac􏰏itnedisuoenorregninaemnoitalsnartlatnozirohtneicehtotdetfihs
ylsuoenorreebnacsnoitac􏰏itnediregn􏰏􏰅larenegnI􏰇dell􏰏nutfelerasrotcarttaerom
sasnoitalsnartgnitpurrocotelbarenluveromdnaeromemoceblliwsnoitaclotsatonsinoitid
􏰆noc gnippawstcatnocelgnisehtecniSA6
xx DD
udthgirtfihslliwstcatnocpitregn􏰍􏰈
􏰇llecionoroVregn􏰏xedni eht ni eilstcatnocecafrus
laeronesuaceberehtsniamertI􏰇devomersiregn􏰏yknipehtnehw
rotcarttaregn􏰏xedni ehtotsetagaporp􏰝D􏰜tcatnocymmudehT􏰤􏰊􏰈􏰇􏰋erugiF
􏰌􏰍􏰈
sagnirrotcarttaehtdnayltnednepednidethgiewebnacsrotcarttalaudividnielihW
􏰇srorretnemngilagnirrotcarttaegralfoecafehtninevestcatnocesohtdengissaeb
otylekileromeraserutaefetairporppahtiwstcatnocotdegralneyllacimanydraeppa
sllecionoroVesohwstnioprotcarttA􏰇srotcarttagnirobhgienfoesohtotnoitalerni
sllecionoroVniatrecknirhsroegralneoslalliwserutaeftcatnocgnihsiugnitsidfo
shtgnertsehtotnoitroporpnisrotcarttaralucitrapfosgnithgiewcimanyD
􏰇sllecmlapehtybdetacavecapsotnisllecionoroVyknip
dnabmuhtehtdnapxednasllecionoroVleehmlapehtknirhsotsuoegatnavdaeb
lliwti􏰅sleeh	mlapeht nahtnoitomfoegnarrediwhcumaevahyknipdnabmuht
ehtecnisionoroVehteziserdnaprawnacsrotcarttaralucitrapfosgnithgiewcitatS
􏰇asrevecivdna􏰅ootseodllecionoroVs􏰛rotcarttaehtsuinollopAesehtforetnecehT􏰇􏰪􏰍􏰈􏰈􏰨senil
thgiartsfodaetsniselcricebotsgnithgiewtnere􏰎idhtiwstcatnocneewtebsrotcesib
tnatsidiuqeehtsesuacsrotcarttaralucitrapotsecnatsidehtgnithgiewylevitacilpit
cionoroV
ehtsesuachcihw􏰅dethgiewnuneebevahsrotcarttaehtrafosdetneserpsA
smargaiDionoroVdethgieWhtiwgniRrotcarttAehtgninuThttsap􏰠􏰣nahterom
􏰑
setatorregnavdnaknirhsotllecionoroVarofnaemtiseodtahW
􏰇 ytitnedi
tcatnocforotacidnignortsanahtecnatsneppaheromsirotcarttanarevotcatnoc
afotnemngilaesicerp􏰅snoitisoptluafedriehtmorfsretemitneclarevesfonoitomfo
segnarevahsregnttnemugraehtdnagnitsetlaciripmehtobstcehwemos gnithgiewehttce􏰐ersyawlalliw
jijiji w􏰳w􏰦d􏰅􏰇e􏰇i􏰅orezhcaorppallitslliwecnatsiddethgiew
􏰉
eht􏰅tes􏰎oecnatsidtnatsnocehttuohti	WlioT
􏰇ecafruseritneehtrevoekat otrotcarttaenofollecionoroVehtesuac nac
gnithgieweguhadnatest adnates􏰎otnatsnocehtfo
noitanibmoceht􏰅smargaidionoroV􏰖dethgiew􏰆dnuopmoc􏰩hcus nitahttcafeht no
ji
sdnepedssenevitceargaidionoroVdethgiewnuehtfoerutcurtsehttce􏰎aton
seodtisuhT􏰇smustnemngissallaot M􏰉gniddatpecxedethgiewnuerastnemngissa
ji
nehwtcethgiewevitiddatnatsnoca
􏰉
􏰅secnatsidrotcartta􏰆tcatnocehtotsthgiewevitacilpitlumynagniylppaerofeB
xirtaMecnatsiDehtotgnithgieWevitiddAtnatsnoCtalaudividni
dnehwsitaht􏰅rotcarttanafopot noylesicerp
fosnoitisopehttahtetonariehtot
drawrofsrotcarttaleehmlapehtevomotefassitisgnithgiewehthtiw􏰅sllecionoroV
leehmlapehtfotnetxedrawpudnalaretalehttimilsgnithgiewehtesuaceB􏰇enil
ralucidneprepaebotseunitnocsleehmlapretuodnarennineewtebrotcesibeht
􏰅emasehterasleehmlapehtneewtebsgnithgiewevitalerehtecniS􏰇sleehmlapeht
dnuoraselcricotnitcartnocotyknipdnaleeh mlapretuoehtneewtebdnabmuht
dnaleehmlaprenniehtneewtebsrotcesibehtsesuacsiht􏰅􏰋􏰈􏰇􏰋erugiFninwohssA
􏰇srotcarttarehtootsecnatsidotderapmocrotcarttaleehmlapaotecnatsidnaedilc
thgiew
􏰡i	􏰍i
ehtecniS􏰇􏰌􏰉􏰤 􏰦 wiew
llamsagnidulcni ybsllecionoroVleeh mlaptcapmocseveihcaSTMehT
􏰇sllecyknipdna
bmuhtehtnahttcapmoceromebotsllecionoroVleehmlapehtrofesneseromhcum
ekamdluowti􏰅sleeh mlapehtnahtretnecdnahottcepserhtiwnoitomfosegnar
rediwhcumevahyknipdnabmuhtehtecniS􏰇sezisllecionoroVyknipdnabmuht
fotsocehttahcumootsllecionoroVleeh mlapehtdegralneevahdluowmc􏰋􏰆fo
snoitisoplacitrevreporpriehtotdrawrofsrotcarttaesehtgnivoM􏰇snoitisopleeh
mlaptluafedderusaemeht nahtrewolsretemitnecelpuocadecalpebotdahsrot
􏰉􏰇􏰍􏰇􏰋􏰇􏰋
􏰇stcatnocrehtootdengissa
ebydaerla yamsrotcartta ybraenrehtollaesuacebylpmis tcatnoc aeviecerllits
nacmargaidionoroVehtmorfdehsinavsahllecesohwrotcarttana􏰅srotcarttarof
gnitepmocerastcatnocelpitlumnehw􏰅revewoH􏰇ecafruseht nonoitisops􏰛tcatnoc
ehtfosseldrager􏰅rotcarttas􏰛llecdehsinavehtotdengissaebtonlliwtcatnocdnah
14 12 10
8 6 4 2 0
−2 −4 −6
0 2 4 6 8 10 12 14 16 18 20 Horizontal Position on Surface (cm)
A2 A3 A4
A7	A6
A5
A1
Vertical Position on Surface (cm)
􏰢􏰍􏰈
􏰇lamronsarafsaeciwtebotdethgiewsleehmlapot
enalpehtnierehwynastcatnocmorfsecnatsidhtiwmargaidionoroVsnoitisopregnegn􏰏ehttahtgnisirprussitios􏰅selcricehttuognipeewselihwmargaid
ionoroVehttakooltondidrohtuaeht􏰅noitisoptluafednidetratsdnahehthguohT
􏰇sllecionoroVrieht ybdebircsmucricylesolcerasregn􏰏yknipdna􏰅xedni􏰅bmuht
ehtfospeewsralucricehtwohetoN􏰇snoitisoptluafedriehtnignitserniamersre
pdnabmuhtehtmorfnoisulcxeybseititneditcerrocrieht
dn􏰏lliwnoitazimitpotnemngissalabolgeht􏰅ecafrusehtgnihcuoteraspitregn􏰏ruof
llaneh W􏰇nwohssadoogsaetiuqebtonlliwylbaborptubtnemngilatcerroceht
fosretemitnecelpuocanihti webllitslli wetamitsetesregn􏰏rehtoemosfI
􏰇ecafrusehtnoerehwynagnitsererasleeh mlaphtobfironoitisoptluafedsti no
deretnecsidnahehtnehwnwohssatcefrepebyllareneglliwsetamitsetes􏰎odnah
ehtybdenimretedsatnemngilalatnozirohehT􏰇tcerrocsignirrotcarttaehtfotnem
􏰆ngilalatnozirohehtsagnolsanoitac􏰏itneditcerrocnitluserlliwegnarnoixe􏰐sti
gnolaerehwynaregn􏰏ynafonwodhcuotdetalosiehttahtsetartsnomedsihT􏰇egnar
noitomeritneehtrevollecionoroVstinihtiwsniamerregnt hcuototnigebselkcunkehtosrednudelrucyllautcaeraspitregnlecionoroVdethgieweht􏰅srotcarttaleehmlapdethgiewesehthtiW
14 12 10
8 6 4 2 0
−2 −4 −6
A2 A3 A4
A7	A6
A5
A1
0 2 4 6 8 10 12 14 16 18 20 Horizontal Position on Surface (cm)
Vertical Position on Surface (cm)
􏰠􏰡􏰈
􏰇egnar
noitomeritneehtrevollecionoroVstinihtiwsniamerregn􏰏hcae
dnasrotcarttariehtrevoyranoitatsniamersleehmlapehttahtetoN
 8 10 12 14 16 18 20 Horizontal Position on Surface (cm)
Vertical Position on Surface (cm)
􏰈􏰡􏰈
􏰇tnemirepxegnipeewsehtfotuotfelerewyeht􏰅gnitsererasregn􏰏
tnecajdariehtelihwyllaretalevomot	moorelttil evahsregn􏰏gnirdna
elddi mehtesuaceB􏰇semitrehtollataecafrusehtnodeniamersmlap
ehttub􏰅egnar bmuhtllufehterutpacotyllaretaltfihsdnaecafrus
eht􏰎otfilotdewollaerewsmlapehttahtetoN􏰇snoitisoptluafed
riehtnignitserdeniamersregn􏰏rehtoehtelihwnoitomfoegnar
riehttuopeews otedamdnapudekciperewsregn􏰏yknipdnaxedni
􏰅bmuhteht􏰅emitataenO􏰇srotcarttaehtrevothgirsnoitisoptluafed
riehtnidetratsdnahs􏰛rohtuaehtfosregn􏰏llA􏰇desopmirepus􏰝swor
cuotdnagnitfilyldetaeperybdenimretedebnacsnoitisopyradnuob
tnerrucehT􏰇ebdluohsyradnuobhcaeerehwsegnarnoitomregn􏰏morfsediced
yllacipytretnemirepxeeht􏰅llecionoroVenofoseiradnuobehtevomnacnoitcnufgni
􏰆thgiewhcaeecniScatnocbmuht􏰅ytilautcanI􏰇ecaf
􏰆ruselohwehtgnirevocllecionoroVbmuhta􏰅llecionoroVenoylnoniatnocdluoc
tcatnocehtybderetnuocnemargaidionoroVehttahtlufrewoposrotcarttabmuht
ehtekamdluoctcatnocarofgnithgiewbmuhtegralyltneiccatnochcaetnihtiwllaflliwtcatnoceht
ylekileromehtdnaprotcartta􏰆tcatnocralucitraparofrotcaferutaefralucitraparegral ehtsuhT
􏰇trapdnahs􏰛rotcarttanevigehthtiwtnetsisnocnisierutaefderusaemehtfiseulav
rellamsnoekatdnaehtfiseulavregral noekat􏰅noitamrofnignihsiugnitsidonsedivorptnem
􏰆erusaemerutaefstinehw􏰈foeulavtluafedanoekatotdengisedsirotcafgnithgiew
hcaE􏰇spihsnoitalererutaefsusrevrotcaflarevesfotcudorpehtsinoitcnufgnithgiew
hcaEehtecniS􏰇stneserperrotcarttaehttaht
trapdnahehtmorfdetcepxeesohthctamtcatnocnevigehtfoserutaefcirtemoeg
ehtrehtehwnodnepedsgnithgiewecnatsidrotcarttaees)
Right Orientation Factor
􏰊􏰡􏰈
fonoitatneiroderovafeht􏰅􏰠􏰍tamuminimasehcaerdna􏰅ytitnedifoevisulcnocni
rppaeroferehtrotcafnoitatneirothgirehT􏰇􏰠􏰍segarevaleeh
o w
ebyradnuobehtrosgnithgiewmlap
dnabmuhtehtecnalabotnekatebtsumeraC􏰇noitacolderisedehtotsevomyra
􏰆dnuobehtlitnunoitcnuf gnithgiewehtfoedutilpmaehtstsujdanehtretnemirepxe
1
00 1 2 3 4 5 6 Contact Size (Normalized Total Proximity)
Thumb Size Factor
􏰋􏰡􏰈
􏰇stcatnocllams
yrevrof msraeppanactrap
dnahynaecniS􏰇sezisregralrofnwodkcabspordrotcafbmuhtehtos􏰅ezispitregn􏰏
tluafedehtsemiteerhtroowtnahtregral hcumebtonnacstcatnocbmuht􏰅sleeh
mlapekilnUlatotybdetacidni
z
bmuhtecniS􏰇rotcaf ezis bmuhtehtstolpyletamixorppadna
ided by Eccentricity
Palm Heel Size Factor
tarehtebotdeotnoitroporpnisesaercnitcatnocpitregnblliwyeht􏰅mehtsnetta􏰐yllufdnasleehmlapehtnostsersdnahehtfo
thgiewllufehtfisuhT􏰇ecafruseritneehtflugnesllecionoroVleehmlapehttaht
egralossemocebrotcafezisleehmlapeht􏰅ezisbmuhtdetcepxemumixamehtnaht
regralsleehmlaproF􏰇yletin􏰏edniesaercnioteerfsirotcafmlapehttpecxerotcaf
ezisbmuhtehtekil hcumsirotcafezisleeh mlapehtsuhTlapehtfooitarehtsusrev iP􏰅rotcafezisleehmlaP􏰤􏰣􏰈􏰇􏰋erugiF
erusserperomsA􏰇rotcafezisleehmlapehtstolpyletamixorppaact (cm)
Palm Separation Factor
􏰍􏰡􏰈
􏰇tcatnocrobhgientseraenstidnaiPtcatnocneewtebecnat
pes mlapw
iFniyletamixorppadettolpsAhcraes
ybseititneditcatnocgniwonktuohtiwderusaemebnacnoitarapestcatnocmumi
􏰆ni M􏰇stniojelbixe􏰐aivrehtonaenoforetemitnecanihtiwdevomebnachcihw
􏰅spitregn􏰏dna bmuht ehtfoeurttonsi sihT􏰇sretemitneclareves ybsregn􏰏eht
morfdnarehtoenomorfdetarapesstcatnocriehtfosdiortnecehtspeekymotana
tsirwtahtsisleeh mlapehtfoerutaefgnihsiugnitsidtnatropmirehtonA
rotcaFnoitarapeSleeHmlaPpeht􏰅syarraedortcelenoituloserwolrofetaruccasseleb
dluowespilledettfoseitimixorphgihehtslecnactubhtnosyekgnisseccasregnetnecdnahraenstcatnoctahthcusdethgiew
ebtsumsrotcarttamlaperofeht􏰅llewsaecafrusehtotnodenetta􏰐sidnahehtfo
tserehtsselnuecafruseht hcuottonodyllacipytsmlaperofehtecniS􏰇smlapdenetta􏰐
fonoitatnemgesmorfspuorgartxerehtodnastcatnocsmlaperofeldnahotgnir
rotcarttaehtforetnecehtraensrotcarttalanoitiddaruofsedulcniSTMehT
sgnithgieWdnasrotcarttAmlaperoFegnnocowt
otnisleehmlapehtfoenostilpsylsuoenorremetsysnoitatnemgesehtfItsedividrotcatnoc
mlapeguhenootnisleehmlapowtehtsegremrehtiesyawlametsysnoitatnemges
ehtfilaitneutcarttaehtfotnemngilaehtroecafrusehtnonoitisopriehtfosseldrageri
esuacebsleehmlapmorfspitregn􏰏tnecajdafosriapgnihsiugnitsidnilufplehyllaic
􏰆epsesirotcafnoitarapesmlapeht􏰅smlapehtsesserpmocyllufsdnahehtfothgiew
ehtsaelbaileremocebylnohcihw􏰅srotcafnoitatneirodnaezisehtekilnU
entseraensti
dnatcatnocehtneewtebnoitarapesehtsasesaercedylkciuqrotcafnoitarapesmlap
lapwjiisbmuhtwjiiewcimanyddnacitatsehtfollA
xirtaMtsoCtnemngissAdethgieWylluFehTilcuEforewophtruofehthtiw
yravstsoctnemngissamlaperofgnikam􏰅niagaderauqseraxirtamecnatsidrotcartta
􏰆tcatnocehtniseirtnemlaperof􏰅srotcarttamlaperofotdengissagniebmorfretnec
dnahmorfyawasmlaprosregn􏰏egaruocsidoT􏰇dnahehtforetnecehtraenraeppa
sllecionoroVriehttahthguoneegralebsgnithgiewmlaperofehtlliwgninetta􏰐
zlatot
mlaproregn􏰏oteudegralsemoceb􏰪n􏰨 HytimixorpdnahlatotehtnehwylnO
zlatot
htnehwllamsosebotdesivedsinoitcnufgnithgiew
zlatot
mlaperof Atsigninettaagnimussapu
rewopdluocmetsysehT􏰇rehtonaenootevitalerelibommierasleehmlapehtecnis
shtgnelregn􏰏nahtsrotarepolaudividnirofottpadadnaerusaemotreisaehcumeb
yllautcadluownoitarapesleehmlap􏰅ylikcuL􏰇rehtonaenootmc􏰊saesolcsaera
sleehmlapesohwsdnahrellamsrofllamsylsuoenorreemocebthgimdnamc􏰋ta
ylprahs􏰎ostucrotcafnoitarapesleehmlapeht􏰅elpmaxeroF􏰇flestignirrotcarttaeht
nahtnoitairavezisdnahotevitisneseromerasnoitcnufgnithgiewerutaefehT
arttaehthcihwrofelpoepylnoehtsuhTfoegnarehttcarttaehtsuhtdnasnoitisopregntpeccanactiecafrusehtsflugnellec
ionoroVmlapanehwro􏰅rotcarttaregn􏰏ynaottcatnocstipuevignactisehsinav
llecionoroVmlapanehwtahtserusnesrotcarttarehtollaedulcnisrotcarttamlap
fosdoohrobhgienegnahcxeehtgnikaMagnolsadecitonneebevahseruliafecnegrevnoc
onisedehtni
detapicitnatonerewhcihwsyawxelpmocnisllecionoroVehtprawnacsgnithgiew
i
suoiravehthguohTnaaehttuoba
scitsitatso
tnemngissaehtsegnahcssecorpnoitac􏰏irevbmuhteht􏰅gnorwsi ytitneditcatnoc
regnrehtrufekatotssecorp
noitac􏰏irevbmuhtasyolpmeSTMeht􏰅esacsihtnI􏰇snoitarapesdnaselgnapitregn􏰏
􏰆bmuhttuobatneinel	ootsemitemossi tnemngissaderauqs􏰆ecnatsidesuacebbmuht
ehtsitcatnocsihtrehtehwetacidniylgnorwyamtcatnocregn􏰏tsomrenniehtfo
tnemngissaeht􏰅serutaefnoitatneiroroezisbmuhtstibihxeylgnortsregnevylisaeredrognisaercninierastcatnocpitregneraserutaef
mlapdnabmuht nehwroecafrusehtgnihcuoterasregn􏰏ev􏰏llanehwelbailer
ylhgiheradohtemtnemngissarotcarttasihtybdecudorpsnoitac􏰏itnediehT
noitac􏰏ireVbmuhT􏰡􏰇􏰋􏰇􏰋
􏰇ecafrusehtotlamroneraspitregn􏰏eht nehw
pitregniFegareva
seitimixorppitregncgnilacsytimixorpnisnoitairav
hcuS􏰇stniopnoitce􏰐niytimixorpehtfonoitarbilacehttpursidsuhtdnaseitimixorp
llaelacsoslanaccirtceleidecafrusehtfossenkcihtehtotsnoitac􏰏idoM􏰇dengilasi m
emocebotsrotcafezisleeh mlapdnabmuhtehtfostniopnoitce􏰐niytimixorpeht
esuacsuhtdnastcatnocllarofseitimixorplatotregralesuacsregnniehtetarbilacotdesueb
nehtdluocnoitarapesleehmlapderusaemsihtdnatsomrenni eht
fienonahtregralemocebotsdnetrotcafsihT􏰇desusimc􏰊􏰆􏰉fonoitarapesegareva
tluafeda􏰅stcatnocregn􏰏owtylnoeraerehtesacnI􏰇ecafrusehtgnihcuotregn􏰏
tsomrenni ehtsiregn􏰏xednirobmuhtrehtehwfosseldragerrucconacegarevaeht
nahtsselnoitarapestsomrenninaecnisenonahtretaergebotdeppilcsirotcafehT
􏰥􏰈􏰜ni m􏰦tcafnoitarapesrenni
q
􏰤noitarapes gva
􏰅stcatnocpitregn􏰏tnecajdarehtoneewtebsecnatsidehtfoegarevaehtotstcat
􏰆nocregn􏰏tsomrennitxendnatsomrennieht neewtebecnatsidehtfooitarehtsa
den􏰏edsitcafnoitarapesrennirotcafnoitarapesrenninaeroferehTmretuoehtebOFdnabsnoitpircsed
ehtroF􏰇etelpmocsinoitaziminimecnatsidrotcarttaehtlitnunwonktoneraseitit
􏰆neditcatnocesuacebsgnithgiewecnatsidrotcarttasadedulcnineebylisaeevahton
dluocyehtecniSticolevlarevesetupmocotsi
tsr
􏰝 NF 􏰯 I F􏰜 􏰞 􏰝 NF 􏰯 I F􏰜
􏰉􏰉
stlusernoitac􏰏itnedifoseirammustneinevnocedivorpscitsitatsesehT􏰇delipmocera
START
combined_thumb_fact > is_thumb_thresh?
Y
combined_thumb_fact &lt; not_thumb_thresh?
Y
INNERMOST	N ASSIGNED TO
THUMB? Y
N
N
INNERMOST	N ASSIGNED TO
THUMB? Y
END
COMPUTE INTER-PATH THUMB FACTORS
COMBINE WITH THUMB SIZE &amp; ORIENTATION FACTORS OF INNERMOST AND NEXT INNERMOST	CONT ACT
SHIFT INNERMOST PATH TO THUMB ATTRACTOR
EXISTING ASSIGNMENTS OK
SHIFT INNERMOST PATHS AWAY FROM THUMB ATTRACTOR
􏰉􏰢􏰈
􏰇mhtiroglanoitac􏰏irevecneserpbmuhtehtfotrahcwolF􏰤􏰈􏰉􏰇􏰋erugiF
1
0
−150	−100	−50	0	50	100	150
Right Inner Angle Factor
􏰊􏰢􏰈
􏰅pitregn􏰏elddi mroxedni ehttsniagadehcniptrats yambmuhteht􏰅erutseghcnip
􏰆itnanagnirud􏰅ecnatsniroF􏰇serutsegnoitatorrognilacsdnahgnimrofrepnehw
segnarelgnadnanoitarapesdeticevobaehtdeecxenetfosresutub􏰅serutsopbmuht
lartuenfognitanimircsidylhgiherasrotcafelgnadnanoitarapesrenniehT
􏰇rotcafelgnallamsyrevagnicudorp􏰅􏰠􏰍sunimdna 􏰠􏰊neewteb
􏰑􏰑
ebylbaborpdluowtcatnoctsomrenni txendnatsomrenni	neewtebelgnaderusaem
eht􏰅pitregn􏰏xedniehtmorfyllautcasitcatnocregn􏰏tsomrenniehtfI􏰇􏰠􏰉􏰈dna
􏰑
􏰇sregn􏰏sade􏰏itnedistcatnoctsomrenni owteht
neewtebelgnarotcevehtsusrev􏰅tcafelgna􏰅rotcafelgnarennithgi Ramixorppaehtnidetce
􏰑􏰑􏰑
siregn􏰏xedniehtotbmuhteht morfrotcevehtfoelgnaehtdnahthgirehtroF
􏰇bmuhtehtsitcatnocregn􏰏tsomrenniehtrehtehwetacidniplehnacstcatnocregnrehtrufsevomylerarbmuhtehtecniS
rotcaFelgnAregniFrennI􏰉􏰇􏰡􏰇􏰋􏰇􏰋
􏰋􏰢􏰈
􏰅regn􏰏tsomretuodnatsomrennineewtebelgnaehtnisegnahcotlanoitroporpylpmis
sawrotcafnoitatorehtfI􏰇evitcelesyreveboslatsumrotcafnoitatorehT
rotcaFnoitatoRpitregniFicudorpyb
ytivitcelessdiasdeepstsomretuodnatsomrenniehtfonaemcitemhtiranahtrehtar
cirtemoegehtgnitupmoC􏰇noitarepoxamehtyborezotdeppilcerahcihwseulav
rotcafevitagenecudorpnoitceridemasehtnisregn􏰏htobfosnoitomlanoitalsnarT
n􏰨OF􏰥􏰪n􏰨IF􏰜􏰯􏰪n􏰨OF􏰜soc􏰰
􏰍
􏰝􏰝􏰪nllarap􏰅snoitceridetisopponidnadeepsemas
ehtyletamixorppataedilsstcatnocregn􏰏tsomretuodnatsomrenniehtsaskaep
rotcafnoisnapxeeht􏰅noitauqeetamixorppagniwollofehtybdenosgnilacs
regn􏰏cirtemmysrofevitceleserahcihwsrotcafnoitatordnanoisnapxehtiwde􏰛RO
yzzuferasrotcafelgnadnanoitarapesrenni ehtgnarenniehtybdetcepxeegnarehtdeecxeoslanacelgnaxednirapesrenni
ehtsesuacsihT􏰇rehtonaenomorfyawaedilspitregn􏰏dnabmuhtehtnehttub
􏰌􏰢􏰈
si hcihw􏰅tcatnoctsomrennitxenehtfoserutaeftnerrucottcatnoctsomrennieht
foserutaef ehterapmocottpmettanoisserpxesihtfosoitargnithgiewerutaefehT
ezi s	b	muht	w
􏰝	NFuhtdenibmoc
􏰤serutaef bmuht􏰛stcatnoctsomrennitxendnatsomrenni
ehthtiwsrotcaftcatnoc􏰆retniesehtsROyllaitnessenoisserpxegniwollofehT
srotcaFbmuhTehtgnitseTdnagninibmoC􏰌􏰇􏰡􏰇􏰋􏰇􏰋
􏰇ecneserpbmuhtforotacidnitsuborasirotcafnoitatoreht
􏰅spitregn􏰏owtneewtebmrofrepottluc􏰑idtubregn􏰏rehtonadnabmuhtelbasoppo
ehtneewtebmrofrepotysaeerarotcafnoitatorsihtezimixamhcihwsnoitomecniS
ri d

otylralucidneprepevomdluohsstcatnocehtesacsihtnitub􏰅snoitceridetisopponi
evomregn􏰏tsomretuodnatsomrenniehtsaskaepwolebnoitauqerotcafnoitator
etamixorppaehT􏰇spitregn􏰏dnabmuhtehtneewtebtovipyranigaminatuoba
noitatorcirtemmysrovaftsumrotcafnoitatoreht􏰅evitceleseromeboT􏰇yranoitats
si tcatnoctsomretuoehtelihwdrawnwodgnitalsnartstratsregn􏰏tsomrenni eht
nehwsahcusnoitomregn􏰏niseirtemmysaotesnopser ni	worgylsuoenorredluowti
i􏰆noN􏰇rotcarttaregn􏰏xedniehtotdrawtuo
tcatnoctsomrenni ehtgnitfihs ybtisedirrevonoitac􏰏irevbmuht􏰅bmuhteht
htiwtcatnoctsomrenniehttupsahmhtiroglatnemngissaehtfI􏰇mc􏰌􏰇􏰉naht
sselebtsumnoitarapesrenniehtrolatnozirohraenebtsumelgnarennieht
rehtiedna􏰅hctamtsumsnoitatneirodnasezistcatnoctsomrennitxendna
tsomrenniehtehcaerebotnoisulcnocsihtroFnseodnoitac
􏰆􏰏irevbmuhttub􏰅seulcetamitsenoitisopdnahtnuoccaotnisekatmhtirogla
tnemngissaehtecnisdegnahcnutfelsimhtiroglatnemngissaehtybedamtcat
􏰆noctsomrenniehtfonoitac􏰏itnediehT􏰇suougibmasitsetnoitacehtnodrawni
tnemngissas􏰛tcatnoctsomrenniehtstfihsnoitac􏰏irevbmuhT􏰇neddirrevoeb
tsummhtiroglatnemngissaeht􏰅rotcarttabmuhtehthtiwtcatnoctsomrenni
ehttupydaerlatonsahmhtiroglatnemngissaehtfIxe􏰪n􏰨rotcafbmuhtdenibmocfI􏰇􏰈
􏰤sesaceerhtgnicudorpral eb
otsoitarehtesuaclliwtcatnoctsomrenniehtnoserutaefekil􏰆bmuht􏰅revewoH􏰇eno
raenniamerotsoitarehtgnisuac􏰅tsomrennitxenehtotralimisebdluohsserutaef
stipitregn􏰏aoslasitcatnoctsomrenniehtfIptcatnocymmudehtsahcustnemngissaeR􏰇dnahehtotdetubirt
􏰆taneebyltnecersahnwodhcuotanehwrognisaercnisiytimixorpdnahlatoteht
hcihwnisegamirofdetucexeylnoeradnaharofsmhtiroglanoitac􏰏irevbmuhtdna
tnemngissaehteroferehT􏰇ecafrusehtgnihcuoterewstrapdnaheromnehwroerut
􏰆soplartuenanidetratssdnahehtnehwedamstnemngissaetaruccadracsiddluoc
egamihcaegnitnemgesretfahctarcsmorfmhtiroglatnemngissaehtgninnuR􏰇ecaf
􏰆rusehtnonwodhcuotstrapdnaheromrouccAnoitac􏰏itnedIgnitehctaR􏰢􏰇􏰋􏰇􏰋
􏰇bmuht ehtsatsomrenni
ehtfonoitaceciwttsaeltaeb
tsumtcatnoctsomrenniehtdnacirtemmysylreporpebtsum
snoitomregn􏰏􏰆bmuht􏰅bmuhtehtsatcatnoctsomrenni ehtfonoitac􏰏itnedireggirt
dnadlohserhthgihehtssaprusotenolagnitcasrotcafnoitatorronoisnapxeeht
rof􏰅esiwekiL􏰇noitamrofnignitanimircsidongnidivorp􏰅enoerasoitarnoitatneiro
dnaezisehtdnaorezerasrotcafyticolevehtnehwssinoitac􏰏irevbmuht􏰅stnemerusaemerutaefsuoiravehtsROyllaitnessero
sdda􏰈􏰌􏰇􏰋noitauqEecniS􏰇snoitarapesdna􏰅selgna􏰅yticolevtcatnoc􏰆retninostset
retcirtssesopminoitac􏰏irevbmuht􏰅setamitsenoitisopdnahgnizilitufodaetsnI
􏰇ytitnedi bmuhtrofs􏰎otucraelchsilbatsesdlohserhtdna􏰝􏰈􏰌􏰇􏰋noitauqE􏰜noisserpxe
noitac􏰏irevbmuhteht􏰅mhtiroglatnemngissaehtfosllecionoroVehtekiL
180 8 cm 6 4 cm 2
4
2
Next Innermost Finger
Innermost Definitely Not Thumb
Keep Ring‘s Innermost
210
Innermost Definitely Thumb
Assignment 6 cm
8
10 cm	300 270
240
􏰢􏰢􏰈
􏰇etamitsenoitisopdnaheht morfseulckaew
niaterotdegnahcnutfelsignirrotcarttaehtottnemngissanopu
desabnoitacbmuhteht􏰅senildehsaddnadilosehtneewtebdnabeht
niseiltcatnoctsomrenniehtfI􏰇bmuhtatonylekiltsomsiti􏰅enil
dehsadehtfothgirreppuehtotseiltcatnoctsomrenniehtfI􏰇bmuht
ayletin􏰏edsiti􏰅enillanogaiddilosehtfotfelrewolehtotseiltcatnoc
tsomrenniehtfI􏰇􏰝retnectaelcric􏰜tcatnocregn􏰏tsomrennitxeneht
fonoitisopehtotevitalersnoigernoisicedehtfotolpralopasisihT
􏰇gnitanimircsidtoneraserutaeftcatnoc􏰆retnirehtonehwelgnadna
noitarapesrennisusrevdnahthgirrofs􏰎otucnoitac􏰏irevbmuhT􏰤􏰊􏰉􏰇􏰋erugiF
􏰣􏰢􏰈
􏰇degnahcebotdahrevenytitnedistios􏰅tluafedtallits
sawetamitsenoitisopdnahehtelihwylbissop􏰅nwoddehcuottsr􏰏ti hcihwniegami
ehtniyltcerrocde􏰏itnedisawti􏰅neewtebnislebalondnadnednagninnigebsti
talebalemasehtsahyrotcejartanehwyllausU􏰇yrotcejart nevigehtgnolanwohs
tsalytitnediehtniaterslebalytitnedituohtiwsworrasuhT􏰇ytitneditrapdnaheht
segnahcmetsysnoitacegamiytimixorphcaeni􏰜petsemithcaetashtapehtotdengissasahmetsys
noitac􏰏itnediehthcihw􏰝􏰗F􏰜seititnedimlapdnaregn􏰏ehthtiwdelebalylevitcel
isoptluafednidnah
elohwehtfonwodhcuotdnanwodhcuotregn
emertxefoyteiravanopudetartsnomedeblliwecnegrevnocnoitacpyllausu
titahtesnesehtnicitsinimretedsimetsysnoitacoitac􏰏itnedielbailerdnaelbatseromhcumsecudorp􏰅nwodsehcuot
regn􏰏rehtonanehwsahcus􏰅yllaitnatsbussesaercninoitamrofniegaminehwsregn􏰏
sngissaerylnohcihwmetsysahcuS􏰇yliraropmetecafruseht􏰎otfil hcihwsregn􏰏fo
noitac􏰏itnedieretaruccaerusneotnopudeilerebdeenylnonehtsetamitsenoitisop
dnahehT􏰇detucexeebtonlliwmhtiroglatnemngissaehthcihwrofsegamiot
stcatnocgnitsixefosnoitac􏰏itnedisuoiverpsdnetxegnikcarthtaptnetsisreP
􏰇gnirrotcarttaehtfosecnarelotnoitatordnahehttsap􏰅noitatoremertxeogrednu
nehttubnoitatneirolartuenani	nwodhcuot hcihwsdnahrofstnemngissafognitfihs
􏰠􏰣􏰈
detarapeserayehtsselnusmlapsade􏰏itnedisimrevendnagnirehtdnuoraylreporp
deredrosyawlaerasregn􏰏elpitluM􏰇ezisegralyleuqinuahcaerstcatnocriehtfi
gnirrotcarttaehtfotahtronoitisopriehtfosseldragerde􏰏itnediylreporpeblliw
􏰅dnahrehtoehtno􏰅sleehmlapdetalosI􏰇mc􏰌tuobanihtiwyllacitrevdnaretemitnec
anihtiwyllatnozirohdengilasi gnirrotcarttaeht􏰅􏰇e􏰇i􏰅noitisopdnahlautcahtiw
tnetsisnocerasetamitsenoitisopdnahfiyltcerrocdet kcabsetamitsenoitisopdnahehtfotfirdehtos􏰅􏰎otfilretfayletaidemmiro
erofebdetlahsierutpacyrotcejarT􏰇yletaidemminigebstpmettanoitac􏰏itnedidna
ecafrusehtfonoigerdetacidniehtninwodsehcuotdnahehtnehT􏰇tnemirepxeeht
gnitratserofebnoitisopdnahtluafedehtotkcabtfirdotdewollasietamitsenoitisop
dnahthgireht􏰅tolpyreveroF􏰇seussinoitacaevahdnadernidettolperasworrarotcarttA􏰇ylno
petsemittsr􏰏ehttarebmuntrapdnahdetaicossariehtybdewollof􏰛A􏰛nahtiw
delebalerasrotcarttA􏰇nwohseraecafrusehtfonoigerdevlovniehtrode􏰏itnedi
gniebstrapdnahehtottnavelersrotcarttaesohtylno􏰅rehtegotetalsnartgnirehtni
srotcarttallaecniS􏰇noitisopdnahdetamitseehtnisegnahcybdesuacsasrotcartta
trapdnahdetcelesfosnoitomehtedulcniosladna􏰅yticirtnecce
􏰅noitatneiroetacidniotspetsemitdetcelestatrapdnahhcaenoderetnecoslaera
􏰝nayc􏰜sespillE􏰇􏰎ostfilronwodsehcuottrapdnahynanehwyllaicepse􏰅spetsemit
detcelestastcatnocregn􏰏llatcennocsenildettodeulb􏰅seirotcejartehtfognimit
evitaleretacidnioT􏰇elbisivylisaeytitnedinisegnahcekamdnaslebalehttuo
daerpsotsesacemosnidevomeblliwdnahehttub􏰅stluserehtnotce􏰎eonroelttil
sahyticolevregn􏰏􏰅noitac􏰏irevbmuhtnisgnilacsdnasnoitatorroftpecxE
15
10
5
0
F2
F2
F2
F3
F3
F2 F3
F4
F4
F2
F3 F4
F5 RA2
F5
F1 F1
−50 5 10 15 20
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰈􏰣􏰈
􏰇regn􏰏xednilautcaehtraen
pusdnerotcartta􏰉Aehtosnwoddnahehtswollof dnatfelstcerroc
etamitsenoitisopdnaheht􏰅detcerroceraseititnedi sA􏰇hcuotspit
􏰆regn􏰏ruofllaecnodetcerrocsnoitac􏰏itnedillahtiw􏰅tifotfeleht
otnwodhcuotsregncarttaregn􏰏xedniehtdna􏰅􏰝􏰉F􏰜regn􏰏xedni
ehtsade􏰏itnedisimyllaitinisiyknipeht􏰅stniartsnoctcatnoc􏰆retni
roerutaef oneraereht elihwllecionoroVregn􏰏xedni eht ni srucco
nwodhcuotykniplanigiroehtecniS􏰇nwodsedilselohwasadnaheht
saslavretniemitralugertaecafrusehtotnognillorsregn􏰏rehtoeht
dnatsr􏰏nwodgnihcuotykniphtiwtfelpotehttastratsdnahehT􏰤􏰋􏰉􏰇􏰋erugiF
15
10
5
0
−5
F2
F3	F4
F2	F3
F4
F5
F5
F5
F5
F3
F4
F3 F2 F5 F2 F4
A2 A3 A4
A5
A1
F3
F4 F2 F5 F3F4
F2
F3 A7 A6
F2	F4 F5	F2 F3 F4 F5
0 2 4 6 8 10 12 14 16 18 20
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
ammhtiroglatnemngissaeht􏰅srenroctfelehttA􏰇yalpotnisemoc
rotcafnoitarapesleehmlapehtsrenrocmottobehttA􏰇tnemngissa
derauqs􏰆ecnatsidforoivahebgnitrostnairavni􏰆noitalsnartehtnoyler
sesacllA􏰇erehwynayltcefrepde􏰏itnedieblliwsleehmlapsulpspit
􏰆regn􏰏ruofrosregn􏰏ev􏰏tahtswolloftisihtmorf􏰥erehwynanoitac􏰏
􏰆itnedisuoenatnatsni􏰅tcefreproftneic􏰑usera worlatnozirohylhguor
anispitregn􏰏ruoftahtswohstnemecalpyrevetuohguorhtnoitac􏰏
􏰆itneditcerroC􏰇􏰝s􏰛xder􏰜stluafedriehttagnitratssyawlasrotcartta
htiwsrenrocruofehttaylevisseccusdecalpsispitregn 14 16 18 20
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰊􏰣􏰈
􏰇􏰝noitator
tsirw􏰜noitaivedranlufoegnar mumixamehttaydaerlasinwohs
sadetsiwt dnahehttubluowsetanidrooctcatnoclatnoziroh
ehtfognitroselpmiStonseodyknipdnagnir neewteb
elgnaehtecnis􏰇􏰋erugiF
nignirrotcarttaehtrofdetalutsopsecnarelotnoitatorregn6 8 10 12 14 16 18 20
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰋􏰣􏰈
􏰇􏰊retpahCfo􏰍􏰣egaP
no􏰊􏰈􏰇􏰊erugiFnidnahsyawedisehtrofderruccosagnigremnoit
􏰆atnemgesdecudni􏰆edortcele􏰆margolellarapdiovaotdaerpsllewtpek
eraspitregn􏰏eht􏰅revewoH􏰇tlusertcerrocsihtrofdedeentonsisleeh
mlapehtfoecneserP􏰇yltcefrepde􏰏itnedisyawlaeratsirwehttanoit
􏰆aivedranlufostimilehtotesiwkcolcyllufdetatordnahanisregniF􏰤􏰡􏰉􏰇􏰋erugiF
15
10
5
0
−5
F4
FF5
FF3
FF2
F1
FF6 F6
FF77
0 2 4 6 8 10 12 14 16 18 20
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰌􏰣􏰈
􏰇tfelrewolehttasi hcihwtcatnocbmuhtehtbargneht
nachcihw􏰝􏰡A􏰜rotcarttaleehmlaprenniehtpunepolliwsleehmlap
htobfognigremroecnesbade􏰏itnedieranoitaivedlaidar
fostimilehtotesiwkcolc
F675
F6	F6 F6
F7 F1
0 2 4 6 8 10 12 14 16 18 20
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰍􏰣􏰈
􏰇detatortonsidnahehtnehw
leehmlaprenniehtrofdetcepxeyllamronnoitisopehtsiregn􏰏xedni
ehtotevitaler bmuhtehtfonoitisopeht􏰅noitator hcumsihtrednu
􏰅esuacebnwodhcuotrevensleehmlapehtfiruccooslanaceruliaffo
trossihTmlapeht morfdesaelersierusserplitnu􏰡Fot
yliraropmetegnahcottfelrewoltatcatnocbmuhtehtotdetubirt
cafrusehtotnosserp
sleehmlapehtsatubacnacsleeh mlaphtobfognigremroecnesbA􏰤􏰣􏰉􏰇􏰋erugiF
15
10
5
0
−5
F2 F2
F1 F1
F3
FF7
FF4	F5 F4
F6
00 2 4 6 8 10 12 14 16 18 20
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
t yb
desuacyllautcaera􏰌F􏰆􏰊Fdelebalstcatnocregn􏰏retuoehttahtetoN
dluowyehtnahtresoolelttil atpekerewsregn􏰏ehttnemirepxesiht
nI􏰇ylreporpdetnemgeserayehtsagnolsaecafruseht noerehwyna
yltcerrocde􏰏itnedi syawlaeranoitarug􏰏nocpirgnepanisregniF􏰤􏰠􏰊􏰇􏰋erugiF
15
10
5
0
−5
F7
F7 F2
F6
F6 F2 F3
A7	A6
0 2 4 6 8 10 12 14 16 18 20
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰢􏰣􏰈
􏰇stcatnocruofroeerhtotnitilpstegtonodstcatnocleeh
mlapgnigralne owtehterusneotti	htiwsyppolsehtgnignirb
􏰅drawpustoohsetamitsenoitisopdnaheht dna􏰡Fdna􏰍Fotdetcer
􏰆rocerasnoitac􏰏itnediehT􏰇ecafruselohwehtrevosllecionoroVrieht
dnapxeotniskcikrotcafezisleeh mlapehttahthguone worgyeht
egamidrihtehtybtuBuot
smlapehttnemirepxesihtnI􏰇ezisllufhcaerdnatuomottobyeht
nehwecafrusnoerehwynayltcerrocde􏰏itnedieraenolasleehmlaP􏰤􏰈􏰊􏰇􏰋erugiF
555
000
F7
F6
F2
F2 F5	F4
A7	A6
A7 A6 A7 A6
F2 F2F4F7 −5 F5 −5 F17 F56 −5 F6
−10
−15
10 12 14 16
a)
−10
−15
10 12 14 16
−10
−15
10 12 14 16
c)
b) Horizontal Position (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰣􏰣􏰈
􏰇mehthtiwyats
􏰝􏰡A􏰅􏰍A􏰜srotcarttaleeh mlapehtdna􏰅sleeh mlapsade􏰏itnedisim
niamerspitregn􏰏eht􏰅evitce􏰎enisirotcafnoitarapeseht􏰅noitarapes
leehmlaplanimonehtmitsenoitisopdnaheht dnahtesuaceb􏰝􏰡F􏰜leehmlapasade􏰏itnedisimyllait
􏰆inisinwodtsr􏰏ehT􏰇ylsuonorhcnysnwodhcuottonodtubmc􏰊
ybdetarapesniamerspitregn􏰏eht􏰝bnI􏰇snoitac􏰏itnediesehtezilib
enotnecajdatontub􏰜spitregn􏰏sade􏰏itnediebotstcatnocehtdna
hsinavotsllecionoroVleehmlapehtsesuachcihwrotcafnoitarapes
leehmlapwolasesuacsihT􏰇lamronekiltrapamc􏰉tuobaeraspit
􏰆regn􏰏tnecajdaeht􏰝a nI􏰇aeragnisnesehtfo􏰝enil	kcalblatnoziroh􏰜
egdemottobehtraensrotcarttaleehmlapehtneewtebdrawpuedils
dnanwodhcuotsregn 5 10 15 20
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰠􏰠􏰉
􏰇pitregn􏰏ehttseraen pusdnerotcartta􏰊Aeht
hguohtneve􏰊Fot􏰋Fmorfdetcerrocstegrevenytitnedipitregnttfel evahdluowylbaborp
mhtiroglatnemngissaeht􏰅tituohtiW􏰇latnozirohraentonsielgna
tcatnocn􏰏dnabmuhteht wohetoN􏰇snoigerpitregn􏰏eht
nidrohcpitregn􏰏elddim􏰆bmuhtarofnoitac􏰏itnedibmuhttcerroC􏰤􏰊􏰊􏰇􏰋erugiF
15
10	F2 F1F1
F3
F3 F23
5
0
F2 F1 F1
F2
FF4 F1
F1
F32 F4
F5
F34
−50 5 10 15 20
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
retnuocetatoryknipdna
bmuhtehtnehwtubtignivig􏰅ylthgilsehcuotbmuhtehtfopitehtylno􏰝c
nI􏰇pitregn􏰏ehtfotahtnahtrellathcumsemocebtcatnocstidnatuo
snetta􏰐bmuhtehtsadetcerrocsinoitac􏰏itnedibmuhteht􏰝bnI􏰇􏰈F
otytitnedibmuhtehtgnitcerroc􏰅pitregn􏰏dnabmuhtehtneewteb
noitomgnilacs hcnipinisibmuhteht
􏰅potehtta􏰝anItcatnocfoytitnediehttub􏰅nostratsretsulcehtdraobehtfoedishcihw
noylivaehdnepedllitstsumnoitac􏰏itnediretsulcesacsihtnI􏰇retsulcdnahtfela
morfelbahsiugnitsidnisraepparetsulctcatnocdnahthgirafstcatnocdnahthgirforetsulcenola
hsiugnitsidhcihwnoitisopretsulcfotnednepedniserutaeftcatnocylnoehteratnem
􏰆ecalpbmuhtevitalerehtdnasnoitatneirotcatnocmlaprennidnabmuhtehT
􏰇rehtoehtotecafrus
ehtfoediseno morfsedilsdnahasasnoitacegdnahemasehtmorfstcatnocllaerusneotdeilppaebtsumstcatnoc
dnahfoseitreporpgniretsulcehttnuoccaotniekathcihwsdohtemdetacitsihpos
erom􏰅eroferehT􏰇dnahthgirehtotsrehtodnadnahtfelehtotdengissapudne
dluowstcatnocs􏰛dnahehtfoemos􏰅sevlahthgirdnatfelehtgnilddartstrehtehwotgnidroccatcatnochcaeotytitnedidnah
ngissaotebdluownoitac􏰏itnedi	dnahfodohtemtselpmis ehT􏰇ecafrusehtfoflah
rehtieotelddi mehtssorcayleerf	maornacdnahatahthcusdeniojecafrusehtfo
sevlahthgirdnatfelehtevahhcihwdnaylsuoenatlumissdnahhtobetadomoccaot
hguoneegralerahcihwsecafrushcuotnedilla
mlanoitatorrognilacsdnah
evisnapxenidevlovnisirot􏰅gnihcuotspitregn􏰏ruof nahtsseleraerehtfitub􏰅 mc􏰋nahteromyb
􏰊􏰠􏰉
ehttsniagasesserpmocyllufhse􏰐ehtlitnuelbailernueraserutaefnoitatneirodna
ezistcatnocehT􏰇regnortsemocebstcatnocgnitsixefoserutaefgnitaugibmasidro
nwodsehcuottrapdnah wenanehwdetaulaveerebylnodeenseititnediregnahseitimixorp
htapllarehtehwgnikcehcybelcycnacsyarrarosnestnerrucehtrofdetucexeeb
otsdeenyllautcamhtiroglanoitacatStcatnoCrofgnikcehCisoppoehtotdnahagnidils
morfsresuegaruocsidoslanacelddi mehtssorcaecafrusehtgnihcrAotnillewnwodsehcuotylneddusretsulcdnahanehwfrusehtfoflahetisoppoeht
otnorafdnahtahtecalpyllanoitnetniyamecafrusehtesuoteerfdnahenoylno
htiwsrotarepoeht􏰅snoitacoldetcepxenunidnahafostnemecalpneddushtiwmetsys
noitac􏰏itnedi dnahehtloofnacrotarepoehthcihwniniamersesachguohT
􏰇snoitisopdnahdetamitseehtotsetalpmetrotcarttadnahthgirdnadnahtfelfo
snoitisopehtgniytybdetnemelpmisinoisividcimanydahcus􏰅􏰎otfilretfasnoitisop
dnahnwonktsalehtniateryliraropmetsetamitsetes􏰎odnahehtecniS􏰇elddi meht
raenyltnecertsomsawdnahhcihwotgnidroccatfelrothgireht drawotgnitfihs
􏰅cimanydeberoferehtdluohsecafrusehtfosevlahthgirdnatfelneewtebnoisivid
ehTeppa
hcihwstcatnoctxeneht􏰅􏰎ostfildnaecafrusehtfoelddimehtotedisthgirehtmorf
sevomdnahthgirehtfi􏰅elpmaxeroF􏰇lufplehsevorposlaybraen􏰎odetfilyltnecer
START
CONTACT PROXIMITIES STABILIZED ?
N
COST LOWEST SO FAR ?
N
Y
Y
END
RETAIN PREVIOUS IDENTIFICATIONS VIA PATH EXTENSION
DEFINE &amp; TRANSLATE LEFT &amp; RIGHT ATTRACTOR TEMPLATES
PICK FIRST CONTOUR
GENERATE PARTITIONING CONTOURS
TENTATIVELY DIVIDE HAND IDENTITIES ACROSS CONTOUR
TENTATIVELY ASSIGN FINGER IDENTITIES WITHIN EACH HAND
EVALUATE BIOMECHANICAL COHERENCE OF PARTITION
PICK NEXT CONTOUR
RECORD PARTITION AS LOWEST COST
N
LAST CONTOUR ?
Y
CHOOSE
LOWEST COST HAND PARTITION
END
ASSIGN FINAL CONTACT IDENTITIES WITHIN EACH HAND
F
􏰌􏰠􏰉
ehtgnitsujdA􏰇rotcarttadengissastidnatcatnochcaeneewtebsecnatsiddethgiew
fomusehtnognidnepedtsoceht􏰅rehtoehtsusrevgnirenonisrotcarttaotstcat
􏰆nocgningissafotsocehterapmoclliwmhtiroglanoitacL􏰞􏰪n􏰨HL􏰦􏰪n􏰨jAL
xfedxoexoex
􏰝􏰉􏰌􏰇􏰋􏰜 jFL􏰞􏰪n􏰨jFL􏰞􏰪n􏰨HL􏰦􏰪n􏰨jAL
yx
􏰤erasgnir
rotcartta RdnahthgirdnaLdnahtfelehtrof􏰝􏰪n􏰨jA􏰥􏰪n􏰨jA􏰜snoitisoprotcartta
lan􏰏ehteroferehT􏰇stes􏰎oregn􏰏detamitsegnidnopserrocrieht ybdetalsnarteb
ylefas nacstnioprotcarttalaudividnI􏰇b􏰊􏰇􏰊erugiFnisdnaheht	wollofsnoigernoit
􏰆atnemgesyppolsehtekiltsuj􏰅dnahstifonoitisopdetamitseeht	wollof otsetalsnart
gnirhcaE􏰇a􏰊􏰇􏰊erugiFninwohssnoitisoptcatnocdnahthgirdnatfeltluafedeht
ot dnopserrocdluohsrehtonaenootevitalersgnirehtfotnemecalptluafedehT
􏰇ecnotadezilituebwontsumsgnirthgirdnatfelhtobtahttpecxe􏰅noitac􏰏itnedi
regn􏰏rofdesu􏰝􏰌􏰇􏰋erugiF􏰜setalpmetrotcarttaehtsaemasehtyllacisabebdluohs
esehT􏰇setalpmetrotcarttadnahthgirdnatfelnoitisopdnaen􏰏edotsipetstsr􏰏eht
􏰅egamitnerrucehtrofdekovniebmhtiroglanoitac􏰏itnedidnahehtdluohS
sgniRrotcarttAthgiRdnatfeLgnicalP􏰉􏰇􏰌􏰇􏰋
􏰇segamisuoiverp morfdetupmocsnoitac􏰏itnedi
tcatnocehtdnetxednaniaterottneic􏰑ussi􏰝􏰊􏰇􏰊noitceS􏰜ssecorpgnikcarthtap
ehtybdemrofrepsanoitaunitnochtap􏰅raeppastcatnocwenondnadezilibatsevah
stcatnocgnitsixefoseitimixorphcihwnisegamiroF􏰇esaercniotseunitnocstcatnoc
wenynafoytimixorplatotehthcihwnisegamiytimixorptneuqesbusrofdnasraeppa
tcatnocwenahcihwniegamiytimixorphcaerofsetucexemhtiroglanoitacimnezodwefaecafrus
􏰍􏰠􏰉
􏰇yltsocyllanoitatupmoc􏰅yrassecenfidna􏰅detacitsihposeromhcumebnac
noititraphcaefonoitaulaveeht􏰅enimaxeotsesehtopyhrewefhtiW􏰇nezodatsom
taotsdnasuohtmorfenimaxeotsnoitatumrepytitnedidnahforebmunehtsecuder
siht􏰅stcatnoctcnitsideromronevesesuacnacdnahhcaetahtgniredisnoC􏰇dnah
etisoppoehtfostcatnocrevossorcropalrevoyllatnozirohdnahenofostcatnoc
hcihwnisesehtopyhllasdiovasruotnoclacitrevaivsnoititrapgnitareneG
􏰇detanimileyletaidemmieradnah
elgnisaybdesuacebnacnahtdnahneviganostcatnoceromezisehtopyhhcihw
sruotnoC􏰇dnahemaseht morferaecafrusehtnostcatnocllatahtsesehtopyheht
eldnahotecafrusehtfosdnethgirdnatfelehttayrassecenoslaerasruotnoC􏰇dnah
thgirehtmorferaruotnocehtfothgirehtotstcatnoclladna􏰅dnahtfelehtmorfera
ruotnocehtfotfelehtotstcatnocllahcihwni􏰅noititrapasaoslanwonk􏰅sisehtopyh
etarapesaotsdnopserrocruotnochcaE􏰇stcatnocfotesdex􏰏arofsesehtopyhtnem
􏰆ngissadetaicossariehtdnasruotnoctnere􏰎ideerhtfoselpmaxewohsc􏰬a􏰍􏰊􏰇􏰋seru
􏰆giF􏰇setanidrooclatnozirohtnecajdaforiaphcaeneewtebyawflahruotnoclacitrev
agnihsilbatsednasetanidrooclatnozirohriehtybstcatnocecafrusllagniredroyb
enodsisihT􏰇tcatnoctnecajdayllatnozirohhcaeneewtebeno􏰅senilruotnoclacit
􏰆revylhguorfotesaen􏰏edotsisesehtopyhelbisnesetarenegot yawtneic􏰑etsom
ehT􏰇derapmoc dnadetarenegebtsumsesehtopyhtnemngissafotes atxeN
sesehtopyHnoititraPelbisualPgnitareneG􏰊􏰇􏰌􏰇􏰋
􏰇􏰎o
tfilyliraropmetsdnahnehwnevesegamiytimixorpevisseccusssorcasnoitac􏰏itnedi
ezilibatsotgnipleh􏰅stnemngissadnahtnecerelbmeserhcihwsesehtopyhtnemngissa
rofstsocevitalerehtsrewolstes􏰎oregn􏰏dnadnahdetamitseehthtiwgnirrotcartta
12
10
8
6
4
2
0
−2 −20 −15 −10 −5 0 5 10 15 20
Left Partition
Right Partition
a)
12
10
8
6
4
2
0
Left Partition
Right Partition
−2 −20 −15 −10 −5 0 5 10 15 20
b)
12
10
8
6
4
2
0
Left Partition
Right Partition
−2 −20 −15 −10 −5 0 5 10 15 20
c) Horizontal Surface Position (cm)
Vertical Surface Position (cm)
􏰡􏰠􏰉
􏰇dnahtfel eht	morferatsereht dna􏰅dnah
thgirehtmorferaecafrusehtfoelddimehtnistcatnocowtehttaht
gninaem􏰅stcatnocfotnemegnarrasihtrofgninoititraptcerroceht
ylbaborpsi 􏰝bfogninoititrapehT􏰇􏰝selcric􏰜stcatnoctnecajdayllat
􏰆nozirohneewtebyawflahdecalpsyawlaerasruotnoC􏰇sesehtopyh
gninoititraptnere􏰎ideerhtgnitaercvnisimhtirog
􏰆lanoitac􏰏itnediregn􏰏rodnah􏰆nihtiwehT􏰇ylgnidroccadetadpuerashtaptcatnoc
llafoseititnedidnahehtdna􏰅gninoititrapdnahlautcaehtsanesohcsillarevotsoc
tsewolehtsahhcihwnoititrapehT􏰇detaulaveneebevahsnoititrapdezisehtopyh
llafostsocehtlitnuruotnocgninoititraphcaerof detaepersissecorpsihT
􏰇noititrapehtroftsoclatot
aniatbootrotcafnoitarapesdnahafolacorpicerehtybdelacsdnarehtegotdedda
erastsocdnahthgirdnatfeldethgieweht􏰅yllaniF􏰇secnatsidrotcarttadethgieweht
ybdezisahpmerednuerahcihwstniartsnoclanoitiddatneserper􏰅wolebdebircsedeb
ot􏰅srotcafesehT􏰇srotcafnoisehocmlapdna􏰅ssendednah􏰅yticolevgnihctulcstifo
slacorpicerehthtiwelohwasadethgiewnehtsitsocdnahhcaE􏰇dnaharoftsoc
gnitt􏰏etalpmetcisabehtstneserpermussihT􏰇srotcarttamlapdnabmuhtrofsrot
􏰆caferutaefnoitatneirodnaezisgnidulcni􏰅noitac􏰏itnediregn􏰏fo􏰋􏰋􏰇􏰋noitauqEni
satnioprotcarttadengissastiottcatnocde􏰏itnediylevitatnethcaemorfsecnatsid
dethgiewfo musehtdnahhcaerofgnitupmocybenodsisihT􏰇stniartsnocnoitar
􏰆apesdnah􏰆neewtebsteemnoititrapehtllewwohdnagnirrotcarttadengissarieht
t􏰏stcatnocde􏰏itnedi ylevitatnetehtllewwohetaulaveottnaemsitsocsihT􏰇noit
􏰆itrapehtroftsocaetupmocotsipetstxeneht􏰅􏰌􏰊􏰇􏰋erugiFotgninruteR
􏰇dnahhcaenihti w
stcatnocotseititnedimlapdnaregntdnadnahtfelehtotruotnoc
ehtfotfelehtotstcatnocynagningissaylevitatnetdnatsomtfelehtsahcusredivid
ruotnoctsr􏰏agnikcipybsnigebnoitazimitpoehT􏰇stniartsnoclacinahcemoibdna
lacimotananwonkyfsitastsebsretsulcnihtiwstnemegnarratcatnocdnasnoitisop
retsulcehttahthcussretsulcdnahthgirdnadnahtfelotnistcatnocehtsnoititrap
sruotnocehtfohcihwenimretedotsi hcraesnoitazimitpoehtfolaogehT
pooLhcraeSnoitazimitpOehTon Factor
􏰣􏰠􏰉
􏰇sdeepsetaredomtasetarutasrotcafeht􏰅tnetniresufo
noitacidniregnortsaevigylirassecentonodsdeepshgihecniS􏰇dengissasiretsulc
nevigehtecafrusehtfoedisehtdnaseiticolevlatnoziroh􏰛stcatnocehtfoegarevaeht
fonoitcnuf asirotcafehT􏰇dnahehtfotsoccisabehtgnisaercedsuht􏰅draobehtfo
edisdengissas􏰛retsulcehtdrawotgnivomerastcatnocs􏰛retsulcdnahanehweulav
􏰇seiticolevtcatnoclatnoziroh
s􏰛dnahthgirehtfoegarevaehtsusrevrotcafnoitceridgnihctulcdnaH􏰤􏰡􏰊􏰇􏰋erugiF
nignisaercniylthgilsybnonemonehpsihtserutpac􏰅􏰡􏰊􏰇􏰋erugiFniyletamixorppa
dettolp􏰅rotcafnoitceridgnihctulcdnahA􏰇edistahtmorfemacylbaborpti􏰅edis
enodrawotgnidilsdnaecafrusehtfoelddimehtninwodgnihcuotdetcetedsidnah
anehweroferehT􏰇thgirehtdrawotgnidilssemuserdna􏰅ecafrusehtfoelddim
ehtninwodkcabsehcuotceriDgnihctulCeht
hguohT􏰇rotcafssendednahehtnidecalpebnacecned􏰏nocdetimil􏰅yletanutrofnU
􏰇dnahetisoppoehtfobmuhtderewolaylurtsiyknipderewolyldesoppusehttaht
hcusdesreveremocebseititnediregn􏰏􏰅gnirrotcarttagnorwehtotstcatnocehtt􏰏
otmhtiroglanoitac􏰏itnedidnah􏰆nihtiwehtsesuacsihtecniS􏰇gnorwylbaborpsi
retsulcehtnistcatnocllaroftnemngissadnahevitatnetehtsesachcusnI􏰇tcatnoc
pitregn􏰏tsomretuotxenehtnahtrewolsretemitnecelpuocanahteromsipitregn􏰏
tsomretuoehtsade􏰏itneditcatnocehtnehwtsocdnahehtgnitsoobybtniartsnoc
􏰇stcatnocregn􏰏tsomretuotxendna
tsomretuoneewtebnoitarapeslacitrevehtsusrevrotcafssendednaHegn􏰏rehtoehtnahtrewolylthgilsylnoebotsdnetyknipehttub􏰅spitregn􏰏
ehtnahtrewolhcumdenoitisopebotsdnetbmuhtehT􏰇ssendednahfonoitacidni
gnortsasevigosladnahemasehtnisregn􏰏rehtootevitaler bmuhtehtfonoitisop
lacitreveht􏰅ecafrusehtfonoigerelddi msuougibmaehtniseil bmuhtehtnehwmorf
sibmuhtadnahhcihwyfitnediplehsrotcafnoitatneirobmuhtehthguohT
rotcaFssendednaH􏰉􏰇􏰌􏰇􏰌􏰇􏰋
􏰈􏰈􏰉
􏰇rotcafnoisehocmlaplan􏰏ehtniatbootdenibmoc
ylevitacilpitlumerasrotcaflacitrevdnalatnozirohehtdna􏰅noitarapeslacitrevehtrof
detaepererapukooleulavrotcafdnatnemerusaemehT􏰇 muminimdnamumixameht
neewtebecnere􏰎idehtgnikatdnasmlaperofrosleehmlapsade􏰏itnedistcatnocllafo
setanidrooclatnoziroh mumini mdna mumixamehtgnidn􏰏ybderusaemyltneic􏰑e
ebnacdaerpslatnozirohehT􏰇stcatnoc mlapneewtebnoitarapeslatnozirohsusrev
rotcafnoisehocmlapehtfoeulavehtswohsstieroferehT􏰇 mc􏰢nahtregralnoigerarevoderettacs
erastcatnocmlapdesoppusehthcihwniretsulcdnahevitatnetarofenowoleb
spordylkciuqrotcafnoisehoc mlapeht􏰅snoititraphcushsinupoT􏰇dnahelgnisa
morfstcatnocmlapeurtrofelbisualpsinahtrediwecafrusehtssorcaderettacs
ebotsrotcarttamlaps􏰛retsulcehtotdengissastcatnocehtsesuacyllaususihT
􏰇srotcarttamlaperofdnaleehmlapotdnahetisoppoehtmorfsregn􏰏artxeehtngissa
otsdnetmhtiroglanoitacehW􏰇mc􏰢tuobanahtregralnoigerarevo
derettacsebtondluohssdiortnectcatnocmlapos􏰅erauqsmc􏰠􏰈tuobasidnahtluda
dehctertstuonafonoigermlaperitneehT􏰇ebnacdnahemasehtmorfsmlaperof
dnastcatnocleehmlaptraparafwohnostniartsnoclufesugnicalp􏰅nosirapmocni
elttilyrevhctertsnac mlapeht􏰅revewoH􏰇mc􏰠􏰉otpugninnapsdnahemasehtfo
bmuhtdnayknipehthtiw􏰅dehctertstuoerasregn􏰏ehtnehwegraldnaesrapsetiuq
emocebnacretsulcaesuacebgnignellahcsisretsulctcatnocgnihsiugnitsiD
rotcaFnoisehoCmlaPcno
desuylnosirotcafssendednahehtos􏰅retsulcdnahde􏰏itnediyletaruccanarofwol
ylsuoenorreebotrotcafssendednahehtesuac nacytiugibmasihT􏰇􏰝􏰌􏰋􏰇􏰋erugiF􏰜
dnahetisoppoehtfoleehmlapretuodnaspitregn􏰏ehtsatnemegnarratcatnocemas
eht evahdnahenofospitregn􏰏dna bmuht eht	hcihwni	ytiugibma nagnitaercoitarapes
dnah􏰆retniehT􏰇sselrosretemitnecwefafosnoitarapesevitisophtiwgnitratspord
otsnigebyllaudargrotcafnoitarapeseht􏰅retsulcemasehtotrehtonaenofosret
􏰆emitnecelpuoc anihti wera hcihwstcatnocfotnemngissaegaruocne oT􏰇tolpeht
ni noitarapesbmuhtevitagenaotsdnopserrocpalrevohcuS􏰇llamsyrevsemoceb
rotcafnoitarapeseht􏰅tievobagnitao􏰐roecafrusehtgnihcuotrehtieelihwecafrus
ehtfonoigerranmulocemasehtgnippalrevoerasbmuhtehttseggusothguonellams
siamertxetes􏰎odnahesehtneewtebecnere􏰎idehtfI􏰇dnuofsisnoitisopregn􏰏tluaf
􏰆edgnidnopserrocriehtottcepserhtiwstcatnocdnahtfelehtfostes􏰎olatnoziroh
ehtfomumixamehtylralimiS􏰇snoitisopregn􏰏tluafedgnidnopserrocriehtottceps
􏰆erhtiwstcatnocdnahthgirfostes􏰎olatnozirohehtfomuminimehtgnidn􏰏yb
derusaemsitI􏰇palrevorohcaorppadnahhcaemorfsbmuhtehtfosnoitisoplatnoz
􏰆irohlautcarodetamitseeht hcihwnisnoititrapfostsoclatotehtsesaercnirotcaf
sihTtnedistcatnoctsomretuodnatsomrenni
ehtneewtebnoitarapeslatnozirohehtsusrevrotcafnoisehocmlaP􏰤􏰣􏰊􏰇􏰋erugiF
1
0 −6 −4 −2 0 2 4 6 8 10 12
Inter−Hand Separation Factor
􏰊􏰈􏰉
draobehtfoediss􏰛dnahtcerrocehtot ylkciuqsedilsretsulcehtfirotneserpsi
bmuhtehtfidnahtcerrocehtotdetubirttaeblliwdnaylreporprehtegotderetsulc
ebsyawlalliwecafrusehtfoelddimehtninwodhcuothcihwdnahafostraP
cerroc
de􏰏itnedieblliw􏰎ostfil dnadraobehtfoedisetisoppoehtotsedilshcihwdnah
amorfstraPisnoitisop
dnahdetamitseehthtiwstratstnemirepxehcae􏰅stnemirepxenoitac􏰏itnediregn􏰏
ehtnisA􏰛􏰇R􏰛naro􏰛L􏰛nahtiwdedecerperaslebalrotcarttadnaregn􏰏􏰅sdnah
thgirdnatfel morfsrotcarttadnasregn􏰏hsiugnitsidoTeserpera􏰝􏰢􏰋􏰇􏰋􏰬􏰈􏰋􏰇􏰋serugiF􏰜stlusernoitac􏰏itnedidnaH
stluseRnoitac􏰏itnedIdnaH􏰍􏰇􏰌􏰇􏰋
􏰇esacsihtnienofoeulavtluafedehtnosekatdna􏰅dnahemasehtotdengissa
􏰇sbmuhtthgirdnatfeleht
neewtebecnatsiddetamitseehtsusrevrotcafnoitarapesdnah10 t
gnirudthgirkcabdevrewsnehtfihsdrawnwoddna􏰎otfilyraropmetstihguohtneve
dnahtfelehtotdetubirttayltcerrocsiriapregnve dna􏰅gnola􏰝􏰋AL􏰇g􏰇e􏰜srotcarttadnahtfelsgnirb􏰅sregn􏰏ehtswollof
etamitsenoitisopdnahtfelehtesuaceB􏰇rewolmcwefaecafrusehtspatdnadnocesarof􏰎ostfilneht
􏰅ecafrusehtfoedisthgirehtotnillewsedilsdnanoitisoptluafedtfelnistratsriapregntrohS􏰤􏰈􏰋􏰇􏰋erugiF
􏰋􏰈􏰉
Vertical Position on Surface (Y axis cm)
15
10
5
0
LA5 LA4 LA3 LA2
LA6	LA7
LA1
RA1
RF1
−−5 −20 −15 calpnoitatneiro􏰠􏰍ahtiwtcatnocenola􏰅ylralimiS􏰇spetsemiterutufniraeppasidnoitamrofni
􏰑
noitatneiroeuqinuehtdluohsnoitac􏰏itnediehtezilibatsottihtiw􏰝􏰈AR􏰜rotcarttabmuhtthgireht
gnignirb􏰅esnopsernitfelsevometamitsenoitisopdnahthgirehT􏰇nwodhcuotnopudnahthgireht
otnoitubirttaetaidemmiesuacdnaE􏰇􏰠􏰉􏰈tuobasielgnarojams􏰛espilletcatnocehttahtetoN􏰇noitatneirostifo
􏰑
eutrivybylelosde􏰏itnediyltcerrocsiecafrusehtfoelddimtfelehtnidecalp0
LF3 LF2
LF3 RF2	a) LF2 RF3
LF3RF3 LF2RF2
RF3 RF2
b)
RF2 RF3
d)
c)
LF3 LF2
−−5 −20 −15 −10 −5 0 5 10 15 20
Horizontal Position on Surface (X axis cm)
􏰇sregn􏰏thgirsanoitac􏰏itnediotlasreverkciuqatubsregn􏰏tfelsanoitac􏰏itnedilaitinihtiw
􏰅􏰝bnisruccoetisoppoehT􏰇dnahtfelehtotdetubirttaerebottisesuacrotcafyticolevgnihctulceht
yticolevdrawtfelstifonoitcetednoputubflahthgirehtnonwodsehcuotriapehtawno
sdnepedytitnediriapehttubytitnedidnahemasehttegspitregn􏰏htob􏰅􏰝ddna􏰝cnisriappitregn􏰏
yranoitats ehtroF􏰇ecafrusfoelddi	mraennoitac5
0
RF2
RRF3 RF4
RF7 RF7
RF5
RF6
RRF1 RF1
−5 −20 −15 −10 −5 0 5 10 15 20
Horizontal Position on Surface (X axis cm)
􏰇elddimehtelddartssleehmlapdnaspitregnafssendednahdnanoitatneiro
bmuhteht􏰅tneserpsibmuhtehtecnisdna􏰅dnahemasehtotdetubirttaerastcatnocllaserusne
rotcafnoitarapesdnaherugiF
􏰡􏰈􏰉
Vertical Position on Surface (Y axis cm)
15
10
5
0
LF5
LF6 LF6
LF4 LF3
LF7
LLF2
LF1
−−5 −20 −15 −10 −5 0 5 10 15 20
Horizontal Position on Surface (X axis cm)
􏰇leehmlapretuoehtnostseryllufdnahehtfothgiewehtnehw
gnihsiugnitsidylnosinosirapmocsihttub􏰅􏰝􏰍FLdelebalylsuoenorre􏰜bmuhtehtotderapmoc􏰝􏰈FL
delebalylsuoenorre􏰜tcatnocleehmlapretuoehtfoezisegralylevitalerehtsiretsulcdnahtfelamorf
retsulctcatnocdnahthgirsihthsiugnitsidylbissopdluoctahtgnihtylnoehT􏰇srotcarttadnahtfeleht
fossenraenehtedirrevootelbanuerayehtesacsihtnidna􏰅edisrafehttadetimilyllanoitnetnisisrotcaf
ssendednahdnanoitatneirobmuhts􏰛dnahafohtgnertseht􏰅seitiugibmagnizilibatsedyllaitnetopdiova
oT􏰇dnahtfelehtsade􏰏itnedisimsidnaecafrusehtfoflahtfelehtotnillewnwodsehcuotdnahthgirA0
LF4 LF3
LLF2
RRF2 RF2
RF3RF4
LLF5
RRF5
RF6 RF6
LF1 LF1 RF1
RF1
LF6	LF7
RRF7 RF7
−5 −20 −15 −10 −5 0 5 10 15 20
Horizontal Position on Surface (X axis cm)
􏰇tlusertcerrocsihtedivorpotetepmocllasrotcafnoisehocmlapdna􏰅srotcafnoitatneiro
bmuht􏰅srotcafssendednah􏰅rotcafnoitarapesdnah􏰆retniehT􏰇sbmuhtehtneewtebthgirtilpsgnieb
sretsulceht􏰅yltcerrocdenoititraperaecafrusehtfoflahtfelehtnoedisybedisdecalpsdnahlluf owTF2
RRF2 RF2
LF5
LLF1 LF1
RF1 RF1
LF7 LF6	LF7
−5 −20 −15 −10 −5 0 5 10 15 20
Horizontal Position on Surface (X axis cm)
ehocmlapehtfossenevitce􏰎e
otlacitircsisleeh mlaptfelehtfoecneserpeht􏰅revewoH􏰇tlusertcerrocsihtedivorpotetepmoc
llasrotcafnoisehocmlapdna􏰅srotcafnoitatneirobmuht􏰅srotcafssendednah􏰅rotcafnoitarapesdnah
􏰆retniehT􏰇sbmuhtehtneewtebthgirtilpsgniebsretsulceht􏰅yltcerrocdenoititraperaecafruseht
foflahtfelehtnoedisybedisdecalpregn􏰏elddimdnabmuhtthgirehtsulpdnahtfeleritneehT􏰤􏰡􏰋􏰇􏰋erugiF
􏰠􏰉􏰉
Vertical Position on Surface (Y axis cm)
15
10
5
0
RF1
LLF5 LF5
LF3 LF4
LLF2
LLF7 LF7
LF1
−−5 −20 −15 −10 −5 0 5 10 15 20
Horizontal Position on Surface (X axis cm)
􏰇evitce􏰎enisi
rotcafgnithgiewstidna􏰅derusaemebtonnacnoisehocmlapehtnni
tfelehtsadenodsleehmlaptfelehtecniS􏰇yltcerrocdenoititraptonera
ecafrusehtfoflahtfel	eht	nodecalpregn
15
10
5
0
LF4LF3 LF2
RRF3 RF3
RF2
LLF5
LLF1
RRF1
−−5 −20 −15 −10 −5 0 5 10 15 20
Horizontal Position on Surface (X axis cm)
􏰇yltcerrocnoitarug􏰏nocdnahsihtseldnahmetsysehtsahregnaecafrusehtfoflahtfel
ehtnoedisybedisdecalpsregnhthcuotdnahafospitregn􏰏ruofeht
tsaeltanehW􏰇snoitac􏰏itnediwennotce􏰎egnizilibatsasahsetamitseehtnidleh
etatsmetsyssuounitnocehttahtserusneoslastcatnocdnahfodiortnecatsujnaht
rehtarseititnediesehtnopusetamitsenoitisopdnahgnisaBnep
hwemossyolpmemetsysnoitac􏰏it
􏰆nedieht􏰅deniartsnocemegnarramlap􏰆regn􏰏lacisnesnongnitcetedfosdohtemlanoitidda􏰅sesac
emosnisihtsesserddarotcaf noisehoc	mlapehthguohT􏰇􏰝􏰢􏰋􏰇􏰋erugiF􏰜srotcartta
mlapelbaliavas􏰛dnahenoehtgnill􏰏ylsuoenorrenetfo􏰅dnahenootnideretsulceb
otsdnahgnihcuotyllaitrap􏰅tnecajdaowtmorfsregn􏰏esuacotsdnetoslaelddim
ehtselddartshcihwdnahagniretsulcrofdedeenrotcafnoitarapesdnahteydessorcnunwodhcuot hcihwsdnahowtfostcatnocehT
􏰇􏰝􏰌􏰋􏰇􏰋erugiF􏰜
dnahtfelehtotdetubirttaebsyawlalliwedisthgirehtmorfrevognidilsdnaha
ybdedecerptonsawhcihwecafrusehtfoedistfelehtotllewnwodhcuotneddus
ynaeroferehT􏰇dnahthgirasade􏰏itnedisimebotecafrusehtfoedistfeleht no
dnahtfel a􏰅elpmaxerof􏰅esuacdluocslasreverevitisopeslaferehw􏰅ecafrusehtfo
sedisraf ehttaton􏰅gnihsiugnitsidtonnetfosi	noitisopdnaherehwecafrusehtfo
elddimehtnidesuylnoerayeht􏰅tneserpsibmuhtehtnehwnevesuougibmaylriaf
eraserutaefyticolevgnihctulcdnacsissensselwap
namuhotelbitpecsuseblliwretpahctxenehtfometsysnoitalupinamcidrohceht
ecniS􏰇snoitidnocgnitareporednuyltcefreptsomlasbmuhtdnasleehmlapmorf
spitregn􏰏sehsiugnitsidmetsyseht􏰅rehtegotesolcerasdnahhtobmorfsregn􏰏nehw
esacehtrofgninoititrapdnahotrorehtegotesolcerapitregn􏰏dnabmuhtataht
esacehtrofnoitac􏰏irevbmuhtotedamebdluocstnemevorpmithgilshguohT
􏰇stce􏰎eedisxelpmocylbaeganamnu
gnisuactuohtiwnoitarug􏰏nocdnahc􏰏icepsstirofdenutebnacsretemaraps􏰛msin
􏰆ahcemdetceleseht􏰅dnimnitpekebtsumsmsinahcemrehtohtiwecnalabhguohT
􏰇noitarug􏰏nocdnahc􏰏icepsafonoitaugibmasidetanimodotsdnetrotcafgnithgiew
romsinahcemnoitac􏰏itnedihcaeesuaceblacitcarpsisretemarapmetsysfogninut
launaM􏰇rehtonaenognitcidartnocylerar􏰅yllufecaepostsixeocsmsinahcemnoit
􏰆ac􏰏itnediynamostahtsierutcetihcraehtfotcepsagnisirprustsomehT
􏰇sdnahhtob
morfstcatnochtiwdetalupopyllufsiecafrusehtnehwtnadnuderemocebsrotcaf
gnithgieweht􏰅suougibmasinoitisopdnahrostnemegnarratcatnocnehwycarucca
noitac􏰏itnedievorpmiotdengisederasrotcafgnithgiewnoititrapdnahdnarotcart
􏰆tasuoiravehtelihw􏰅ylralimiS􏰇llewsaesacsihtnitnadnudersetamitsenoitisop
􏰌􏰉􏰉
ehtfoseludomruofeht􏰅txeN􏰇STMehtnonoitalupinamcidrohcfotnempolev
􏰆edehtdecneu􏰐ni hcihwairetircngiseddnaselpicnirpnoitcaretniretupmocihwsecivedtupnifoweiverahtiwstratsretpahcehT
􏰇snoitomregn􏰏decnalabnumorfmodeerffoseerg
􏰆edgnilacsdnanoitatortcartxeyllargetniotsregn􏰏ralucitrapfosnoitomgniretl􏰏
dnagnithgiewrofseuqinhcettneserplliwretpahcsihT􏰇sregn􏰏elpitlumfosnoitom
gnikcartyltnednepednirofseigolonhcetgnisneshcuot􏰆itlumfoyticracsehtotdna
sdaphcuotforotcafmrofllamsehtotdetubirttaebylbaborpnacsihTregn􏰏rodnah morf
lortnocfoFOD􏰆􏰉nahteromdeviredevahot nwonksienoonidiova􏰅noitarug􏰏nocdnahniegnahcelpmis
ahtiwgnirutsegdna􏰅gnitniop􏰅gnipyt neewtebylsuoenatnatsni hctiwsotrotarepo
STMehtelbaneretpahcsiht ni	debircsedsanoitatnemelpmisti	dnatpecnocsihT
􏰇tuoyalyeklanoitnevnocanognipytrofdevreserebdluohssregn􏰏laudividnifoytivit
􏰆casuonorhcnysaelihw􏰅gnitserdnahro􏰅serutsegdnammoc􏰅gnitniopetaitinidluohs
sregn􏰏elpitlumfonwodhcuotsuonorhcnys􏰤tpecnoclevonanopudednuofsiscihparg
dna􏰅sdnammoc􏰅txetfoyrtnerofseuqinhcetfonoitargetnisihT􏰇STMehtnonoital
􏰆upinamcidrohcdnagnipytetargetniotdeilppaerasretpahcsuoiverpnidepoleved
seitilibapacnoitac􏰏itnedi dnagnikcarteht wohetartsnomedlliwretpahcsihT
NOITALUPINAMCIDROHC
􏰌retpahC
namrofrepfoxedni eht yb
dedividksatehtrof DIytluc􏰑idfoxedniehtotlanoitroporpsiksatgnitegrata
rof	T Memittnemevomehttahtsetats	waleepsgnitegratcisabehT
ecnamrofrePgnitnioPdnawaLyberutsopgnitserdnahehtmorflennahcwenatcelesoslanacsrotare
wollayb
ytilibixe􏰐gnippamdnascimonogresevorpmignitagsihT􏰇rezingocernoitomdrohc
ehtnihtiwslennahcnoitalupinamcidrohcfonoitcelesetagoslaslangisnoitazinorhc
􏰆nystesbusregniF􏰇slangisnoitalupinamrodnammocetairporppaehtsetareneg
dnasnoitceridralucitrapnisdrohcregn􏰏ralucitrapfonoitomstcetedrezingocer
noitomdrohcehT􏰇dnahhcaenonoitomregn􏰏morfmodeerffoseergedelbissecca
ylsuoenatlumisteytnednepedniruofsretl􏰏rotcartxenoitomdnahehT􏰇gnitserro
􏰅gnidils􏰅dezinorhcnysebotdnuofretalerahcihwesohtslecnactubsesserpyeksa
snwodhcuotregniwsrezingocernoitomdrohc
dnarotcetedgnipytehtsdeefrotcetednoitazinorhcnystesbusregn􏰏ehT
􏰇rezingocernoitomdrohcehtdna􏰅rotcartxenoitomdnaheht􏰅rotcetedgnipyt
eht􏰅rotcetednoitazinorhcnystesbusregncartnisgallaropmeT
yaleDgnikcarTylnoesneshcihwsecivedybnahtregnrepfoxednirehgiha􏰅oslA􏰇evom
tsumdnahehtecnatsidehtesaercedodtubsemitgnitegratlatotesaercedtonod
sdeepsdnahretsaftasnoitomrosrucegralyletanoitroporpsidsesuachcihwsnoitcnuf
refsnartnoitomrosrucotnoitomesuomraenilnontahtneebevahstluserelbaton
eromehtfoemoSahsylsuoiravot waLiFfonoisrev
emosotatademitgnitegratt􏰏yllacipytseidutsecnamrofrepecivedgnitnioP
oitalumrof rehtO􏰇tegrat ehtfo htdiwlatnozirohehtsi
Wdna􏰅tegrat
ehtotnoitisopregn􏰏rodnahgnitratseht morfecnatsidlatnozirohehtsi Aerehw
W
􏰉
􏰝􏰉􏰇􏰌􏰜
􏰝 􏰜gol􏰦DI
A􏰉
􏰤sadesserpxeebdluocytluc􏰑idfoxedniehttahtdnuofyllanigiro
􏰪􏰢􏰊􏰨sttiF􏰅aerategratranmuloc􏰅llatadrawotsyawedisdnahehtgnivomfoksat
􏰢􏰉􏰉
ylraeehtfosllabkcartdetareponiop
dnagnipytgnitargetnitastpmettaelbisivtsomehtbyeKlacinahceMniseciveDgnitnioPgniddebmEroppusotstpmettatsapezylanadnasecivednoitalupinamtcerid
gnitsixefoseitilibapacehteniltuolliwweiversihTeht
􏰅sehcaorppaesehtximretniyleerfnacsresutahtosdevlovesaherawtfosecafretni
resulacihparghguohTicepsetercsidnI􏰇tcejboneercs􏰆nona
foeuhdna􏰅ezis􏰅noitisopehtsahcus􏰅ecapsetanidroocemosnisretemarapfotnem
􏰆tsujdalanoitceridib􏰅suounitnocsevlovninoitalupinaMoit
􏰆alupinamneewtebstsixe ymotohcid􏰅tnevmucricottlucslangislortnocnoitomtsritac􏰏itnedi
regn􏰏foecnegrevnoc􏰅noitazilibatsytimixorpregnitompeekotdengisedsirezingocernoitomdrohcSTMeht
􏰅sihtfothgil nI􏰇snoitadargedecnamrofrepesehtledomot waL􏰛sttiFotsnoisnetxe
esoporpsrohtuahtobdna􏰅stluserralimisdah􏰪􏰋􏰍􏰨nam􏰎oH􏰇􏰙􏰋􏰈􏰉ybsrorrenoit
ohtemnoitargetniSTMehthguohT
􏰇edomgnitniopniniamerotnwodyekhtdnagniggarddnagnitniop
tatsafsaeciwttuoballitssawesuomehtopnillits
elihwyekrehtonagnisserpybdehsilpmoccasi	gnikcilCiedomgnitnioPcitsyojralimiS
􏰇sksatgnitnioperuprof kcits
gnitniopehtnahtretsaf􏰙􏰌􏰉deniameroslaesuomehT􏰇dn􏰏otregn􏰏ehtroftegrat
llamsahcussi kcitsgnitniopehtesuacebuohttahtdnuofyllautcayehT􏰇yrasse
􏰆cennusawecivedgnitniopehtroftuoyalyekeht􏰎ognihcaerecniskcitsdeddebme
ehthtiwretsafebdluowgnipytdnagnitniopfoerutximagnivlovnisksattahtdn􏰏
otdetcepxeyehtdnatregn􏰏
ehtybkcitsehtotdeilppaecroflanoitceridehtotlanoitroporpsiyticolevretniop
esuoMcitsgnitnioptniopkcarTehterutaefoteunitnocspotpalabihsoTdnaMBI
smialc􏰅MEOdaphcuotyramirp
eht􏰅􏰇cnI􏰅scitpanyS􏰇skcilcesuommodnargnisuac􏰅gnipytelihwsbmuhtriehthtiw
dapehtpatyllatnediccaotycnednetaevahsresuemos􏰅tnemevorpminaylniatrec
sisihtelihW􏰇rabecapsehtwolebdetacolsdaphcuotdezisesuomahti w
nahtreroop 􏰙􏰢􏰈ylnosawecnamrofrepgnitnioP􏰇srehtoehtlladelrucdnasregn􏰏
xedniehtdednetxeresuehtnehwnagebedomgnitniopdna􏰅draobyekehttanwod
koolotdetnuomsawaremacehT􏰇draobyekarevoserutseggnitniopkcartotmetsys
noitingocererutsegdnahdesabttniopdluowsresusuhT􏰇gnitniopsignir
ehtnoitceridehtderrefniyalpsidpotpalafosrenrocehttadetnuomsrosnesderarfni
ruoF􏰇regn􏰏xedniehtnognirgnittime􏰆derarfninadecalpylralimis􏰪􏰍􏰊􏰈􏰨krutkoG
dnatrebiS􏰇kcilcdnaretniopehtevomotgnirettelapehttsniagagnirsulytseht
burdnagnipytelihwsgnirehtraewdluocsresU􏰇regn􏰏xedniehtotgnirettelap
adnabmuhtehtotgnirsulytsadehcattarrehtiegnitniopregn􏰏
esnesotneebsahhcaorppalufsseccusyllaicremmocsseltubgnitseretninA
draobyeKaevobAserutseGgnitnioPgnitceteDiopdnagnipytdexi mniecnamrofrepllarevorettebtcepxedluow
enoeroferehT􏰇sdaphcuotdnaskcitsyojynitesehtnahtesuomafotahtotresolc
hcumebdluohs ycaruccadnadeepsgnitniopstideirruh wohnognidnepedsmehtylpmissignipytotgnitniopmorfdnagnitniopot
gnipytmorfhtobgniogSTMehtrofemitgnimohehteroferehT􏰇ecafruseht􏰎otfil
spitregn􏰏esehtretfaemitynaemusernacgnipyT􏰇spitregn􏰏tnecajdaeerhtgnidils
dnagnicalpylsuonorhcnysybdetaitiniebnacgniggard􏰅ylralimiS􏰇ylsuonorhcnys
ecafrusehtnospitregn􏰏tnecajdaowtgnicalpyb􏰝woremohrevoyltceridgnidulcniesserpyekroskcilcnottubdetalu
􏰆memorfsnoitomgnitniophsiugnitsidnetfosdaphcuotdnasneercshcuoT
sneercSdnasdaPhcuoT􏰋􏰇􏰉􏰇􏰈􏰇􏰌
􏰇dnahtnanimodehthtiwgnitniopelihwdnahtnanimod􏰆non
ehthtiwdnuorgkcabehtgninnapsahcussnoitalupinamlaunamibsedulcerphcaorp
􏰆pagnitniopdednah􏰆eno􏰅gnipytdednah􏰆enoeht􏰅yllaniF􏰇desurevoeblliwdnah
enonisnodnetroselcsumfotesbusynatahtdoohilekilehtsesaercedseodSTM
ehtsasdnahhtobrevoylneveseitivitcaesehtgnidaerps􏰅selcsumemasehtyltcaxe
daoltonodgnitniopdnagnipyttahtgnimussA􏰇revodnarevoselcsumgnisuom
ehtylnoesulliwdnahgnisuomehtdna􏰅delbuodsidnahenorofdaolgnipyteht
esuacebsdraobyekdednah􏰆enoehtrofretaergylbamuserpsisdnahehtniseirujni
esurevofoksireht􏰅gnitniopdnagnipytfostnuomalauqeseriuqer ksats􏰛rotarepo
ehtfineve􏰅revewoH􏰇sdnahhtobrevoyllauqegnipytdnasnoitalupinamhtobdaerps
hcihw􏰅sgnippamSTMehtnahtytiralclanoitazinagroeromsahrehtoehtotnoit
􏰆ac􏰏icepsetercsiddnadnahenootsksatnoitalupinamfonoitacollatcirts A
􏰇emehcsgnipytdrohcrotuoyalyekwen
anraeltsumrotarepoehttahtsisecivedesehtfoegatnavdasidyramirpehT􏰇dnah
enofohcaerysaenihtiwerasrettelllaerusneotstuoyalrosemehcsgnidrohceuqinu
esuhcihwesoprupsihtrofsdraobyekdednahhtoehthtiwkcuptelbatgniwardro
esuomagnitalupinamelihwdnahenohtiwsdnammocyekgniussirognipyT
sepyTrehtOehtr
snoitatupmocevisnetniro􏰅aremac􏰅stnemhcattagnirehtylraelC􏰇noitatskrow􏰉
ogidnIscihparGnociliSzHM􏰆􏰠􏰌􏰈anospf 􏰡taregn􏰏ehtkcartylnodluochcaorppa
􏰉􏰊􏰉
lanoisnemid􏰆eerhtdnaowtroflortnocnoitomfoFOD􏰆􏰍􏰆􏰊otni ytilibixe􏰐regn􏰏dna
dnahfoegnarediwsihtecuderdnaerutpacotdepolevedneebevahsecivedfoegnar
ediwA􏰇retnecmlapehtmorfderusaemsanoitatneirodnanoitisopdnahllarevoeht
morfemocwtfosdrohcotsnoitalupinamrehtodna
gnitniopgnippamylnoybmelborpsihtdnuorastegSTMehT􏰇evisnopsernudna
hsiggulsmeesrosrucehtekamdluownoitomhcumsihtsserppusroretl􏰏otgni
􏰆tpmettA􏰇tluserylisaenacecafrusehttaderusaemnoitomlaretal􏰖􏰋􏰟􏰈sahcumsa
htiwspatgnicnal G􏰇ecafrusehtstcatnocregn􏰏ehtnehwtneserpebllitsyamhcihw
riaehtnisiregn􏰏ehtelihwnoitomregn􏰏laretaltnac􏰏ingisstrapmignidnetxedna
gnihcaerkciuQ􏰇snoigeryekhcuototsregnnocyarranahcuS􏰇etarelotnacnoitomregn􏰏laretalfo
gnidlohserhtnahtecafrusehtgnolasnoitomlaitnegnateromecudorpotsdnetsnoig
i
siedomrosrucstnetaphtobnIlaretal􏰩􏰅 wols aseodresuehtfi rosaerayektcnitsid
otnidedividylentahtnoitpmussaehtsisdohtemeseht
nitnerehnI􏰇skcilcsadednetnierahcihwecafrushcuotehtnospatgnirudruccolliw
eridottnemevomgnitimilellortnoc
ebnactahtesohteramodeerffoseergedlargetnItcurts
lautpecrepksatdnaerutcurtslortnocecivedneewtebecnednopserrocehT
ytilibarapeSluoc
snoitacilppaerawtfosgnitsixellatsomlaodD􏰉hcuselihW􏰇stcejbodna
stnemucodlanoisnemid􏰆owtgnilacsdnagnitatortatpedaylralucitrapeblliwSTM
ehttahtstseggussihTwsevorpmiecnamrofrep
􏰤taht􏰇latebocaJfo
noitressaeht	ybdezirammus ebnacisop􏰅evitalerdnaetulosba􏰅yrator
dnaraenilfonoitanibmoc ynafosrecudsnarterasecivedtupni􏰇􏰇􏰇
􏰤tahteugra􏰇lateyalnikcaM􏰅elpmaxeroF􏰇secivedesehtrofsemehcsnoitaulavedna
noitacbevitisnes􏰆ecrofFOD􏰆􏰍edulcnisecivedhcuS􏰇sksatnoitalupinamlacihparg
􏰋􏰊􏰉
lliwstnenopmoctnanimod􏰆noneht􏰅sixanagnolaylraensinoitomfonoitcerideht
gnitacidni􏰅setanimodmodeerffoseergednoitalsnartro􏰅gnilacs􏰅noitatorehtfoeno
nehwtuB􏰇slanogaidhtaptsetrohsssorcanoitalupinamtsafgniwollasuht􏰅snoitcerid
lanogaidniylurterahcihwmodeerffoseergedrostnenopmocelpitlumnisnoitom
suoenatlumisssaplliwsretl􏰏noitomehT􏰇ytilibarapesdnaytirgetnineewtebesim
􏰆orpmocaekirtsotyrtlliwretpahcsihtnidetneserprotcartxenoitomdnaheht
􏰅􏰪􏰢􏰍􏰈􏰨􏰇lateiahZfoecafretni􏰆rezodllubkcits􏰆gnitniop􏰆laudtnecerehtekiL
􏰇gnitatordnagnimooz􏰅gninnapsuoenatlumisgnitneverp
􏰅secivedesehthtiwtluc􏰑idyrevsiemitatamodeerffoseergedowtnahteromfo
noitalupinamapesderedisnocebdluohsmodeerffoseergedgnillorcseseht􏰅revewoH
􏰇ecimrofdradnatsmodeerffoseergedowtehtnahteromedivorp􏰪􏰉􏰌􏰨maLdnakcilliG
foesuoMrelloRehtnorellorregn􏰏elddimehtdnaesuomtnioPllorcSMBIehtno
kcitsgnitniopehtsahcuseci mrofslortnocgnillorcsyrailixua􏰅elpmaxeroF
􏰇sexarehtoehtgnola
tegratehtmorfyawarosrucehtegdunyamnoitomdnahniseitilibatsni􏰅sexafotes
enonitegratehtnoni orezsresusaesuacebecnamrofreptruhnacylsuoenatlumis
elbaliavamodeerffoseergedlargetniynamootgnivahtahtsesacemosnidnuof
oslaevahsrehcraeserrehtO􏰇ksatehtfotaht htiwdehctamebdluohsecivedeht
foytilibarapesfoytilargetniehtsuhT􏰇rettebdemrofreptnemtsujdanoitisopdna
tnemtsujdarolocneewtebhctiwsotnottubahtiwesuomFODaroloctcejbognignahcsa
hcussksatelbarapesylevitingocroftahtdnuof osla􏰇latebocaJ􏰅revewoH
􏰇noitceridlanogaidanisitegratehtfilanogaidehtgnolalevartotdewollafiylkciuq
eromtegratahcaeryllausunacenoecnisgnisirprustonsisihT􏰇modeerffoseerged
largetnieerhtgnidivorpecivedahtiwdellortnoctseberasnoisnemidowtnitcejbo
nagninoitisopdnagnizisylsuoenatlumissahcussksatlargetnitahtdnuofanoitnev
rhtiwdnahderreferpehtfonoitisopehtesnesyllanretni nac
sresutahtmialc􏰠􏰊􏰆􏰌􏰈dnuof􏰪􏰠􏰣􏰨􏰇la
tekuhcnageL􏰇stegratllamsdrawotstnemevomnoisicerptaretsafsi dnahthgir
ehtdna􏰅sulytsro􏰅llabkcart􏰅esuomhtiwgnitniopecnatsid􏰆gnoltaretsafsidnah
tfelehttahtde􏰏irev􏰪􏰡􏰡􏰨􏰇latehsabbaK􏰇gnitpmorptuohtiwygetartsegasudnah
lellarapehtdetpadayllautcastcejbusdna􏰅redilsahtiwdelacsrehtoehtelihwkcup
ahtiwdenoitisopdnahenonehwksatgninoitisopdnagnilacsdexi maniesaercni
ecnamrofrep􏰙􏰌􏰉􏰆􏰌􏰈adnuofsreyMdnanotxuB􏰅seidutstseilraeehtfoenonI
􏰇dnahderreferp􏰆nonehthtiwegap
ehtetalupinamottondetcurtsnierastcejbusfi􏰙􏰠􏰉otpusporddeepsgnitirwdnah
tahtdnuof􏰪􏰌􏰌􏰨senehtAdnadraiuG􏰅dnahthgirehthtiwgnitirwdnahroflamitpo
repapfoeceipafonoitatneirodnanoitisopehtpeekotdnahtfelriehtesuyllamron
elpoeptahtgnicitoN􏰇ytinummocnoitcaretniretupmoc􏰆namuhehtninoitatnemi
nufotdevloveevahdnahderreferp􏰆non
ehtfosnoitomesraoc eht tahtstisop hcihw􏰅􏰪􏰌􏰌􏰨 ledomniahccitameniks􏰛draiuG
􏰇sdnahhtobgnisunoitalupinamsuoenatlumisotsrefernoitalupinamlaunami B
noitalupinaMlaunamiBniagdeepsehteviecer
dluohssrotareposuhT􏰇sixaehtgnolayltcaxesruccolortnoctahtosdesserppuseb
􏰍􏰊􏰉
􏰅srelursadepahserahcihw􏰅recudsnartlacisyhpahtiwhcae􏰅slootelbapsargtcnitsid
etacovdayehT􏰇llewsanoitceleslennahcotdnetxedluohsecnednopserrocerutcurts
lautpecrepova􏰅slennahcdrohcregn􏰏artxestifoenootgniggard
etacollaoteerfsiSTMehT􏰇gniggardroftnaemesohtmorfgnitnioproftnaem
snoitomregn􏰏hsiugnitsidotecneuqesgnimitgardlbatehtno
gnisserpenodnatelbatehtrevognirevohsulytsaneewtebecnereisfoecneuqesgard􏰆patemosreb
􏰆 mucehtdiovaottiswollaoslaSTMehtfoytilibapaclennahcelpitlumehT
uomrevonoitceleslennahcSTMfoegatnavdayramirpehT􏰇gnitserdnahelohw
rofdevresersihtnevesehttub􏰅slennahcneveshsiugnitsidnacSTMehtfometsys
noitac􏰏itnediregn􏰏ehT􏰇snotuuberomynamre􏰎oyamsmetsysDACrofskcup
dezilaicepSssedomevitanretla
etavitcaotesuomanosnottubgnisserpotsdnopserrocnoitceleslennahC
noitceleSlennahCnamlaunamibekamdluocsihT􏰇ecapsemasehtninoitalupinamlaunamibdna
gnipyttroppusotelbaebotnwonkecivedylnoehtsiSTMeht􏰅revoeroM􏰇mehtrof
detiusyltcefreperaytilibanoitac􏰏itnedidnahdnaecafrusediwsti􏰅STMehtnotey
derolpxe neebtonevahseuqinhcetnoitalupinamlaunamibhcuselihWgnitsidotsdnanoitazinorhcnyS􏰉􏰇􏰌
􏰇nwodtupnehwecafrus krow
ehtgnirettulcybgnipytgnidepmisloottuobayrrowotevahtonlli wyehtniagadna
􏰅ecnamrofrepgnihctiws􏰆edomrolootrettebeveihcaylekillliwyeht􏰅tuoyaddnani
yadsksatemaseht htiwdecaferasrotarepoSTMfisuhT􏰇rehtonapukcipdna
lootenonwodtupotdeenonsierehtecnissuoenatnatsniylbarapmocsiSTMeht
noslennahcneewtebgnihctiwsiddaeriuqeryameroferehtdnagnixelpitlumecapscirenegot
tsesolcsinoitceleslennahccidrohcstacilppaniatrecrofdetius􏰆llewdnaelba
􏰆dnemmocsislootdezilaiceps􏰅elpitlummorfkcipyllacisyhpotyticapacsihT
ecivedsseldroc􏰅elpitlumyfitnedidnakcartnachcihw􏰅ygolonhcettelbat
mocaWeht􏰅niagA􏰇rewols yltnac􏰏ingisoslasitcejbolautrivhcaegarddna􏰅tceles
􏰅rosrucehtetalupinamotecivedenognisuybgnixelpitlum􏰆emiT􏰇skcupdelebal
sahcusseciveddexelpitlum􏰆ecapscireneghtiwnahtseciveddexelpitlum􏰆ecaps􏰅dezi
􏰆laicepsesehthtiwneercsnostcejbognivomylmodnarruofgnikcarttaretsafera
stcejbustahtdn􏰏yehT􏰇neercsnoemasehtkool hcihwstcejbolautrivgnidnopser
􏰆rocfotesagnitalupinamfoksateht hctamotrotorro􏰅skcirb􏰅serauqselbahcterts
GET ANY PATH RECENTLY CREATED BY HAND PART TOUCHDOWN
N
N
START
PATH PROXIMITY JUST CROSSED KEYPRESS THRESH ?
Y
PATH IDENTIFIED AS FINGER NOT PALM?
Y
Y PATH'S N DEBOUNCE Y
FIND CLOSEST KEY REGION
HAND SLIDING/ WRITING ?
OK ? N
N
ANY KEY REGION NEARBY ?
Y
APPEND KEYPRESS QUEUE ELEMENT TO TAIL OF FIFO KEYPRESS QUEUE
CREATE KEYPRESS QUEUE ELEMENT CONTAINING PATH ID, CLOSEST KEY &amp; PRESS TIMESTAMP
􏰢􏰊􏰉
􏰇ssecorpnoitartsigersserpyekehtfotrahc	wolF􏰤􏰈􏰇􏰌erugi F
􏰣􏰊􏰉
ynA􏰇eueuqsserpyekehtniecnotsohehtotnoissimsnartderussatonsilobmysyek
s􏰛tnemelena􏰅revewoH􏰇tsohehtotdettimsnarteblliwpatregn􏰏hcaemorfslobmys
yekhcihwniredroehtsex􏰏suhttI􏰇dlohserhtytimixorpsserpyekehtssapyeht
nehwybsnwodhcuotregn􏰏sredroylevitce􏰎eeueuqsserpyekehttahtetoN
􏰇sregn􏰏rehtoybsnwodhcuotroftiawrossecorp
otseunitnocpoolehttsatadtnemelesserpyeksihtsdneppanehtpetslan􏰏ehT
􏰇dlohserhtytimixorpsserpyekehtdessorcregn􏰏ehtnehwgnitacidnipmatsemita
dna􏰅noigeryektsesolceht􏰅ytitnediregn􏰏dnaxednihtapehtgniniatnocdetaerc
sierutcurtsatadtnemelesserpyeka􏰅regn􏰏ehtotesolcsinoigeryekagnimussA
􏰇pitregn􏰏ehtforetemitnecanihtiwsnoigeryekoneraerehtfiderongiebyam
nwodhcuotehT􏰇􏰝􏰍egaPno􏰈􏰇􏰈erugiFees􏰇g􏰇e􏰜tuoyalyekYTREWQden􏰏ederp
yx
ani yektseraenehtdn􏰏otdesusi􏰝􏰪n􏰨iF􏰅􏰪n􏰨iF􏰜diortnecpitregn􏰏ehtfonoitisop
tnerruceht􏰅stsetnoitartsigeresehtsessapnwodhcuotregn􏰏ehtgnimussA
􏰇nwodhcuot wenehtfopmatsemitehthtiwnosirapmocrof
devreserpsihtapdloehtyb􏰎otfiltsalehtfopmatsemitehtnoitavitcaernopU􏰇tops
emasehtrevonwodkcabsehcuotylkciuqdna􏰎ostfilregn􏰏ehtfihtapdlos􏰛regn􏰏
agnitavitcaerybgnitsetecnuobedsetatilicaf􏰝􏰊􏰇􏰊􏰇􏰊noitceS􏰜eludomgnikcarthtap
ehT􏰇gnipytsadednetnitonylraelcsnoitomdnahmorfsesserpyekfonoitareneg
diovaotrotcetedgnipytehtybderongiebtsumnwodhcuotehtsesacesehtnI
􏰇nwodhcuottnerrucehtotroirps m􏰠􏰌􏰈􏰬􏰠􏰠􏰈naht
sselecafruseht􏰎odetfiltrapdnahemaseht􏰅􏰇e􏰇i􏰅gnitsetecnuobedsliafti 􏰒
􏰇noitalupinamcidrohcanidevlovniyltnerrucsihtiwdetaicossasitidnaheht􏰒
􏰇regn􏰏afodaetsnimlapasade􏰏itnedineebsahhtaptcatnocsti􏰒
􏰤eurterasnoitidnocgniwollofehtfoynasselnueueuqsserpyekanideretsiger
siti􏰅dlohserhtllamsasessaprusytimixorptcatnocstidnanwodsehcuottrapdnah
busrof
sehcraestsr􏰏mhtiroglanoitcetednoitazinorhcnyseht􏰅deveirterneebevahegami
ytimixorptnerrucehtrofsrekramelcycefildnaseititnedihtapehtretfA
semiTesaeleRdnasserPybshtaPgnitroSbsemitesaelerdnasserpderusaemeht
􏰅sdlohserhtytimixorprewolhtiwtub􏰅􏰝􏰈􏰇􏰌􏰇􏰈􏰇􏰍noitceSees􏰜setaremarfegamiretsaf
htiwelbarelotebdluohssdlohserhtrehgiH􏰇ytimixorppitregn􏰏egarevaehthtf􏰏􏰆eno
tuobaottesyltnerrucerasdlohserhtytimixorpesehT􏰇􏰎otfillatototroirpdlohserht
ytimixorpesaelerawolebspordhtaptcatnochcaehcihwtaemitehtdnadlohserht
ytimixorpsserpasdeecxetsr􏰏htaptcatnochcaehcihwtaemitehtdrocersrekram
elcycefilehtelihw􏰅sregn􏰏dezinorhcnysfosnoitanibmocrosdrohctnere􏰎idhsiug
􏰆nitsiddnashtapmlaperongiotdedeeneraseititnediehTerrucehttupnisasekatssecorpeht􏰅dnahhcaenihti W􏰇dnahhcaeotdengis
􏰆sastcatnocehtrofyltnednepednidetaepersissecorpnoitcetednoitazinorhcnys
sihTrhcnysregn􏰏ehtfotrahcwo􏰐aswohs 􏰉􏰇􏰌erugiF
rotceteDnoitazinorhcnySehT􏰉􏰇􏰉􏰇􏰌
􏰇sdnahhtob
morfsnwodhcuotregn􏰏ehtsredrodnasedulcni eueuqsserpyekeht􏰅wolebrotceted
noitazinorhcnysehtniyletarapesdnahhcaerofdeniatniamsesaelerdnasnwodhcuot
regn􏰏fostsilderedroehtekilnU􏰇eueuqetadidnacsserpyekaderedisnocebdluohs
eueuqsserpyekehtesnessihtnI􏰇tsohehtotdettimsnartgnieberofebeueuqeht
morfdeteledebottnemelenaesuacnacsregn􏰏fotesbusdezinorhcnysafotrap
tsserp
gniebsahcussnoitcesgniwollofehtnidessucsidebotsnoitidnocforebmunafo
GET HAND'S CURRENT PATH PARAMETERS &amp; ID'S
DELETE ASSOCIATED KEYPRESS QUEUE ELEMENTS
A
ANY PRESSES SYNCED
SEARCH FOR FINGER SUBSETS PRESSED OR RELEASED SIMULTANEOUSLY
N
# FINGER	Y RELEASES
SYNCED > 2?
A
C
SYNCED	Y FINGERS SIMULTANEOUSLY B
LIFTING ?	?
SYNC MARKER PENDING
N
N
?? YY
SET SYNC TIME MARKER
# FINGER
PRESSES SYNCED > 2 ?
Y
N
PAUSE SENDING OF ASSOCIATED KEYPRESS QUEUE ELEMS
DELETE ASSOCIATED KEY QUEUE ELEMS
DELETE ASSOCIATED KEY QUEUE ELEMENTS
TOUCHING	Y OR HALTED TOO
LONG ? N
CLEAR SYNC MARKER
RESUME KEY QUEUE SENDING
N
N
LIFTING
Y
􏰈􏰋􏰉
􏰇ssecorpnoitcetednoitazinorhcnysregn􏰏ehtfotrahc	wol F􏰤􏰉􏰇􏰌erugi F
B
SYNCED FINGERS DOWN BRIEFLY ?
Y
N
DELETE ASSOCIATED KEYPRESS QUEUE ELEMENTS
SIGNIFICANT	Y LATERAL MOTION ?
C
N
CHORD HAS TAP EVENTS ?
N
LOOKUP CHORD FROM SYNCED FINGER ID'S
RESTING CHORD: NO EVENTS GENERA TED
Y
APPEND CHORD TAP EVENTS TO COMM QUEUE
C
􏰉􏰋􏰉
􏰇nois
􏰆simsnartdnanoitcetedpatdrohcgniwohs􏰉􏰇􏰌erugiFfonoitaunitnoC􏰤􏰊􏰇􏰌erugiF
􏰊􏰋􏰉
ehtgnihcuot yltnerrucsregn􏰏llaedulcnitonyamtesbussserpdezinorhcnyseht
esuacebdesusitesbus mretehT􏰇dle􏰏tibytitnediregn􏰏asayltneinevnocderotssi
tesbussihtgnisirpmocseititnediregn􏰏fonoitanibmocehtdna􏰅tesbusdezinorhcnys
ehtsadedrocersidezinorhcnysebotdnuofsesserpregn􏰏tnecerfotestsegralehT
􏰇sm􏰠􏰌􏰈tuobadnasm􏰠neewtebyraveroferehtnacnoitcetednoitazinorhcnyssserp
rofdlohserhtlaropmetehT􏰇dnahrehtieno􏰎otfilronwodhcuotdetaler􏰆gnipyt
tsalehtecnisemitehtnosdnepedosladlohserhteht􏰅sedomnoitalupinamcidrohc
dnagnipytneewtebsiseretsyhemosedivorpoT􏰇noitazinorhcnysrofdetsetgnieb
sregn􏰏forebmunehtotnoitroporpniylthgilssesaercnidlohserhtlaropmetehtfo
edutingameht􏰅dnahehtssorcanwodhcuotninoisicerpmietadomoccaoT
􏰇detacidnisisesserpregn􏰏
tnecertsomneht gnomanoitazinorhcnys􏰅yrtnetnecertsomhtnehtfodlohserht
laropmetanihtiwsi yrtnetnecertsomehtfoemitsserpehtfI􏰇seirtnetsiltnecer
tsomev􏰏ro􏰅ruof􏰅eerhtehtfosemitsserpgnirapmocybdnuofsiev􏰏otpusregn􏰏
tnecertsomehtfoeromroeerhtgnomanoitazinorhcnyS􏰇suonorhcnysaderedisnoc
sisserpregn􏰏tnecertsomeht􏰅tonfI􏰇dezinorhcnysderedisnocerasesserpregn􏰏
owteht􏰅dlohserhtlaropmetanahtsselsisemitsserpowteht neewtebecnere􏰎id
ehtfI􏰇derapmoceratsilehtniseirtnetnecertsomowtehtfosemitsserpeht
􏰅sesserpregn􏰏tnecertsomowtehtneewtebnoitazinorhcnysrofkcehcoT
stesbuSregniFdezinorhcnySrofgnihcraeS􏰉􏰇􏰉􏰇􏰉􏰇􏰌
􏰇stsetnoitazinorhcnysrehtotsomdnastsilesehtmorfdedulcxe
erasesaelerdnasesserpleehmlap􏰅noitalupinamcidrohcrognipytnietapicitrap
ylevitcaotton􏰅gnitsererasdnahehtelihwsmraerofehttroppusotsismlapehtfo
noitcnufyramirpehtecniS􏰇semitesaelerhtaprofdeniatniamsitsiletarapestub
rali	mis	A􏰇semitsserphtapotgnidroccaderedroeberoferehtlli wtsilehT􏰇dlohserht
ytimixorpsserpehtsessorcregn􏰏ehtsatsilaniemitsserpstihtiwgnolahtapregn􏰏
hcaegnidrocerybenodebnacsihTngistuohtiwsregn􏰏eromro
owtfotesbusafo􏰎otfilsuonorhcnysybdewollofnwodhcuotdezinorhcnyS􏰒
􏰇pat drohcafotraprognitsertsujrehtie
erasregn􏰏ehttahtgnitacidni􏰅delecnacerasesserpyekdetaicossaeht􏰅dnoces
aflahtuobanahteromecafruseht noniamersregnlersieueuqsserpyekeht nodlohehtos􏰅sesserpyekylekil
tsomerasnoitomregn􏰏eht􏰅􏰎otfilsuonorhcnysafoesacnI􏰇noitazinorhcnys
rofdekcehcebnacsesaelersregn􏰏ehtlitnudettimsnartgniebmorfsyekdeta
􏰆icossaehtforehtiestneverphcihweueuqgnissecorpsserpyekeht nodecalp
sidlohaos􏰅flestinisuougibmasisregn􏰏owtfonwodhcuotdezinorhcnyS􏰒
􏰇noitazinorhcnys􏰎otfiltuobanwonk
signihtynaerofeb􏰅yletaidemmidelecnacebotsregn􏰏esohthtiwdetaicossa
sesserpyekehtsesuacsyawlasregn􏰏eromroeerhtfonwodhcuotdezinorhcnyS􏰒
􏰇dezinorhcnyserewsregn􏰏esohtfosnwodhcuot
lanigiroehtrehtehwfosseldrager􏰅delecnacebotsregn􏰏esohthtiwdetaicos
􏰆sasesserpyekehtsesuacsyawlasregn􏰏eromroeerhtfo􏰎otfildezinorhcnyS􏰒
􏰤swollofsadeziram
􏰆musebnac􏰉􏰇􏰌erugiFnirotcetednoitazinorhcnysehtybnekatsnoitcaehT
snoitcAdnasnoisiceDrotceteDnoitazinorhcnySraessisemitesaelerybdetros
seititnedihtapfotsilehT􏰇nwodhcuotylsuoenatlumisyehtsagnihcuotsniamer
teysregn􏰏rehtonahtreilraehcumnwodsehcuotregnhcaerofdetacollaebtsumlennahcenoos􏰅spatdrohcybdetareneg
ebot	deenyletulosbahcihwstneve ylnoehteraskcilcesuoM􏰇secivonybgnipyt
gnirudgnitser dnahegaruocneotytpmetfeleboslayamatdrohcehT
􏰌elbaTnidetartsullieraslennahc
euqinuehT􏰇dnahrepslennahceuqinunevesylnoeraerehtdna􏰅lennahcdrohc
emaseht otreferllaspitregn􏰏forebmunemasehtgniniatnocsnoitanibmoc􏰅sregn􏰏
eromroowtrofseititnedifosnoitanibmocelbissop􏰍􏰉eraerehthguohT􏰇patdrohc
ehtnisregn􏰏fonoitanibmocehtot dengissasdnammocdraobyekroskcilcesuom
sahcusstnevetupni ynarofelbatpukool akcehcotdesusitesbusdezinorhcnys
ehtrofseititnediregn􏰏fodlespatdrohcriapregn􏰏dezinorhcnysmorfsekirtsyektnecajdasuonorhcnysaylerab
etaitnere􏰎idot	wolsootylpmissietarnacsyarrarosnesspf 􏰠􏰌ehtsesacenilredrob
wefani􏰅noitazinorhcnys􏰎otfildnanwodhcuotfognigarevahcushtiwneve􏰅revewoH
􏰇noitcetedtsuboreromrofecnodedlohserhtdnarehtegotdeddaerasecnere􏰎idemit
nwodhcuotdna􏰎otfilriapregn􏰏eht􏰅yletarapesecnere􏰎idemit􏰎otfildnaecnere􏰎id
emitnwodhcuotehtgnidlohserhtfodaetsnI􏰇gnipytnidevlovnitonerasregn􏰏esoht
ngiserusasisregn􏰏eromroeerhtfonoitazinorhcnys􏰎otfilronwodhcuotrehtieos
􏰅sregn􏰏owtnahteromssorcallitslliw
erehttub􏰅dezinorhcnyssraeppa􏰎otfilronwodhcuotrehtietahtylkciuqosrehtona
otyekenomorfllorsregn􏰏eht􏰅syektnecajdagnikirtsnehwsemitemosesuacebesac
laiceps asadetaertebtsumnoitcetednoitazinorhcnysriapregn􏰏tahtetoN
􏰍􏰋􏰉
􏰇spitreg
􏰆nocIlennahC
oitac􏰏itnediehtybdegnahcsiytitnedis􏰛htapeht􏰒
􏰤eurtemocebsnoitidnocgniwollofehtfoynafihtaptcatnocdetaicossa
stifo􏰎otfilotroirpemitynatadeteledebnactnemeleeueuqdaehsihT􏰇etadidnac
sserpyekdilavninasaeueuqehtmorfdeteledneebronlobmysyekdetaicossasti
dettimsnartrehtiensahhcihwnwodhcuotregn􏰏tsedloehtstneserpertnemeleeueuq
daehsihT􏰇eueuqsserpyekehtfo daehehttatnemeleehttakeepotsi	petstsr􏰏
ehT􏰇retupmoctsohehtotstnevedetaicossarolobmyss􏰛yekehtgnidneserofeb
esaelerregniwspetsehtswohsiodnunahtgnitavarggassel
hcumsisihttub􏰅tnesebotkcilcehtrofemitdnocesadrohcregn􏰏ehtpatot
evahyamresueht􏰅gnipytretfaecalpni ebydaerlaotsneppahrosrucesuomehtfI
􏰇edilsdrohcgninevretninagniriuqer􏰅gnikcilcerofebrosrucesuomehtnoitisoperot
deenlliwresuehtgnipytretfayllausuecnistsolebotspatdrohclanoitnetnisesuac
ylerartitey􏰅rosructxetehtnoitisoperylmodnarnachcihwskcilcesuomsuoirups
sdiovasihT􏰇edilsdrohclaretalgninevretninatuohtiwsserpyekdilavaswollof
ylkciuqhcihwpatdrohctsrnoitareneglatnediccatsniaganoituacerprehtrufasA
skcilCesuoMlatnediccAgnidiovAslennahctsomnodetarenegebylefasnacstnevetupnifoegnar
ediwag
tnevehguohT􏰇rohtuaehtybdesusgnippamtnevepatdrohcrehtofoselpmaxerof
A
ELEMENT PATH STILL IDENTIFIED AS FINGER ?
N
B
DELETE CURRENT ELEMENT FROM KEYPRESS QUEUE
PICK ELEMENT AT HEAD OF KEYPRESS QUEUE
C Y
Y
PATH IN A	N SYNCHRONIZED
SUBSET ?
TIME SINCE PRESS &lt; TAP TIMEOUT
Y
Y
FINGER SLID TOO FAR ?
N
FINGER LIFTED ?
N
KEY REGION A MODIFIER ?
B
N
C
Y
APPEND PRECEDING MODIFIERS &amp; ELEMENT'S KEY REGION SYMBOL TO HOST COMM QUEUE
SKIP TO NEXT ELEMENT IN QUEUE
B PATH MOST
Y
PROXIMITY PROFILE IMPULSIVE ?
Y	FINGERS TOUCHING
N
Y
TIME SINCE FINGER PRESS > TAP TIMEOUT ?
? NN
A
􏰢􏰋􏰉
􏰇ssecorpnoissimsnartdnaecnatpeccasserpyekehtfotrahcwolF􏰤􏰋􏰇􏰌erugiF
􏰣􏰋􏰉
snoigerre􏰏idomynarofeueuqehtfodaehehtdrawotkcabnacslliwegatsnoissi m
􏰆snarteht􏰅egatsnoissimsnartehtsehcaeryllufsseccusdnapatyekdilavasitnemele
txenehtfI 􏰇daehehtgniteledtuohtiweueuqeht nitnemeletxenehtotsecnavda
gnissecorp􏰅yekre􏰏idomarevosiregn􏰏ehttubdetfilteytonsahregn􏰏s􏰛tneme
􏰆ledaehehtfi􏰅 􏰶tla􏰵ro􏰅 􏰶lrtc􏰵􏰅 􏰶tfihs􏰵sahcussyekre􏰏idomeldnahoT
syeKre􏰏idoMgnildnaH􏰈􏰇􏰊􏰇􏰉􏰇􏰌
􏰇wolebdebircsedsa􏰅sserpcitamepytasaro􏰅gnitserdnahhtiwsserpevislupmi
nasa􏰅yekre􏰏idomasanoissimsnartdesuacllitsevahyamti􏰅tuoemitpateht
gnideecxeemitarofecafrusehtnodeyatsevahyamtihguohtnevetubtotdettimsnartsi
lobmysyekehtsarotarepoehtotkcabdeefrofdnuosgnikcilcasetarenegoslaSTM
ehT􏰇eueuqehtfodaeheht morfdeteledsitnemelesserpyekehtdnatsohehtot
dettimsnartsilobmysyekdetaicossaeht􏰅patyeklamronasayfilauqotnwodhcuot
retfahguonenoosdhcuotesohwregn􏰏ehtrehtehwskcehctxenmhtirogla
eht􏰅stsetevobaehtybdeteledneebtonsahtnemelesserpyekehtgnimussA
􏰇noitalumekcilcnottubesuomroyekrofpatregn􏰏elgnisa
dnanoitalupinamrosrucesuomrofedilsregn􏰏elgnisayolpmehcihwsdaphcuotyb
detarelotsinahtnoitompatgnicnalgeromhcumsisihT􏰇syekrofgnihcaerylkciuq
nehwrucconetfohcihwsnoitompitregn􏰏gnicnalgetadommoccaotdewollaeranoit
􏰆omlaretalfosretemilli	mlareves􏰅ecafrusehtnognipyt hcuotebyamsresuesuaceB
􏰇􏰝wolebees􏰜dlohregn􏰏citamepytroyekre􏰏idomatonylraelcsidna􏰎otfil
tuohtiwdnocesaflahtuobanahteromecafrusehtnoneebsahtcatnoceht􏰒
􏰇noitcessuoiverpehtni
debircsedsatesbusregn􏰏dezinorhcnysaforebmemaebotdnuofsihtapeht 􏰒
􏰠􏰌􏰉
ytivislupmieht􏰅yekagnitarenegtuohtiwregn􏰏desiaranwodtesyltnegotrotarepo
eht wollaoT􏰇gnimit􏰎otfiltneuqesbusfosseldrager􏰅dettimsnarteblliwlobmys
s􏰛yektaht􏰅noigeryekaotnodepporddnaecafruseht􏰎odesiarylsuonorhcnysasi
regn􏰏aemithcaenehT􏰇ecafrusehtnognitserdnahafosregn􏰏ev􏰏llahtiwtrats
tsumresueht􏰅daetsnI􏰇sserphcaeretfa􏰎otfilregn􏰏kciuqeriuqertonseodhcihw
edomecnatpeccayekdnocesasahSTMeht􏰅ecafrusehtevobasregn􏰏ehtfonois
􏰆nepsusegaruocnetonseodhcihwerutsopgnipytevitanretlanaedivorpoT
􏰇secneuqes neewtebecafrusehtnokcabsdnahehttser
otsliafrotarepoehtfierityllautnevelliwsmra􏰛srotarepoehttahttpecxeerutsop
gnipytelbatpeccasisihT􏰇ecafrusehtevobasdnahehtgnitao􏰐tuohtiwylkciuq
depytebtonnacsdrow􏰅yllacisaB􏰇sdnahehttroppusotecafruseht nognitsersreg
􏰆n􏰏tsomhtiwsecneuqesgnol epytottluc􏰑idyrevtisekamnoitarenegyekesuacot
ecafruseht􏰅patylpsirc􏰅􏰇e􏰇i􏰅􏰎otfil ylkciuqsregn􏰏tahttnemeriuqereht􏰅revewoH
􏰇noissimsnarttuohtiwdeteledebdnatuoemit yllaitnesselliwyeht􏰅sdnocesillim
derdnuhelpuocanihtiwdetfiltonerayehtsagnolsatub􏰅rotcetednoitazinorhc
􏰆nys eht	ybdetadilavni	ebtonlli wdnasuonorhcnysayllaitnesseerasesserprettal
esehT􏰇tahtretfadnocesabmuhtehtdnaretaldnocesayknipehthtiwylsuonorhc
􏰆nysawollofyehttub􏰅ecafrusehtnospitregn􏰏lartnecehtgnicalpylsuoenatlumis
ybgnitserdnahnigebsemitemossrotarepoesuacebyrassecensisihT􏰇sserpyek
agnikovnituohtiwylsuonorhcnysaecafrusyekeht noregn􏰏ehttserotrotarepo
ehtrofyawasedivorptniartsnocgnimitsiht􏰅sehctiwsyeklacinahcemfodlohserht
ecrofnoitavitcaehtekiL􏰇tnesebotyekarofsdnocesillimderdnuhwefanihtiw
􏰎okcabtfildnaecafrusehtnoregn􏰏ehthcuottsumsrotarepoyllamroN
sdnaHdednepsuSmorfspaTlluFotsevitanretlA􏰉􏰇􏰊􏰇􏰉􏰇􏰌
􏰇snoigerre􏰏idomgnidecerpynafosganahnehwmpw􏰠􏰍tuobaotelbaniattaetargnipytmumi
􏰆xamehttimilotsraeppasihTitregn􏰏gni
􏰆tao􏰐fonoitalugereht􏰅ecafrusehtevobagnitao􏰐smlapspeekrotarepoehtfI
􏰇derrajtegspitregn􏰏ehttahtdrahosecafrusehtekirtsotelbissopmitubllatisekam
oslaecafrusehtotsregnkirtsotyrassecenlevartdrawnwodeht
secudersihT􏰇􏰖􏰋􏰟􏰈saelttilsaebnacthgiehregn􏰏gnitao􏰐egareva􏰅detnalpsmlap
ehthtiW􏰇hcuotyllatnediccamehtgnitteltuohtiwecafrusehtevobatao􏰐spitregn􏰏
tahtthgiehehtgnitalugerylluferaceromrofecnereferasasevressmlapehtfo
noitisopdex􏰏ehT􏰇ecafrusehtnodetnalperasmlapehtnehwgnitao􏰐spitregn􏰏
ehtpeekotdedeensrosnetxetsirwehtybnoitrexeehtsecuderepolsdrawnwodehT
apeht
􏰅􏰠􏰈􏰆􏰌ybdrawnwodepolsotpalehtnodecalpsiSTMehthcihwnierutsopdnah
􏰑
etaidemretninahtiwelbaniatboebotmeessdeepsgnipyttsebehT􏰇sdeepsgnipyt
stimilyletamitluecafrusdrahahtiwsnoitomgnippatfonoitcaretniwohtuobaedam
ebnacskramer wefa􏰅draobyeklacinahcemanognipytsaetaruccasaSTMehtno
gnipyt hcuotekamotyrasseceneblliwsmhtiroglanoitingocerecneuqesgnipytdna
snoitacolyekfokcabdeefelitcatsahcusstnemecnahnelanoitiddahguohT
sdeepSgnipyTlaitnetoPsevititeperhtiwelpoeprofylniamdednetnisitisuht
􏰥􏰝mpwgnitsera	morf
gnipythcuS􏰇nwodhcuotregn􏰏ehtmorfdetarenegsiyekonhtotgnidroccaderusaemsiel􏰏orpytimixorpehtfo
􏰉􏰌􏰉
􏰇yltnednepednidesseccaebotresuegareva
ehtmorfytiretxeddnagnitsiwtelbanosaernueriuqerdluowdnasegnardetimilevah
modeerffoseergedesehtfoynam􏰅revewoH􏰇noitatneirobmuhtfomodeerffoeerg
􏰆edenodna􏰅ecafrusehtotytimixorproerusserppitregn􏰏laudividninimodeerffo
seergedev􏰏􏰅ecafrusehtotlaretalnoitomnimodeerffoseergednetdleiyecafrusa
nostcatnocregn􏰏fostnemerusaeM􏰇yltnednepednistniojehtfollaevomottluc􏰏
􏰆fidtiekamsnoitatimilegaknilnodnet􏰅rettamlacitcarpasatub􏰅denibmocstnioj
regn􏰏llanitnemevomfomodeerffoseerged􏰊􏰉sahdnahhcae􏰅yllacinhceT
noitcartxEnoitoMdnaH􏰊􏰇􏰌
􏰇sregn􏰏ero mro owtfosedils
dezinorhcnysybdetaitini ebylnonacsnoitalupinamcidrohcecnisdetarelotylisae
erasnoigeryekpatyehtsasregn􏰏elgnisfosnoitomgnicnalG􏰇sregn􏰏gnitserrehto
tfilotgnivahtuohtiwslobmysyeketarenegotecafrusehtnodesserpylevislupmi
rodeppatdnadetfilebnacsregn􏰏􏰅gnitserecnO􏰇owtrodnocesarofsregn􏰏rehto
htiwgnolaerehtniamer dnayltnegecafruseht nodecalperarotesbusesaeler
rosserpregn􏰏dezinorhcnys afosrebmemerayehtsagnol sasyekonetareneg
ecafrusehtnotserotdednetnisregniF􏰇resuehtotkcabdeeftpmorpgnirusne
􏰅dekaepsaheslupmi ehtrodesaelersiregn􏰏ehtsanoossaslobmysyeketareneg
ecafrusehtnosesserpevislupmirospatpsirC􏰇draobyeklacinahcemdradnatsa
foseitilibapacgnitserdnahdnagnipytehthtobetalumeyllacimonogreotecafrus
hcuot􏰆itlumehtswollasuhtevobadebircsedssecorpnoitcetedgnipytehT
yrammuSgnipyT􏰋􏰇􏰉􏰇􏰌
􏰇gnirrajpitregn􏰏lanoisacco
nitluseryamsihT􏰇tcapmiregn􏰏fossenevislupmiehtninoitairaveromesuacosla
thgiehpitregnnidnepedstnenopmoc
yticolevelacsylraenilnonoslalliwti􏰅yranoitatsniamersregn􏰏emosnehwneveniag
noitommrofinuedivorpoTtcart
􏰆xeehtotnignideelbmorfsnoitomgnilacsdnanoitatorniseitimrofinu􏰆nontneverp
oT􏰇snoitatordnasgnilacshtiwylsuoenatlumisdemrofreprohtiwlargetniebot
noitalsnartroftlucarttenaotpuddayehtyllausu􏰅􏰍􏰇􏰌erugiFnineeseb
lliwsA􏰇yltcefreplecnactonodsgnilacsdnasnoitatordnahgnirudsregn􏰏ehtllafo
snoitomlanoitalsnartehttahtsikcabwardylnoriehT􏰇gnizisdnanoitatortcejbofo
sksatnoitalupinamlacihpargehtotylevitingocdnopserrocyeht􏰅bmuhtelbasoppo
ehtedulcniylevitiutnihcihwsnoitomezilituyehtesuacebmrofrepotysaesnoitom
dnahesehteraylnotoN􏰇tunagniwercsnunehwsasregn􏰏ehtneewtebrodilraj
agniwercsnunehwsatsirwehttuobarehtienoitatordnahdna􏰅sregn􏰏ehtgnidnet
􏰆xerognixe􏰐ylmrofinuybgnilacsdnah􏰅noitalsnartdnahelohwebnacsnoitom
regn􏰏dnadnahcisabeht􏰅dedeeneramodeerffoseergedruofylnonehW
􏰇erehdetcartxe
eblliwyticolevnimodeerffoseerged􏰋ylno􏰅ecafrusehtssorcaerusserptlitdnah
nisecnere􏰎id morfdeniatboebylbisualpdluocesehthguohT􏰇sexalacitrevdna
latnozirohehttuobadedeeneramodeerffoseergedlanoitatoreromowt􏰅snoisnemid
eerhtninoitalupinamFOD􏰆􏰍llufroF􏰇enalpecafrusehtnihtiwgniziserrognimooz
dna􏰅enalpecafrusehtnihtiwnoitator􏰅noitalsnartlacitrev􏰅noitalsnartlatnoziroh
eramodeerffoseergedcisabruofeht􏰅snoisnemidowtnI􏰇scihparglanoisnemid􏰆owt
fonoitalupinamrofmodeerffoseergedhguonemodeerffoseergedelbavresbo􏰍􏰈eht
morfnaelgotsimhtiroglanoitcartxetnenopmocnoitomehtfoesoprupehT
􏰋􏰌􏰉
eroferehT􏰇sregn􏰏lartnecehtfognixe􏰐ehttnemelpmocotgniliaf􏰅yranoitatsniam
􏰎idehttub􏰅noitalsnart
tenorezrofslecnacsnoitomriehtfomusehttahtos􏰅deepsemasehtylhguorta
snoitceridetisoppoylraennilevart􏰝􏰌F􏰜yknipdna􏰝􏰈F􏰜 bmuhtehT􏰇gnilacsdnah
evitcartnocagnirudregn􏰏hcaefoseirotcejartswohs􏰍􏰇􏰌erugiF􏰅noitatordnagni
􏰆lacsgnirudruccohcihwsnoitomregn􏰏decnalabnuehtetartsullirehtruf oT
􏰇orezottesylpmiseraseiticolevgnilacs
dnanoitatoreht􏰅ecafrusehtgnihcuoteradnahehtmorfsregn􏰏owtnahtsselfI
􏰇seiticolevnoitatorrognilacsdnahetupmocotyrassecenerasregn􏰏gnitcatnocowt
tsaelta􏰅noitatneirotcatnocbmuhtnisegnahcmorfdetcartxesiyticolevlanoitatora
sselnU􏰇derusaemsinoitalsnarterofebsnoitomlanoitatordnagnilacsmorfderusaem
ebtsumstnenopmocyticolevralopeht􏰅sdeepstnenopmocralopehtnodnepedlliw
egarevayticolevnoitalsnartehtnisregn􏰏ralucitrapfosgnithgiewehtecniS
noitcartxEtnenopmoCnoitatoRdnagnilacShtnoitcnufoteunitnoc
lliwtI􏰇shtapregn􏰏ehtfogniredroreporpnopuylnosdnepedyllaermhtiroglanoit
􏰆cartxeeht􏰅revewoH􏰇bmuhtelbasoppoehtmorfsishtapesehtfoenofimrofrepot
rotarepoeht rof	reisaeerasnoitator	dnasgnilacs􏰅rehtonaenofoeraspitregn􏰏eht
nahtsregn􏰏rehtoehtfotnednepednieromhcumsinoitombmuhtecniS􏰇dezisahp
􏰆meedebnacsnoitaluclactnenopmocnoitomralucitrapedargeddluowhcihwsleeh
mlaprosregn􏰏niatrecfonoitomtahtosdedeenerasnoitac􏰏itnediehT􏰇snoitaluc
􏰆lacnoitomehtni desuebotseiticolevlaretal dnaseitimixorpehtniatnocshtap
esehT􏰇dnahnevigehtrofshtaptcatnocde􏰏itnediehttupnisasekatmhtirogla
ehT􏰇􏰌􏰇􏰌erugiFninwohseramhtiroglanoitcartxenoitomehtfospetsehT
mhtiroglAnoitcartxEehtotstupnI􏰈􏰇􏰊􏰇􏰌
START
AT LEAST 2 FINGERS DOWN ?
Y
N
GET HAND'S CURRENT PATH PARAMETERS &amp; ID'S
SET ROTATION AND SCALING VELOCITIES TO ZERO
INIT TRANSLATION WEIGHTINGS TO FINGER PROXIMITIES
DECREASE TRANSLATION WEIGHTING OF RELATIVELY SLOW FINGERS
DECREASE TRANSLATION WEIGHTING OF CENTRAL FINGERS AS POLAR COMPONENT SPEEDS INCREASE
COMPUTE TRANSLATION VELOCITY AS WEIGHTED AVERAGE OF FINGER VELOCITIES
DEAD-ZONE FILTER ALL COMPONENTS BY FRACTION OF FASTEST COMPONENT
GET CURRENT AND PREVIOUS POSITIONS OF INNERMOST AND OUTERMOST TOUCHING FINGERS
COMPUTE SCALING VELOCITY FROM CHANGE IN SEPARATION BETWEEN INNERMOST AND OUTERMOST
COMPUTE ROTATIONAL VELOCITY FROM SEPARATION AND CHANGE IN ANGLE BETWEEN INNERMOST AND OUTERMOST
COMBINE WITH ROTATION AND SCALING ABOUT A FIXED POINT BETWEEN THUMB AND OTHER FINGERS
END
􏰌􏰌􏰉
􏰇seiticolevregn􏰏laudividnimorfseiticolevnoitalsnart
dna􏰅noitator􏰅gnilacsdnahgnitcartxerofmhtiroglaehtfotrahcwolF􏰤􏰌􏰇􏰌erugiF
15
10	F2
5
0
−5
F3
F3
F4
F5
F2
F7 F7
F4
F5
F1
F1
F6 F6
0 2 4 6 8 10 12 14 16 18 20
Horizontal Position on Surface (X axis cm)
Vertical Position on Surface (Y axis cm)
􏰍􏰌􏰉
􏰇noitalsnart
drawnwodtenanitluserdnadelecnacnuogsregn􏰏􏰝gnirdna􏰅elddi m
􏰅xedni􏰜lartnecehtfosnoitomeht􏰅yranoitatserasmlapehtecnistub
􏰅lecnacsnoito myknipdnabmuhtehtsnoitalsnartsadeddafitahtetoN
􏰇gnilacsdnahagnimrofrepnehwseirotcejartregn􏰏gnixe􏰐lacipyT􏰤􏰍􏰇􏰌erugiF
􏰡􏰌􏰉
en􏰏ekamottnawyamsrotarepotahtsisregn􏰏tsomretuodnatsomrenniroyknip
dnabmuhtehtotdetcirtsereraevobasnoitatupmocehtnosaerrehtonA
􏰇noitalsnart
htiwlargetnimodeerffoseergedgnilacsdnanoitatorehtgnikamsuht􏰅elohwasa
gnitalsnartoslasidnahehtfinevenoitomdnahfostnenopmocgnilacsdnanoitator
ynaerutpacsnoitauqeesehT􏰇stnenopmocgnilacsdnanoitalsnartehtsastinuemas
ehtottitrevnocotnoitarapestnerrucehtybdeilpitlumsielgnaniegnahcehT
􏰝 􏰌􏰇 􏰌􏰜
􏰳
􏰝􏰪n􏰨 OF􏰥􏰪n􏰨IF􏰜d
􏰕􏰸
􏰰
t􏰋
􏰝􏰪 􏰈
􏰯n􏰨	OF􏰥􏰪 􏰈
􏰯n􏰨 I F􏰜
􏰯􏰝􏰪 n􏰨	OF􏰥􏰪 n􏰨 I F􏰜
􏰍􏰍
rv
􏰦􏰪n􏰨 H
􏰤regn􏰏tsomretuodna
rv
􏰕􏰸
tsomrenniehtneewtebelgnaniegnahcehtmorfdetupmocsi Hyticolevlanoitator
dnaheht􏰅ylralimiS􏰇orezebotdemussasinoitarapesniegnahceht􏰅egamiytimixorp
suoiverpehtgnirudgnihcuottonsawsregn􏰏tsomretuorotsomrenniehtfoenofI
􏰇OFdnaIFsregn􏰏eht neewtebecnatsidnaedilcuEehtsi􏰝􏰪n􏰨OF􏰥􏰪n􏰨IF􏰜derehw
t􏰋
􏰝􏰪􏰈 􏰯n􏰨 OF􏰥􏰪􏰈 􏰯n􏰨IF􏰜d 􏰯􏰝􏰪n􏰨 OF􏰥􏰪n􏰨IF􏰜d
sv
􏰝 􏰋􏰇 􏰌􏰜
􏰦 􏰪 n􏰨
H
􏰤OFregn􏰏tsomretuodnaIFregn􏰏tsomrenni
sv
ehtneewtebecnatsidniegnahcehtmorfdetupmocsi HyticolevgnilacsdnahehT
􏰇stnemerusaemnoitatordnagnilacsdnahlaitiniehtrofdesuerasregn􏰏gnihcuot
tsomretuodnatsomrenniehtfosnoitisopsuoiverpdnatnerrucehtylno􏰅yknipdna
bmuhtneewtebtsetaergyllausueranoitomregn􏰏nisecnere􏰎idehtecniS
􏰇sregn􏰏fonoitanibmocynamorfsnoitomfomusehtniraeppalliwnoitalsnart
latnozirohtena􏰅smlaperofehttaderetnecnahtrehtardex􏰏mraerofhtiwtsirweht
taderetnecsinoitatorehtfitahttpecxe􏰅snoitatordnahgnirudruccoanemonehp
ralimiS􏰇noitalsnartlacitrevtenaotpusddagnilacsdnahagnirudseiticolevregn􏰏
lartnecfo musehtdna􏰅snoitombmuhtdnayknipehtneewtebecnere􏰎idehtnaht
sselyllaususiregn􏰏rehtoynadnaregn􏰏lartnecafonoitomneewtebecnere􏰎ideht
􏰢􏰌􏰉
ehtstcepxerotarepoeht􏰅revewoH􏰇regn􏰏hcaefoseiticolevlaretalehtegareva
ylpmisotebdluowseiticolevnoitalsnartdnahetupmocotyawtselpmisehT
noitcartxEtnenopmoCnoitalsnarT􏰊􏰇􏰊􏰇􏰌
􏰇orezebotsgnilacsrosnoitatortniopdex􏰏ehtsesuacnoitom
lanoitalsnartesacniegarevananahtrehtarnoitarepomumixamaaiv􏰌􏰇􏰌􏰬􏰋􏰇􏰌snoit
􏰆auqEfostlusereht htiwdenibmocera􏰝􏰪n􏰨
ri d
ri d
crv csv H􏰅􏰪n􏰨
H􏰜seiticolevgnitluserehT
􏰝􏰡􏰇􏰌􏰜
􏰝􏰝􏰪n􏰨cF􏰥􏰪n􏰨IF􏰜
􏰝􏰝􏰪n􏰨cF􏰥􏰪n􏰨IF􏰜
􏰯􏰪n􏰨
􏰯􏰪n􏰨
cF􏰜nis 􏰰
IF􏰜nis 􏰰
􏰍
􏰍
deepsdeepscrv
􏰪 n􏰨
c F 􏰰􏰪 n􏰨
ri d
I F	􏰯	􏰦 􏰪 n􏰨	H
q
􏰝􏰍􏰇􏰌􏰜􏰝􏰝􏰪n􏰨cF􏰥􏰪n􏰨IF􏰜􏰯􏰪n􏰨cF􏰜soc􏰰
􏰍
ri d
􏰝􏰝􏰪n􏰨cF􏰥􏰪n􏰨IF􏰜
􏰯􏰪n􏰨
IF􏰜soc 􏰰
􏰍
deepsdeepscsv
􏰪 n􏰨
c F 􏰰􏰪 n􏰨
I F	􏰯	􏰦 􏰪 n􏰨	H
q
􏰤seiticolevgnilacsdnanoitatoreht
tnemelppusottsomretuoehtnahtrehtogO􏰵c 􏰵I􏰤cFfsregncsronoitator agnirudregn􏰏tsomretuoehtevom
ylreporpotsliafrotarepoehtesac nI􏰇yratnemelpmoctonerasnoitomregn􏰏ehtro
gnirruccosinoitalsnartdnahelohwtnac􏰏ingisynafiorezhcaorppayehT􏰇sregn􏰏
ehtneewtebtniopdex􏰏atuobagnilacsdnanoitatorcirtemmysotevitisnesylno
erassecorpnoitac􏰏irevbmuhtehtnideilppa􏰠􏰌􏰇􏰋􏰬􏰍􏰋􏰇􏰋snoitauqE􏰅revewoH
􏰇ykniprobmuhtyranoitatsehtottcepserhtiwgnilacsronoitator
foecnaraeppaehtesuac dluowsnoitalsnartregn􏰏lartnecesuacebelbissopebton
dluowsiht􏰅􏰌􏰇􏰌􏰬􏰋􏰇􏰌snoitauqEhtiwdegarevaerewbmuhtehtdnasregn􏰏lartneceht
neewtebselgnarosecnatsidnisegnahcfI􏰇yranoitatsniameryknipdnabmuhteht
elihw􏰅gnirdna􏰅elddim􏰅xedni􏰅􏰇e􏰇i􏰅sregn􏰏lartnecehthtiwsnoitalupinamgnitalsnart
􏰣􏰌􏰉
ehtgnilacsnwodybylsselmaessihthsilpmocca wolebsalumrofehT􏰇yranoitatsera
yknipdnabmuhtehtelihwsnoitalupinamlanoitalsnarten􏰏gnikammorfsregn􏰏
lartnecehttneverptonseodtihguoht􏰅snoitatordnasgnilacsdnahgnirudsregn􏰏
lartnecehtrofsgnithgiewnoitalsnartehtsesaercedpetsgnithgiewlan􏰏ehT
􏰇yranoitats
tubecafrusehtnognitsersrehtoehtgnipeekelihwsregn􏰏owtroenoylnognivom
ybdesseccaebdluowdnahwningised􏰆dedia􏰆retupmocsahcussnoitacilpparofdeppiksebnacpets
sihttahtetoN􏰇ecnednepeddeepsehtfohtgnertsehtstsujdawtprewopehterehw
fomumixamehtybdedivid
deepsstiotnoitroporpnignithgiewnoitalsnartsgnivomtsetsafehtgnidn􏰏ybenodeb
nacsihT􏰇detulidtonsisregn􏰏gnivomyllanoitnetnifoniagyalpsidotlortnoceht
tahtosyranoitatsylevitalererahcihwsregn􏰏fosgnithgiewehtsesaercedpetstxen
ehT􏰇ysioneromebyamstnemerusaemyticolevdnanoitisopriehtecnisecneu􏰐ni
sselevahylthgilhcuotylnohcihwsregn􏰏􏰅ylralimiS􏰇seiticolevorezriehthtiwega
z
􏰆revaehtetulidtonodecafrusehtgnihcuottonsregn􏰏tahtserusnesihT􏰇􏰪n􏰨iF
wv	wv
􏰳􏰪n􏰨	iF􏰅􏰇e􏰇i􏰅yti mixorptcatnoclatotsti otdezilaitinitsrissaylluferacrotcartxetnenopmocnoitalsnartehteroferehT
􏰇snoitomregn􏰏lartnecdelecnacnuybdesuacsnoitalsnartten
suoirupsotevitisnessiegarevaelpmisa􏰅dnahehtgnitatorrognilacsylsuoenatlumis
sirotarepoehtfi􏰅eromrehtruF􏰇yranoitatsgnitsereraemosfineve􏰅devomgnieb
erasregn􏰏ynamwohfosseldragertnatsnocebotniagyalpsidotlortnocronoitom
􏰠􏰍􏰉
dlohserhtdeepsanahtsselseiticolevtupnirof yticolevtuptuoorezsecudorphcihw
retl􏰏enoz􏰆daedetarapesahguorhtdessapsitnenopmocyticolevhcaE􏰇stnenopmoc
yranoitatsylraendnastnenopmoctnanimodneewtebegakaelnoitomgniniamereht
evomerotyrassecensigniretl􏰏raenil􏰆noN􏰇rehtonanievomotgnitpmettayliram
􏰆irpelihwmodeerffoeergedenofostnemerusaemnisnoitabrutrepronimesuacllits
nacnoitalsnartro􏰅noitator􏰅gnilacsdnahgnirudnoitomregn􏰏nevenu􏰅rehtonaeno
fotnednepednierastnenopmocyticolevtnatluserehttahtyawahcusniseiticol
􏰆evnoitalsnartdna􏰅gnilacs􏰅noitatorehterusaemotnekateracehtetipseD
gniretliFenoZdaeDv ywv 􏰈􏰦i iF iF
􏰌
P
􏰈 􏰦i
x wv
iF
xv xwv
iF iF
􏰌
xv
P 􏰦􏰪n􏰨 H
􏰈 􏰦i
􏰌
P
y v	xv
iewnoitalsnartehthti W
wv
y wv
x wv
wv
y wv
x wv
􏰇i
􏰅sdeepstnenopmocralopehtybdegnahcnuerasregn􏰏tsomretuodnatsomrenni
ehtfosgnithgiewnoitalsnartehT􏰇􏰣􏰇􏰌noitauqEni mretgnilacsdnahafokcal
s v
x wv
eht ybdetacidnisa􏰅􏰪n􏰨
Hyticolevgnilacs dnahybdetce􏰎aebtondeen􏰪n􏰨
cF
gnithgiewnoitalsnartlatnoziroheht􏰅saibnoitalsnartlatnozirohhcumesuacton
seodgnilacsdnahecnistahtetoN􏰇tsomretuodnatsomrenniehtneewteberagO
􏰵c 􏰵I􏰤cfseititnediesohwsregn􏰏lartnecehtotylnodeilppaerasnoitauqeesehT
s v
r v
hs e r ht r al o p
joctnac􏰏ingisemoceb
seiticolevgnilacsdnanoitatorehtfosedutingamehtsasgnithgiewnoitalsnartlartnec
􏰈􏰍􏰉
wohetoN􏰇sregn􏰏ehtgnidnetxeelihwpusedilsnehtdnaesiwkcolc􏰆retnuocgnitator
dnagnidnapxesregn􏰏htiwelcricanisedils dnahthgireht􏰅􏰡􏰇􏰌erugiFnI
􏰇sretl􏰏enozdaedhtdiw􏰆elbairavehthguorhtdessapneebevah
yehtretfastnenopmocitarapesroelgnaniegnahcehtmorfylno
deviredsienildehsadeht􏰅gnilacsdnanoitatorroF􏰇stnenopmocnoitalsnartehtrof
yticolevnoitalsnartnisegarevadethgiewstneserpersriapllaneewtebnoitarapesroelgna
nisegnahcehtfoegarevaehtstneserperenildettodeht􏰅stnenopmocgnilacsdnanoit
􏰆atorehtroF􏰇stnenopmocnoitalsnartehtrofseiticolevnoitalsnartregn􏰏foegareva
elpmisastneserperenil􏰝neerg􏰜dettodeht􏰅tolphcaenI􏰇ecafrusehtssorcasedils
dnahartotnoitatordnahmorfsnoitisnartsuoirupserongi
otsiseretsyhretliwenozdaedehtfoseicnednepeD􏰇orezotdesserppusyleritneeblliw
tnenopmocyticolevnoitalsnarteht􏰅sdeepsnoitatorfo􏰌􏰟􏰈nahtsselerasdeeps
noitalsnarttahtosgnitatoryliramirpsirotarepoehtfituB􏰇tnecrepwef aybdeeps
noitalsnartehtelacsnwodoteblliwtce􏰎eylnoehtdna􏰅deepsnoitalsnartlautcaeht
otderapmocelbigilgenebnehtlliwhtdiwenozdaednoitalsnartsiht􏰅gnitalsnart
yliramirpsirotarepoehtfI􏰇retaergsirevehcihw􏰅sdeepsgnilacsronoitatorehtfo
􏰌􏰟􏰈tuobaottesebnacenozdaednoitalsnartehtfohtdiweht􏰅ecnatsniroF
􏰇sdeepstnenopmoctsapdna
tnerrucfonoitubirtsidehtotgnidroccaseiravenozdaedhcaefohtdiwrodlohserht
deepseht􏰅revewoH􏰇dlohserhtehtdeecxetahtseiticolevtupnirofdlohserhtehtdna
deepstupniehtneewtebecnere􏰎idehtotnoitroporpnisdeepstuptuosecudorptub
0.5
0
−0.5 0.5
0
−0.5 0.5
0
−0.5 0.5
0
−0.5
0
0
0
0
200	400	600
200	400	600
200	400	600
200	400	600
800	1000	1200
800	1000	1200
800	1000	1200
800	1000	1200
Time (ms)
1400	1600	1800	2000
1400	1600	1800	2000
1400	1600	1800	2000
1400	1600	1800	2000
Scaling Velocity (m/s)	Rotation Velocity (m/s)	Vert. Trans. Velocity (m/s)	Horiz. Trans. Velocity (m/s)
egarevadethgiew􏰆regn􏰏eradehsad􏰅snoitomregn􏰏fo
segarevamrofinuerasenildettoDelihwelcricaniseunitnocdnaheht􏰅sm􏰠􏰠􏰊􏰈
􏰬􏰠􏰠􏰡morF􏰇esiwkcolc􏰆retnuocgnitatordnagnidnapxesregnetcartxestnenopmocyticoleV􏰤􏰡􏰇􏰌erugiF
􏰊􏰍􏰉
􏰇snoitalsnartregn􏰏
foegarevamrofinuehtmorfdetupmocsitinehwnoitalsnartlatnozirohnisecnab
􏰆rutsidegralehtecitoN􏰇stnenopmocgnilacsehtotnirevossorclaitnatsbusgnisuac
􏰅esiwkcolcdetatoryllufsemocebdnahehtsaetarapesotdnetyknipdnabmuhtehT
􏰇emitehtflahtuobastnenopmocrehtoehthtiwecnerefretnisserppusotelbaylnosi
gniretliF􏰇emoselbuorteromsiesacnoitator dnahehT􏰇desusiseiticolevregn􏰏lla
foegarevamrofinuehtnehwecnerefretninoitalsnartlacitrevegralehtecitoNoitalsnartehtniecne
􏰆refretnielbaecitonesuacnoixe􏰐dnanoisnetxeregn􏰏ehthguohT􏰇esiwkcolckcab
dnaesiwkcolc􏰆retnuocsetatordnaniaganwodsehcuotdnaheht􏰅y􏰐eirbpudekcip
gniebretfA􏰇ylhtooms kcabxe􏰐dnadnetxetsr􏰏sregn􏰏eht􏰅􏰣􏰇􏰌erugiFnI
􏰇emit
ehtfotsomdeorezstnenopmocgnilacsdnanoitatorehtpeekotelbasigniretl􏰏
enoz􏰆daed􏰅sselehtreveN􏰇erutsopregn􏰏evitalernistfihsthgilsmorfstnenopmoc
gnilacsdnanoitatorehtotniegakaelthgilsllitssierehthguoht􏰅emasehtylraen
erastnenopmocnoitalsnartehtfosnoisrevllA􏰇dexalerniamerstsirwdnasregn􏰏
elihwwobleehtmorfelcricaniylhguorsedilsdnahelohweht􏰅􏰢􏰇􏰌erugiFnI
􏰇emitehtfotsomdeorez
sniamertnenopmocnoitatorderetl􏰏enoz􏰆daedeht􏰅sregn􏰏ehtgnidnetxednapugni
􏰆dilsylnonehwtahtetoN􏰇spitregn􏰏neewtebnoitarapesdnaelgnanisegnahckaew
ylevitalerehtybdetulidtonerayehtesuacebgniretl􏰏enoz􏰆daedretfanevesecne
􏰆re􏰎idegarevaeht nahtregnorts yllautcaerasecnere􏰎idyknip􏰆bmuhteht􏰅sgnilacs
dnasnoitatorroF􏰇rehtegotlamehtorezreventubtahwemossnoitalsnartdethgiew
􏰆regn􏰏ehtelacsnwodsretl􏰏enozdaedehT􏰇sgnilacsdnasnoitatorgnortsehtgnirud
sregn􏰏lartnecehtfosnoitomdecnalabnuehterongi yehtesuacebsnoitalsnartega
􏰆revaelpmisehtnahtrellamshcumerastnenopmocnoitalsnartdethgiew􏰆regn􏰏eht
0.5
0
−0.5 0.5
0
−0.5 0.5
0
−0.5 0.5
0
−0.5
0
0
0
0
200	400	600
200	400	600
200	400	600
200	400	600
800	1000	1200
800	1000	1200
800	1000	1200
800	1000	1200
Time (ms)
1400	1600	1800	2000
1400	1600	1800	2000
1400	1600	1800	2000
1400	1600	1800	2000
Scaling Velocity (m/s)	Rotation Velocity (m/s)	Vert. Trans. Velocity (m/s)	Horiz. Trans. Velocity (m/s)
􏰋􏰍􏰉
􏰇gniretl􏰏enoz􏰆daedretfasegarevadethgiew􏰆regn􏰏eradilos
dna􏰅segarevadethgiew􏰆regn􏰏eradehsad􏰅snoitomregn􏰏fosegareva
mrofinuerasenildettoD􏰇sedilsdnahehtsaerutsopriehtnirucco
stfihsevissapthgilsyldetbuodnutub􏰅ylevitcadetatortsirwehtron
dexe􏰐tonerasregn􏰏ehT􏰇sevawenisocdnaeniselbmeserotstnenop
􏰆moclacitrevdnalatnozirohehtgnisuac􏰅elcrichguoranisevomdnah
ehT􏰇noitalsnartdnah􏰆elohwmorfdetcartxestnenopmocyticoleV􏰤􏰢􏰇􏰌erugiF
0.5
0
−0.5 0.5
0
−0.5 0.5
0
−0.5 0.5
0
−0.5
0
0
0
0
500	1000
500	1000
500	1000
500	1000
1500	2000
1500	2000
1500	2000
1500	2000
Time (ms)
2500	3000
2500	3000
2500	3000
2500	3000
Scaling Velocity (m/s)	Rotation Velocity (m/s)	Vert. Trans. Velocity (m/s)	Horiz. Trans. Velocity (m/s)
􏰌􏰍􏰉
􏰇gniretl 􏰏enoz􏰆daedretfasegareva	dethgiew
􏰆regn􏰏eradilosdna􏰅segarevadethgiew􏰆regn􏰏eradehsad􏰅snoitom
regn􏰏fosegarevamrofinuerasenildettoD􏰇esiwkcolckcabneht
dnaesiwkcolc􏰆retnuocsetatordnahelohweht􏰅sm􏰠􏰠􏰠􏰊􏰬􏰠􏰠􏰠􏰈morF
􏰇ylhtoomskcabxe􏰐dnadnetxesregn􏰏eht􏰅s m􏰠􏰠􏰠􏰈otpU􏰇snoitomgni
􏰆lacsdnanoitatordnahetarapesmorfdetcartxestnenopmocyticoleV􏰤􏰣􏰇􏰌erugiF
􏰍􏰍􏰉
􏰇tce􏰎e
onevahowtroregn􏰏afos􏰎otfilrosnwodhcuotretalosedilsafogninnigebeht
tadex􏰏ebnoitceleslennahcehtrehtehwro􏰅segnahcecafrusehtgnihcuotsregn􏰏
fonoitanibmocehtfiedilsafoelddimehtniegnahcdluohslennahcdetceleseht
rehtehwsirezingocernoitomdrohcafongisedehtninoitseuqtnatropmimA
noitceleSlennahC􏰈􏰇􏰋􏰇􏰌
􏰇yltnednepednidnahhcaerof
detaepersissecorpnoitingocernoitomdrohcehttahtetoN􏰇rotcetednoitazinorhcnys
ehtmorfslangisnoitazinorhcnystesbusregn􏰏sevieceroslarezingocernoitomdrohc
ehTitnediehttupnisaseriuqertisuhT􏰇stnenopmocnoitomdetcartxe
ehtfosdeeps dnasnoitceridehtnognidnepedstnevenoitalupinamrodnammoc
etairporppatuosdnesti􏰅noitomnisidnahehtecno􏰅nehT􏰇edilsdnahafogninnigeb
ehttadetcelessahrotarepoehtnoitalupinamcidrohchcihwsregnI􏰇metsysnoitalupinam
cidrohcdnagnipytehtfoeludomlan􏰏ehtsirezingocernoitomdrohcehT
noitingoceRnoitoMdrohC􏰋􏰇􏰌
􏰇stnen
􏰆opmocnoitalsnartdetcartxeehtotnisnoitomgnilacsronoitatordnahtcefrepmi
foegakaelsserppusyletelpmocotyrasseceneblliwseicnednepedhtdiwenoz􏰆daed
fonoitazi mitporehtruf􏰅stnenopmocgnilacsdnanoitatorotnisnoitomlanoitalsnart
mrofinu􏰆nonmorfegakaeltneverpyllufsseccussretl􏰏enoz􏰆daeddetnemelpmiyltner
􏰆rucehthguohT􏰇STMehtnonoitalupinamFOD􏰆􏰋largetni wollayletamitlulliw
sihT􏰇stnenopmocnoitomdetcartxeehtfoecnednepedniehtsevorpmiyltaerggni
􏰆lacsrognitatorsi dnahehtelihwsnoitomyknipdnabmuhtehtgnirovaF
snoisulcnoCnoitcartxEnoitoM􏰍􏰇􏰊􏰇􏰌
􏰹􏱁􏱁 &amp;􏱁􏰹%􏱀􏱃􏰾 %􏰿# &amp;􏰾􏰿 $􏰼􏱁􏰼􏰺%􏰼􏰻 􏰺􏰿􏰹􏱃􏱃􏰼􏱁
State C
All Liftoff Immediately
Lateral Motion
Additional T ouchdowns/ Some Liftoffs
State M
Liftoff of All Fingers
􏰡􏰍􏰉
rebmunehtfitahtsimargaidetatssihtfocitsiretcarahcelbatontsomeht􏰅revewoH
􏰇stnevegniggardsetareneglennahcregn􏰏owteht􏰥noitomehtotnoitroporpnideta
􏰆renegerastnevegnitniop􏰅detcelesneebsahlennahcregn􏰏elgnisehtfI􏰇sruccoM
etatsnoitalupinamehtotnoitisnarta􏰅detcetedsinoitomlaretalnehw􏰅esiwekiL
􏰇etatsnoitceleslennahcehtnielihwdetnuocspitregn􏰏forebmunmumixamehtno
sdnepednottubesuomdetalumeehtfoytitnediehT􏰇deussisi kcilcnottubaerehw
􏰅Tetatspatehtotsrucconoitisnarta􏰅ylkciuq􏰎otfilsregn􏰏roregn􏰏ehtfI􏰇detcet
􏰆edsi􏰎otfillatotronoitomlaretallitnusnwodhcuotlanoitiddastnuocCetatS􏰇C
etatsnoitceleslennahcehtotFetatsgnitao􏰐ehtmorfsrucconoitisnarta􏰅nwod
sehcuotregn􏰏elgnisanehWgiF
􏰆hcuothcetigoLtnecernodesuenihcametatsnoitomdrohcehtebotsraeppasihT
􏰇􏰠􏰈􏰇􏰌erugiFfomargaidetatselpmisehtybdetartsullisinoitulosremrofehT
snoitanibmoCregniFwolloFslennahCnahcroftahtsisnwodhcuot
lanoitiddagnitarelotrofnosaerrehtonA􏰇sedilsgnirudemitatasdnocesrofecafrus
ehtevobadednepsussregn􏰏niatrecgnidlohmorfeugitafehtotnosirapmocniselap
slennahchctiwsotsmgnomseriuqersyawlalennahctnereebdluohssrotarepo􏰅esiwekiL􏰇lennahc drohcadetcelessahnoitanibmocregn􏰏
dnanoitomlaitiniriehtecnonoitalupinamdnahelohwrofecafrusehtotsregn􏰏
dednepsusllaroynapordotmodeerfehtsrotareposevigSTMeht􏰅eroferehT
􏰇􏰪􏰡􏰈􏰈􏰨emordnyss􏰛niavreuQeDsahcusseirujniesurevootenorperarab
ecapsehtevobahgihsbmuhtriehtdnepsusohwstsipytsa􏰅yllacimonogredabeb
dluocsihT􏰇ecafrusehtevobahgihdrohcehtni dedulcnitonsregn􏰏esohtdnepsus
thgimsrotarepo􏰅stnediccahcusgniraeF􏰇resworbbewarofdnammoc􏰖kcaB􏰩eht
gniussiotgardlatnozirohamorfhctiwsdluocedilsregn􏰏􏰆eerhtagnirudbmuhteht
fonwodhcuotlatnediccana􏰅elpmaxeroF􏰇enorp􏰆tnediccadnagnisufnocyreveb
dluocytitnediregn􏰏gnihcuotniegnahcyrevenopuslennahcgnihctiwS􏰇gniggard
dnagnitnioptsuj	nahtsnoitalupinamdnasdnammocfo yteiravrediwhcumaot
deppamerahcihw􏰝􏰈􏰇􏰌elbaTees􏰜slennahcdrohceromwef aetiuqsre􏰎oSTMeht
􏰅tsriF􏰇snosaerforebmunarofSTMehtnonoitalupinamcidrohcrofnesohcton
sawti􏰅noitacilppaniatrecrofsuoegatnavdaebyamngisedsihtdna􏰅slennahceerht
ylnosahhcihwdaphcuotllamsarofngisedetairporppanasisihtelihW
lennahCsteSnoitanibmoCregniFlaitinI􏰉crotarepoehtsuhT􏰇noitceleslennahcehtfoetadpu
narofCetatsotsnruterlortnoc􏰅􏰎otfillatottuohtiwsegnahcsregn􏰏gnihcuotfo
􏰣􏰍􏰉
deepsnoitaitiniehttahtylekilsseltignikam􏰅esaelerronwodhcuotregn􏰏wenhcae
retfasm􏰠􏰌tuobarofdelacsnwoderasnoitomdnahdetcartxednaregn􏰏􏰅ecafrus
ehtdehcaerevahdrohcanisregn􏰏llaerofeblennahcehtfoni kcolerutamerp
tneverpoT􏰇wolebdessucsidebotecneuqesnoitazinorhcnyslaicepsehtteemyeht
sselnunoitalupinamgnirudnoitceleslennahcehttce􏰎atonlliws􏰎otfilrosnwodhcuot
regn􏰏lanoitiddA􏰇nidekcolsinoitceleslennahcehtupinamehtotnoitisnart
ehtgnireggirterofebedilstsumsregn􏰏ehtraf wohrotsaf wohenimretedhcihw
sdlohserhtdeepsedulcnisretemarapytivitisnesnoitomehTregn􏰏fonoitanibmocehtsesurotceleslennahceht􏰅􏰋􏰇􏰉􏰇􏰉􏰇􏰌noitceS
ninoitingocerpatdrohchtiwsA􏰇noitomdnahroregn􏰏laretalrofdnaecafruseht
gnihcuotsregn􏰏fonoitanibmocehtnisegnahcrofskcehcyllaunitnoc CetatS
noitceleSlennahC􏰤CetatS􏰈􏰇􏰉􏰇􏰋􏰇􏰌
􏰇margaidehtninwohstonsihcihw􏰅gnipytsadeterpretni
esruocfoeraspatregn􏰏elgniS􏰇􏰊􏰇􏰉􏰇􏰉􏰇􏰌noitceSnidebircsedylsuoiverpsasregn􏰏
eromroowtfonwodhcuotsuonorhcnysgniwollofesaelersuonorhcnysclennahcasuhT􏰇ecafrusehtgnihcuotebotdnaha morf
sregn􏰏owttsaeltaseriuqerCetatsotFetatsmorfnoitisnartehtidtsr􏰏ehT
􏰇􏰈􏰈􏰇􏰌erugiFnidemmargaidsingisedenihcametatstnareloteromshtsseccaotnoitaitiniedilsretfayltrohsnwodbmuhtehttesnacsrotarepo
gers
􏱂􏰹􏱃􏱀!&amp;􏱁􏰹%􏱀􏱃􏰾 %􏰿# &amp;􏰾􏰿 $􏰼􏱁􏰼􏰺%􏰼􏰻􏰺􏰿􏰹􏱃􏱃􏰼􏱁
Some Liftoffs
$􏰼􏱁􏰼􏰺%􏱀􏱃􏰾􏱃􏰼􏱅 􏰺􏰿􏰹􏱃􏱃􏰼􏱁􏰽# 􏱂 $􏱆􏱃􏰺􏰿# 􏱃􏱀􏱇􏰼􏰻 $&amp;ly
Hand Motion Additional
Synchronized Liftoff &amp; Touchdown of Finger Subset
Subset Touchdown
Synchronized Liftoff of Finger Subset
State C	Touchdowns Asynchronous/
State M
State SC
State ST
All Liftoff Delayed
Additional T ouchdowns
Liftoff of All Fingers
Liftoff of All Fingers
􏰇ecafrusehtnognitsererasregn􏰏tsomrollanehw
sregn􏰏fotesbusagnipporddnagnitfilylsuonorhcnysybdetarenegebotspatdrohcdnadetceleseb
otlennahcdrohcwenawollaTSdnaCSsetatsdna􏰅sedilsdnahotesnopsernistnevenoitalupinam
dnasdnammocseussi MetatS􏰇ecafrusehtevobagnitao􏰐Fetatsnistratsdnahehtnehwgnippat
drohcdnanoitceleslennahcwollaTdnaCsetatS􏰇rezingocernoitomdrohcSTMehtrofmargaidetatS􏰤􏰈􏰈􏰇􏰌erugiF
􏰠􏰡􏰉
􏰈􏰡􏰉
ybTSetatshguorhtlennahcwenehtnodeussiebnacpatdrohcaCSetatsmorF
tnisregn􏰏
fonoitanibmoceht	morfdenimretedsilennahc weneht􏰅CSetatsniecnO
􏰇rettulcdiovaot
margaidehtninwohstonsinoitisnartsihttub􏰅Mhguorhtgniogtuohtiwyltcerid
Cetats	morf􏰅􏰇e􏰇i􏰅gnidilstuohtiwecafruseht nognitser neebevahsregn􏰏llaretfa
deretneeboslanactI􏰇ecafrusehtotkcabmehtgnippordylsuonorhcnysdna􏰅ev􏰏fo
tuoeerhtroowttsujyllausu􏰅sregn􏰏ehtfoemosgnitfilylsuonorhcnysyb Medom
noitalupinamlennahcgnitsixenamorfderetneebnacCSetatsnoitceleslennahc
dezinorhcnysehT􏰇slennahcegnahcdnaenihcametatsehtteserot􏰎otitfilot
decrofgniebretfadednepsusdnahehtpeekotdnetsyawlayamsrotarepoesiwrehto
esuacebtnatropmisisihT􏰇ecafruseht􏰎osregn􏰏llagnitfiltuohtiwecafrusehtno
gnitsersidnahehtnehwslennahcegnahcotyawaedivorpTSdnaCSsetatS
noitceleSlennahCtesbuSdecnySla
pukcipotrotarepoehtgnicroftuohtiwetaitininehtlliwedomnoitalupinamehT
􏰇drohcehtrofderisedsregn􏰏ehtfotserehtgnidilsdnagnippordyltneuqesbusyb
sihttcerrocylsselmaesotmehtswollasnwodhcuotsuonorhcnysafoecnareloT􏰇yras
􏰆secenerasregn􏰏elpitlumtahtgnittegrof􏰅ecafrusehtnoregn􏰏enoylnognidilsdna
gnicalpybedilsatratsotyrtylsuoenorreyamsrotarepoecivonerehocesuacebyrassecentonsieno􏰅llafotsriF􏰇MetatsotCetatsmorfnoitis
􏰆nartehtroftnemeriuqernoitazinorhcnysnwodhcuotonsierehttahtetoN
􏰇tnerehoc
yllaitinisinoitomdrohcerusneotsdeepsregn􏰏gnirobhgienfonoitcarfanihtiweb
dnadlohserhtdeepsehtssapotderiuqererasregnwdlohserht
􏰉􏰡􏰉
metsysehtswollasihT􏰇yltnednepedniderug􏰏nocebnacdrohchcaerofstnevetupni
ehtdna􏰅yltnednepedninoitcnufdnahhcaerofsrezingocernoitomdrohcehT
sgnippaMdrohC􏰊􏰇􏰋􏰇􏰌
􏰇stnemelpmi yltnerrucrezingocernoitomdrohcehthcihw
serutsegnoitomlanoitceridehtswohs􏰉􏰇􏰌elbaT􏰇tucdnocesamrofrepotedilswen
anigebdnadnahehtpukciptsumrotarepoehtsuhT􏰇edilsrepecnodeussiylnoera
sdnammochcusgnirusne􏰅dnammocehtfoecnaussitsr􏰏ehtretfagnitargetnisesaec
rezingocernoitomeht􏰅detaeperebotdeenmodleshcihwdnaelbisreverylisaeton
erahcihwtucsahcussdnammoctohsniehtsteser􏰅dehcaersidlohserhtanehwyek
etairporppaehttuosdnesnehtrezingocernoitomehT􏰇noitceridralucitrapani
noitomdnahsetargetniti􏰅syek worraehtsahcuselbisrevererasnoitcaesohwsyek
gnitideroF􏰇dluowdaphcuotroesuomaekiltsujretupmoctsohehtotstekcap
noitomesuomsdnes dnaslavretniemitraluger􏰅llamsrevostnenopmocyticolev
detcartxeehtsetargetniylpmistipinamfoepyt
ehtnognidnepedsyawtnere􏰎idlarevesnisetarepoedomnoitalupinamehT
noitalupinaM􏰤MetatS􏰊􏰇􏰉􏰇􏰋􏰇􏰌
􏰇sdrohcregn􏰏nommoctsom
ehtesoohcotyrassecensregn􏰏eerhtroowtehtsserpdnatfilylnodnaecafruseht
nognitsersdnahehtfothgiewehtfotsomevaeldaetsninactub􏰅noitalupinama
gnitratserofebecafrusehtmorfdnahgnitserelohwatfilotevahtonseodrotarepo
ehttahtegatnavdaehtsre􏰎oCSetats􏰅niagA􏰇erofebsanoitceleslennahcweneht
niskcol oslanoitisnartsihT􏰇sregn􏰏gnitserehtfonoitomehtfoevitcepserri􏰅etats
noitalupinamehtotkcabnoitisnartaesuaclliwtesbussihtnisregn􏰏ehtfonoitom
laretaltnerehoc􏰅esiwekiL􏰇tesbusregn􏰏emasehtgnipporddnagnisiarniagaecno
 ni l a c s	d n a h	e vi s n a p x E
hgir
elbisreverri􏰅noitalsnart
nwodropuelbisreveR
􏰇thgirro
tfelnoitalsnartelbisreveR
􏰇nwodro
punoitalsnartelbisreveR
􏰇noitcerid
ynani􏰝edils􏰜noitalsnarT
􏰇􏰝tohs
􏰆eno􏰜ecafrusnopatfeirB
noitoMdrohCfoepyTnocInoitoM
􏰇STMeht
ybdezingocerserutsegnoitomlaretaldnasnoitalupinamelpmisehTidetxetrofrohtuaehtybdesusgnippamehtrofanadna􏰅pat
drohca􏰅gnilacs dnahevitcartnocrohcnipaotdetacollaylevitiutni ebnacetsap
dnaypoc􏰅tucsdnammocgnitidenommoceht􏰅elpmaxeroF􏰇serutsegdnammoc
tohs􏰆enorof devreserebnacbmuhtehtedulcniyllaitini hcihwsdrohcregniF
􏰇gnilacsronoitatorgnitpmetta
nehwecafrusehtot bmuhteht ddaylnodeenrotarepoehT􏰇drohcdetceleseht
gnignahctuohtiwnoitomdrohcgnitaitiniretfaemitynanwodhcuotnacbmuhteht
ecnis	bmuhtehtedulcni	ot evahtondluowsdrohcesehT􏰇tcejbodnuorgerof	aetator
dnaeziser􏰅etalsnartdluocdnahthgirehtnidrohcgnidnopserrocaelihwdnuorgkcab
yalpsidehtetatordnaupmoctsohroF
􏰇txetfonoitcelesrof
snoitanibmocworratfihsetarenegdluowspitregn􏰏eerhtdna􏰅noitomfonoitcerid
ehtotgnidnopserrocsdnammocyek worraetarenegdluowtnemevomriappitregn􏰏
tfel􏰅ecnatsniroF􏰇dnahtfelehtnosdrohcotdetacollaerasnoitalupinamrosruc
txetgnidnopserrocelihw􏰅ecafrusehtfoflahthgirdnadnahthgirehtotevoba
dessucsidsadetacollaebdluocsnoitalupinamrosrucesuom􏰅ylevitanretlA
􏰇dnahrehtienosregn􏰏ruoffosedilsot
detacollaebdluocgnillorcs wodni W􏰇ecafrusehtnospitregn􏰏eerhtfopatelgnisa
ybdetarenegyllacimonogreebdluocskcilc􏰆elbuoddna􏰅ecafrusehtfoflahrehtieno
riappitregn􏰏afopat aybdetarenegebdluowskcilcesuomyramirP􏰇ecafruseht
foflahrehtienogarddnatniopdluocdnahrehtieyawsihT􏰇sdnahhtobnodrohc
pitregn􏰏eerhtaotgardnottubesuomdnasdnahhtobnodrohcriappitregnere􏰎idynamnisdnahneewtebsksatetacollaot
􏰌􏰡􏰉
􏰇retpahclan􏰏dnatxeneht nirohtuasihtfo
ydutsesacani	neeseblliwsa􏰅rotarepodelliks arofllewetiuqkrowydaerlayeht
􏰅ecitcarprogniniarttuohtiwtiesudnaSTMehtotpuklawnacenoynatahthguone
foorptellubteytoneraretpahcsihtfosmetsysehtelihWoitceleslennahcAorpehtotdel hcihw􏰅noitalupinamlacihpargecnahneottubgnipytroftonereh
desuneebevahsdrohcdnahlluFbylbailerylriaftuoyal yekehtrevonoitalupinamcidrohc
morfdehsiugnitsidebnacgnipyttahtgnieblatnemadnuftsomeht􏰅noitcaretni
retupmoc􏰆namuhrofstpecnoclevonlarevesdecudortnioslasahretpahcsihT
􏰇tsomretuoottsomrennimorfredroreporpniniamersnoitac􏰏itnedi
regn􏰏ehttahtseriuqerrotcartxenoitomdnahehT􏰇nwodhcuots􏰛regn􏰏tsr􏰏eht
fosm􏰠􏰠􏰈tuobanihtiwegrevnoc dnatsuboreboslatsumnoitac􏰏itnedi	dnahdna
regn􏰏􏰅spitregn􏰏forebmundnabmuhtehtfoecneserpehtnopusdnepednoitceles
lennahcnoitalupinamecniS􏰇rezingocernoitomdrohcdna􏰅rotcartxenoitomdnah
􏰅rotcetedgnipyt􏰅rotcetednoitazinorhcnysehtybderongiyllufebnacyehttaht
oshcussade􏰏itnediebtsumstcatnocmlaP􏰇elbailerebotnoitcetednoitazinorhc
􏰆nystesbusregn􏰏rofdesusemitesaelerdnasserpregnputliubebnacSTMehtnosnoitalupinamcidrohcelitas
􏰆revdnagnipytgnitargetnirofmetsysawohdetartsnomedsahretpahcsihT
snoisulcnoC􏰅yliratnemomSTMehtdeirtevahohwseugaellocdna
sdneirffosecneirepxeehtnopudesaB􏰇snoitalupopresufoseidutslamrof􏰅evisnetxe
eromyberutufehtnidenimaxeebdluohshcihwseussituognitniop􏰅doirepsiht
revoSTMehtfoesumorfsnoitavresbodnasnoisserpmiymreresivdayM􏰇stlusereraperpdnanoitatressidsihttideotretupmoc
lanosrepyramirpymnoecivedtupnielosehtsaSTMehtdesuevahI nehtecniS
􏰇􏰣􏰣􏰣􏰈yraunaJylraelitnuesuyliadtroppusothguonellewnoitcnuftondiderawtfos
STMeht􏰅􏰢􏰣􏰣􏰈rebmevoNni detelpmocsawSTMehtfonoitcurtsnochguohT
rohtuAehtfoydutSesaCdnalainomitseT􏰈􏰇􏰍
􏰇STM
ehtnonoitalupinamlaunamibronoitingocergnitirwdnahtroppusotyrasseceneb
dluowhcihwsmetsysgnitarepolaicremmocdnaSTMehtotstnemecnahneerutuf
fonoissucsidahtiwdneI􏰇erutufehtni STMehtfoscimonogrednaycneic􏰑eeht
etaulaveylevitcejbodnayllamroferomdluochcihwseidutsesacISRdna􏰅eugitaf
mret􏰆gnol􏰅ytilibasueniltuoI􏰅txeN􏰇noitatressidsihteraperpotSTMehtfoseitilib
􏰆apacnoitalupinamcidrohc dnagnipytehtgnisuelihwsmotpmysISRymfoyduts
esacadnasecneirepxeymfolainomitsetdeliatedahtiwsnigebretpahcsihT
SNOISULCNOCDNA
yhplareves
dnasrotcodowtdeksaIhguohTnirorotcuddabmuhtaerotyltnerappaI􏰅ylesnetnirellor
elbailernu􏰅gninoitcnuflamsihtgnisukeewaretfA􏰇draobyeksiseni Kaotdeddadah
Iecivedrellorbmuhttfelaninoitcnuflamax􏰏otdeliafIogasraeyflah􏰆a􏰆dna􏰆eerht
nehwsa􏰅yawdrahehtdenrael neebsahssenerawascimonogresihtfohcuM
􏰇skeewrofsdnahym
setaticapacnitahtnoitamma􏰐nignilaripsesuacnacyadrehtonarofretupmocehtfo
esudeunitnoctahtsnrawhcihwsmraerofehtniniapgninrub􏰅peedehtdna􏰅ecneuq
􏰆esnoctuohtiwderongiebyllausunachcihw􏰅sseneroselcsumlaic􏰏repusneewteb
ecnere􏰎idehtlletnacItahtosmlapdna bmuht􏰅sessen􏰎its	hctiwsyeksuoiravhtiwsdraobyekdradnats􏰅􏰠􏰊􏰈hw
htiwesaeehtos􏰅ssalcehtfotserehtnahtretsafhcumycaruccadnadeepsdeniag
I􏰅loohcshgihninamhserfasassalcgnipyttsr􏰏ymgniruD􏰇egarevaevobaylde
􏰆tbuodnusinoisicerpdnaytiretxedlaunamym􏰅sraeyevlewtrofecnamrofreponaip
lacissalcnisnosseldeviecerIesuaceBbahnoitomegnartsenotsaeltasahSTM
ehtdeirtsahohwnosrepwenyrevE􏰇smhtiroglaSTMehtesufnochcihwsnoitom
diovaylsuoicsnocnuI􏰅etargetniotdesoppuseranoitalupinamcidrohcdnagnipyt
eht wohfonoitonaevahIesuaceb􏰅elpmaxeroF􏰇eeseroftonnacIsyawnisemit
􏰆emos􏰅desaibtahwemosyldetbuodnu maI􏰅erawtfosSTMehtforepolevedsA
rotaulavEnasassentiFyM􏰈􏰇􏰈􏰇􏰍
􏰢􏰡􏰉
􏰇stneveesuomdetalumeaivswodniwfognillorcssuounitnocdelbane􏰪􏰋􏰨ted
􏰆uAlaumaSybllorcstoHdellacnoisnetxemetsysgnitarepona􏰅revewoH􏰇yrassecen
erewsrevirdecivedmotsucon􏰅erawdrahnislocotorpesuomlairesdnadraobyek
naotdetcennocsawSTMehT
sdohteMdnatnempiuqE􏰉􏰇􏰈􏰇􏰍
􏰇meht
rebmemerotesuapllitstsumItahtylerarosdemrofrepesohtmorfsnoitalupin
tneot
serutsegdemrofrep􏰆yltneuqerfrofhguonegnol STMehtdesuevahI􏰅elpmaxeroF
􏰇tonnacsecivonfonoitalupopahtiwlairttrohsatahtstce􏰎ednaesumret􏰆gnolno
noitamrofni edivorpnacenosihtsahcus ydutsesachtnom􏰆eerhta􏰅yllaniF
􏰇etanimileotdeliafdahemitdna􏰅yparehtlacisyhp􏰅secivedtupnievitanretla
rehtohcihwlairtehtotpugnidaelsmotpmysmret􏰆gnolotlairtSTMsihtgnirud
smotpmysymerapmocylraelcnacIsuhToirpshtnomehtgnirudecneH􏰇draobyekta􏰐anahteromyrujnisiht
etabrecaxeotdemeessbmuhtehtnosesopmitisyekgnitidednaerutsopdesiareht
irw
ehthguorhtsessapnodnetroxe􏰐regn􏰏xednitfelehterehwssenrednetdnaniap
gnisuactuohti wdraobyeklacinahce mynanoyadrepowtroegapanahteromepyt
otelbaneebtonevahI􏰅noitacavnoskeewrofeerf niapgniyatsretfanevE
􏰇erevessselemocebevahswoblehtobnisitilydnocipeeht
sahcusstopstohsitinodnetrehtoymnispu􏰆era􏰐saneve􏰅emoselbuortdeniamer
sahyrujni dlosihttub􏰅sesicrexegninehtgnertsdnagnihctertshguorhtytilibixehtretfashtnomweftsradeniatnochcihwedoc C
sahcustxetgniretneetcerrocdnatideotysaeossawtiesuaceB􏰇draobyek
lacinahcemanodluowIsasrorreynamsaeciwttuobagnikam􏰅STMehtnoetuni m
repsdrow􏰠􏰍ot pufodeepslamronymtaroirpdraobyeklacinahcemahtiwepytotelbaneebdahInaht
erom􏰅STMehthtiwsegap􏰍􏰆􏰊depytIsyadynam􏰅sselehtreveN􏰇snoitceswenegral
roftnatsissagnipytadnatxetfostnuomaetaredom􏰆ot􏰆llamsfoyrtnerofSTMeht
gnirreferp􏰅doirepsihtgniruderawtfosnoitingocer hceepsfoesudediovaIolsehtfoesuaceB􏰇epytotorpSTMehtfoytilibapac
noitalupinamcidrohcdnagnipytehthguorhtsnoitiddadezis􏰆egapdnagnitidella
demrofrepInehT􏰇emrof ni	mehtepytnosrepadahdnadnahybtxetehtfo
snoitcesgnolfotfardtsr􏰏ehttuoetorwI􏰅oslA􏰇dehcattallabkcartkart􏰆esuoMdna
draobyek􏰋nuSahtiwnoitatskrownuSanogninnurbaltaMhtiwdettolperew
serug􏰏ehtfoyna M􏰇atadlacihpargdnatxettupni	otsnaemrehtonodeilerIsemit
ta􏰅noitatskrowyramirpymrofecivedtupnielosehtsawSTMehthguohT
nuetingdEorci MybtidEkcilSlausiVtxetdnasecnerefer􏰅sedocgnittamrof􏰅erofereht􏰥rossecorpdrowlanoitnevnocahtiw
E
nahtrehtaregaugnalgnittesepytXTLehtninettirwsawnoitatressidsihT
A
frusrehguoraseilpmikcabdeef
elitcateromesuacebylesicerpyrevdenimretedebotsdeennoitseuqdnocesehT
􏰧STMehtnognipyt hcuotfoycaruccadnaotnoitpadadeeps
otyrassecensisnoitisopworemohdnayekehtfokcabdeefelitcat hcumwoH􏰇􏰉
􏰧nraelenoyrevenacdnanraelotmehtekattiseodgnolwoh􏰅􏰇e􏰇i
􏰅woremohrevoydaetssdnahriehtpeekotnraelotelpoeproftisidrahwoH􏰇􏰈
􏰤erutufehtnidenimaxeebot
deenseussi owt􏰅esacynanI􏰇draobyek YTREWQlanoitnevnocanosadetnalsnaht
rehtar􏰅siseni Keht noekil􏰅STMeht nolacitrevthgiartserasnmulocyekcitebahpla
ehttahtsieussignitaobmysehttagnikool ybkcepdnatnuhotelbaneeb
evahkcabdeefelitcatynatuohtiwSTMeht noepytotdeirtyliratnemomevahohw
elpoepehtllA􏰇larutantonyltnerappasititub􏰅sihtodotdenraelyllufsseccusevah
IdnasailEnhoJhtoB􏰇woremohrevoderevohsregn􏰏ehtelihwecafrusehtno
smlapymtserotelbatrofmocoslasawti drawnwodgnipolspal ymnoSTMeht
htiW􏰇gnitfirdtuohtiwworemohevobariaehtnidednepsusylluferacebotdah
yehtroelitcatonsawerehtos􏰅htoomsyltcefrepsawecafrusehttub􏰅rekram
ahtiwecafrusehtnoslobmysyekeht werdI􏰇lairtSTMehtfohtnomtsr􏰏eht
rofdesutuoyalyekYTREWQehtfonoitidnerasi􏰍egaPno􏰈􏰇􏰈erugiF
􏰇semittagniyonnaemoceb
didhcihw􏰅STMehtnodetnirpslobmysehttaecnalgaderiuqeryllausutuoyal yek
tpecxeyawatnewgnipytetaredom
seinapmoccayllausuhcihwsmraerofymnigninrubehtlla􏰅skeewelpuoctxeneht
revoSTMehtfoesudeunitnocIsA􏰇ecnarelotdnahgnitserdnaytilibailermetsys
nistnemevorpmi ymoteudsawtifiroSTMehtfostnemeriuqerlarutsoptnere􏰎id
ylthgilsehtotdetpadayllan􏰏dahydobymesuacebsawsihtrehtehwyastonnac
IyletanutrofnU􏰇yawatnewssen􏰎itsredluohsdnakcenymtniopsihttA
skeeWhtruoFdnadrihT􏰉􏰇􏰋􏰇􏰈􏰇􏰍
􏰇lennahcdetcelesehtgnilbasid
rognihctiwstuohtiwnoitalupinamfoelddimehtniecafrusehtotpordotsregnnu
rocitarretahwemosnoitalupinamretniopedamoslametsysnoitalupinamcidrohc
ehtnisguB􏰇snoitavitcayeksuoirupsgnisuactuohtiwecafrusehtnosdnahym
tserotemwollatondluowhcihwsguberawtfosSTMotsihtdetubirttaI􏰇niap
redluohsdnakcenetucadecneirepxeIowtrokeewtsr􏰏ehtgnirudtgnileefehtdahI􏰇siseniKehthtiwdahyltnecertiekiltxenehtotyadenomorf
pudliubtondidniaprepeedtubu
roirpmorfgnirevocerllitssawI􏰅ecivedehtgnisuowtrokeewtsr􏰏ehtgniruD
skeeWowTtsriFocebdetrats
tuoyalkarovDde􏰏idomehtnognipyt􏰅skeewowttuobanihtiw􏰅revewoH
skeeWhthgiEdnahtneveS􏰋􏰇􏰋􏰇􏰈􏰇􏰍
􏰇eugitafmraemosdnassen􏰎itsredluohsdnakcenymni
ecnegruseradesuacsihT􏰇tuoyalngierofehtdnuorayawymdn􏰏otnwodgnikooldna
ecafrusehtevobasdnahymgnidlohyltnatsnocflesymdnuofniagaecnoI􏰅syekrof
dnuoraleefothcihwhtiwsegdeyekondnatuoyalkarovDehtnosecneuqesnoitom
regn􏰏foteyyromemrotomongnivaH􏰇sihtdetabrecaxeylbaborpSTMeht morf
kcabdeefelitcatfokcalehTlrerarehtfoemostpekoslaI􏰅karovDot YTREWQ
morfnoitisnartehtnigninraelerdeepsoT􏰇syek􏰛i􏰛 dna􏰛u􏰛ehtdeppawsI􏰅gnihcterts
regn􏰏xednieziminimotdehsiwIecnistub􏰅slewovesehtforehtiegnivlovnishpargid
ezimitpootsihtdidkarovD􏰅ylbamuserP􏰇􏰛u􏰛yekemohxedniehtsahsilgnEninetfo
saeciwtsraeppaerIsyadelpuoctsr􏰏ehtretfA
􏰇rekramahtiwecafrusehtnoslobmysehtgniwarderdnael􏰏noitarug􏰏nocani
snoitisopdiortnecyekfognippawsderiuqerylnotuoyalSTMehtgnignahC􏰇syek
woremoh morfsi daoltahtfotsomdnaedniehtdezilituhcihwtuoyalyeka
nraelotebdluowyrujniregn􏰏xednigniggansihtfoecafehtniecnarudnegnipyt
ymevorpmiotyawylnoehtdedicedIlairtehtfokeewhtfe􏰎idesaercedotdeppawseraronmorfdeppawsera
􏰛u􏰛 dna􏰛i􏰛etoN􏰇lairtehtfokeewhtf􏰏ehtnignitratsrohtuaehtybdetpodatuoyal yekkarovDderiehttsertonnacsrotarepofiyliraropmet
esaercni yamsredluohsehtnognidaollarutsoptahttseggusyeht􏰅hcuocafotsermra
hgihehttsniagadepporpdaehymhti wseivomgnihctawsahcusseitivitcarehtoyb
nothguorbneebevahdluocniapredluohsdnakcenfostuopsfeirbymhguohT
􏰇pudliubotsnigebnoitamma􏰐ni
roeugitaferofebyadrepgnolsasemitna􏰅snepllabgnillortoliP􏰅snottubesuomlacitposahcussecivedecrof
noitavitca􏰆orezhtiwsecneirepxesuoiverpymhtiwtnetsisnocsisihT􏰇draobyek
lacinahcemanosaecnarudnehcumsasemiteerhtroowtevahI􏰅STMehtnognipyt
ecnarudneetinalesopmocoteerftlef
Itahtsraeyruofniemittsrvarggaosemacebrevenregnreveylraenyadrepsegap􏰍􏰆􏰊depytI
􏰅elbaliavanusawtnatsissagnipyt ymtuboevahstopskaewrehtoymroregn􏰏sihtnissenrednetroniapfosecnerrucer
dlimylnoym􏰅stuoyalyekgnihctiwsretfakeewtsr􏰏ehtecniStcalatnediccaynamdiovalli wnoitarepoSTMgninrael
tsr􏰏erasecivonnehwsnoitalupinamcidrohccisabtsomehttubllagnilbasiD
􏰇erutufehtnidedeeneblliweromyldetbuodnudna􏰅sihtsserdda
oterawtfos STMehtotedamneebydaerlaevahsnoitac􏰏idomlareveS􏰇rorrerieht
odnuot	woh wonktondnadesufnocemocebyamyeht􏰅dnammocdrohcroyeka
etavitcayllatnediccaodyehtnehwcnysylesicerpot	wonktonyamyehtcaehT
􏰇sesactsomnidecitoneblliwsnoitresnihcusserusnesnoitavitcayek
llamorfkcabdeefelbiduA􏰇decitonsirorreehtsagnolsatcerrocotysaeerayeht
suhT􏰇yeknoitcnufaybnoitacovnidnammocanahtrehtarretcarahccitebahplana
fonoitresni evlovnisnoitavitcalatnediccahcustsom􏰅noitisopemohraenyllausuera
sdnahehtdnaSTMehtfoelddi mehtnierasyekdnammocehtecniSht hcuotyllatnedicca
lliwI yadasemitfoelpuoca􏰅ylralimiS􏰇nettirwrevoebdluocstnetnocdraobpilc
ehthcihwni􏰅ecneuqesetsap􏰆tucafoelddimehtnisienosselnuecneuqesnocon
evahlliwrorreremrofeht􏰅ypocotdeppamsipatregn􏰏􏰆bmuhtehtecniS􏰇erutseg
noitatorasaerutseggnilacsdnaharopatregnrrorre
noitingocererutsegehtsaemasehttuobasi sehcuotlatnediccaoteudsrorrefo
rebmuneht􏰅emroF􏰇melborpaneebyllaertonsahsiht􏰅snoitavitcalatnedicca
ynaodnuot wohswonkohwrotarepodelliksasAnoctuohtiwemit ynataecafrus
eht nosregn􏰏ev􏰏lladnasmlapecalpylsuonorhcnysnacenohguohT􏰇ylekilerom
ecafrusehthti wtcatnoclatnediccaybnoitavitcalatnediccasekamoslati􏰅segatnavda
cimonogresuoivbosahSTMehtfoecrofnoitavitcaorezyllaitnesseehtelihW
snoitavitcAlatnediccAdnasrorrEnoitingoceRsenevislupmiehtgnirusaem
edulcnisnoitporehtO􏰇sesserpyeksadeterpretnieblliwecafrusehttsniagasehsurb
latnediccatahtdoohilekilehtrewollliwdlohserhtsihtgnisiaR􏰇detcetederaelcyc
efilpatehtfosdneehttaseitimixorpthgilsehtylnosemitemosefoytimixorpniskaepehtesuacebtub
poehtybstcapmipitregnoitavitca
yekrofdlohserhtytimixorpeht􏰅elpmaxeroF􏰇spf􏰠􏰌tnerrucehtfodaetsnispf
􏰠􏰠􏰈foredroeht nosetaremarfegami yti mixorperiuqerlliwgniretl􏰏hcus􏰅elbailer
eboT􏰇snoitavitcalatnediccatneverposlanacgniretlmsemocebnosrepaecnO􏰇spatdrohcregn􏰏ruofroeerht
nahtyllatnediccadetavitcaebotylekilsseldnalufesuylralucitrapsidnammoc
ypocehtrofpatregn􏰏􏰆bmuhtehT􏰇noitarepocisabrofdedeenyllaersignikcilc
esuomyramirpot deppampatriapregn􏰏dnahthgirehtylnO􏰇spat drohcsa
deppammeht gnivahtuohtiwylefaseromsdnammocesehtsseccanacsecivonos
dna􏰅thgirehtnokcilc􏰆elbuod
dnadnahtfelehtnoepacsEotdeppamesehtevahIvoN􏰇skcilcesuomrosdnammocsuoirupsgnitarenegtuohtiwsregn􏰏ev􏰏
nahtreweftsernacsecivonosspatdrohctsomelbasidottnatropmiyllaicepsesitI
rofrep􏰅slen
􏰆nahcnoitalupinamrosrucehtnotnednepedylsuoicsnocnuemacebIelihW
􏰇gnitibihnidnaevitimirpylddo
smees	wonecivedgnitniopenola􏰆dnats agnisU􏰇gniugirtnisawrosrucagnivom
saylisaesagnitidefoegatssuoiverpynaottnemucoda􏰖kcabllor􏰩otelbagnieb
􏰅yltneuqerfsadesutonerewoderdnaodnuetin􏰏nironoitelpmocenildnadrowsa
hcusserutaeftidekcilSlausiVfonoitalupinamrofslennahcehthguohTnahc
ehtdesuyltneuqerfI 􏰇detnargroftiekatotsagnillorcsrogniggard􏰅gnitniop
dnagnipytneewtebylsuoenatnatsni hctiwsotytilibaehtotdemotsuccaosemaceb
ylkciuqI􏰇denoisivnedahIsadiutksed
dnagnitiderofmetsysnoitalupinamcidrohcehtdesuylnoevahIhguohtnevE
ecnamrofrePnoitalupinaMcidrohChT􏰇􏰫e􏰫naybdewollofkcilcesuomyramirpasagn􏰏xednidnaelddimehtotdetacolla􏰛ht􏰛gniebelbatontsomehtneppahtonseodsiht􏰅tuoyalyekYTREWQehtni
traparafosderettacserasretcarahctneuqerftsomehtecniS􏰇patdrohcriapregn􏰏
asadeterpretniylsuoenorreebotmehtgnisuac􏰅suonorhcnysylraeneblliwspat
􏰅noisseccuskciuqnisyektnecajdayllatnozirohowtgnippatnehwsemitemoSrepoehtotstnemeriuqergnimitecnuobedgnitpadayltnegnirtsrospat
rdpitregn
htiwmednatnillewosnoitcnufyehtecnisecneirepxegnitideehtmrofsnartyllaer
hcihwserutsegetsapdna􏰅ypoc􏰅tucehtsiti􏰅revewoH􏰇citamotuaemocebotmeht
rofhguonenetfoelipmocedocdna􏰅drawrofdnakcabresworb􏰅esolcwodniw􏰅evas
el􏰏􏰅ecalperdnahcraesdrowrofserutsegdnammoctohs􏰆enoehtdesuevahI
􏰇sretl􏰏noitavitcalatnediccaronoitomehtotstnemevorpmirehtruf
eriuqeryletin􏰏edgnitarytefasecivonwol atubgnitarytissecenecivonhgihaevah
hcihwsnoitanibmoCiticimonogrewohton􏰅srorrenoitingocerronoitavitcalatnediccaotsi
noitomdnanoitanibmocregn􏰏ehtenorp wohnaemI ytefasyB􏰇secivonrofsitiefas
wohdna􏰅secivonybnoitalupinamIUGlarenegrofebthgimtiyrassecenrolufesu
woh􏰅tidemrofrepIyltneuqerf woh􏰅􏰇e􏰇i􏰅emrofsawtilufesuwohotgnidroccadetar
sinoitalupinamdnadnammochcaE􏰇noitalupinamrodnammocdetonehtetareneg
ot yrassecensnoitomdnanoitanibmocregn􏰏ehttsilselbatgnippamehT
􏰇snocinoitomdnalennahcdrohcehtrofsdnegelgniniatnochwdnahhcaerofsgnippamnoitomdnapatdrohcehtliatedhcaertuohtiwnoitisopdnahtnerrucehttademrofrepebsyaw
rohcfonoitingocerecnis􏰅revoero M􏰇yekagnisserpnahttrouBybdesuopseserutsegfo􏰇gnimohtuoyalyeK
S􏰟gniggarD
􏰊	􏰌	􏰇 kcilc􏰆elbuod
nottubesuomyramirP
ybrof
dedee N	l uf es U
nottubesuomyramirP
noitcAnoitoMlennahC
IUG drohCdnaH
thgi R
ef aS
dnapUegaP
noitoMlennahC
IUG drohCdnaH
tf e L
ef aS
􏰇slennahcnoitalupinamdnahtfelrofsgnippaMepO
S ypoC
􏰇􏰝draobpilcot􏰜tuC
noitcAnoitoMlennahC
IUG drohCdnaH
thgi R
ef aS
dedee N	l uf es U

elipmoc
􏰇􏰝edocCrofnoit
􏰆autcnupretniop eht􏰜	􏰶􏰆
􏰇􏰝rabune mssecca otaobpilcmorf􏰜etsaP
􏰇􏰝draobpilc ot􏰜 ypoC
􏰇􏰝draobpilcot􏰜tuC
noitcAnoitoMlennahC
IUG drohCdnaH
tf e L
ef aS
dedee N	l uf es U
iwollofehT􏰇rosrucesuomehtesuottnatculerregnolonmaI􏰅STMeht
nonoitalupinamrosructxetroesuomhtiwllewyllauqeetargetniserutseggnitide
tohsiKymnodellatsni
dahIsrellornoitalupinamrosructxetehtdnasyektohdraobyekforovafnirosruc
esuomehthtiwgnitidedennuhsIerofebsyawlAtnosdnammocgnitidednanoit
anuserutsegpool􏰆nepo􏰅suoenatnatsniera
snoitomrehtoehTnip􏰆itna
regn􏰏􏰆bmuhtadna􏰅titcelesotpatdrohcpitregn􏰏􏰆􏰊rehtonasmrofrep􏰅decalper
ebotdrowehtrevoesuomehtsevom􏰅pat drohcpitregn􏰏􏰆bmuhtahtiwnoitceles
ehtseipoc􏰅drowtnemecalperehttcelesdnakcilcylralu
􏰆citrapasihpargaraptnere􏰎idamorfdrowrehtonahtiwdrowagnicalpeR
􏰇rosructxetehtgnivomretfadraobpilcehtgnisoltuobayrrow
tondnadraobpilcehtottxetdetcelesehtypocrotucrehtieotesoohcnacenoecnis
elbixe􏰐eromhcumerayehttey
foecneinevnoceht hctamserutseggnitideSTMesehT􏰇rabloota􏰎odesseccaeb
otdahsnottubetsapdnatucfiekatdluowtiemitehtflahtuobaofrepebnacecneuqes
eritneeht􏰅xelpmocdnuosyamecneuqessihtelihWs pu􏰆wollof asulp􏰅syadevitucesnocruofrevodaerpssnoissesetuni m
􏰆yteninruoffotsisnocdluowlairtAwslairt ytilibasufolaogehT
slairTytilibasU􏰈􏰇􏰉􏰇􏰍
􏰇seidutseugitaf
gniebdrihtehtdnavelamroF
snoitaulavEerutuF􏰉􏰇􏰍
􏰇sregn􏰏yknipdnagnirehtrednugnilrucropugnidlohelihwsregn􏰏elddim
dnaxedniehtgnidilsnahtnoitanorpmraerofsseleriuqeryehtesuacebriapyknip
dnagnirehtgnisureferpnetfoI􏰇spitregn􏰏fotesralucitrapaesuotgnirebmemer
nahtreisaeroysaesasmeesspitregn􏰏forebmunralucitrapaesuot gnirebmemer
􏰥noisufnocevitingocynaesuacotmeestonseodnoitanretlahcuS􏰇erutsopdnah
ymyravotriappitregn􏰏yknipdnagnirehtdna􏰅riappitregn􏰏gnirdnaelddim
eht􏰅riapelddimdnaxedniehtneewtebetanretlanetfoI􏰇noobaetiuqebottuo
snrutoslagnitnioprofspitregn􏰏owtfonoitanibmocynaesuotmodeerfehT
􏰇rosrucesuomeht gnisudrawkwaerahcihw
noitalupinamrabllorcssahcussksatetaivboslennahcgnillorcspitregn􏰏􏰆􏰋eht􏰒
􏰇draobyekdnaesuomehtneewtebsnoitomgnimohsetanimileSTMeht 􏰒
􏰇snoitomdrawkwaesehtmrofreptondeenrotarepoehtossgardpat
dnaskcilc􏰆elbuodfonoitalumeotlennahcnoitalupinamasetacidedSTMeht 􏰒
􏰇sdaphcuot ynit
􏰌􏰣􏰉
limafpolevedotSTMehtgnisustnemngissassalc
dezidradnatsnokrowotdewollaebdluowstcejbusgnipyt sa hcus
snoitarepoSTMfolairotutevitcaretninanietapicitrapdluowstcejbusehtsetunim
􏰌􏰈txeneht roFaem
ecnamrofrepehT􏰇emitfotnuomaemasehtrofdetaulaveebdluowSTMehtno
ecnamrofrepdeniartnunehTtiwecnamrofrepdehtswohstnehwretsemes
ehtfoelddi mehtniecalpekatdluowlairtehtdna􏰅ssalctrarognitfard􏰅gnimmarg
􏰆orp􏰅noitisopmocemasehtfosrebmemebylbareferpdluowstnedutseht
ksatgnitidenesohcehtroferawtfoshtiwrailimafydaerlaerewohwstnedutsevlewt
tuobAusaemotretal
ppppp
pp
ppppp
p
ppppp
p
STM
dradnatS􏰤tset􏰆tsoP
ecitcar P
lairotuT
STM
dradnatS􏰤tseter P
􏰈􏰈yaD􏰋yaD􏰊yaD􏰉yaDcsroeriannoitseuqniap
dradnats aretupmoceht gnisutuoll􏰏ot deksaebosladluowyehT􏰇ydutsehttuo
􏰆hguorhtsmotpmysniapfogolyliadaniatniamdluowstcejbuS􏰇smraerofrosdnah
ehtotseirujniniartsevititeperereves􏰆ot􏰆etaredomfoyrotsihaevahohwflesym
ekilslaudividnifodesopmocebdluowseidutsesacISReht nistcejbusehT
seidutSesaCISRnoitarepoSTMfotesbusemosroSTMehtno
ycneic􏰑eretaergeveihcanacelpoepfienimreteddluowydutsahcus􏰅yllaniF􏰇nrael
otemoselbuorttsomehteraseitivitcaSTMhcihwetacidniosladluowsnoitaulave
ehtgnirudSTMehtnoytivitcadnahllafosgoLlehthsilbatsedluowydutsahcuS
nikovnirofsdohtemevitanretlaytluc
􏰇noitaulaveevisseccushcaenidesuebdluowsegnahcralimisgniriuqertnemucodelp
􏰆mastnere􏰎ida􏰅sksatnoitaulaveehthtiwytirailimafnihtworgegaruocsidoT
fodneehttA􏰇draobyekdnaesuomdradnatsno
ecnamrofrepfonoitaulaverehtonahtiwdnedluowsnoissesnoitneterdnanoisses
htruofehtrtnocoT􏰇setunim􏰌􏰈tsalehtninoitaulaveecnamrofreprehtonahtiwdnedna
􏰅setuni m􏰠􏰍rofstnemngissassalcnoSTMehtfoesudellortnocnuwolla􏰅STMeht
nonoitaulaveetunimadluowSTMehtfoecrofnoitavitca
laminimeht􏰅mret􏰆gnolehtnitahtebdluowsisehtopyhehT􏰇wollofylbaborpdluow
ecivedrehtiemorftnemevorpmimret􏰆muidemtub􏰅secivedwenriehtotdesugnitteg
elihwsmotpmysfogninesrowmretek
dradnatsriehtmorfhctiwsstcejbusemosevahotebdluowecivedwenaotgnihctiws
morfstce􏰎eobecalpelbissoptuotoorotyawenO􏰇secivedtupnirehtofoesohtnaht
tnere􏰎idyllacidareraleefdnaepahsstiesuacebelbissopmiebdluowSTMehtfossen
keewarofsecivedtupniremrofriehtot
kcabhctiwsotdeksaebdluowstcejbus􏰅skeewowttsaeltarofuaetalpadehcaerdah
snrettapegasuSTMdnasmotpmysniapretfA􏰇tnatsnocdeniamerniapdetroper
elihwegasuSTMyliadroecnarudnenisesaercnisahcusanemonehpesopxethgi m
sgol hcuS􏰇yadhcaeretupmocehttaemitriehtetamitseotevahtondluowstcejbus
osyadhcaeegasufoytisnetnidnasnoititeperlatotfokcartpeekdluowSTMehtno
erawtfosgniggolytivitcadnaH􏰇sitinodnetregn􏰏xednitfelymsahcusseirujniereves
ylralucitrapetadomoccaotyrassecensadetpadaebdluowerawtfosSTMehT
􏰇skeewlarevesrof ti esuot eunitnoc dnaesuretupmocllarof
STMehttpodadluowstcejbusnehTbnraelotsnoisseslairotuteno􏰆no􏰆enoowteviecer
dluowstcejbuS􏰇slevelmotpmysecnereferhsilbatseylmr􏰏otydutsehtfoowtro
keewtsr􏰏ehtgnirudsecivedtupnilanoitnevnocriehtesuoteunitnocdluowstcejbuS
otino motselcsumkcendna􏰅redluohs􏰅rosnetxeregn􏰏􏰅roxe􏰐
regn􏰏detcelesnosrosnes􏰝margoymortcele􏰜GMEhtiwdett􏰏tuoebdluowstcejbus
􏰅snoissesnoitaulavegniruD􏰇troppusdnahgnitserfoeergedehtdnagnipytgnirud
ecroftcapmiregntnohT􏰇nesohcebdluowstsi
􏰆pytnoitpircsnartroseiratercessahcusslliksgnipytdooghtiwstcejbusneT
􏰇STMeht ottpadastcejbussaegnahcsecrof
tcapmiregn􏰏dnagnitser dnahfosnrettap woherusaemosladluowtI􏰇STMeht
nognipyt gnirudspuorgelcsumsuoiravfosnoitrexeevitalereht ytivitcalacirtcele
elcsumhguorhterusaemdluohsydutsehteroferehT􏰇tnatropmiebyamecafruseht
foepolsdrawnwodeht􏰅smlapriehttserodyehtfi dna􏰅gnipytelihwecafrusehtno
smlapriehttserstcejbusrehtehwnodnepedotylekilsisihT􏰇sekortskciuqekamot
ecafrusehtevobadednepsusylluf niamertsumsregn􏰏ehtecnisgnipyttsafgnirud
draobyekdradnatsarofnahteromebyamnoitrexeelcsumredluohsdnarosnetxe
regnfsselebotdetcepxeerayehtiwdemrofrepebnacsnoitalupin
􏰆amcidrohcecniS􏰇ydobehttuobadetubirtsidsi STMehtnognipytgniruddaol
krowelcsumeht wohenimretedotebdluowydutseugitafehtfolaogenO
seidutSeugitaFgnipyTnirug􏰏nocdnahdezilaic
􏰆epseromfonoitatnemgesti mrepdluohssyarraedortcelenoituloserrehgiH
noituloseRyarrAdesaercnIilgenotspordnoisnet hcus􏰅revefi􏰅ylkciuq woh
dnaSTMehtotnoitpadalaitinignirudecneirepxelliwelpoepnoisnetartxehcum
wohetacidnidluownoissestsaldnatsr􏰏ehtneewtebs􏰛GMEfonosirapmoC􏰇sdnah
gnitao􏰐morfeugitafdiovaotsrotarepoSTMllarofsyawtseggusdluocnoitrexe
wolyllaicepsehtiwstcejbusnidevresbosnrettapytivitcagnipyT􏰇smlapgnitao􏰐ro
smlapgnitserhtiwdeziminimsinoitrexerosnetxeregn􏰏rehtehwhsilbatseosladluoc
sihT􏰇noitrexeelcsumdnanoisnepsusdnahneewtebknilehtfohtgnertsehthsilbat
􏰆sedluocsnrettapgnitserdnahsusrevsnrettapnoitrexeelcsumfosisylanA􏰇emit
revoSTMehtnosdnahgnitserelbatrofmoceromemacebstcejbusrehtehwetacidni
dluowsnoissesnoitaulaveevisseccusgnirudsecrofllecdaolSTMfosisylanA
􏰇STM
ehtnognipytgnirudnoitrexeelcsumfonoitpadamret􏰆gnolerusaemothtnoma
rofslavretnikeew􏰆enotasnoitaulaveruoh􏰆owtrofnruterdluowyehT􏰇snoitatskrow
lanosrepriehttabojehtnoesurof STMnanevigebdluowtcejbushcae􏰅snoisses
hcusfosyadwefaretfA􏰇eugitafrefniotslevelnoitanegyxoeussitnisegnahcerus
􏰆aemdluocneuqerfGMEnaidemnisesaerc
􏰆eD􏰇derotsebdluowswodniwgnippalrevoetunimoissesecitcarpgnipytSTMetunimtgniyrterewstcejbuselihwnoissessihtgnirud
elihwidrohcht kcaltubylkciuqdnaylpmiseromdemrofrepeb
nachcihwserutsegnoitalupinamcidrohcehttnemelppusdluocsihT􏰇􏰪􏰉􏰋􏰈􏰨􏰉􏰟SOrof
nePnisaslobmyscitebahplaralucitrapgniwardybsorcamdnammocekovniotdesu
ebdluocedomgnitirwdnah􏰅ylevitanretlA􏰇tuoyalyekehtnollatatsixetonodhcihw
ronwodgnikooltuohtiwtuoyal yekehtnohcaerotdrahrehtieerahcihwslobmys
dnanoitautcnupfoyrtnerof ylnoti noylerthgi mstsipytdooGonodohwroSTMehtno
gnipytelbuortevahohwsrotarepOsadluownoitingocergnitirwdnahfonoitargetnI
noitingoceRgnitirwdnaHsulytsgnikni
ehtsadekcartebdluowregn􏰏xedniehtylbareferP􏰇epytotorpSTMtnerruceht
hti wgnitniopdnagnipyt neewtebhcti ws hcihwsegnahcnoitarug􏰏nocehtsaysaesa
tsujegnahcnoitarug􏰏nocdnahahtiwedomgnitirwdnahotsehctiwsrotarepoeht
hcihwnimetsysanoisivnenaceno􏰅noitalupinamcidrohcdnagnipytnidesusnoit
􏰆arug􏰏nocdnahehtmorfelbahsiugnitsideraspirgneptahtgniwonK􏰇􏰝􏰣􏰇􏰉erugiF􏰜
noitarug􏰏nocdnahdesolcyllaitrapehtfospitregn􏰏retuoehtmorfpirgnepehtfo
selkcunkretuoehthsiugnitsidylbailernacniselkcunkyfitnediyltcerroclliwmetsysnoitac􏰏
􏰆itnediregn􏰏gnitsixeeht􏰅ylreporpdetnemgeseraselkcunkretuodnasregn􏰏dehcnip
ehtfitahttseggusstnemirepxeyranimilerPrutsidrojam
gnitatipicerp􏰅hcraesehtmorfdedulcxeyleslafebotswortnecajdaroemasehtni
sedortcele	yna mesuac	dluoc regnihwnrettaphcraesnoitatnemgestnerrucehthtiwtuB􏰇sdiort
􏰆nectcatnocnisesaibelbaecitonylerabylnoesuacdluohsedortceleelgnis afoeruliaf
aevahlliwedortceleenoynasahcumosdiortnectcatnocehtbrutrepllitsdluocedortceledab
elgnisa􏰆namhcraesnoitatnemgesehtfineveesuacebepytotorptnerrucehtfoyarrarosnes
esraocylevitalerehtnorettamyllaertonseodsihT􏰇􏰝􏰌􏰇􏰉􏰇􏰊noitceS􏰜amixamlacol
morfseiradnuobtcatnocrof hcraesdrawtuoehttpursidylisaenacstnemerusaem
ytimixorpelacsllufroaliavaemocebsyarrarosnesrenomerahcihwsmhtiroglanoitatnemgeS
noitatnemgeStnareloTtluaF􏰋􏰇􏰊􏰇􏰍
􏰇noitatnemgeselbailerroftserotde􏰏idomebdluocmetsysehtelpoepesehtroF
􏰇􏰌retpahCnidebircsedsnoitalupinamcidrohcesicerpehtgnimrofrepelbuortevah
yamsregnevinU􏰊􏰇􏰊􏰇􏰍
􏰉􏰠􏰊
gnitarepotsomhguohT􏰇latnemadnuferomera􏰅noitalupinamdednah􏰆owtsuoen
􏰆atlumis􏰅􏰇e􏰇i􏰅tupnimaertslaudrofyrassecenstnemecnahneerawtfosehT
􏰇stelbatgniwardotc􏰏icepsrennamaniFOD􏰌ylnostroppusyltnerruc
tamroftupnIXehtneve􏰅revewoH􏰇swodniWXot􏰪􏰢􏰈􏰈􏰨snoisnetxetupnIXehtsahcus
slocotorptelbatgniwardybdesserddayllaitraperaseussiesehTisopevitalerroyticolevotevitanretlanasalanoitpoebdluohs
gninoitisopetulosbaroftamrofegassemapedniev􏰏tsaeltadnanoitalupinamfomodeerf
foseergedxistsaeltaotdednetxeebdluohsstamrofegassemesehTitalupinamtroppus
avaJdna􏰅swodni WX􏰅􏰉􏰟SO􏰅􏰢􏰣swodni WnisegassemesuomtnerruC􏰇dessecorp
yltnerrucerasegassemdraobyekdnaesuomerehwlevelemasehttaseueuqtneve
otnisegassemnoitalupinammaerts􏰆laudetaroprocnidluohsskrowemarfecafretni
resulacihpargdnasmetsysgnitarepo􏰅ytilibapacsihtfoegatnavdallufekatoT􏰇sresu
retupmocfonoitalupopredi whcumaot ytilibapacnoitalupinamlaunamibgnirbot
laitnetopehtsahSTMehtprogni wardrofsrevirddezilaicepsybdetroppusyltnerruc
erastelbatgniwardsahcussecivedFOD􏰆hgiH􏰇􏰪􏰈􏰍􏰨mehttroppussmetsysgnitarepo
weferoferehtdnasecivedtupnielbaliavaylediwwefraeserhguohT
􏰇STMehtybdelbanesnoitcaretniFOD􏰆hgihdnalaunamibhcirehtfoselpmaxe
cisabtsomehtylnoeraercni􏰅sedortcelemargolellaraptuognisahpyB􏰇esuyliadrednurohtuaeht
etartsurftonotsallamsosydaerlasisrorreesehtfoycneuqerfeht􏰅edilslanoitcerid
aromlaparobmuhtehtsezingocersimyllanoisaccometsysehthguohToitacilppagniwardani kni
gniyalnehwesirasahcusseussinoitazinorhcnysgnitseretnihtiwlaedotevahlliw
sremmargorP􏰇detalupinamgniebtcejbodnuorgerof rehtororetniopehtswarder
dnasevomdaerhtrehtoehtelihwmaertsdnahtfelehtnisegassemotesnopserni
dnuorgkcabeht warderdnaevomdluowdaerhtenopdnaetarapestpekebotsdeen
dnahgninnaptnanimod􏰆nonehtmorfmaertstupnieht􏰅ylsuoenatlumisdetroppus
ebotsnoitarepohcusroF􏰇maertstupniretniopelgnisehtnigardnottubyraili
􏰆xuanasadetavitcayllaususignillorcsesuacebyltnednepedniteyylsuoenatlumis
retniopehtevomdnallorcstonnacrotarepoeht􏰅smetsystnerrucni􏰅revewoH
􏰇thgirehthtiwtinognitirwelihwdnahtfelehthtiwrepapfoeceipatneiro
yllamitpoyeht nehwgnicitonnevetuohtiwyadyrevesihtodelpoeP􏰇dnahtnani
􏰆modehthtiwgnitniopelihwdnahtnanimod􏰆nonehthtiwdnuorgkcabehttneiro
dna􏰅eziser􏰅napotlarutanylevitingocdnalufesusititaht􏰪􏰍􏰢􏰨nwohssahhcraeser
􏰅thgirehthtiwesuomdnadnahtfelehthtiwrosructxetehtgnillortnocnahtgnisuf
􏰆noceromneveebdluowdnahhcaerof rosrucesuomagnivahelih W􏰇rosrucemas
ehtlortnocllayehtosmaertsenootnidegremeraecivedhcaefosdnammocnoitom
eht􏰅ylsuoenatlumisdetcennocebotecivedgnitniopenonahteromwollasmetsys
uomhtobecalperot
hguonelacitcarpSTMehtekamlliwlliksgnipythcuotycagelroftroppusdnaytilib
􏰆apacnoitalupinamlacihpargevisnetxefonoitanibmoceht􏰅tsebtA􏰇smraerofdna
stsirw􏰅sregn􏰏otseirujniniartsevititepermorfgnire􏰎uselpoeprofepohwensreac hcihwsnoitalupinamcidrohceseht
nrael otgnilliweblliwohwdna􏰅noitomregn􏰏esicerpylevitalerrofstnemeriuqer
esehtretsamotsecivonrof ebtilliwdrah woheweroferehtdnateemotrotarepo􏰆delliksarof
ysaeeranoisicerprofstnemeriuqeresehttahtetacidniSTMehtgnitareposecneir
ahtmrofinu
eromsnoixe􏰐regn􏰏dnadezinorhcnysrettebebtsumsnwodhcuotregn􏰏􏰤noisic
􏰆erprotareporof dnameddesaercnisiylsselmaesossnoitalupinamcidrohcFODamrofrepoteudsrorrenamuhemosdnah
suougibmatahwemosfonoitingocerytluafnodemalbebnachcihwsrorreesoht
esehtfotnenopmocnommoc A
􏰇􏰪􏰡􏰈􏰈􏰅􏰌􏰋􏰅􏰡􏰊􏰨emordnyslennutlaprac
dna􏰅emordnyss􏰛niavreuQeD􏰅sitilydnocipe􏰅sitivonysonet􏰅sitinodnetsahcusseirujni
esurevoecivedtupniotetubirtnocllaemityrevocertneiccnocafosselsierutarepmet woltserresuehtsselnuderetnuocneeb
otylekiltonsiecroflacinahcemdezilacol􏰅elpmaxeroF􏰇esuretupmocotyllanigram
ylnoylppasrotcafksiresehtfoemoSirtsudnifoseidutslacigoloimedipesuoiraV
ISRrofsrotcaFksiRaegamadeussit tfos	hcihwrednusnoitidnoceht
etamylekil
tsomehteraseussittfosnamuH􏰇slairetams􏰛msinahcemhcaenisnoitatimillacisyhp
ehtfognidnatsrednunasemussangisedgnireenigne ynafonoitaulavelluF
SREENIGNEROFSCIMONOGRE
AxidneppA
alucric
doolbgnirednihybereht􏰅seussitsserpmocserutsoptniojemertxE􏰇eussitfoyrevocer
gniwolsrehtruf ybnoititeperfostceitnenopxenataamuartorci	mmorfrevocerotseussitehtswollagnitseR
􏰇nitessessecorpyrotamma􏰐nidnaegamadeussitmret􏰆gnol􏰅dlohserhtamuartevital
􏰆umuclacitircatsapsetalumuccasdaolevititepermorfamuartorcimehtnehwtaht
sezisehtopyhtaksirehthguohtnebnacnoititeperhgihdnaecrofhgihhtoberiuqer
hcihwsbojniyrujnifoksiretaleryllautcasiegamadnodnetfoeergedeht
􏰅yrevocerlacigoloisyhpdnalacinahcemreporpstneverpnoititeperesuaceB
􏰇slanigiroehtnahtylevitce􏰎essel
detneirodnarekaewtubcitsaleeromerasreb􏰏negalloctnemecalperehtesuaceb
htgnertslanigiroriehtniageryllufrevensnodnetdegamadsllec
rell􏰏dnasreb􏰏negallocfoxirtamcitsaleocsivasiflestieussitnodnetehT
egamaDeussiTtfoSninoititepeRsesaercnisnodnetybraenfognillewsnehw
rucconacsemordnyslennutfoegamadevrenlufniapehTeussitehtfonoitamma􏰐niot
􏰡􏰠􏰊
esurevootetubirtnocnacsnottub􏰎itsfognikcilcevititeperehtdnacgnomatahtstseggussisylanaevobaehT
seciveDtupnIfosecroFnoitavitcAliraropmet
nacgnipytgnirudderetnuocnesecrofpitregnutsA􏰇yrujniesurevo
esuacyamerusopxedegnolorphcihwevobadlohserhtdesoporpanrdsiverbsilaidariprac
rosnetxefonoitanegyxonae mdnuoftsisrep􏰖eugitafycneuqerf wol􏰩eht􏰅􏰝CVM􏰜noitcartnocyratnu
􏰆lovmumixamfo􏰙􏰍􏰇􏰠ylnosawecrofgnippirgegarevaehthguohtnevE􏰇noitarepo
esuomfosruoh􏰊retfaesuomehtfosedisehtpirghcihwselcsumehtnieugitaf
tnac􏰏ingisderusaem􏰪􏰋􏰡􏰨􏰇latenosnhoJ􏰇esudraobyekdnaesuomfolacipytsnoitid
􏰆nocnoititeper􏰆hgih􏰅ecrof􏰆etaredomehttadevresboneebevaheugitaffoserusaem
lacigoloisyhpdetaveleamseirujniesehtfoerutanlaudargehtelihW
􏰇􏰪􏰊􏰈􏰈􏰨 stneirtundnanegyxoelpma
tuohtiwrevocertonnacylraelceussitdeniartSeknoitcartnoccitatsgnidloH􏰇emordnyslennutlapracfoesuac
dnangisahtobderedisnocnrutnisierusserplanacfonoitaveledeniatsuS􏰇lanac
lapracehtnierusserpdiuavitcalatnediccadiovadnasdnahgnitsertroppusotdeeneht􏰅ecrof
gnipytnessellliwsngisedhctiwsyekhcihwetacidniseidutsesehthguohT
ppednierews􏰛GMEroxet didyehtsa􏰅kcilcelbiduanadna
kcabdeefecrofniegnahctpurbanadecudorphcihwf􏰪􏰌􏰋􏰨drareG􏰇tce􏰎e
tnac􏰏ingisondahlevarterpelihwlppatce􏰎aoslanac
􏰅tuognimottoberofebtubtcatnoclacirtcelegnikamretfasesserpedyekehtecnatsid
ehttsideht
arevotub􏰅ecrofekamgnisaercnihtiwsesaerc
tecrof eromsemiteb
tsumhcihwecrofehtamuH
utsoptsirwdrawkwaehtdezilartuenyllufsseccusevahsdraobyek
cimonogresuoiravnistilpsehT􏰇􏰪􏰋􏰉􏰨gnippatthgilylemertxednaserutsoplartuen
egaruocnehcihwsdaphcuoteviticapacybdesserddaerasnrecnocesehT􏰇seirujni
􏰣􏰠􏰊
􏰇ecafrusstievobadednepsussemitemoserasdnahehtecnis
sdnamedlarutsopevissecxeesopmitonseodSTMehttahterusnedluohs hcraes
􏰆ererutuf􏰅osl A􏰇dlohserhtamuartevitalumuclacitircehtfoecnadiovadnalennut
lapracehtedisnierusserpninoitcuderotsetalsnartsihttahtyfirevdluohshcraeser
erutuFl􏰆daphcuotforovafnirehtegotla
sehctiwserusserpetanimileotneebsahnoitatressidsihtninekathcaorppaehT
STMehtotecnaveleR􏰋􏰇A
􏰇stsertsirwmr􏰏sahhcihwsiseniKehtsahcussdraobyekcimonogrenoelbarelot
smees N
roperlan􏰏
ehtsnoitalopretninmuloctfeldnaelddimdesaibnuehthtiwdegarevasinoitalop
􏰆retninmulocthgirehtnehW􏰇erehtedortcelepotehtsehcuotylnotcatnoceht
esuacebdesaibylerevessinmulocthgirehtylnodna􏰅lauqeylhguorerasnmuloctfel
dnaelddi mehtfospalrevotcatnocehte􏰈􏰇BerugiFnitcatnocrediwehtroF
􏰇mm􏰌􏰆􏰊fororrenoitisop
aotsdnopserrocsiht	mc􏰉􏰇􏰈fognicaps	woredortcelenaroF􏰇wor potehtot yaw
ehtfo􏰋􏰟􏰊tuobaebotnoitisopdetalopretniehtdnaedortceleelddimpotehtno
langisregnortsaesuaclliwhcihwostcatnocehtesuacebsihttceadtnocllafosnoitisoplautcaehT􏰇sworneewt
􏰆ebsitcatnocanehwdesaibsitisnoitidnocniatrecrednu􏰅setartsulli􏰈􏰇BerugiF
satub􏰅woranoderetnecerastcatnocfidesaibnusyawlasinoitisoplacitrevdetal
􏰆opretniehTtnocpitregn􏰏naht
regralerasgnicapswornehwnoitalopretnilacitrevdiayllareneg􏰢􏰇􏰈􏰇􏰉noitceSni
decudortnisyarraedortcelemargolellarapfosegdewdesrepsretniehthguohT
SYARRAEDORTCELEMARGOLELLARAP
NOSESAIBNOITALOPRETNILACITREV
BxidneppA
a) b) c) d) e) f) g)
setuorotyrassecenerasnmulocneewtebspagehT
􏰇worhcaeniderusaemytimixorpevitalerehtetacidniotton􏰅mehtpal
􏰆revostcatnocehterehwsedortcelewormottobmorfsedortcelewor
potehtetaitnere􏰎idyllausivottnaemylnoeraerug􏰏sihtnisgnidahs
margolellarapehttahtetoN􏰇worpotehtnoderetnecebotdetalop
􏰆retni eblliwthgirehtnoenoehtelihwwor	mottobehtnoderetnec
ebotdetalopretnieblliwtfelehtnoenoeht􏰅sworneewtebderetnec
era􏰝gnistcatnocynitehthguohtnevetahtosstcatnocrellamsrof
snesrowsaibehT􏰇wolylsuoenorreebdluownoitisoplacitrevdetal
􏰆opretniriehtretnecnmulocfothgirehtottsujerewyehtfI􏰇hgih
ylsuoenorreebotnoitisoplacitrevdetalopretniriehtgnisuac􏰅mottob
ehtnahtsedortcele worpotehtfoerompalrevodnaretnecnmuloc
elddimfotfel ehtottsujerastcatnoceht􏰝edna􏰝bnituB􏰇swor
ehtneewtebyawflahsadetalopretniyltcerrocebotnoitisoplacitrev
riehtgnisuac􏰅swormottobdnapotfosnoitroplauqepalrevollitstub
snmulocelddimehtnoderetnecerastcatnocehto
snoitroplauqepalrevorohcuotdnasnmulocelddi mdnatfelehtneewt
ontubsworedortcele marg
htgnitartsullimargaiDemgniruderoferehT􏰇yrotcejartregn􏰏ehtnisnoitallicso
lacitrevsasevlesmehttsefinamsesaibeht􏰅ecafrusehtssorcayllatnozirohgnivomsi
regn􏰏aneh W􏰇e􏰈􏰇BerugiFdnab􏰈􏰇BerugiFfoesohtneewtebsesaibetaredomerom
tibihxednasrosnesehtotesu􏰎ideromrorediwraeppaottcatnocehtsesuacsiht
htgnitneverpn􏰏
ehtneewtebrevoccirtceleidkcihtyllaitnatsbusagnisuos􏰅ytimixorphse􏰐otlanoit
􏰆roporpylesrevnisiecnaticapacedortceledesnes􏰅tsriF􏰇sesaibnoitalopretnilacitrev
esehtfostce􏰎eehtetaroilemaotsdohtemforebmunasyolpme STMehT
noitalopretnImargolellaraProfdiortneClacitreVraenilnoNamorfdeniatboeb
dluowhcihwlangisytimixorpehtottnelaviuqeerasmusnmulocdetupmoceseht
􏰅raenil erasrosnes yti mixorpeht dnaelucsuni merasmargolellarapneewtebspag
lacitrevehtsagnolsA􏰇smusnmulocehtgnisuyllatnozirohgnitalopretninehtdna
nmulochcaenislangisytimixorpehtgnimmusybdevorpylisaesisihT􏰇noitatupmoc
diortneclatnozirohnisesaibtnacwolmm􏰍llufa􏰅wormottobehtnoderetnecsa
detropereblliwnoitisopstios􏰅edortceletfelmottobehtspalrevoylnotcatnoctfel
ehttahtllamsoserastcatnocehtereH􏰇gnicaps woraflahsuni mrosulpsahcumsa
ebnacrorrenoitisopeht hcihwnisnoitalopretnitcerroceht
esuacebelucsunimemocebsnmuloclarehpirepnopalrevodesaibybdesuacsrorre
gnirusne
􏰅retemillimanahtsselotsesaibnoitalopretniehtetaroilemaspetsesehthguohT
j
r apz
G
yarramargolellaraprof GdiortneclacitrevdevorpmiehtnehT
j
j
G
E
E
G eclacitrevehteb Gdna GteL􏰇snmuloclartnecehtfosnoitalopretnidesaibnu􏰅regnortsehtezisahpme
otytimixorplatots􏰛nmulochcaeerauqssalumrof diortnecraenilnongniwollofeht
ceht gnithgiewylraenilfodaetsnI
􏰇noitalopretni
lacitrevdesaibaetubirtnocyamdnayrehpireptcatnocehtnosinmulocthgireht
tahtnoitacidnidrawrofthgiartstsomehtsisihT􏰇snmulocelddimrotfeleht morf
tahtnahtssel hcumsi nmulocthgireht	morflangislatotehtos􏰅edortcelepoteht
fopitehttanmulocehtfoedistfel ehtspalrevoylnotcatnocehtnmulocthgir
desaibehtnitahtetoN􏰇owtforotcaf atsaeltaybsrorreehthsinimidrehtruf
nacylnevedeppalrevoerahcihwretnecehttasnmulocesohtforovafnitcatnoc
ehtfoyrehpirepehttasnmulocdesaibyllaitnetopehtsezisahpmenocdezisceleesaercedotsiyleritne
sesaibehtetanimileotyawnwonkylnoehT􏰇stcatnocpitregn􏰏thgildnaytivitisnes
noitomrosruchgihnevignoitomrosrucneercs􏰆nonisnoitallicsoelbaecitonesuac
􏰌􏰈􏰊
􏰇crotcarttaraeneht
drawotetagaporpdnarotcarttarafehtfotuodeppawsebottcatnocehtesuaclliw
hcihw􏰅otcatnocehtsarotcesibralucidnepreptnavelerehtfoedisemasehtnoera
srotcarttaesehttaht gnicitonybde􏰏ireveboslanactI􏰇xrotcarttasatcatnoceht
morfecnatsidemaseht􏰝crayvaeh􏰜stniopfoelcricehtnihtiweilyehttahtgniciton
ybyllausivde􏰏irevebnacsihT􏰇xnahttcatnocehtotresolceraxrotcarttarafeht
ottnecajdaddnaasrotcarttaehtesuacebte msi	noitidnocecnegrevnocsihtttatsesolcs􏰛tcatnocehtotxrotcartta
rafehttcennoctsumecneuqesbussihtaeni gnippawstaht hcusecneuqesegnahcxeesi w
􏰆riapehtfoecneuqesbusaebtsumereht􏰅crotcarttatsesolcehtotgnirehtdnuora
etagaporpottnemngissas􏰛tcatnocehtroF􏰇crotcarttatsesolcstimorfgnirehtfo
edisetisoppoehtnoxrotcarttanaotdezilaitini ylroopsicetanutrofnuetartsnomed􏰉􏰇Cdna􏰈􏰇CserugiF
􏰇GNIR
ROTCARTTANANOHCRAESLAIROTANIBMOC
DEZILACOLROFSPARTECNEGREVNOC
CxidneppA
d
d
c
x
a)
c
b)
c
x
c)
a
a
b
b
b
d
a
rt􏰝cni gnirehtfogniprawehtdna􏰝bfognir
ehtniytivacnocehttuB􏰇edisraenehtno􏰝c􏰜rotcarttatsesolcehtot
gnirehtfoedisetisoppoehtno􏰝x􏰜rotcarttanamorftnemngissa􏰝o􏰜
s􏰛tcatnocaetagaporpnacsrotcarttatnecajdafognippawsesiwriap
osralucricyltcefrepsignirrotcarttaehtgrevnocgniwohssmargaiDpllA􏰇noitalergniredroesiw
giFfostnemngissadetatorehttubnahcxeesiwriapehtsesaercnisihT􏰇nekatsisriaphcusllarevo
tsoctnemngissamuminimehtsecudorphcihwpawsehtdna􏰅srotcarttarehtolladna
rotcarttamlapehtneewtebderedisnoceraspawsesiwriap􏰅rotcarttamlapasehcaer
ecneuqesegnahcxeehtnehwsuhT􏰇gnirehtnirotcarttatxenehttsujfodaetsni
srotcarttallaedulcniotsrotcarttamlaprofdoohrobhgienegnahcxeehtfoeziseht
gnisaercniybyllufsseccusdesserddaneebsahsihT􏰇mlapasade􏰏itnedisimdnarot
􏰆carttamlapanideppartteglliwtcatnocpitregn􏰏ayllacipyT􏰇metsysnoitac􏰏itnedi
ehtnidevresboneebyllautcaevahcf eht notnemngissa
s􏰛tcatnocehtgnippart􏰅srotcesibralucidneprepdnacratnatsidiuqeehtfoedistuoera
rotcarttarafehtottnecajdasrotcarttaehtniagAibralucidnepreptnecajdaehtfoedisemasehtnosixrotcarttatahtgniton
ro xhguorhtsessaphcihwcratnatsidiuqeehtedistuoeil	ddnaasrotcarttatnecajda
ehttahtgnitonybyllausivde􏰏irevsisihT􏰇xrotcarttamorfepacsetonnactcatnoc
ehtdnadetpeccaebtonlliwsrotcarttatnecajdaesehthtiwspawsesiwriapsuhT􏰇d
dnaasrotcarttatnecajdastinahtotcatnocehtotresolcebotxrotcarttasesuac
sihT􏰇xtagnirrotcarttaehtni ytivacnocasniatnocb􏰈􏰇CerugiFfoesacehT
XX O
O
O XX
a)
O
XX
O
O
O
O
XX
b)
XX O
O
O
O
XX
c)
aceb
mumini mlacolelbatsaoslaera􏰝bnistnemngissadetatoreht􏰅reve
􏰆woH􏰇􏰝ani nwohssitnemngissa muminimlabolgehT􏰇stcatnocfo
llufgnirrotcarttacirtemmysyltcefreparofmuminimlacollanoitatoRotcarttaregn􏰏lautcaehtdnuorasgnicaps
rotcarttaniseirtemmysaehtesuacebylbamuserpliwdnastsoctnemngissaehtesaercnidluowcemngissaehtrofmhtiroglawenAmhtiroglasisabgnitanretlaehTevititepmoc Arusaemytisnetniniapeerhtfonosirap
􏰆moca􏰤selacsniapfossenevisnopseRzilitUeciveDtupnI draobyeKroesuoM
retupmoC􏰇renga W􏰇 RcirEdnanortcel EnoecnerefnoCEIPSnI􏰇noitcurts
􏰆nocnoitcaretnidecnavdaroftnemadnufaaRosgnideecorPnI􏰇melborptnemngissa
ehtrofmhtiroglanoitaxalersuonorhcnysadetubirtsidA􏰊􏰨
􏰇􏰡􏰢􏰣􏰈􏰅kroYweN􏰅retyurGedretlaW􏰇sledoMnoitazimitpO
suounitnoC􏰇molbdnaSsiuoLvlosrof dohtemhtapgnitnemguatsetrohsehT
􏰅noitcaretnIretupmoCiannoitseuQsemoctuOdnaHnagihci	Mehtfognitset ytidilavdna
ytilibaileRaMdeilppAetercsiDalB􏰇M􏰇MnI􏰇secivedtupnifoecaps
ngisedehTnamuHfoygolohcysPehTortcelE􏰟􏰌􏰣ihc􏰟ihcgis􏰟gro􏰇mca􏰇www􏰟􏰟􏰤ptth
ocsi	DevitcejbuSdna􏰅ytivitcAelcsu M􏰅ecroFgnipyTnokcabdeeFyrotid
􏰆uAdna􏰅ecaPgnipyT􏰅ssenpmoccimonogresisenikehtfonoitaulavecimonogrenA􏰇gnaWiaT
dna􏰅samohT􏰇EtreboRnI
naciremA􏰇gnipytelihweugitaffotnempolevedehtdnaecrof nossen
argelacsitlumaivsisylanadnanoitatnemgesegamIrumonoTubonihsoYdnaotomukuFikaasa M
gnitnioPdnadraobyeKdetargetnI􏰇reyaartS􏰇HdivaDdnaznarF􏰇JkcirtaPcepssdrawoTmnamuhehtfoyticapacnoitamrofniehTetarlamitpofongisedehTretnIehTnet
dnasnodnet ot amuartevitalumucfostcepsalacinahcemoiBjbOA􏰇Pyhtomi T􏰅eipselli GdivaDwoH􏰇eegaM
aruaLdna􏰅llisiduRennairaM􏰅madAnasuS􏰅nedloHanitirKoPoCnosnoitcasnarTEEEI􏰇srobhgienraenderahsnodesaberus
Mdnagap􏰅noitingoceRnrettaPecnerefnoClanoitanretnIht􏰌ehtfosgni
􏰆deecorPnI􏰇serutcipegnarninoitcetedyradnuoBebmece D􏰅lanruoJ
s􏰛bboDosgnideecorPnI 􏰇ksatgnipytdegnolorpagnirud
serutsoptsirwnisegnahCpsnartdnasdnah􏰆owt􏰅stelbatnodesabmgidarapiugafongisedehT
􏰇notxuBlli Bdna􏰅leduaBsamohTamcihcrareihgnisu
ecnamrofreptrepxefostimilehTyM􏰪􏰈􏰢􏰨
􏰇􏰍􏰊􏰣􏰈􏰅gizpieL
􏰅galreV􏰆eimedakA􏰇nehparGnehcildnenudnunehcildneredeiroehTagnitroS􏰤gni mmargorPretupmoCfotrAehTferp􏰆nondnaderreferpehtnisecivedtupniretupmocgnisu
ecnamrofrepnamuHenilesrapsdna
esnedrof	dohtemhtapgnitnemguatsetrohS􏰈segap
􏰅􏰋emulov􏰅yteicoSygoloiBenicideMni gnireenignEEEEIehtfoecnerefnoC
launnAhtnamheLnevetSdnanosnhoJpmoCnisrotcaFnamuHnoecneref
􏰛stti Fgnisusecivedtupni
fonoitaulavediparehtrofloot AcaFnamuHnoecnerefnoC􏰉􏰣􏰛 IHCehtfosgnideecorPnI􏰇sksatlanoisnemid
􏰆owtotwaL􏰛sttiFgnidnetxE􏰇notxuBmailliWdnaeizneKcaMttocS􏰇I􏰅srotide􏰅IIIssenruF􏰇A􏰇Tdnadle􏰏raB􏰇 WnI􏰇gnitupmoc
decnavdarofseuqinhcetnoitcaretnidnasecivedtupnIrofniehtnoetonAfeciveDtupnIdaphcuoTAgnisUrofmetsyS􏰪􏰈􏰣􏰨
nisrossertSlacisyhPrehtOdnaytivitcAdnaHoterusopxEgniyfitnauQrof
dohteMlanoitavresbOnafonoitaulavEdnatnempoleveDanoitatnemelpmIb􏰆erutsega􏰆emarFrosneSehTtfonoinap
􏰆moCnI􏰇stnemnorivnepotksed􏰆nonrehtodnaecapsytivargorci mniesurof
retupmocelbaraewAHC
ehtfonoinapmoCnIKtrautSdna􏰅nostreboR􏰇 GegroeG􏰅yalnikcaMimretedasagaLitisopmraerofdnatsirw􏰅dnah􏰅regn􏰏foelorehT􏰤serusserplanaclapracart
itluMEEEIpmoCevreNlarehpireP􏰤semordnySlennuT
􏰇ztiweikraMiwelpoeprofsdraobyeKiuQharobeDdnailleracsaPlimEfosnoitacilppA
dnastpecnoC􏰆snoitallesseTlaitapSnoit
􏰆anretnIehtfosgnideecorP􏰅noitcaretnIretupmoC􏰆namuHniegaugnaLngiS
dnaerutseGleHdnarekloidutS􏰇rereiGuoJ
􏰇noitcartnocyratnulovfoslevelwolhtiwsesaercednoitanegyxoelcsummraeroF
taer G􏰅sserPytisrevinUegdirbmaC􏰇snoitauqE
laitnere􏰎iDlaitraPfonoituloSlaciremuN􏰇sreyaMnietSretnuGdna􏰅nell A􏰇Pyhtomi T􏰅tessi BnehpetSpmoCe􏰅􏰇laterepaiD􏰇DnI
􏰇gnitnioprofsnoitcnufnoitom Mllortnoc
tnemurtsnignikcartmuHitcadnaerutsopgnitarepodraobyeKitadilavyranimilerp􏰍􏰣􏰣􏰈ehtfosgnideecorP􏰇sdnuorgkcabxelpmoctsniagaserutsopdnah
fonoitac􏰏issalctsuboR􏰇grubslaMrednovhpotsirhCdna hcseirTnehcoJivahebdnaecnaraeppaehtnoselpic
tnisegnahccihpargoymortcelE
Gfosretemarapngisedyalrevodnalautcurtsfostceennutlaprac
dnasrotcaflanoitapuccO􏰋􏰅enicideMlairtsudnIfolanruoJhsitirB􏰇yrtsudninisredrosidamuart
evitalumuctsirwdnatrebiSoitalupinamlacihparG􏰇sailEnhoJdnanamretse WenyaWitlumfognigami
eviticapacrof sutarappadnadohtemA􏰣􏰈yluJ􏰅􏰡􏰈􏰬􏰈segapuomlanoisnemidtupmoC
lausiVehT􏰇tnemecalptcejbolautrivrofnoitisopdnahgnisUgesegamirofmhtiroglatneidargelacsitlumAhcemoibdnayrtemoporhtna􏰤dnahsecivedtupniruoffoyduts AnotraBortosinAagivand􏰊launamibafonoitatnemirepxednangiseD􏰤􏰖tepraCcigaM􏰖eht
fohcraesnI􏰇rekleSdeTdnaimodeerffoeergedxisniecnamrofrepnamuHretlarof kcipdoogdraobyeks􏰛pirgofnId
televawgnisugnirotinomeugitafelcsuM</Text>
        </Document>
        <Document ID="182">
            <Title>Evaluation of practice-based approach</Title>
            <Text>10.3 Evaluation of practice-based approach
</Text>
        </Document>
        <Document ID="216">
            <Title>Statement of Originality</Title>
            <Text>I will deliver original:

- Performance, Artwork and Artefacts;
- Software product and creative production techniques;
- Innovative control systems and interfaces that will be applicable hopefully beyond the domain of performance;

MEMO: To be amended

I hereby declare that this thesis has not been and will not be submitted in whole or in part to another University for the award of any other degree.




Signature:




Ian John Grant</Text>
        </Document>
        <Document ID="105">
            <Title>6.2.5 Optical Input</Title>
        </Document>
        <Document ID="438">
            <Title>Ideas for Online Seminars</Title>
        </Document>
        <Document ID="327">
            <Title>Quotations and Notes - Digital Work</Title>
        </Document>
        <Document ID="294">
            <Title>REGRegister_Ian_Grant_28 Aug 2009_revision_001.doc</Title>
            <Text>

Application to Register for a Research Degree Programme
(to be completed by the proposed supervisory team and the student)

In completing this form you should refer to the relevant sections of the Research Degree Regulations (Part 9 of the UEL Manual of General Regulations) and the UEL Code of Practice for Postgraduate Research Programmes.

This form should be typewritten wherever possible. 

Confirmation of registration will be sent to the student’s and the Director of Studies’ UEL email address. 

When fully completed, this form must be submitted to the nominated individual in the School - usually the Research Administrator or Officer to the RDSC, accompanied by Form SDN for each supervisor nominated.

1. Student’s Details

Full name
Ian Grant
UEL student number
u0751966
Enrolled for  (Please Tick)
MPhil


Prof Doc


PhD via MPhil
x
Prac Doc


PhD Direct

MPhil by Publication


PhD (Eur)

PhD by Publication

Title of Professional/Practitioner Doctorate Programme (if applicable)
SMARTlab Practice-based PhD Programme
Date of enrolment
July 2008
Current Mode of Study (Please Tick)
Full Time

Part Time
x
Requested date for start of registration (Registration must occur within 6 or 12 months of the date of enrolment for FT/PT students respectively and may be backdated to that date)
July 2009
Declaration of previous registration. If you have a previous period of registration elsewhere which you wish to transfer and to form part of your registration at UEL, please give the dates and location of this registration. Proof of this period of registration must be provided.
none
School
SMARTlab Digital Media Institute / School of Computing, Information Technology and Engineering
Name of Collaborating establishments (if any)
n/a
If you are conducting your research outside the UK, please provide brief details of where and how you will be supported in your research.
n/a
If you are in receipt of any scholarship, award etc in connection with the proposed research programme, please give brief particulars
n/a
If you have undertaken any training or have prior experience that you feel supports this form, please give details and dates

Methods in Arts Research, PhD research seminars. University of Manchester, 1994-1995 I registered for a PhD 1994-1997 at Manchester University, where  
there were mandatory sessions for MPhil / PhD registrants. The sessions included: Literary Studies, Textual Analysis, Hermeneutics,  Anthropological Methods in Performance, Historiography,  Interpretative approaches, Phenomenology and other Qualitative  approaches (mainly in Educational Research but across performance  studies).

Using Computers in Qualitative Research, a series of workshops organised by SAGE and Qualitative research software company QSR (on NuDist)

If you have made any publications that you feel support this form, please give references.

Ian Grant (2008) Transdisciplinary Digital Art: Sound, Vision and the New Screen, Chapter: Experiments in Digital Puppetry: Video Hybrids in Apple’s Quartz Composer. Communications in Computer and Communication Science. Edited by Randy Adams, Steve Gibson, Stefan Muller Arisona. Springer. 342-258. 


(July 2007) Of Minnie the Moocher and Me: Explorations in Digital Puppetry. Video Hybrids in Apple’s Quartz Composer Digital Puppetry Performance Workshop and Paper (to be published in proceedings). Digital Art Weeks Festival 2007 / DAW07. ETH Zurich, Zurich, Switzerland. www.digitalartweeks.ethz.ch

(February 2007) Talking Toys and Digital Puppetry. Artificial Intelligence and Simulation of Behaviour (AISB) ’07 at Newcastle University, Newcastle Upon Tyne, 2-5 April 2007.

(Jan 2001) Finding the Wooden Voice in Puppetry Into Performance: A Users Guide. London: Theatre Museum, Central School of Speech and Drama and the Puppet Centre Trust. 29-31. ISBN 09537729-42 

(June 2000) From Craft to IT: The Art of the Puppet Technology. A paper delivered at Digital Scenography, a conference held at the University of Kent, UK. 

(1996) Formation of a Research Design: Towards a Critical Ethnography of Educational Theatre: Poster Abstract (and Review by Joyce Wilkinson) in Somers,john, ed. 1996. Drama and Theatre in Education: Contemporary Research. North York, Canada: Captus Press.


2. The Programme of Research 

Proposed Title of Thesis
Expressivity and the Digital Puppet:
Mechanical, Digital and Virtual Objects
in Games, Art and Performance
Aim of the Investigation

The current study explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

In this context 'innovative' means both emerging, new, technology or established technologies that are being re-defined by their communities of use and are finding new applications within the performing arts, particularly puppetry performance.

I aim to explore the related contexts of digital puppetry, real-time animation, mimetic and non-mimetic kinetic objects, automata, 'cybernetic sculpture', performance systems and the technological interfaces to such phenomena. 

I aim to create evaluate and create puppet/object theatre performances/installations that use original software and hardware systems that are designed to explore 'performance expressivity', with reference to relevant historical, art, entertainment and technological precedents.

I wish to theorise and form a taxonomy of 'expressivity' in relationship to digital domains and puppetry. By 'expressivity', I refer to different domains of action including: voice, face, body, hands and gesture.

Scope and Focus: finding patterns in an interdisciplinary study

The scope may first appear broad and the research focus wide. However, the researcher agrees with the statement:

"a cross-disciplinary approach yields information about patterning that is not visible from any single discipline." (Dorosh, 2008, 14, my emphasis)

Initially the surveyed areas and practice are diverse. The research progresses with the solid intent to articulate the patterns between (and the inter-relatedness of) cultural forms, and control systems, across computer media and performance art disciplines. In forming a taxonomy of 'expressivity' in relationship to digital control-systems and puppetry, a broad range of phenomena will need to be studied.

Focus is to be achieved through the practical work and case studies detailed in the ‘Proposed Plan of Research’, below.


Details of your proposed research in lay terms

Many innovations in contemporary computing and the way we interact with machines have applications beyond the domains for which they are designed.

Computer vision for gesture recognition, touch surfaces (like the Apple’s iPhone, Microsoft’s Surface and TUIO systems), wireless control devices, accelerometers and game control devices, like data gloves, can be used beyond their originally designed purpose. Increasingly these technologies and the means to programme them are finding their way into the hands of non-specialists and are crossing boundaries of practice. The present study works in an interdisciplinary way between human-computer interaction/interface design, computer programming and the performance practice of puppetry, evaluating the reciprocal opportunity for innovative practice.

In my practice and research, I aim to create low-cost hardware and software that allows a skilled and unskilled puppeteers to control dynamic physical objects (like a robot) or a virtual object (like a game character) in ways that calibrate and test new interfaces for their expressive potential. Coming from a background in puppetry, where a special approach to gesture, movement and mechanism applies, I wish to find a playful fusion between emerging technology and performance traditions.

The established puppetry traditions, as categorised by control mechanisms and form: rods, shadows and glove, (one may add the direct manipulation of objects and the related area of toys and automata), have fascinating parallels with moving objects in the virtual worlds of interactive art and games. The thesis will clearly establish and test the boundaries of these parallel forms.


Proposed plan of work, including its relationship to previous work, maximum 4,000 words. Please include in your discussion a description of the research methodologies and explain why these methodologies are the most appropriate for the task. Include a list of references for all works cited.

Focus and Scope  
The proposed study is interdisciplinary and seeks to establish patterns between diverse cultural forms and technological practices. The focus for the study is created by analysing specific case studies in practice that involve tactile and gestural interaction with interfaces for expression and improvisation.

The current work has a practical, experimental and media archeological approach that seeks to explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

Although the area seems broad (for example, puppetry is a large subject within performance studies, has numerous world traditions and a long history), I find focus through particular practical case studies where I design and test interfaces for expression and improvisation centered around gestural interaction, touch, optical and physical capture of motion.

Puppetry has distinct practices in various world traditions, but in computer space can be defined as the expressive use of interfaces to control objects. It is necessary, for the thesis, to offer an extended definition of ‘object’, which becomes interesting in computational contexts as does ‘expressive behavior’ and ‘affect’.

In exploration of practices akin to puppetry, the research draws on instances of performance, games and installation art practice in wider cultural practice and the practical explorations of the author. This focus is multi-disciplinary. 

The case studies are focused in the following ways: 

(1) Physical interfaces to on-screen (or virtual) performing and expressive objects and 
(2) Physical control of physical performing and expressive objects. I foresee multiple interfaces and approaches to objects will be designed, tested and used and may be shared and adapted between case studies.

Research Questions
Through practical, theoretical and historical research I wish to explore and answer the following research questions:

Primary Question: What is 'expressivity' in the context of computer controlled and user controlled physical and virtual objects?

Three main overarching questions
(1) How do new and emerging technologies facilitate innovative techniques of design and control of puppet-like objects?

(2) What are the most effective designs for interactive tools to create and sustain experiences of expressive play?

(3) How do the domains of traditional puppetry and emerging interactive technologies relate?

Sub-Questions: Definitions and Detail
The following questions further refine and dimensionalise the key research questions.

What defines a digital puppet?
What defines a virtual puppet?
Can we describe the distinctions between virtual and physical kinetic expressive systems?
     - What is the difference between an automata and a puppet in the context of virtual 
       creatures, animatronics (performance robotics) and artificial-life?

How do current performance technologies facilitate expression?

What is 'expressivity' in the context of computer controlled objects?

What happens to 'expressive acts' when mediated through interactive technology? Eg.
	- Gestural
	- Vocal
	- Whole Body (Viscerality and the wider sensorium of the body etc.)
	- Facial
	- Kinetics, movement and touch

What are the qualitative distinctions between live and captured movement?
How can we use movement capture as an analytic tool within puppetry studies?
How can we use movement capture as a tool for play within puppet performance?
What are the most effective methodologies for studying interdisciplinary puppetry practice?
To what extent is it useful to compare diverse virtual and physical kinetic forms as forms of digital puppetry?

New Interfaces for Expression
What are the most effective interactive interfaces to capture the expressive acts of puppeteers?

How do we best describe the 'expressive potential' of custom interfaces for puppetry?

What are the perceptions of traditional puppeteers towards new technology and new interfaces for expression?

 
Methods and Methodologies – A Media Archeology of Kinetic Behavioural Sculpture
The writing seeks to historicise in and around the spaces occupied by digital (or virtual) puppets, the avatar, automata, the robot, artificial life, and the animatronic. All these phenomena can be grouped, as by Reas (1996), in the realm of 'kinetic behavioural sculpture':

“A behavioral kinetic object is a dynamic system, meaning it changes with time. This system is composed of a source of energy, inputs, outputs, and a control architecture which converts the information from the inputs into information which stimulates the outputs. These elements sum to form the complete object, but other elements may be added to provide mass or form.” (Reas 1996, 22)

An emerging methodological approach called media archeology, will be used to trace how traditional puppetry forms, often described by their mode of interaction/media, (rods, shadows, strings and gloves) - map into current paradigms of interaction with virtual worlds, digital objects, games, and automated animation; and associated areas of digitally enhanced dolls, toys and automata. Media Archeology is a field that reflects on today's technologies by linking them to the socio-technical histories out of which they emerged.

“There is a gang of artists, theoreticians, and artist-theoreticians who have a very strong affinity (moreover, one that links them to a figure such as Artaud): they burn and burn up in the endeavour to push out as far as possible the limits of what language and machines, as the primary instances of structure and order for the last few centuries, are able to express and in doing so to actually reveal these limits” (Siegfried Zielinski, 2009, my emphasis)

Ethnographic Methods - Thick Description of Performance and User Testing
My experience in ethnographic methodology also assists to bring the study within a particular social, cultural way of seeing.  How to systematically observe and perform thick description (Geertz, 1977) of contexts such as software design and study is relatively unique, but common in anthropology when interpreting performance culture and forms, such as puppetry.

Media archeology, in the way documents and artefacts are studied has an affinity with historical ethnography. The moment of study is past as well as present. I will apply ethnographic methods of theme analysis – observation, thick description, coding and dimensionalising -  to support a broader semiotic approach to the cultures and histories of digital puppetry.

“The concept of culture I espouse…is essentially a semiotic one. Believing with Max Weber, that a man [sic] is an animal suspended in webs of significance he himself has spun. I take culture to be those webs, and the analysis of it to be therefore not an experimental science in search of law but an interpretive one in search of meaning.” (Geertz, 1977, 5)

Sherry Turkle (2005) has developed an approach to understand how we think and express through evocative objects. I argue digital puppets are a special class of evocative object. In addition to Turkle, I will consider multiple approaches to theorising the object in cultural practice: again this is drawn from a variety of domains (e.g. John Dewey's expressive object in Art as Experience (Dewey, 2005) to Baudrillard's complex system of objects (Baudrillard, 1988). I have started this work in relation to automata and talking toys (see the published works).

Puppetry, Kinetic Sculpture and Gestural Interaction
All styles of puppet manipulation rely on gestural interaction. The study considers the performative/expressive potential of numerous computer interaction systems that utilise tactility and touch, whole-body, face, hand interactions. In a cultural study of expressive automata and toys, the study considers the role of the sonic in puppetry: the rhythmic gesture, sound and voice.

I wish to explore: How do new and emerging technologies facilitate innovative techniques of design and control over puppet-like objects and create experiences of expressive play?

The human body in movement and the computational capture of such movement to make meaning through expressive acts mediate by evocative objects – is another way to state the primary exploration of the thesis.

"The world of objects and needs would thus be a world of general hysteria. Just as the organs and functions of a body in hysterical conversion become a gigantic paradigm which the symptom replaces and refers to, in consumption objects become a vast paradigm designating another language through which something else speaks," (Baudrillard:1988, 10-29)

Plan of Literature Review, Research, Practical Work and Writing
Below (FIG: GANTT), I supply a static image of a dynamic Gantt chart detailing an approximate time-plan of how the practical work relates to the evaluation of literary research material and writing activity.

The GANNT chart is a snapshot produced by an interactive project management tool, OmniPlan, and is dynamic. It changes as research, practice and writing activities develop. A current, zoomable, snapshot can be found here:

http://www.daisyrust.com/phd/dissertation_timeplan_current_ian_grant.png 



Establishing Cultural Patterns through Practice
I will produce and evaluate: 

(i) Performance, Artwork and Kinetic Behavioural Sculpture; 
(ii) Original software, software art and computer based creative production techniques; 
(iii) Innovative control systems and interfaces that will be applicable hopefully beyond the domain of performance; 
(iv) Papers, videos, websites and published outcomes that will contribute to the thesis, documentation and other elements of the research.

As a minimum, I plan to make and test: 
(i) Several expressive, physical objects; 
(ii) Several virtual objects, that are controlled by
(iii) Several innovative control systems, in prototype, including touch surfaces, optical motion/expression/gesture capture, digital input devices.

It will be ideal if the systems are tested in performance or in installation contexts. Standard HCI user testing methods, including focus groups, cognitive walkthroughs, Goals, Operators, Methods, and Selection Rules (GOMS) analysis, will be employed, evaluated and discussed where relevant.

Relationship to Previous Work: Comparative Case Studies of Practice
The thesis will analyse case study material, including software and performances by the author, other artists, puppeteers, animators and sculptors. Case studies include:

* Theo Jansen's Walker and his Staadcreatures: Covering ideas of Virtual Creatures, Object Orientedness, Automata and Computer Simulated Physics; Cybernetic Sculpture; See (Jansen 2007)


Figure 1: Theo Jansen Walker As Physical, Virtual and Textual (Code) Object
* Surfaces and Shadows: Synchronous and Asynchronous Techniques: 
	- Lotte Reiniger, Stop motion animator
* Phil Worthington “Shadow Monsters”. Animation and Computer Vision: AR installations;

Figure 2: Phil Worthington “Shadow Monsters” 2005
* Golan Levin: New media artist. (Reface [Portrait Sequencer] 2007), Snout (2008) Double-Taker (Snout) (2008) Opto-Isolator (2007) and other projects: face and expression recognition; automata and robotics; video hybrids;

  
Figure 3: Golan Levin - physical and virtual projects (2006-2009)
* Embodied Interactions: finger, hand, whole-body, gestural, eye and facial interactions;

Examples of how these areas map with my own work can be see in Ian Grant “Video Hybrids, explorations in digital puppetry”, presented as a performance piece, software and documentary write up (see sample chapter).

Dissertation Writing Practice
The thesis type-setting is automated using a LATEX template (created by the author using UEL standards) and a BIBTEX database. 

Registration Document References

Baudrillard, Jean and Mark. Poster. Selected writings (of) Jean Baudrillard. Stanford University Press, Stanford (Calif.), 1988.

Dewey, John. Art as Experience. Perigee Trade, 2005.

Dorosh, D. Patterning: The Informatics of Art and Fashion. PhD Thesis. UEL. Awarded in June 2008.

Geertz, Clifford.The Interpretation of Cultures. Basic Books, NY. 1977.

Jansen, Theo. The Great Pretender. 010 Uitgeverij, 2007.

Levin, Golan. Projects. http://www.flong.com/projects/ Date modified: 2009. Date Accessed: 1st June 2009.

Reas, Casey. Behavioral kinetic sculpture. Master’s thesis, MIT, 1996.

Turkle, Sherry. Computer Games As Evocative Objects: From Projective Screens. To Relational Artifacts  in Raessens, Joost and Jeffrey Goldstein “Handbook of Computer Game Studies” MIT Press, London, 2005.

Zielinski, Siegfried. Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. MIT Press, 2008.

Zielinski, Siegfried. Media Archaeology. http://www.ctheory.net/articles.aspx?id=42#text%201 Date modified: 7th Nov, 1996. Date Accessed: 1st June 2009.

For an extended abstract and sample writing, please see the sample chapters supplied for annual review AY08-09.

Current Research Bibliography
The current bibliographic database (BIBTEX) contains 180 references already considered in the literature review. For brevity, I only include references included in several important categories. An asterisk (*) indicates critical and key texts. There may be some repeated entries where items are cross-referenced. I (mostly) exclude instructional texts in computer technologies (e.g. software and programming text-books, e.g. iPhone programming, OpenGL Red Book, OpenCV, etc.). 

Puppetry, theory, history, technology, digital puppetry; 
[1]	Bacon, M. No Strings Attached: Inside Story of Jim Henson’s Creature Shop. Virgin Books, 1997.
[2]	Baird, B. The Art of the Puppet. With illustrations. Ridge Press, Inc., 1965.
[3]	Bar-Lev, A., Bruckstein, A. M., and Elber, G. Virtual marionettes: a system and paradigm for real-time 3d animation3d animation. The Visual Computer 21 (2005), 488–501.
[4]	Baran, I., and Popović, J. Automatic rigging and animation of 3d characters. Proceedings of the 2007 SIGGRAPH conference 26 (2007).
[5]	Barnes, C., Jacobs, D., Sanders, J., Goldman, D., Rusinkiewicz, S., Finkelstein, A., and Agrawala, M. Video puppetry: a performative interface for cutout animation. SIGGRAPH Asia ’08: SIGGRAPH Asia 2008 papers (Dec 2008).
[6]	Baudelaire, C. The philosophy of toys. In Essays on Dolls, I. Parry, Ed. Syrens, London, 1994.
[7] *	Bell, J. Strings, Hands, Shadows: A Modern Puppet History, illustrated edition ed. Wayne State University Press, 2000.
[8] *	Bell, J., Ed. Puppets, Masks, and Performing Objects. MIT Press, Boston, 2001.
[9]	Bicat, T. Puppets and Performing Objects: A Practical Guide. The Crowood Press Ltd, 2007.
[10]	Blumenthal, E. Puppetry and Puppets: An Illustrated World Survey. Thames &amp; Hudson, 2005.
[11]*Collective author. PUCK no. 9 :Images virtuelles Thematic review. Institut international de la Marionnette, CHARLEVILLE-MEZIERES, 1996.
[12]*Conner, S. Dumbstruck: A Cultural History of Ventriloquism. Oxford University Press, New York, 2000.
[13]	Currell, D. Puppets and Puppet Theatre. The Crowood Press Ltd, 1999.
[14]	Hall, V. Mike (the talking head). http://mambo.ucsc.edu/psl/mike.html, Date Created: nodate. Date Accessed: 01/02/2007.
[15]	Jurkowski, H. Aspects of Puppet Theatre. Puppet Centre Trust, 1988.
[16]	Jurkowski, H., and (Editor), P. F. A History of European Puppetry from Its Origins to the End of the 19th Century: Volume 1. Edwin Mellen Press Ltd, 1996.
[17]	Jurkowski, H., and (Editor), P. F. A History of European Puppetry: The Twentieth Century. Volume 2. Edwin Mellen Press Ltd, 1998.
[18]*Kaplin, S. A puppet tree: A model for the field of puppet theatre. TDR (1988-) 43 (Oct 1999), 28–35. Puppets, Masks, and Performing Objects.
[19]	Kaplin, S. A puppet tree - a model for the field of puppet theatre. In Puppets, Masks, and Performing Objects, J. Bell, Ed. MIT Press, Boston, 2001, pp. 18–25.
[20]*Knep, B., Hayes, C., Sayre, R., and Williams, T. Dinosaur input device. CHI ’95: Proceedings of the SIGCHI conference on Human factors in computing systems (May 1995).
[21]*Lecoq, J. The Moving Body: Le Corps Poetique, revised edition ed. Methuen Drama, 2002.
[22]	Mack, J. Masks: The Art of Expression, new edition ed. British Museum Press, 1996.
[23]	Magnenat-Thalmann, N. The making of a film with synthetic actors. Leonardo. Supplemental Issue 1 (Jan 1988), 55–62. Electronic Art.
[24]	Mazalek, A., and Nitsche, M. Tangible interfaces for real-time 3d virtual environments. ACE ’07: Proceedings of the international conference on Advances in computer entertainment technology (Jun 2007).
[25]*Meschke, M., and Sörenson, M. In Search of Aesthetics for the Puppet Theatre. Indira Gandhi National Centre for the Arts, New Delhi, 1992.
[26]	Ninomiya, D., Miyazaki, K., and Nakatsu, R. Networked virtual marionette theater. In Technologies for E-Learning and Digital Entertainment. Springer Berlin / Heidelberg, 2008.
[27]	Obraztsov, S. My Profession. Fredonia Books, 2001.
[28]	Parry, I. Essays on Dolls. Syrens, London, 1994.
[29]	Rilke, R. M. Dolls: On the wax dolls of lotte pritzel. In Essays on Dolls, I. Parry, Ed. Syrens, London, 1994.
[30]	Schönewolf, H. Play with Light and Shadow: Art and Techniques of Shadow Theatre. Littlehampton Book Services Ltd, 1969.
[31]*Segal, H. B. Pinocchio’s Progeny : Puppets, Marionettes, Automatons and Robots in Modernist and Avant-Garde Drama. Johns Hopkins University Press, Baltimore, 1995. Normal Loan 791.53 SEG.
[32]	Shi, X., Zhou, K., Tong, Y., Desbrun, M., Bao, H., and Guo, B. Mesh puppetry: cascading optimization of mesh deformation with inverse kinematics. International Conference on Computer Graphics and Interactive Techniques (2007).
[33]	Silk, D. William the Wonder-Kid: Plays, Puppet Plays, and Theater Writings. Sheep Meadow Press, U.S., 1996.
[34]	Tillis, S. Toward an Aesthetics of the Puppet: Puppetry as a theatrical art. Greenwood, New York, 1992. Normal Loan 791.53 TIL.
[35]*Tillis, S. The art of puppetry in the age of media production. In Puppets, Masks, and Performing Objects, J. Bell, Ed. MIT Press, Boston, 2001, pp. 172–183.

3D Graphics, Programming, Virtual and Augmented Reality
[1]*  Fiala, S. C. M. Augmented Reality: A Practical Guide: The Complete Guide to Understanding and Using Augmented Reality Technology. Pragmatic Bookshelf, 2008.
[2]	Gobbetti, E., Balaguer, J., and Thalmann, D. Vb2: an Architecture for Interaction in Synthetic Worlds. Proceedings of the 6th annual ACM symposium on User interface software and technology (1993), 167–178.
[3]	Govindaraju, V., Djeu, P., Sankaralingam, K., Vernon, M., and Mark, W. Toward a Multicore Architecture for Real-Time Ray-Tracing. MICRO ’08: Proceedings of the 2008 41st IEEE/ACM International Symposium on Microarchitecture (Nov 2008).
[4]	Junker, G. Pro OGRE 3D Programming (Expert’s Voice in Open Source). APress,US, 2006.
[5]* Mazalek, A., and Nitsche, M. Tangible interfaces for real-time 3d virtual environments. ACE ’07: Proceedings of the international conference on Advances in computer entertainment technology (Jun 2007).
[6]	Salti, S., Schreer, O., and Stefano, L. Real-time 3d arm pose estimation from monocular video for enhanced hci. VNBA ’08: Proceeding of the 1st ACM workshop on Vision networks for behavior analysis (Oct 2008).
[7]*	Zhai, S. Human Performance in Six Degree of Freedom Input Control. PhD thesis, University of Toronto, 1995.
Augmented Reality Techniques
[1]*	Billinghurst, M., Kato, H., and Poupyrev, I. Tangible augmented reality. SIGGRAPH Asia ’08: SIGGRAPH ASIA 2008 courses (Dec 2008).
[2]	McQuiggan, S., Rowe, J., and Lester, J. The effects of empathetic virtual characters on presence in narrative-centered learning environments. CHI 2008, April 5–10 Papers (2008).
[3]	Mistry, P., Maes, P., and Chang, L. Wuw - wear ur world: a wearable gestural interface. CHI EA ’09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).
[4]	Molyneaux, D., and Gellersen, H. Projected interfaces: enabling serendipitous interaction with smart tangible objects. TEI ’09: Proceedings of the 3rd International Conference on Tangible and Embedded Interaction (Feb 2009).
[5]*	Wang, R., and Popović, J. Real-Time Hand-Tracking with a Color Glove. SIGGRAPH ’09: SIGGRAPH 2009 papers (July 2009).

Digital Art, Objects, Virtuality and Performance
[1]* Blundell, B. G. An Introduction to Computer Graphics and Creative 3-D Environments. Springer, 2008.
[2]*	Candlin, F., and Guins, R. The Object Reader. Routledge, 2008.
[3]	Candy, L., and Edmonds, E. Explorations in Art and Technology: Intersections and Correspondence. Springer, 2002.
[4]*	Dixon, S. Digital Performance, annotated edition ed. The MIT Press, 2007.
[5]	Ede, S. Art and Science. I B Tauris &amp; Co Ltd, 2008.
[6]	Gabor, D. Technological civilisation and man’s future. In Cybernetics, art and ideas, J. Reichardt, Ed. Studio Vista, London, 1971, pp. 18–24.
[7]	Grau, O. Virtual Art: From Illusion to Immersion (Leonardo Book S.), rev sub ed. The MIT Press, 2003.
[8]*	Grau, O. MediaArtHistories. The MIT Press, 2007.
[9]*	Jansen, T. Theo Jansen: The Great Pretender. 010 Uitgeverij, 2007.
[10]	Mealing, S. Computers and Art, 2nd revised edition ed. Chicago University Press, 2008.
[11]	Reichardt, J., Ed. Cybernetics, art and ideas. Studio Vista, London, 1971.

Computer Vision Techiques for Digital Puppetry
[1]*	Barnes, C., Jacobs, D., Sanders, J., Goldman, D., Rusinkiewicz, S., Finkelstein, A., and Agrawala, M. Video puppetry: a performative interface for cutout animation. SIGGRAPH Asia ’08: SIGGRAPH Asia 2008 papers (Dec 2008).
[2]	Cabral, M., Morimoto, C., and Zuffo, M. On the Usability of Gesture Interfaces in Virtual Reality environments. CLIHC ’05: Proceedings of the 2005 Latin American conference on Human-computer interaction (Oct 2005).
[3]	Datcu, D., and Rothkrantz, L. Facial Expression Recognition in Still Pictures and Videos Using Active Appearance Models: a Comparison Approach. Proceedings of the 2007 international conference on Computer systems and technologies (2007).
[4]	Salti, S., Schreer, O., and Stefano, L. Real-time 3d arm pose estimation from monocular video for enhanced hci. VNBA ’08: Proceeding of the 1st ACM workshop on Vision networks for behavior analysis (Oct 2008).
[5]*	Vlasic, D., Baran, I., Matusik, W., and Popović, J. Articulated mesh animation from multi-view silhouettes. SIGGRAPH ’08: SIGGRAPH 2008 papers (Aug 2008).


Tangible Interaction and Embedded Control Technologies
[1]*	Barnes, C., Jacobs, D., Sanders, J., Goldman, D., Rusinkiewicz, S., Finkelstein, A., and Agrawala, M. Video puppetry: a performative interface for cutout animation. SIGGRAPH Asia ’08: SIGGRAPH Asia 2008 papers (Dec 2008).
[2]	Billinghurst, M., Kato, H., and Poupyrev, I. Tangible augmented reality. SIGGRAPH Asia ’08: SIGGRAPH ASIA 2008 courses (Dec 2008).
[3]	Fishkin, K. A Taxonomy for and Analysis of Tangible Interfaces. Personal and Ubiquitous Computing 8 (2004), 347–358.
[4]*	Guo, C., Young, J., and Sharlin, E. Touch and Toys: New Techniques for Interaction With a Remote Group of Robots. CHI ’09: Proceedings of the 27th international conference on Human factors in computing systems (Apr 2009).
[5]	Mazalek, A., and Nitsche, M. Tangible interfaces for real-time 3d virtual environments. ACE ’07: Proceedings of the international conference on Advances in computer entertainment technology (Jun 2007).
[6]	Mistry, P., Maes, P., and Chang, L. Wuw - wear ur world: a wearable gestural interface. CHI EA ’09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).
[7]	Molyneaux, D., and Gellersen, H. Projected interfaces: enabling serendipitous interaction with smart tangible objects. TEI ’09: Proceedings of the 3rd International Conference on Tangible and Embedded Interaction (Feb 2009).
[8]	Vertegaal, R., and Poupyrev, I. Eek!  a mouse!  organic user interfaces: tangible, transitive materials and programmable reality. CHI EA ’09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).
[9]	Weller, M., Do, E., and Gross, M. Posey: instrumenting a poseable hub and strut construction toy. TEI ’08: Proceedings of the 2nd international conference on Tangible and embedded interaction (Feb 2008).
[10]	Weller, M., Gross, M., and Do, E. Tangible sketching in 3d with posey. CHI EA ’09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).
[11]	Zaman, B., Abeele, V., Markopoulos, P., and Marshall, P. Tangibles for children,: the challenges. CHI EA ’09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).

Tangible Interaction and Embedded Control Technologies (interactive toys)
[1]	Newman, M. Interactive Barney: Good or evil?  Conferees worry about where computerized ’character’ toys are going next. http://www.post-gazette.com/businessnews/19990521barney1.asp, Date Modified: 21/05/1999. Date Accessed: 01/03/2007.
[2]	Parent, A. Read Reviews of Hasbro Aloha Stitch Doll 3570 at eOpinions. http://www.epinions.com/content_163285929604? linkin_id=8003929, Date Created: 28/11/2004. Date Accessed: 01/02/2007.
[3]	Shenk, D. Behold the Toys of Tomorrow (The Atlantic Online - Digital Culture). http://davidshenk.com/webimages/atlantic1.htm, Date Created: 07/01/1999. Date Accessed: 01/02/2007.
[4]	Strommen, E. F. When the Interface is a Talking Dinosaur: Learning Across Media with ActiMates Barney. http://www.playfulefforts.com/archives/papers/CHI-1998.pdf, Online PDF of published work. Date Written: 1998. Date Accessed: 01/03/2007.
[5]	Strommen, E. F. Learning from Television With Interactive Toy Characters As Viewing Companions. http://www.playfulefforts.com/archives/papers/SRCD-1999.pdf, Online PDF of published work. Date Written: 1999. Date Accessed: 01/03/2007.
[6]*	Strommen, E. F. Interactive Toy Characters as Interfaces For Children. http://www.playfulefforts.com/archives/papers/IA-2000.pdf, Online PDF of published work. Date Written: 2000. Date Accessed: 01/03/2007.
[7]	Strommen, E. F. Play?  Learning?  Both...or neither?  http://www.playfulefforts.com/archives/papers/AERA-2004.pdf, Online PDF of unpublished work. Date Written: 2004. Date Accessed: 01/03/2007.


Methods (Cultural Studies, Performance Studies and Theatre Anthropology)

[1]*	Candlin, F., and Guins, R. The Object Reader. Routledge, 2008.
[2]*	Eugenio Barba, N. S. A Dictionary of Theatre Anthropology: The Secret Art of the Performer. Routledge, 1991.
[3]*	Turkle, S. Evocative Objects: Things We Think with. The MIT Press, 2007.
[4]	Zielinski, S. Media Archaeology, November 1996.
[5]*    Zielinski, S. Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. MIT Press, 2008.



Summary of the elements of the investigation that are novel, original or creative and that may constitute production of original knowledge or an original interpretation of existing knowledge
- A deepening of the material and thought and cultural philosophy surrounding puppetry studies and technology;

- Original Performance, Artwork and Artefacts;

- Original Software products and creative production techniques;

-  New hardware control systems and interfaces for expressive play that will be applicable hopefully beyond the domain of performance and digital puppetry;

- Papers and published outcomes that will contribute to the thesis, documentation and other elements of the PhD.

Details of facilities available for the investigation, including fuinding and location
Self funded, private studio, work based workshop, teaching and seminar spaces at TVU
Relationship between work to be undertaken in the collaborating establishment and that to be undertaken at the sponsoring establishment or elsewhere (if relevant)
n/a
A Health and Safety audit is required in respect of all proposed laboratory experiments and/or fieldwork. 
Does this investigation require laboratory experiments and/or fieldwork? (Please Tick)
Yes

No
audience studies and user testing – not sustained fieldwork
If Yes, a mandatory copy of the audit, signed by the Student and the Director of Studies, is attached (Please Tick)
Yes

No

Does the programme of research involve work that would require the approval of the University’s Ethics Committee prior to that part of the project commencing? (Please Tick)
Yes

No
x
If Yes, has a form for approval already been made to the University’s Ethics Committee?  (Please Tick) 
You may not proceed with this part of your research until approval has been granted.
Yes

No

Will the programme of research lead to output(s) which will have commercial form(s) and/or intellectual property of potential value? 
If yes, you must  inform the Knowledge Transfer Office
Yes
Some original software but all is based on open-source principles so will share-and-share-alike (using creative commons licenses)
No


3. Proposed Supervisory Team
Form SDN - Nomination as a Supervisor/Director of Studies should be appended for the Director of Studies and each supervisor nominated. The RDSC will use information supplied about current and past supervisions to ensure that this proposal does not breach the maximum number of PGR students that can be supervised without consent.

Nomination of Director of Studies
Name and title
Dr Leslie Hill
School
SmartLab Digital Media Institute / School of Computing, Information Technology and Engineering
Email address (preferably an institutional or official email address)
leslie@smartlab.uk.com
Holds a PhD/Professional Doctorate? (Please Tick)
Yes
x
No

Number of UK HEI research degree students currently supervised (excluding this one)
MPhil

Professional Doctorate


PhD including PhD (Eur)
8
Practitioner Doctorate

Number of UK HEI research degree students successfully supervised to completion for
MPhil

Professional Doctorate


PhD including PhD (Eur)
6
Practitioner Doctorate


Nomination of Second Supervisor
Name and title
Dr Esther MacCallum-Stewart
School (and for supervisors external to UEL, their institution and full postal address)
SmartLab Digital Media Institute / School of Computing, Information Technology and Engineering
Email address (preferably an institutional or official email address)
esther@smartlab.uk.com
Holds a PhD/Professional Doctorate? (Please Tick)
Yes
x
No

Number of UK HEI research degree students currently supervised (excluding this one)
MPhil

Professional Doctorate


PhD including PhD (Eur)
4
Practitioner Doctorate

Number of UK HEI research degree students successfully supervised to completion for
MPhil

Professional Doctorate


PhD including PhD (Eur)

Practitioner Doctorate


Nomination of Third Supervisor (if applicable)
Name and title

School (and for supervisors external to UEL, their institution and full postal address)

Email address (preferably an institutional or official email address)

Holds a PhD/Professional Doctorate? (Please Tick)
Yes

No

Number of UK HEI research degree students currently supervised (excluding this one)
MPhil

Professional Doctorate


PhD including PhD (Eur)

Practitioner Doctorate

Number of UK HEI research degree students successfully supervised to completion for
MPhil

Professional Doctorate


PhD including PhD (Eur)

Practitioner Doctorate

 

Overall supervisory experience and activity of the Proposed Supervisory Team
Number of UK HEI research degree students currently supervised (excluding this one)
MPhil

Professional Doctorate


PhD including PhD (Eur)
12
Practitioner Doctorate

Number of UK HEI research degree students previously successfully supervised to completion for
MPhil

Professional Doctorate


PhD including PhD (Eur)
6
Practitioner Doctorate

If the combined experience and activity of the proposed supervisory team does not meetthe requirements stipulated in UEL’s Research Degree Regulations, please provide a short statement justifying why consent is sought and why this particular supervisory team is most suitable for the programme of research.


4. Nomination of Advisors

Nomination of First Advisor, if applicable
Name and title

Current position, department and institution

Postal address 

Email address (preferably an institutional or official email address)

Previous posts held 

Qualifications

Nomination of Second Advisor, if applicable
Name and title

Current position, department and institution

Postal Address 

Email address (preferably an institutional or official email address)

Previous posts held 

Qualifications



5. Student’s Declaration

I Confirm
	•	that I wish to apply to be registered as a student for the postgraduate research award indicated at the head of this form. 
	•	that the particulars given in this form are correct.
	•	that except with the specific permission of the PGR Review Sub-Committee, any written component of the programme must be submitted in English and I must also undertake an oral examination in English.

Student IAN GRANT
Signed:

Date:  28 Aug 2009

6. Supervisory Team’s declaration


We confirm 
	•	that we support this form and believe that the Student has the potential to complete the programme of work proposed
	•	that, as required by our University’s research degree regulations, we are not ourselves currently receiving supervision on a research degree programme at any HEI
	•	that, if applicable, we agree to the request for the backdating of registration

We recommend that the applicant be registered for a research degree

Director of Studies
 Signed:

Printed:
Date:
Second Supervisor
Signed:

Printed:
Date:
Third Supervisor  (if applicable)
Signed:

Printed:
Date:








7. Academic Referees
Please nominate two referees to whom the School Research Degrees Sub-Committee may refer for advice. One nominee must be external to the Research Degrees Sub-Committee.


First Referee
Name and title
Dr Helen Paris, Reader
Postal address 
Drama Dept, School of Arts, Brunel University
Uxbridge, Middlesex UB8 3PH
Email address (preferably an institutional or official email address)
helen.paris@brunel.ac.uk
Second Referee
Name and title
Jeremy Gardiner, Professor of Digital Arts
Postal address 
School of Art and Design, TVU
Grove House 
1 The Grove 
Ealing, W5 5DX
Email address (preferably an institutional or official email address)
Jeremy.gardiner@tvu.ac.uk
8. Dean of School’s declaration


I confirm that the University and School facilities and resources detailed in this form, together with other appropriate resources, such as supervisor(s)’ Time, will be available for the duration of the programme of research

Dean of School (or nominee)
 Signed:

Printed:
Date:

9. Document Log
Once sections 1-8 have been completed, this form, along with form(s) SDN, should be submitted to the relevant School RDSC for consideration and the reference for the pertinent minute(s) recorded below. If recommended, the form and form(s) should then be sent to the PGR Review Sub-committee for approval along with the relevant extract from the minutes (either attached to this form or pasted at its end). Please note that the form will not be processed until the Graduate School has received the minutes of both committees in their entirety. 

Recommended by School RDSC
 Date of RDSC: 

Minute ref:
Approved by PGR Review Sub-committee
Date of PGR Review:

Minute ref:



























Nomination as a Supervisor/Director of Studies 
Short Curriculum Vitae Form
(to be completed by the nominated individual)


In completing this form you should refer to the relevant sections of the Research Degree Regulations (Part 9 of the UEL Manual of General Regulations) and the UEL Code of Practice for Postgraduate Research Programmes.

This form should be typewritten wherever possible. 

Confirmation of appointment will be sent to the student’s and the supervisory team’s email addresses.

This form, to accompany Form REG: Application to Register for a Research Degree Programme or Form CSA: Application for a Change in the Supervisory Team, is to be submitted to the nominated individual in the School - usually the Research Administrator or Officer to the RDSC. 


1. Supervisor Type

Is the nomination for a Supervisor or Director of Studies? (Please Tick)
Supervisor

Director of Studies
x


2. Proposed Supervisor’s Details

Name and title
Dr Leslie Hill
Current position, Department and Institution
Principle Research Fellow UEL SMARTLab
Previous posts held 
Director Curious International Ltd 1996-present
NESTA Dream Time Fellow 2003-2005
Head of Media, London College of Music and Media, Thames Valley University
Sr. Lecturer / Artist Fellow, Institute for Studies in the Arts, Arizona State University 1997-2000

Email address (preferably an institutional or official email address)
leslie@smartlab.co.uk


3. Proposed Supervisor’s Qualifications and Experience

Qualifications
PhD Representations of Women in British Drama 1890-1817, University of Glasgow
MA Shakespeare Text &amp; Performance, Shakespeare Institute, Stratford Upon Avon, University of Birmingham
Double BA English &amp; Philosophy Magna Cum Laude, University of New Mexico

Current research or professional practice
Socially engaged art projects, performance, filmmaking, ‘sci-art’ projects, biofeedback interfaces.  Academic writing in field of Performance Studies

the proposed supervisor is not themselves being supervised on a research degree programme at any HEI (Please Tick)
Yes

No X

Please list up to six publications which are of most relevance to this proposal
Publication 1
Leslie Hill &amp; Helen Paris, Performance and Place, (London: Palgrave MacMillan) 2006.
Publication 2
Leslie Hill &amp; Helen Paris, ‘Curious Feminists’ in Feminist Futures, Elaine Aston and Geraldine Harris (eds.), (London: Palgrave Macmillan) 2006.

Publication 3
Leslie Hill, ‘Red Lantern House’ in China Live: Reflections on Contemporary Performance Art, (London: Live Art UK) 2005.
Publication 4
Leslie Hill and Helen Paris, ‘One to One’ in We Love You: Reflections on Audiences (London: Goethe Institute and ACE) 2005.
Publication 5
Leslie Hill &amp; Helen Paris with Dr Upinder Bhalla, ‘On the Scent’ in Talking Back to Science: Art, Science and The Personal Bergit Arends and Verity Slater (eds.), (London: The Wellcome Trust) 2004.
Publication 6
Leslie Hill &amp; Helen Paris, 'On the Scent', Performance Research, (Cambridge: Routledge) September, 2003.


4. Proposed supervisor’s Confirmation

I confirm that the above is a true and accurate record of my experience and qualifications
Signed:
Printed:
Date:






































Nomination as a Supervisor/Director of Studies 
Short Curriculum Vitae Form
(to be completed by the nominated individual)


In completing this form you should refer to the relevant sections of the Research Degree Regulations (Part 9 of the UEL Manual of General Regulations) and the UEL Code of Practice for Postgraduate Research Programmes.

This form should be typewritten wherever possible. 

Confirmation of appointment will be sent to the student’s and the supervisory team’s email addresses.

This form, to accompany Form REG: Application to Register for a Research Degree Programme or Form CSA: Application for a Change in the Supervisory Team, is to be submitted to the nominated individual in the School - usually the Research Administrator or Officer to the RDSC. 


1. Supervisor Type

Is the nomination for a Supervisor or Director of Studies? (Please Tick)
Supervisor
x
Director of Studies



2. Proposed Supervisor’s Details

Name and title
Dr Esther MacCallum-Stewart
Current position, Department and Institution
Postdoctoral Research Fellow, Games and Interactive Media, SMARTlab UEL.
Previous posts held 
Associate Tutor, University of Sussex (2001-2006)
Researcher, University of Sussex (2002-2006)
Email address (preferably an institutional or official email address)
neveah@gmail.com


3. Proposed Supervisor’s Qualifications and Experience

Qualifications
PhD. ‘The First World War and Popular Culture’ (University of Sussex)
2006
BA English Literature and Cultural Studies. (1st Class)
2000
Current research or professional practice
Postdoctoral Research Fellow, Games and Interactive Media, SMARTlab UEL: digital games, players, player and game narratives and tribal behaviour. Along with Microsoft community affairs  at UELI am also developing the concept of Stealth Learning in online games
Board Member: DiGRA (Digital Games and Research Association)
the proposed supervisor is not themselves being supervised on a research degree programme at any HEI (Please Tick)
Yes

No
x
Please list up to six publications which are of most relevance to this proposal
Publication 1
Tribal Fusions: Social Worlds, Players and Games. This book investigates the role of players and groups in online worlds, their behaviour as active agents in a developing narrative space and how the represent themselves online. (this is the project I am currently working on. Manchester University Press are reviewing the proposal. 50 000 words have been written so far)

Publication 2
Stealth Learning and the Digital Dividend (MIT Press) (Badshah, Goodman and MacCallum-Stewart) Part of the (E)Mergencies series, this book discusses how players can learn in online environments by drawing on an increasingly rich backdrop of virtual narrative experience. (accepted and first draft is completed - goes to press August 2009) 

Publication 3
Digital Culture, Play and Identity; A World of Warcraft Reader
MIT Press: ISBN: 0262033704 (2008)
Two chapters in this:

‘Never Such Innocence Again’, War and Histories in World of Warcraft 

The Playing of Roles: How does roleplay affect gameplay in  World of Warcraft? (with Justin Parsler, The University of Brunel)

Publication 4
‘Real Men Carry Girly Epics – Normalising Gender Bending in Online Games
Eludamos Vol 1 #2 (2008)
http://www.eludamos.org/index.php/eludamos/article/view/35/54

Publication 5
The International Journal of Performance Arts and Digital Media Vol 3 #2.1 (2007)
‘The Warfare of Imagined Homes – Building Identities in Second Life’ 
ISSN: 14794713

Publication 6
Lost on a Desert Island – Convergence Texts and Digital Games.
Games and Culture Vol 4 #3 2008. (This may change to vol 5 #1 depending on a preceding special edition)

4. Proposed supervisor’s Confirmation

I confirm that the above is a true and accurate record of my experience and qualifications
Signed: 
Printed:
Date: 



</Text>
        </Document>
        <Document ID="183">
            <Title>Evaluation of questions, objectives and aims</Title>
            <Text>10.4 Evaluation of questions, objectives and aims
</Text>
        </Document>
        <Document ID="217">
            <Title>figure_008_hot_minnie</Title>
        </Document>
        <Document ID="106">
            <Title>6.2.6 Computer Vision</Title>
        </Document>
        <Document ID="439">
            <Title>Sally Jane Norman</Title>
            <Text>Sally Jane Norman

http://vimeo.com/3393021
Sally Jane Norman - Music and Machines IX

Contexts

Taxonomies of Gesture As Encodings of Resistance?

subject object instrument constraints of triangulations

negotiation of relational dynamics


Polynesia - Internet "The weaving together of people"

Relational weaves - collaborating

aethestic experience or action
aesthetic event


Wool

Emergence of the indivuated event phenomena

See hand written notes

instruments

"Puppeteers are a particular breed of performing artist who have a particular relationship to an interface, which is obviously imbued with agency. I am not talking necessarily about anthropomorphic interfaces - I am talking about sometimes very abstract puppeteering systems. So you've got relics of instrumental gesture that will haunt the mess of motor-codes that we are using to play todays instruments. "

she goes on to talk about:


relics of instrumental habits / vestiges



Rabardel, P:

On artefacts: instrumentalisation / instrumentation

"Verillion and Rabardel works in a sociocultural tradition, from the assumption that artefacts mediate and shape human agency, the scope of their work is human computer interaction." From http://www.geogebra.org/publications/2008-Misfeldt-Cerme6.pdf
SEMIOTIC INSTRUMENTS: CONSIDERING TECHNOLOGY AND REPRESENTATIONS AS COMPLEMENTARY
Morten Misfeldt



Sally Jane Norman - Digitally Mediated Performance - 13.12.07

&lt;http://video.google.com/videoplay?docid=-4224459561267792302#>



Distributed spaces

---
BELOW FROM: re:mote auckland :: sally jane norman (2005)
http://www.archive.org/details/sally

motion capture workshop (pre internet)charleville
15 different cultural geo origins: and "when you started to team people up to do a collective manipulation of a puppet which could of been themselves  a sort virtual entity that they were constructing as a tandem or if there were three of them, as in the Japanese Bunraku technique, then what was quite staggeringly obvious was the cultural rhythms that were invested in their gestures. 

- myth of the universal gestural rhythms 
- what is interesting to play with is the difference...

idiosyncrasies 
harvest them!
share them!
resist homogenisation...

robotic theatre


audio lab

differences in scale - what is intimate space, shared space,  geographically distributed, cosmically distributed, 
Being WITNESS to 

industrial ventures...

remote piloting of space craft - a form of pupeeteering...
---



Sally Jane Norman Director, CultureLab, University of Newcastle http://www.ncl.ac.uk/niassh/culturelab/ http://www.ncl.ac.uk/niassh/culturelab/sjn.htm Doctor Norman's research spans performing arts, cultural theory, and technology. She has served many organisations dedicated to transdisciplinary research, including the Daniel Langlois Foundation in Montreal, the "Vida" Artificial Life Competition organised by Telefonica in Madrid, the Scientific Council of the Fine Arts Division of the French Ministry of Culture, and the Hexagram Creation and Research Committee in Montreal.  Sally Jane Norman was born in Aotearoa/ New Zealand in 1953, where she studied music and dance, and earned Bachelor and Master of Arts degrees from the University of Canterbury. She pursued her research in Paris with a Doctorat de 3ème cycle (PhD) at the Institut d'Etudes théâtrales, Université de Paris III (1980), and a Doctorat d'état ès Lettres et Sciences humaines (1990). Parallel to French and English publications including studies for UNESCO and the French Ministry of Culture, Doctor Norman has run live performance workshops with digital tools at the International Institute of Puppetry in Charleville-Mézières, Studio for Electro-Instrumental Music (STEIM) in Amsterdam, Zentrum für Kunst und Medientechnologie (ZKM) in Karlsruhe, and European Festival of Digital Creation in Valenciennes. She organised the 1992 New Images and Museology Conference at the Louvre, and co-organised STEIM's 1998 TOUCH Festival and Symposium in Amsterdam, and IRCAM's Dance and Technology session at the 2003 Resonance Festival. Doctor Norman worked on EU R+D projects at the ZKM from 1996-1998 and became Director of the Angoulême site of the Ecole supérieure de l'image (ESI) in 1999. Nominated Director General of the school's two sites (Angoulême/ Poitiers) in 2001, she launched ESI's pioneering Digital Arts Doctoral Programme. 

Sally Jane Norman (Newcastle University)
Sally Jane’s background and interests are in live performance, art &amp; technology, and interdisciplinary research. She funded her Doctorat d'état (Institut d’études théâtrales, Paris III) by working as a scientific translator. After authoring studies on new media for UNESCO and the French Ministry of Culture, and leading a major digital images conference at the Louvre, she worked on EU Framework projects at the Zentrum für Kunst und Medientechnologie in Karlsruhe. In parallel, she developed performance technology initiatives as research director of the International Institute of Puppetry in Charleville-Mézières and artistic co-director of the Studio for Electro-Instrumental Music (STEIM) in Amsterdam. As Director General of the Ecole supérieure de l'image (Angoulême/ Poitiers), she subsequently secured a multi-million euro funding contract and launched a practice-based Digital Arts PhD with Poitiers University. Since autumn 04, as founding director of Culture Lab, a £4M SRIFfunded interdisciplinary digital laboratory working with Newcastle’s three faculties (Humanities, Science, Medicine), her role is to seed and host a wide range of interdisciplinary research projects.

Member of the AHRC Knowledge Transfer Peer Review Panel, she lead Newcastle input to an FP6 EU creative software project, is Co-Investigator on an ESRC project on sustainable business models in the creative industries, and was Principal Investigator on an EPSRC-AHRC e-science pilot (performing arts, bioengineering, computing). Funding and research bodies for which she has ensured consultancy include the Swedish Strategic Research Foundation, Hexagram, Daniel Langlois Foundation and Fonds québécois de recherche sur la société in Canada, Telefonica Foundation in Madrid, Netherlands Quality Agency (post-graduate arts programmes), and the Portuguese Foundation for Science and Technology. As a stubborn believer in the power of collaborative, interdisciplinary energies to spearhead innovative cultural and technological processes, she tends to naturally work in unclassifiable discomfort zones.


BELOW SOURCE: http://www.m-cult.org/performingplaces/presenters/norman.htm (Nov 2006)

PERFORMING PLACES - MEDIA AND EMBODIMENT IN THE URBAN ENVIRONMENT
Helsinki


SALLY JANE NORMAN

Director, Culture Lab, Newcastle University (UK)

Theatres of Individuation: Shaping Shared Aesthetic Experience

Gilbert Simondon describes individuation as a permanent process in the living being since, contrary to inorganic entities, the living being is not just the result but the theatre of individuation. This statement is taken as a starting point to define theatre as a metastable system rich in different potentials (form, matter, energy), whose mediation yields a particular kind of aesthetic experience, namely, a celebration of shared space and time.

Theatre designates both a physical location, and the live performance activity this location hosts. It is characterised by permanent renegotiation of the temporal and spatial boundaries on which individuation of all live art depends. This paper relates selected historical cases and theories of theatrical individuation to current performance experiments in technologically innervated urban settings. Mediaeval encounter-patterns and other mobile theatre forms, which implement different kinds of boundaries to those employed in dedicated architectural edifices, provide a backdrop for descriptions of current experiments producing technologically layered, wilfully ambivalent spaces that lend themselves to multiple aesthetic readings.

In conclusion, shared aesthetic experience in our massively extended sensorium is seen to need new shaping powers, to heighten moments of metasynchronicity and proxemics of metagesture that can inform future theatres of individuation.


Sally Jane Norman , citizen of Aotearoa/ New Zealand and France, is a cultural theorist/ practitioner whose research is focused on live art and technology, author of studies for UNESCO, the French Ministry of Culture, and the French National Scientific Research Centre (CNRS). Docteur d' é tat (Paris III), co-/organiser of workshops, performances, and seminars exploring human interactions in digital environments at institutions including the International Institute of Puppetry - Charleville-M é zi è res, Zentrum für Kunst und Medientechnologie - Karlsruhe, Studio for Electro-Instrumental Music - Amsterdam, IRCAM - Paris. Engaged on EU Framework projects since 1997. From 2001-2004, Director General of the Ecole sup é rieure de l'image (Angoulême/ Poitiers). Currently Director of Culture Lab, a new interdisciplinary research facility at Newcastle University which hosts creative practice-led collaborations that extend and challenge uses of digital tools.  

http://www.ncl.ac.uk/culturelab




</Text>
        </Document>
        <Document ID="328">
            <Title>A Description of Shadow Play</Title>
            <Text>[check out][#schonewolf69]

[Currell (2007)][#currell07]

The domain of puppetry is a broad area and reflects performance and artefacts of vastly variated form and style.

Animated objects or things, and bringing an illusion of agency or life-likeness,  is at the heart of all puppetry. Live control of performing objects and the co-presence of the operator / puppeteer is also key to most established definitions of the puppet. This brings a fundamental exclusion, in definition, of recorded film animation techniques and contemporary acts of digital animation

However, traditional shadow craft has been a prominent element in recent research and papers surrounding computer graphics and real-time systems. [LIST THEM WITH A SHORT PRECIS]

[Tan kian lam (2008)][#Tan-Kian-Lam:2008uq] Real-Time Visual Simulation and Interactive Animation of Shadow Play Puppets Using OpenGL
[Li and Hsu (2007)][#Li:2007p1864] - An Authoring Tool for Generating Shadow Play Animations With Motion Planning Techniques.

[Mitra and Pauly (2009)][#Mitra:2009p2019] Shadow Art
[Saltz (2004-2005)][#Saltz:2004ly] Virtual Vaudeville projects

This goes hand in hand with the computer simulation of light and shadows and virtual space.

[Jurkowski (1996)][#jurkowski96] strikes a set of distinctions 

: "Human imagination can give life to all simulacra... . Another form of animation has a physical dimension, and refers to indirect (mechanical) or direct (human) manipulation. Although mechanical figures are sometimes termed "puppets", the puppet in its pure form is a figure manipulated by direct human agency, that is, by the puppeteer.
The digital practices I'm researching and discovering are hybrid and often combine elements of pre-made animation, procedural computer generated animation (e.g. physics simulation animating 'rigidbodies', or the 'motion planning' approaches of [Li and Hsu (2007)][#Li:2007p1864] and, relatively unique to my current project, the role of  real-time puppeteer controlled animation for performance, mediated by computer software.

The term 'performance animation' attempts to capture the hybrid disciplines that challenge 

IS A PROJECTION / PROJECTED IMAGE A SHADOW?
Fundamentally yes.

:"A broad definition of a shadow puppet would be: a two-dimensional figure shown against a semi-translucent screen and lit so that an audience on the opposite side of the screen can see the shadow of the figure only" [p.139][#Wright:1986kx]

The design manufacture and operation of any puppet is key to it's expressive qualities

For figurative silhouettes, the posture, gesture and poise, 

Aesthetic qualities of the object itself (translucency, colour, cutting and finish)

For the purposes of comparison of shadow puppets

hinges, joints and constraints to movement
use of torque gravity and friction for animation

rods- stability, control, points/centres of force
3 rods, some on joints, moveable parts of the puppet, others fixed

use of light 

a fusion of primary, secondary (sometimes more) points of shadow-casting illumination - creates complex multiple shadows, umbra and penumbras for the same object

lighting effects, 

outdoor flame as shadow-casting  - in wayang / java - spiritual contexts for
the flickering flame casts an subtly animated shadow

Some shadow puppet traditions (NAME THEM) have control rods with pins, or sharp protuberances on the ends enabling them to be removed, while an object is in use, and stuck in other parts of the figure, enabling more subtle and flexible control over movement.

[mise-en-scene]

An emerging theme: puppetry (as an artform) has been 'on the brink of belonging' an outsiders pursuit. Research papers on digital forms of shadow theatre herald the demise of live, traditional craft based pursuits: 

:"The art of shadow play is slowly disappearing simply due to the lack of interest in the younger generation. There is a need for us to promote and provide greater accessibility to, and preserve this masterpiece using present technologies such as digital media." [SOURCE]

So digital media is seen as part of the solution, rather than part of the problem live forms face.

Digital Analogies

Constructional / functional histories of puppetry often miss or underplay the wider cultural contexts. I've grouped the thesis into the traditional technical (shadows for example) only to emphasise parallel, associated phenomenon in the digital domain. Firmly placing a consideration of puppetry into a radical  'multidisciplinarity', like Jurkowski, as simulacra, as theatre, as tools or instruments for human expression.</Text>
        </Document>
        <Document ID="295">
            <Title>Successful Upgrade Text</Title>
            <Text>Expressivity and the Digital Puppet:
Mechanical, Digital and Virtual Objects
in Games, Art and Performance

Aim of the Investigation
The current study explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

In this context 'innovative' means both emerging, new, technology or established technologies that are being re-defined by their communities of use and are finding new applications within the performing arts, particularly puppetry performance.

I aim to explore the related contexts of digital puppetry, real-time animation, mimetic and non-mimetic kinetic objects, automata, 'cybernetic sculpture', performance systems and the technological interfaces to such phenomena. 

I aim to create evaluate and create puppet/object theatre performances/installations that use original software and hardware systems that are designed to explore 'performance expressivity', with reference to relevant historical, art, entertainment and technological precedents.

I wish to theorise and form a taxonomy of 'expressivity' in relationship to digital domains and puppetry. By 'expressivity', I refer to different domains of action including: voice, face, body, hands and gesture.

Scope and Focus: finding patterns in an interdisciplinary study

The scope may first appear broad and the research focus wide. However, the researcher agrees with the statement:

"a cross-disciplinary approach yields information about patterning that is not visible from any single discipline." (Dorosh, 2008, 14, my emphasis)

Initially the surveyed areas and practice are diverse. The research progresses with the solid intent to articulate the patterns between (and the inter-relatedness of) cultural forms, and control systems, across computer media and performance art disciplines. In forming a taxonomy of 'expressivity' in relationship to digital control-systems and puppetry, a broad range of phenomena will need to be studied.

Focus is to be achieved through the practical work and case studies detailed in the 'Proposed Plan of Research', below.

Details of your proposed research in lay terms
Many innovations in contemporary computing and the way we interact with machines have applications beyond the domains for which they are designed.

Computer vision for gesture recognition, touch surfaces (like the Apple's iPhone, Microsoft's Surface and TUIO systems), wireless control devices, accelerometers and game control devices, like data gloves, can be used beyond their originally designed purpose. Increasingly these technologies and the means to programme them are finding their way into the hands of non-specialists and are crossing boundaries of practice. The present study works in an interdisciplinary way between human-computer interaction/interface design, computer programming and the performance practice of puppetry, evaluating the reciprocal opportunity for innovative practice.

In my practice and research, I aim to create low-cost hardware and software that allows a skilled and unskilled puppeteers to control dynamic physical objects (like a robot) or a virtual object (like a game character) in ways that calibrate and test new interfaces for their expressive potential. Coming from a background in puppetry, where a special approach to gesture, movement and mechanism applies, I wish to find a playful fusion between emerging technology and performance traditions.

The established puppetry traditions, as categorised by control mechanisms and form: rods, shadows and glove, (one may add the direct manipulation of objects and the related area of toys and automata), have fascinating parallels with moving objects in the virtual worlds of interactive art and games. The thesis will clearly establish and test the boundaries of these parallel forms.


Proposed plan of work, including its relationship to previous work
Focus and Scope  
The proposed study is interdisciplinary and seeks to establish patterns between diverse cultural forms and technological practices. The focus for the study is created by analysing specific case studies in practice that involve tactile and gestural interaction with interfaces for expression and improvisation.

The current work has a practical, experimental and media archeological approach that seeks to explores the interface between traditional puppetry and emerging computer technologies, through historical, theoretical enquiry, case studies and practical experiments. The thesis will evaluate and test with users (puppeteers, audiences, animators and programmers) the expressive qualities of innovative interactive systems.

Although the area seems broad (for example, puppetry is a large subject within performance studies, has numerous world traditions and a long history), I find focus through particular practical case studies where I design and test interfaces for expression and improvisation centered around gestural interaction, touch, optical and physical capture of motion.

Puppetry has distinct practices in various world traditions, but in computer space can be defined as the expressive use of interfaces to control objects. It is necessary, for the thesis, to offer an extended definition of 'object', which becomes interesting in computational contexts as does 'expressive behavior' and 'affect'.

In exploration of practices akin to puppetry, the research draws on instances of performance, games and installation art practice in wider cultural practice and the practical explorations of the author. This focus is multi-disciplinary. 

The case studies are focused in the following ways: 

(1) Physical interfaces to on-screen (or virtual) performing and expressive objects and 
(2) Physical control of physical performing and expressive objects. I foresee multiple interfaces and approaches to objects will be designed, tested and used and may be shared and adapted between case studies.

Research Questions
Through practical, theoretical and historical research I wish to explore and answer the following research questions:

Primary Question: What is 'expressivity' in the context of computer controlled and user controlled physical and virtual objects?

Three main overarching questions
(1) How do new and emerging technologies facilitate innovative techniques of design and control of puppet-like objects?

(2) What are the most effective designs for interactive tools to create and sustain experiences of expressive play?

(3) How do the domains of traditional puppetry and emerging interactive technologies relate?

Sub-Questions: Definitions and Detail
The following questions further refine and dimensionalise the key research questions.

What defines a digital puppet?
What defines a virtual puppet?
Can we describe the distinctions between virtual and physical kinetic expressive systems?
     - What is the difference between an automata and a puppet in the context of virtual 
       creatures, animatronics (performance robotics) and artificial-life?

How do current performance technologies facilitate expression?

What is 'expressivity' in the context of computer controlled objects?

What happens to 'expressive acts' when mediated through interactive technology? Eg.
	- Gestural
	- Vocal
	- Whole Body (Viscerality and the wider sensorium of the body etc.)
	- Facial
	- Kinetics, movement and touch

What are the qualitative distinctions between live and captured movement?
How can we use movement capture as an analytic tool within puppetry studies?
How can we use movement capture as a tool for play within puppet performance?
What are the most effective methodologies for studying interdisciplinary puppetry practice?
To what extent is it useful to compare diverse virtual and physical kinetic forms as forms of digital puppetry?

New Interfaces for Expression
What are the most effective interactive interfaces to capture the expressive acts of puppeteers?

How do we best describe the 'expressive potential' of custom interfaces for puppetry?

What are the perceptions of traditional puppeteers towards new technology and new interfaces for expression?

 
Methods and Methodologies – A Media Archeology of Kinetic Behavioural Sculpture
The writing seeks to historicise in and around the spaces occupied by digital (or virtual) puppets, the avatar, automata, the robot, artificial life, and the animatronic. All these phenomena can be grouped, as by Reas (1996), in the realm of 'kinetic behavioural sculpture':

"A behavioral kinetic object is a dynamic system, meaning it changes with time. This system is composed of a source of energy, inputs, outputs, and a control architecture which converts the information from the inputs into information which stimulates the outputs. These elements sum to form the complete object, but other elements may be added to provide mass or form." (Reas 1996, 22)

An emerging methodological approach called media archeology, will be used to trace how traditional puppetry forms, often described by their mode of interaction/media, (rods, shadows, strings and gloves) - map into current paradigms of interaction with virtual worlds, digital objects, games, and automated animation; and associated areas of digitally enhanced dolls, toys and automata. Media Archeology is a field that reflects on today's technologies by linking them to the socio-technical histories out of which they emerged.

"There is a gang of artists, theoreticians, and artist-theoreticians who have a very strong affinity (moreover, one that links them to a figure such as Artaud): they burn and burn up in the endeavour to push out as far as possible the limits of what language and machines, as the primary instances of structure and order for the last few centuries, are able to express and in doing so to actually reveal these limits" (Siegfried Zielinski, 2009, my emphasis)

Ethnographic Methods - Thick Description of Performance and User Testing
My experience in ethnographic methodology also assists to bring the study within a particular social, cultural way of seeing.  How to systematically observe and perform thick description (Geertz, 1977) of contexts such as software design and study is relatively unique, but common in anthropology when interpreting performance culture and forms, such as puppetry.

Media archeology, in the way documents and artefacts are studied has an affinity with historical ethnography. The moment of study is past as well as present. I will apply ethnographic methods of theme analysis – observation, thick description, coding and dimensionalising -  to support a broader semiotic approach to the cultures and histories of digital puppetry.

"The concept of culture I espouse…is essentially a semiotic one. Believing with Max Weber, that a man [sic] is an animal suspended in webs of significance he himself has spun. I take culture to be those webs, and the analysis of it to be therefore not an experimental science in search of law but an interpretive one in search of meaning." (Geertz, 1977, 5)

Sherry Turkle (2005) has developed an approach to understand how we think and express through evocative objects. I argue digital puppets are a special class of evocative object. In addition to Turkle, I will consider multiple approaches to theorising the object in cultural practice: again this is drawn from a variety of domains (e.g. John Dewey's expressive object in Art as Experience (Dewey, 2005) to Baudrillard's complex system of objects (Baudrillard, 1988). I have started this work in relation to automata and talking toys (see the published works).

Puppetry, Kinetic Sculpture and Gestural Interaction
All styles of puppet manipulation rely on gestural interaction. The study considers the performative/expressive potential of numerous computer interaction systems that utilise tactility and touch, whole-body, face, hand interactions. In a cultural study of expressive automata and toys, the study considers the role of the sonic in puppetry: the rhythmic gesture, sound and voice.

I wish to explore: How do new and emerging technologies facilitate innovative techniques of design and control over puppet-like objects and create experiences of expressive play?

The human body in movement and the computational capture of such movement to make meaning through expressive acts mediate by evocative objects – is another way to state the primary exploration of the thesis.

"The world of objects and needs would thus be a world of general hysteria. Just as the organs and functions of a body in hysterical conversion become a gigantic paradigm which the symptom replaces and refers to, in consumption objects become a vast paradigm designating another language through which something else speaks," (Baudrillard:1988, 10-29)

Plan of Literature Review, Research, Practical Work and Writing
Below (FIG: GANTT), I supply a static image of a dynamic Gantt chart detailing an approximate time-plan of how the practical work relates to the evaluation of literary research material and writing activity.

The GANNT chart is a snapshot produced by an interactive project management tool, OmniPlan, and is dynamic. It changes as research, practice and writing activities develop. A current, zoomable, snapshot can be found here:

http://www.daisyrust.com/phd/dissertation_timeplan_current_ian_grant.png 

Establishing Cultural Patterns through Practice
I will produce and evaluate: 

(i) Performance, Artwork and Kinetic Behavioural Sculpture; 
(ii) Original software, software art and computer based creative production techniques; 
(iii) Innovative control systems and interfaces that will be applicable hopefully beyond the domain of performance; 
(iv) Papers, videos, websites and published outcomes that will contribute to the thesis, documentation and other elements of the research.

As a minimum, I plan to make and test: 
(i) Several expressive, physical objects; 
(ii) Several virtual objects, that are controlled by
(iii) Several innovative control systems, in prototype, including touch surfaces, optical motion/expression/gesture capture, digital input devices.

It will be ideal if the systems are tested in performance or in installation contexts. Standard HCI user testing methods, including focus groups, cognitive walkthroughs, Goals, Operators, Methods, and Selection Rules (GOMS) analysis, will be employed, evaluated and discussed where relevant.

Relationship to Previous Work: Comparative Case Studies of Practice
The thesis will analyse case study material, including software and performances by the author, other artists, puppeteers, animators and sculptors. Case studies include:

* Theo Jansen's Walker and his Staadcreatures: Covering ideas of Virtual Creatures, Object Orientedness, Automata and Computer Simulated Physics; Cybernetic Sculpture; See (Jansen 2007)


Figure 1: Theo Jansen Walker As Physical, Virtual and Textual (Code) Object
* Surfaces and Shadows: Synchronous and Asynchronous Techniques: 
	- Lotte Reiniger, Stop motion animator
* Phil Worthington "Shadow Monsters". Animation and Computer Vision: AR installations;

Figure 2: Phil Worthington "Shadow Monsters" 2005
* Golan Levin: New media artist. (Reface [Portrait Sequencer] 2007), Snout (2008) Double-Taker (Snout) (2008) Opto-Isolator (2007) and other projects: face and expression recognition; automata and robotics; video hybrids;

  
Figure 3: Golan Levin - physical and virtual projects (2006-2009)
* Embodied Interactions: finger, hand, whole-body, gestural, eye and facial interactions;

Examples of how these areas map with my own work can be see in Ian Grant "Video Hybrids, explorations in digital puppetry", presented as a performance piece, software and documentary write up (see sample chapter).

Dissertation Writing Practice
The thesis type-setting is automated using a LATEX template (created by the author using UEL standards) and a BIBTEX database. 

Registration Document References

Baudrillard, Jean and Mark. Poster. Selected writings (of) Jean Baudrillard. Stanford University Press, Stanford (Calif.), 1988.

Dewey, John. Art as Experience. Perigee Trade, 2005.

Dorosh, D. Patterning: The Informatics of Art and Fashion. PhD Thesis. UEL. Awarded in June 2008.

Geertz, Clifford.The Interpretation of Cultures. Basic Books, NY. 1977.

Jansen, Theo. The Great Pretender. 010 Uitgeverij, 2007.

Levin, Golan. Projects. http://www.flong.com/projects/ Date modified: 2009. Date Accessed: 1st June 2009.

Reas, Casey. Behavioral kinetic sculpture. Master's thesis, MIT, 1996.

Turkle, Sherry. Computer Games As Evocative Objects: From Projective Screens. To Relational Artifacts  in Raessens, Joost and Jeffrey Goldstein "Handbook of Computer Game Studies" MIT Press, London, 2005.

Zielinski, Siegfried. Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. MIT Press, 2008.

Zielinski, Siegfried. Media Archaeology. http://www.ctheory.net/articles.aspx?id=42#text%201 Date modified: 7th Nov, 1996. Date Accessed: 1st June 2009.

Example Writing
For an extended abstract and sample writing, please see the sample chapters supplied for annual review AY08-09.


Current Research Bibliography
The current bibliographic database (BIBTEX) contains 180 references already considered in the literature review. For brevity, I only include references included in several important categories. An asterisk (*) indicates critical and key texts. There may be some repeated entries where items are cross-referenced. I (mostly) exclude instructional texts in computer technologies (e.g. software and programming text-books, e.g. iPhone programming, OpenGL Red Book, OpenCV, etc.). 

Puppetry, theory, history, technology, digital puppetry; 
[1]	Bacon, M. No Strings Attached: Inside Story of Jim Henson’s Creature Shop. Virgin Books, 1997.
[2]	Baird, B. The Art of the Puppet. With illustrations. Ridge Press, Inc., 1965.
[3]	Bar-Lev, A., Bruckstein, A. M., and Elber, G. Virtual marionettes: a system and paradigm for real-time 3d animation3d animation. The Visual Computer 21 (2005), 488–501.
[4]	Baran, I., and Popović, J. Automatic rigging and animation of 3d characters. Proceedings of the 2007 SIGGRAPH conference 26 (2007).
[5]	Barnes, C., Jacobs, D., Sanders, J., Goldman, D., Rusinkiewicz, S., Finkelstein, A., and Agrawala, M. Video puppetry: a performative interface for cutout animation. SIGGRAPH Asia ’08: SIGGRAPH Asia 2008 papers (Dec 2008).
[6]	Baudelaire, C. The philosophy of toys. In Essays on Dolls, I. Parry, Ed. Syrens, London, 1994.
[7] *	Bell, J. Strings, Hands, Shadows: A Modern Puppet History, illustrated edition ed. Wayne State University Press, 2000.
[8] *	Bell, J., Ed. Puppets, Masks, and Performing Objects. MIT Press, Boston, 2001.
[9]	Bicat, T. Puppets and Performing Objects: A Practical Guide. The Crowood Press Ltd, 2007.
[10]	Blumenthal, E. Puppetry and Puppets: An Illustrated World Survey. Thames &amp; Hudson, 2005.
[11]*Collective author. PUCK no. 9 :Images virtuelles Thematic review. Institut international de la Marionnette, CHARLEVILLE-MEZIERES, 1996.
[12]*Conner, S. Dumbstruck: A Cultural History of Ventriloquism. Oxford University Press, New York, 2000.
[13]	Currell, D. Puppets and Puppet Theatre. The Crowood Press Ltd, 1999.
[14]	Hall, V. Mike (the talking head). http://mambo.ucsc.edu/psl/mike.html, Date Created: nodate. Date Accessed: 01/02/2007.
[15]	Jurkowski, H. Aspects of Puppet Theatre. Puppet Centre Trust, 1988.
[16]	Jurkowski, H., and (Editor), P. F. A History of European Puppetry from Its Origins to the End of the 19th Century: Volume 1. Edwin Mellen Press Ltd, 1996.
[17]	Jurkowski, H., and (Editor), P. F. A History of European Puppetry: The Twentieth Century. Volume 2. Edwin Mellen Press Ltd, 1998.
[18]*Kaplin, S. A puppet tree: A model for the field of puppet theatre. TDR (1988-) 43 (Oct 1999), 28–35. Puppets, Masks, and Performing Objects.
[19]	Kaplin, S. A puppet tree - a model for the field of puppet theatre. In Puppets, Masks, and Performing Objects, J. Bell, Ed. MIT Press, Boston, 2001, pp. 18–25.
[20]*Knep, B., Hayes, C., Sayre, R., and Williams, T. Dinosaur input device. CHI ’95: Proceedings of the SIGCHI conference on Human factors in computing systems (May 1995).
[21]*Lecoq, J. The Moving Body: Le Corps Poetique, revised edition ed. Methuen Drama, 2002.
[22]	Mack, J. Masks: The Art of Expression, new edition ed. British Museum Press, 1996.
[23]	Magnenat-Thalmann, N. The making of a film with synthetic actors. Leonardo. Supplemental Issue 1 (Jan 1988), 55–62. Electronic Art.
[24]	Mazalek, A., and Nitsche, M. Tangible interfaces for real-time 3d virtual environments. ACE ’07: Proceedings of the international conference on Advances in computer entertainment technology (Jun 2007).
[25]*Meschke, M., and Sörenson, M. In Search of Aesthetics for the Puppet Theatre. Indira Gandhi National Centre for the Arts, New Delhi, 1992.
[26]	Ninomiya, D., Miyazaki, K., and Nakatsu, R. Networked virtual marionette theater. In Technologies for E-Learning and Digital Entertainment. Springer Berlin / Heidelberg, 2008.
[27]	Obraztsov, S. My Profession. Fredonia Books, 2001.
[28]	Parry, I. Essays on Dolls. Syrens, London, 1994.
[29]	Rilke, R. M. Dolls: On the wax dolls of lotte pritzel. In Essays on Dolls, I. Parry, Ed. Syrens, London, 1994.
[30]	Schönewolf, H. Play with Light and Shadow: Art and Techniques of Shadow Theatre. Littlehampton Book Services Ltd, 1969.
[31]*Segal, H. B. Pinocchio’s Progeny : Puppets, Marionettes, Automatons and Robots in Modernist and Avant-Garde Drama. Johns Hopkins University Press, Baltimore, 1995. Normal Loan 791.53 SEG.
[32]	Shi, X., Zhou, K., Tong, Y., Desbrun, M., Bao, H., and Guo, B. Mesh puppetry: cascading optimization of mesh deformation with inverse kinematics. International Conference on Computer Graphics and Interactive Techniques (2007).
[33]	Silk, D. William the Wonder-Kid: Plays, Puppet Plays, and Theater Writings. Sheep Meadow Press, U.S., 1996.
[34]	Tillis, S. Toward an Aesthetics of the Puppet: Puppetry as a theatrical art. Greenwood, New York, 1992. Normal Loan 791.53 TIL.
[35]*Tillis, S. The art of puppetry in the age of media production. In Puppets, Masks, and Performing Objects, J. Bell, Ed. MIT Press, Boston, 2001, pp. 172–183.

3D Graphics, Programming, Virtual and Augmented Reality
[1]*  Fiala, S. C. M. Augmented Reality: A Practical Guide: The Complete Guide to Understanding and Using Augmented Reality Technology. Pragmatic Bookshelf, 2008.
[2]	Gobbetti, E., Balaguer, J., and Thalmann, D. Vb2: an Architecture for Interaction in Synthetic Worlds. Proceedings of the 6th annual ACM symposium on User interface software and technology (1993), 167–178.
[3]	Govindaraju, V., Djeu, P., Sankaralingam, K., Vernon, M., and Mark, W. Toward a Multicore Architecture for Real-Time Ray-Tracing. MICRO ’08: Proceedings of the 2008 41st IEEE/ACM International Symposium on Microarchitecture (Nov 2008).
[4]	Junker, G. Pro OGRE 3D Programming (Expert’s Voice in Open Source). APress,US, 2006.
[5]* Mazalek, A., and Nitsche, M. Tangible interfaces for real-time 3d virtual environments. ACE ’07: Proceedings of the international conference on Advances in computer entertainment technology (Jun 2007).
[6]	Salti, S., Schreer, O., and Stefano, L. Real-time 3d arm pose estimation from monocular video for enhanced hci. VNBA ’08: Proceeding of the 1st ACM workshop on Vision networks for behavior analysis (Oct 2008).
[7]*	Zhai, S. Human Performance in Six Degree of Freedom Input Control. PhD thesis, University of Toronto, 1995.

Augmented Reality Techniques
[1]*	Billinghurst, M., Kato, H., and Poupyrev, I. Tangible augmented reality. SIGGRAPH Asia ’08: SIGGRAPH ASIA 2008 courses (Dec 2008).
[2]	McQuiggan, S., Rowe, J., and Lester, J. The effects of empathetic virtual characters on presence in narrative-centered learning environments. CHI 2008, April 5–10 Papers (2008).
[3]	Mistry, P., Maes, P., and Chang, L. Wuw - wear ur world: a wearable gestural interface. CHI EA ’09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).
[4]	Molyneaux, D., and Gellersen, H. Projected interfaces: enabling serendipitous interaction with smart tangible objects. TEI ’09: Proceedings of the 3rd International Conference on Tangible and Embedded Interaction (Feb 2009).
[5]*	Wang, R., and Popović, J. Real-Time Hand-Tracking with a Color Glove. SIGGRAPH ’09: SIGGRAPH 2009 papers (July 2009).

Digital Art, Objects, Virtuality and Performance
[1]* Blundell, B. G. An Introduction to Computer Graphics and Creative 3-D Environments. Springer, 2008.
[2]*	Candlin, F., and Guins, R. The Object Reader. Routledge, 2008.
[3]	Candy, L., and Edmonds, E. Explorations in Art and Technology: Intersections and Correspondence. Springer, 2002.
[4]*	Dixon, S. Digital Performance, annotated edition ed. The MIT Press, 2007.
[5]	Ede, S. Art and Science. I B Tauris &amp; Co Ltd, 2008.
[6]	Gabor, D. Technological civilisation and man’s future. In Cybernetics, art and ideas, J. Reichardt, Ed. Studio Vista, London, 1971, pp. 18–24.
[7]	Grau, O. Virtual Art: From Illusion to Immersion (Leonardo Book S.), rev sub ed. The MIT Press, 2003.
[8]*	Grau, O. MediaArtHistories. The MIT Press, 2007.
[9]*	Jansen, T. Theo Jansen: The Great Pretender. 010 Uitgeverij, 2007.
[10]	Mealing, S. Computers and Art, 2nd revised edition ed. Chicago University Press, 2008.
[11]	Reichardt, J., Ed. Cybernetics, art and ideas. Studio Vista, London, 1971.

Computer Vision Techiques for Digital Puppetry
[1]*	Barnes, C., Jacobs, D., Sanders, J., Goldman, D., Rusinkiewicz, S., Finkelstein, A., and Agrawala, M. Video puppetry: a performative interface for cutout animation. SIGGRAPH Asia ’08: SIGGRAPH Asia 2008 papers (Dec 2008).
[2]	Cabral, M., Morimoto, C., and Zuffo, M. On the Usability of Gesture Interfaces in Virtual Reality environments. CLIHC ’05: Proceedings of the 2005 Latin American conference on Human-computer interaction (Oct 2005).
[3]	Datcu, D., and Rothkrantz, L. Facial Expression Recognition in Still Pictures and Videos Using Active Appearance Models: a Comparison Approach. Proceedings of the 2007 international conference on Computer systems and technologies (2007).
[4]	Salti, S., Schreer, O., and Stefano, L. Real-time 3d arm pose estimation from monocular video for enhanced hci. VNBA ’08: Proceeding of the 1st ACM workshop on Vision networks for behavior analysis (Oct 2008).
[5]*	Vlasic, D., Baran, I., Matusik, W., and Popović, J. Articulated mesh animation from multi-view silhouettes. SIGGRAPH ’08: SIGGRAPH 2008 papers (Aug 2008).


Tangible Interaction and Embedded Control Technologies
[1]*	Barnes, C., Jacobs, D., Sanders, J., Goldman, D., Rusinkiewicz, S., Finkelstein, A., and Agrawala, M. Video puppetry: a performative interface for cutout animation. SIGGRAPH Asia ’08: SIGGRAPH Asia 2008 papers (Dec 2008).
[2]	Billinghurst, M., Kato, H., and Poupyrev, I. Tangible augmented reality. SIGGRAPH Asia ’08: SIGGRAPH ASIA 2008 courses (Dec 2008).
[3]	Fishkin, K. A Taxonomy for and Analysis of Tangible Interfaces. Personal and Ubiquitous Computing 8 (2004), 347–358.
[4]*	Guo, C., Young, J., and Sharlin, E. Touch and Toys: New Techniques for Interaction With a Remote Group of Robots. CHI ’09: Proceedings of the 27th international conference on Human factors in computing systems (Apr 2009).
[5]	Mazalek, A., and Nitsche, M. Tangible interfaces for real-time 3d virtual environments. ACE ’07: Proceedings of the international conference on Advances in computer entertainment technology (Jun 2007).
[6]	Mistry, P., Maes, P., and Chang, L. Wuw - wear ur world: a wearable gestural interface. CHI EA ’09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).
[7]	Molyneaux, D., and Gellersen, H. Projected interfaces: enabling serendipitous interaction with smart tangible objects. TEI ’09: Proceedings of the 3rd International Conference on Tangible and Embedded Interaction (Feb 2009).
[8]	Vertegaal, R., and Poupyrev, I. Eek!  a mouse!  organic user interfaces: tangible, transitive materials and programmable reality. CHI EA ’09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).
[9]	Weller, M., Do, E., and Gross, M. Posey: instrumenting a poseable hub and strut construction toy. TEI ’08: Proceedings of the 2nd international conference on Tangible and embedded interaction (Feb 2008).
[10]	Weller, M., Gross, M., and Do, E. Tangible sketching in 3d with posey. CHI EA ’09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).
[11]	Zaman, B., Abeele, V., Markopoulos, P., and Marshall, P. Tangibles for children,: the challenges. CHI EA ’09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems (Apr 2009).

Tangible Interaction and Embedded Control Technologies (interactive toys)
[1]	Newman, M. Interactive Barney: Good or evil?  Conferees worry about where computerized ’character’ toys are going next. http://www.post-gazette.com/businessnews/19990521barney1.asp, Date Modified: 21/05/1999. Date Accessed: 01/03/2007.
[2]	Parent, A. Read Reviews of Hasbro Aloha Stitch Doll 3570 at eOpinions. http://www.epinions.com/content_163285929604? linkin_id=8003929, Date Created: 28/11/2004. Date Accessed: 01/02/2007.
[3]	Shenk, D. Behold the Toys of Tomorrow (The Atlantic Online - Digital Culture). http://davidshenk.com/webimages/atlantic1.htm, Date Created: 07/01/1999. Date Accessed: 01/02/2007.
[4]	Strommen, E. F. When the Interface is a Talking Dinosaur: Learning Across Media with ActiMates Barney. http://www.playfulefforts.com/archives/papers/CHI-1998.pdf, Online PDF of published work. Date Written: 1998. Date Accessed: 01/03/2007.
[5]	Strommen, E. F. Learning from Television With Interactive Toy Characters As Viewing Companions. http://www.playfulefforts.com/archives/papers/SRCD-1999.pdf, Online PDF of published work. Date Written: 1999. Date Accessed: 01/03/2007.
[6]*	Strommen, E. F. Interactive Toy Characters as Interfaces For Children. http://www.playfulefforts.com/archives/papers/IA-2000.pdf, Online PDF of published work. Date Written: 2000. Date Accessed: 01/03/2007.
[7]	Strommen, E. F. Play?  Learning?  Both...or neither?  http://www.playfulefforts.com/archives/papers/AERA-2004.pdf, Online PDF of unpublished work. Date Written: 2004. Date Accessed: 01/03/2007.


Methods (Cultural Studies, Performance Studies and Theatre Anthropology)
[1]*	Candlin, F., and Guins, R. The Object Reader. Routledge, 2008.
[2]*	Eugenio Barba, N. S. A Dictionary of Theatre Anthropology: The Secret Art of the Performer. Routledge, 1991.
[3]*	Turkle, S. Evocative Objects: Things We Think with. The MIT Press, 2007.
[4]	Zielinski, S. Media Archaeology, November 1996.
[5]*    Zielinski, S. Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. MIT Press, 2008.

Summary Of The Elements Of The Investigation That Are Novel, Original Or Creative And That May Constitute Production Of Original Knowledge Or An Original Interpretation Of Existing Knowledge
- A deepening of the material and thought and cultural philosophy surrounding puppetry studies and technology;

- Original Performance, Artwork and Artefacts;

- Original Software products and creative production techniques;

-  New hardware control systems and interfaces for expressive play that will be applicable hopefully beyond the domain of performance and digital puppetry;

- Papers and published outcomes that will contribute to the thesis, documentation and other elements of the PhD.</Text>
        </Document>
        <Document ID="184">
            <Title>Summary of findings</Title>
            <Text>10.5 Summary of findings 
</Text>
        </Document>
        <Document ID="218">
            <Title>figure_007_full_title</Title>
        </Document>
        <Document ID="107">
            <Title>2D and 3D Output</Title>
            <Text>4.3 2D and 3D Output
</Text>
        </Document>
        <Document ID="329">
            <Title>Notes</Title>
            <Text>Myron Kruger
Shadows are an intriguing computational phenomena and, in an art context, have been the focus of several recent computer graphics papers, [TODO SIGGRAPH, SIGGRAPH Asia]. Performative and puppetry contexts have been considered and technically simulated. But most studies stop short of becoming a tool for expression and performance.

The present thesis is aiming to evaluate the full extent of contexts where digital puppetry is presented. This study includes contexts where the performative and puppeteering contexts of control are secondary to the visual and semiotic ones. 

The current chapter focuses on the theme of the silhouette and shadow puppet.

There is a lot in:

[][#Ren:2005:LSF]

: "Shadow play is a Chinese art that has not yet transformed into a digital form." [p.1601][#Li:2007p1864]

[][#Tan-Kian-Lam:2008uq]

The work of [Lam et. al][#Tan-Kian-Lam:2008uq], drew together techniques for real-time simulation of shadow puppets, animation and visual look drawn from Chinese shadow puppet traditions. Using standard OpenGL techniques, the paper explores texturing, animation, depth of field blurring, intensity of light source and procedural algorithms for (automatic) animation. 

From the perspective of performance and puppetry, although real-time image generation techniques were examined, no real-time performance system was described. The paper mentions the potential for real-time physics simulation in creating expressive animation, but doesn't develop or propose any techniques for performer-object control. 

The paper mentioned an important and emerging theme of my work, that of preservation of traditional crafts an the importance of digital media preserving and promoting cultural heritage.
The real-time features of OpenGL: texturing properties, blending, animation and timers,  lighting, shaders for blurring screen elements

real-time raytracing
gpu programmable shaders

hard-ware acceleration of games physics

[TODO: SEQUENCE OF ANNOTATES IMAGES OF THE IK APPROACH TO ONE OF MY MULTIJOINTED FIGURES]

[TODO: SUMMARISE THIS QUOTATION FROM LAM]
 we have provided several solutions are provided in visual simulation and animation of virtual shadow play’s puppet. Firstly, texture mapping and blending techniques are used instead of rendering technique in order to allow fast and interactive display in real time environment. Besides, several techniques that use various themes (lighting) and special effects such as blurring to bring the right atmosphere to the virtual shadow play are proposed. Hierarchical modeling method is adopted in order to model a realistic animation for the puppet to include real time elements that allow playing of shadow play naturally in virtual environment. Previous works are not interactive and require manual pre-ordering of the play using key-framed approach.</Text>
        </Document>
        <Document ID="296">
            <Title>PhD Admin</Title>
        </Document>
        <Document ID="185">
            <Title>Summary of problems</Title>
            <Text>10.6 Summary of problems
</Text>
        </Document>
        <Document ID="219">
            <Title>figure_002_cab_roto</Title>
        </Document>
        <Document ID="108">
            <Title>6.3.1 Game Engines</Title>
            <Text>4.3.1 Game Engines
</Text>
        </Document>
        <Document ID="20">
            <Title>nime2005_228</Title>
            <Text>Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
On Interface Expressivity: A Player-Based Study
Cornelius Poepel Academy of Media Arts	Department of Music
Peter-Welter-Platz 2	The University of Birmingham D-50676 Ko ̈ln	Edgbaston
+49 221 20189355	Birmingham B15 2TT, UK cp@khm.de
ABSTRACT
While many new interfaces for musical expression have been pre- sented in the past, methods to evaluate these interfaces are rare. This paper presents a method and a study comparing the potential for musical expression of different string-instrument based musical interfaces. Cues for musical expression are defined based on re- sults of research in musical expression and on methods for musical education in instrumental pedagogy. Interfaces are evaluated ac- cording to how well they are estimated to allow players making use of their existing technique for the creation of expressive music.
Keywords
Musical Expression, electronic bowed string instrument, evaluation of musical input devices, audio signal driven sound synthesis
1. INTRODUCTION
The Conference on New Interfaces for Musical Expression has presented a lot of challenging new interfaces in the past that can be used for musical performances. Regarding the question whether they are usable for musical expression, evaluation is often done by the developer or a small number of people. While the interfaces are frequently said to be highly usable, it often remains unclear which aspects of music and its expression the interfaces were designed for. A reason might be that evaluation methods are rare. According to the workshop proposal of NIME 01 [9] it is one of the goals of NIME to explore interfaces focusing on all aspects related to their potential for musical expression and to identify criteria for evaluating musical interfaces.
This paper presents a method to identify criteria for the evalua- tion of instrument-like controllers. The criteria include expressive cues found to be important in evaluating musical expressivity by research in music psychology. Indicators for a quantitative mea- surement are based on playing techniques known to be relevant for these cues. A measuring instrument is built to measure the player- estimation about the behaviour of instruments. Experimental stud- ies and results are presented and discussed.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
NIME05, Vancouver, BC, Canada Copyright 2005 Copyright remains with the author(s).
2.	RELATED WORK
A lot of researchers address the topic of musical expression. Ex- amples may be found in Jorda` [4] or Blaine and Fels [1]. However, musical expression is one of the topics among others. A measuring instrument to evaluate the specific potential for expressive music of interfaces is not found. Wanderley and Orio present a method based on tools used in research on Human Computer Interaction (HCI) to evaluate interfaces [11]. It includes evaluation of learnability, ex- plorability, feature controllability and timing controllabillity. Isaac uses these tools and modifications evaluating an accelerometer and a Korg Kaosspad KP 2 [3]. The evaluation method presented in this paper differs from previous work since measurement method- ology is based on the expressive skills of musicians and the player- estimation of the interface-ability to deal with it.
3.	MUSICAL EXPRESSION
Emotional expression plays a key role in musical expression [5]. Performers communicate musical expression to listeners by a process of coding. Listeners receive musical expression by de- coding. Performers code expressive intentions using expressive- related cues (Brunswikian lens model, cited in [5]). Extensive work has been done to identify most relevant cues. These cues include: tempo, sound level, timing, intonation, articulation, timbre, vibrato, tone attacks, tone decays and pauses.
String instruments are known to be highly useful for musical ex- pression. Performances with string instruments fall into the cate- gories of music, explored by expressivity research [6]. The skills developed by string players offer the potential for the development of musically expressive string based interfaces. Where exactly can these skills be found? According to [2], instrumental playing tech- niques play a key role in enabling a string player to create musical expression.
There are, of course, a lot of other factors that may influ- ence expression in musical performance especially when listener- evaluated. Juslin groups them into: piece-related, instrument- related, performer-related, listener-related and context related fac- tors [6].
Important for the present research are: - the factors that influence musical expression, - the cues musicians use for coding expressive intentions - and the methods string players use to create these cues with their instruments. Interfaces built to use the expressive skills of string players will need to deal as well as possible with those methods.
4.	EXPERIMENTAL STUDIES
According to present evaluation methods of musical interfaces the potential for musical expressivity can hardly be evaluated be-
228
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
Env follower
Pitch follower
Scaling table
Pickup Signal
found in [8]. To compare the interfaces on the same base, simple FM-
Synthesis is chosen. Leveling the problems of the ZETA System [7] in timing-accuracy, a synthesizer sound is chosen coming closer to the timbre of a string instrument than simple FM does. It may be expected that timbre accuracy will be higher valuated. Addition- ally, a traditional instrument-body is used to give the participant a hardware-feel closer to the original instrument. All instruments are given a similar small reverberation. To allow a better distinction be- tween acoustic and electronic sound all instruments are transposed one octave down. Speakers (stereo) used are Adam P-11.
The envelope follower of instrument B and C is mapped to each carrier oscillator using the same tables. It is mapped using differ- ent tables to each modulation index. Synthesis is implemented in MaxMSP. The Max object fiddle∼ [10] is used for pitch and ampli- tude following.
ASDSS may be estimated to present nuances of sound variation applied by the player to the interface in the sound result even if they are not tracked by the pitch and envelope follower [8]. With respect to this assumption it is hypothesized that instrument C will be higher estimated than instrument B and A. According to the ex- perience of the author it is hypothesized that instrument A would be estimated lower than instrument B and C. Since instrument A and B are driven by pitch and amplitude following it is hypothesized that instrument B will be valuated closer to A than to B.
4.2 Operationalization
In order to measure musical expressivity it has to be operational- ized with a set of indicators. Existing research states a set of factors to be relevant [6] in performance of musical expression. While the present study focuses only on the instrument and the player, only following of those factors may be seen as relevant: Performer-related: - The performers technical skills - The performers motor precision - The performers emotion-expressive style Instrument-related: - Acoustic parameters available - Instrument specific aspects of timbre, pitch, etc.
The mentioned instrument-related factors primarily need to be enabled by sound-synthesis. It is the task of the interface to bring performer-related factors via mapping to sound synthesis and (by doing this) adequately to sound. Focusing on the potential for mu- sical expression it may be seen as relevant to measure whether play- ers estimate the interface to be capable of doing this task.
According to operationalization, indicators have to lie in the field of the performer related skills and the estimated instrument response. The selected skills have to be relevant for the men- tioned expressive cues (Section 3). These cues may be put into five interface-relevant cue-groups (Examples for playing techniques re- lated to the groups are written in brackets):
1. Timing accuracy: tempo, timing, pauses (e.g. pizzicato, colle ́, spicatto, short notes) 2. Pitch accuracy: intonation, vibrato (e.g. different notes with pauses, legato tones, glissando, different vibratos)
3. Dynamics accuracy: sound level (e.g. crescendo, decrescendo, pp, mf, ff, sfz) 4. Articulation accuracy: articulation, tone attacks, tone decays (e.g. de ́tache ́, martele ́, spicatto, scratching)
5. Timbre accuracy: timbre (e.g. go into the sound, pull the sound with the bow, change of bow-bridge distance)
Indicators to measure accuracy of the interface are the specific responses of the instrument to the related playing techniques.
Amplitude
Oscillator
x
+
Oscillator
Frequency
Modulation Index
x
Ratio
Amplitude
Frequency
Audio Out
Figure 1: Flow Chart instrument B
cause the question of which kind of musical expression the inter- face will be used for cannot be answered [11]. Assuming an evalua- tion method that tests an interface by estimating whether it is usable to create a specific kind of musical expression, the assumption of Wanderley and Orio may be confirmed. However, if key techniques to create musical expression could be found, it might be a solution to estimate the interface according to these key techniques.
In this research it is assumed that playing techniques of string players related to the task of musical expression can be seen as a valid base to create musical expression. It is further assumed that playing technique is used to create expressive cues with the aim of coding expressive intention into sound.
The intention of the experiment lies in a comparison of the player estimated potential for musical expression of string instru- ment based electronic instruments for sound synthesis. Interfaces of these instruments are intended to use the mentioned specific skills of a string player for musical expression. Three instruments, based on three different structures concerning interface-mapping- synthesis are compared. The estimation of the players is measured by letting them evaluate whether the interface allows a player to create cues necessary for musical expression.
4.1	The Devices
The instruments are: - Instrument A: ZETA Midi Viola-bridge mounted on a traditional viola, Midi-Synthesizer Kawai K5000, program A044. - Instrument B: Electric viola, pitch and amplitude following, map- ping, simple FM-Synthesis. - Instrument C: The same electric viola, pitch and envelope follow- ing, mapping, ASDSS (Audio Signal Driven Sound Synthesis) [8] implemented with a modified FM-Synthesis (s. Figure 1).
Compared to traditional synthesis methods, ASDSS uses the au- dio signal of the interface as a control parameter. This parameter is the main controlling signal to drive the synthesized sound. This synthesized sound is indirectly shaped by control parameters like pitch or amplitude. A broader description of this method may be
229
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
4.3	Design of Experiment
Each test took about 90 minutes. 13 participants (trained string players) evaluated the instruments. Participants structure was: 7 women and 6 men, 8 professionals (earning their money with the instrument) and 5 amateur musicians. The musicians were mem- bers of professional orchestras of Cologne, Jazz-Violinists and am- ateur musicians. Participants were introduced to the experiment and tested each of the three instruments (20 min), they played and answered according to the questionnaire (60 min.) and they were interviewed (10 min.). The questionnaire gave 18 instructions to play specific musical tasks. Each task was followed by one of the three questions:
- Is the instrument transparent according to the playing method used? - Is the relation gesture - electronic sound adequate? - Is the intended sound result well represented in the perceived sound result?
The 18 tasks were related to playing techniques according to the five cue-groups (11 tasks), to scales performed with different artic- ulation (2 tasks) and to musical phrases (5 tasks). The players had to play and compare all three instruments (changing order) with each task. A five point Likert-scale (1: not usable, 5: very good) was used. The participants evaluated each instrument with each task. When done with the 18 tasks, the questionnaire asked whether the participant considers the instrument as being usable for musical expression (in general) and as being usable for the personal musi- cal expression. After finishing the questionnaire participants were interviewed on their experiences with the instruments.
5. RESULTS
Data from the questionnaires was analyzed by calculating arith- metic mean and median. It was analyzed whether personal factors like gender, amateur/professional and interest in electronic sounds matters in player estimation of the instruments.
5.1	Quantitative Results
According to the five point Likert-scale the overall values (mean) are: Instrument: A: 1,90; B: 3,59; C: 3,82 (Figure 2, see All partic- ipants). Analyzing different groups of people the following results (overall) were found: All results of the person-groups show the ranking-order of instrument A low, B higher and C highest (B and C much closer together than A and B). Analyzing in detail showed that participants with low interest in electronic sounds (8 partici- pants) estimated the instruments B and C more different while par- ticipants with high interest in electronic sounds (5 participants) es- timated instrument B very close to instrument C (Figure 2). Women and men did not estimate instrument C and B noteworthy different but women estimated instrument A higher in having potential for the creation of expressive music than men did.
Analyzing the cue-groups (section 4.2) the highest difference be- tween estimation of instrument C and B was found in dynamics. Results of other cue groups, scales with different articulation and musical tasks are presented in Figure 3.
On the question whether the instrument is considered as being usable (in general) for musical expression 4 participants estimated instrument C most high, 2 participants estimated instrument B most high and 7 participants estimated both with the same value on the five point scale. Analyzing the 18 tasks of participants consider- ing instrument B highest showed they consider instrument C on 3,5 (mean) tasks better than B. Analyzing participants considering in- strument B higher showed they consider instrument B on 2 (mean) tasks better than C.
Figure 2: Player estimation on all 18 tasks (mean).
Figure 3: Player estimation on cue-groups, scales and musical phrases (mean).
5.2	Reliability and Validity
In order to examine the difference of the three instrument- estimations an analysis of variance (ANOVA) was done. A sig- nificant difference between the three was observed (F = 29.76, crit- ical F = 3.26 and p &lt; 0.01). Reliability within a cue group was examined by calculating the correlation of tasks asked twice. Cor- relation ranged between 0.40 - 0.74 (average 0.58). It may be con- cluded reliability of some tasks is low. The reasons may be seen in the modifications of tasks when asked the second time. Partic- ipants distinguished to a greater extent than expected between the different playing techniques belonging to one of the five cue-groups (section 4.2). Since reliability of some tasks is not high, the values presented in the graphs may be seen as tendencies but not as exact results.
To estimate validity the mean of data regarding all participants on all 18 tasks is compared with the mean of data found on the question whether the participants consider the instrument usable for musical expression (Table 1).
Table 1: Values to estimate validity
18 tasks (mean)
considered expressivity (mean)
Instrument A
1,90
1,77
Instrument B
3,59
3,77
Instrument C
3,82
4,00
230
Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME05), Vancouver, BC, Canada
5.3 Interviews
Since participants ranked instrument B and C different (2 esti- mated B best, 4 estimated C best, 7 estimated equally) it was inter- esting to get comments on the reasons for their opinion. Concern- ing instrument B participants mentioned that it was more precise and better controllable in fast passages. Drawbacks of instrument B were seen in the cold sound and the poor response in slight tonally alterations. Mentioned reasons why instrument C was evaluated higher were: it feels more like a string instrument, it responds bet- ter to my input, I can play more warmly with it. A drawback men- tioned on instrument C was the dirty timbre when playing ff. For both instruments it was often mentioned that they were to heavy and that the C- and G-strings had a different timbre than the D- and A-strings. Another drawback mentioned on both instruments was the bubbling sound by scratching or playing or noisy tone attack (pitch detection problems). A general conviction was, that if this disturbance could be disposed instruments would be much more usable. A request mentioned twice was to built a black box of- fering instrument B and C for stage use and without bubbling. In general instrument A was seen as very poor in response and only playable with extensive modification of playing technique.
6. CONCLUSION
While the result confirmed the hypothesized ranking, instrument B was much closer to C than expected. One of the reasons may be seen in the chosen implementation of ASDSS. The modified FM- Synthesis causes a strong tonal influence especially when played with a strong bow pressure close to the bridge. The resulting sound covers the ability of ASDSS to transmit nuances in sound not cov- ered by the following system. A question might be why instrument C was estimated better in dynamics since B and C were using the same amplitude follower. Reasons may be found in the fact that timbre-dynamics play a big role in performing different dynamics. The instrument has to be capable of translating small variances in timbre-dynamics accurately to the sound. These small variances will cause a different sound output in instrument C because the au- dio signal with all its spectral qualities is directly connected to the carrier oscillator (Figure 1). Instrument B will correspond only to the predefined mapping of the amplitude follower to modulation index and amplitude.
In the presented experiment instruments are estimated using skills and techniques of an experienced string player. It may be expected that new instruments offer new possibilities. Observing the participants it was detected that some players try very soon to explore specific sounds like e.g. the bubbling sound when scratch- ing. Some participants considered these sounds to be usable for specific expressive tasks. Since the test focuses on the skills al- ready developed, such instrument-potentials are not represented in evaluation. However, it may be assumed that valid factors lie in the traditional playing techniques. Majority of the participants tested the instruments in the opening phase of each experiment with mu- sical tasks of their repertoire trying to figure out the possibilities and limitations of the instruments.
The general method of evaluating controllers presented in this paper may be found to be useable for the estimation of new instrument-like interfaces. It allows considering specific expressive-related factors of the instrument more thoroughly. This paper has to leave out details of results due to limited space. How- ever, the presented method shows a differentiated picture of the tested instruments which may be of interest for their developers.
7. OUTLOOK
It might be interesting to explore how musicians estimate the three instruments when tested for a longer period of time. It is known from synthesists that specific instruments, interfaces or sounds stay interesting over a long period of time while others do not. It also might be interesting to use the presented method to measure the player estimation on the expressive potential of other controllers. However this would have to assume that their concept on expressivity relates to expressive cues found in performance re- search and that playing techniques may be defined to create these cues. If the cues of existing research would not fit to their idea of musical expression, it would be interesting to know how this idea may be described and how it relates to existing research on musical expression.
8. ACKNOWLEDGMENTS
The author wishes to thank the electronic studio of the Music Academy in Basel, Scott D. Wilson, Carolin Demuth, Eric Lee, Jin Hyun Kim, Georg Trogemann, the German Research Foundation and all the string players taking time for the experiment.
9. REFERENCES
[1] T. Blaine and S. Fels. Contexts of collaborative musical experiences. In Proceedings of the 2003 Conference on New Instruments for Musical Expression (NIME-03), pages 129–134, May 22-24 2003.
[2] I. Galamian. Grundlagen und Methoden des Violinspiels. Addison-Wesley Publishing Company, Frankfurt/M, 2. Ed., 1988.
[3] D. Isaacs. Evaluating Input Devices for Musical Expression. Thesis 2003, http://www.itee.uq.edu.au/ markp/publications/ DavidIsaacsThesis.pdf, 2003.
[4] S. Jorda`. Digital instruments and players: Part I efficiency and apprenticeship. In Proceedings of the 2004 Conference on New Instruments for Musical Expression (NIME-04), pages 59–63, June 3-5 2004.
[5] P. N. Juslin. Music and Emotion, chapter Communicating Emotion in Music Performance: A Review and Theoretical Framework, pages 309–337. Oxford Univ. Press, 2001.
[6] P. N. Juslin. Five facets of musical expression: a psychologists perspective on music performance. Psychology of Music, 31(3):273–302, July 2003.
[7] Y. Lilit and I. Fujinaga. ZETA violin techniques: Limitations and applications. Journal SEAMUS, 13(2):12–16, July 1998.
[8] C. Poepel. Synthesized strings for string players. In
Proceedings of the 2004 Conference on New Instruments for Musical Expression (NIME-04), pages 150–153, June 3-5 2004.
[9] I. Popurev, M. J. Lyons, S. Fels, and T. Blaine. Workshop proposal: New interfaces for musical expression. In Proceedings of the 2001 Workshop on New Interfaces for Musical Expression. SigCHI, April 1-2 2001.
[10] M. S. Puckette, T. Apel, and D. D. Zicarelli. Real-time audio analysis tools for Pd and MSP. In Proceedings of the 1998 International Computer Music Conference (ICMC-98), pages 109–112, October 1-6 1998.
[11] M. M. Wanderley and N. Orio. Evaluation of input devices for musical expression; borrowing tools from HCI. Computer Music Journal, 26(3):62–76, 2002.
231</Text>
        </Document>
        <Document ID="21">
            <Title>Title Brainstorms</Title>
            <Text>Improvisation and Expressivity and the use of Mechanical, Digital and Virtual Objects in Games and Performance

Expressivity of the Mechanical, Digital and Virtual Object

Performance and the Mechanical, Digital and Virtual Object

Expressivity and 

Performer, Expressivity and the Technological Object

Puppetry and Metaphors of Control in Technology Enhanced Performance





</Text>
        </Document>
        <Document ID="186">
            <Title>Future trends</Title>
            <Text>10.7 Future trends</Text>
        </Document>
        <Document ID="109">
            <Title>6.3.2 3D Content Creation Software</Title>
            <Text>4.3.2 3D Content Creation Software
</Text>
        </Document>
        <Document ID="22">
            <Title>outline2</Title>
            <Text>Flow and Metaphoric Sound-Based Composition
Creative Compositional Technique for Computer Music
Abstract Summary (English) Summary (Dutch) Motivation
Thesis Outline
Jan van der Tempel H.K.U./U. of Portsmouth jan@www2.hku.nl 31.01.08
I. Introduction A. What is the aim of this research? B. What are key definitions?
Electronic music composition process
the design of electronic instruments and the manner of their use in the composition and/or performance of organized sound
Human-Machine Interaction
Flow
the compositional and technical factors, visible and hidden, involved in the composition process of the electronic musician
a state in which the mental (compositional) and physical (technical) processes are harmonized, enabling high creativity
C. [mission statement] ex. This paper presents a novel composition method for computer music composers and sound designers. The method is designed to enhance the creative process. It attempts to induce flow by harmonizing human- machine interaction through the use of sound-based metaphors. The efficacy of the composition method will be tested in practice, and will be evaluated according to our investigation of the psychological, musical, and technological factors involved.
II. Flow in Electronic Music Composition Processes A.	What is flow
B.	What conditions are necessary for flow 1. Challenge vs. Ability
C.	How do these conditions translate in the electronic music composition process
1. Novelty vs. Familiarity 2. Reason vs. Intuition 3. Human-Machine Interaction
D.	How to harmonize human-machine interaction? 1. A binding, integrating, synchronizing agent: Metaphor
E. Related issues 1. Psychological
2. Technical 3. Philosophical
III. Hypothesis
A.
Change the relationship with tools
1.
2.
3.
Past occurence of purposeful metaphoric composition a.	Traditional
i.	Programmatic composition ii. Opera/ballet iii.	Mimetic approaches
b.	Modern i. Jazz
ii.	Improvisation in general c.	Post-modern
i. Ambient
ii. Sound-scapes/-objects/-maps A means to transcend the composer/technology duality
a.	Should integrate imagination and interaction b.	Should be coherent, repeatable, transferable c.	Should be (relatively) universal applicable d.	Avoids an infinite regress of sources
Metaphor as a unifier of subjective/objective a.	Literary metaphor b.	Psychological metaphor c.	Metaphor in music and sound
B.
The composition technique 1. Description
2. Use
3.
a.	Functions i.	As a stand-alone composition method
ii.	Combined/integrated in process b.	Approaches
i.	As a creative tool ii.	As an experiment iii.	As a performance iv. As an exercise
v.	As a game c.	Potential users
i.	Sound designers ii. Composers iii. Producers iv.	Other creatives – Crossovers
Is it new a.	Literature
b.	Research c.	Interviews d.	Reinventing the wheel
a.	Format i.
Typed document -Step by step guide -Tutorial -Extra materials
b.	Principles contained
i.
ii.
Imagination guided by sound design -Sound as embodiment of “characters” -Instruments as “puppets”
Music guided by imagination -Perform the puppets -Body, mind, and machine synchronized -Characters and stories arise by default
4. Hypotheses a.	The Audio Puppet technique enhances creative flow
i.	It harmonizes interaction and imagination ii.	It is coherent, repeatable, transferable
IV. Evaluation
A.
B.
Factors for evaluation 1. Queries
iii. iv. v.
It is (relatively) universal applicable It avoids an infinite regress of sources Enhances “flow”
a.	Does it synchronize imagination and interaction b.	Is it coherent, repeatable, transferable c.	Is it (relatively) universal applicable d.	Does it avoid an infinite regress of sources
2. Limitations a.	Quantifiability
3. Data
i.Test surveys ii. Testimony iii. Aesthetic evaluation
b.	Subjectivity c.	Lack of control d.	Near-infinite variables
a.	Personal assessment of creative flow b.	Rate of successful application c.	Rate of correct application d.	Amount of inspiration at each step
Experiment description 1. Method
a.	Application of method by test group (workshop) b.	Survey test group c.	Observe test group’s musical output
2. Resources a.	composition method document b.	Music creation tools c.	Survey document
3.	Study population a.	40-80 participants
b.	Composers, sound designers, producers
c.	1st	year, 4th	year, post graduate, professional
d.	Interact through workshop/lecture 4.	Data collection
a.	Electronic – e-mail, digital audio
C. Results 1.	Data representation
a.	Subtotals, totals b.	Graphic data display c.	Summaries
2. Flaws a.	Margin of error
b.	Blind spots c.	Potential anomalies
D. Analysis 1.	Identifiable trends
2.	Answers to queries a.	Does it synchronize imagination and interaction b.	Is it coherent, repeatable, transferable c.	Is it (relatively) universal applicable d.	Does it avoid an infinite regress of sources e.	Subjective experience of “flow”? Creativity?
3.	Further
discussion
V. Conclusion A. Does the method harmonize human-machine interaction?
B. Does it induce flow? C. Does it have merit in other aspects? D. Other
APPENDIX</Text>
        </Document>
        <Document ID="23">
            <Title>a_foundation_for_emotional_expressivity</Title>
            <Text>1
A Foundation for Emotional Expressivity
Anna Ståhl
SICS Box 1263 SE-164 29 Kista, Sweden annas@sics.se
Petra Sundström
DSV, KTH/SU Forum 100 SE-164 29 Kista, Sweden petra@dsv.su.se
Kristina Höök
DSV, KTH/SU Forum 100 SE-164 29 Kista, Sweden kia@dsv.su.se
Abstract
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright © 2005 AIGA | The professional association for design.
To express emotions to others in mobile text messaging in our view require designs that can both capture some of the ambiguity and subtleness that characterizes emotional interaction and keep the media specific qualities. Through the use of a body movement analysis and a dimensional model of emotion experiences, we arrived at a design for a mobile messaging service, eMoto. The service makes use of the sub-symbolic expressions; colors, shapes and animations, for expressing emotions in an open-ended way. Here we present the design process and a user study of those expressions, where the results show that the use of these sub-symbolic expressions can work as a foundation to use as a creative tool, but still allowing for the communication to be situated. The inspiration taken from body movements proved to be very useful as a design input. It was also reflected in the way our subjects described the expressions.
Keywords
Affect, Handheld Devices and Mobile Computing, Interaction Design, User-Centered Design/Human- Centered Design, User studies
Project/problem statement
Emotional communication between people depends on what media is used and the specific qualities that characterize that media channel. It is also dependent on the shifting situations in which the communication takes place.
In face to face communication the spoken word is only one part of the communication. The emotional content of what is said is conveyed also in subtle ways through
tone of voice, facial expressions and body language. Parts of this happen unconsciously, but are still important for the understanding and interpretation of meaning and the emotional content of a message.
In attempts to mirror real life meetings, like in videoconferencing, most of the emotional content gets through, but not exactly as in face to face communication. This form of communication holds different qualities and therefore affects the emotional communication in another way.
In text messaging such as SMS (Short Messaging Service), IM and email the communication unfolds in different ways. The media specific qualities open up possibilities for us to convey different emotions than what we transmit unconsciously in face to face situations, we can hide where we are and we can choose to answer at a later stage. The emotional communication channel in text messaging is often very narrow, leaving the user with smilies and emoticons that hardly mirror the shifting situations in which the communication take place. This opens up for emotional augmentation of text messages that can hold the media specific qualities and allow for the shifting communication situations.
Our goal in this project is to add media channels to mobile text messages for emotional expressivity, this is realized in the eMoto system. The targeted user group for this service is women around 30. This group has consequently been used in the design process and user studies. One of the problems with adding media channels for emotional expressivity concerns how to capture the subtleness and richness that characterizes emotional communication and still keep the media specific qualities within text messaging. The way we express ourselves is ambiguous and allows for many different social processes to take place. The emotions themselves are not clear-cut singular states that need to be expressed, neither should the design be context- free or simplistic signs. For example, a shrug can mean
2 many different emotions depending upon context and how it is mediated [25]. It may be a shrug asking for
forgiveness or it may simply mean I do not know. Emotions are not residing solely in the brain, but also in our bodies, [5] and [4]. Using sub-symbolic expressions when adding media channels, such as colors, shapes, animations, sounds or haptics, may offer an alternative route where we address more of this subtleness. This may also allow for an intuitive, physical reaction to the expressions, which in turn will address the physical nature of emotional experiences. The question posed here is whether we can mirror some of the subtleness and ambiguity of emotional interaction through creating a richer medium than what is available in mobile text messaging today, but still keep the media specific qualities? Below we present the resulting design expressed in the mobile service named eMoto and a user study of the expressions we designed. Our aim is to show that the way we realized ambiguity in the design of the expressions, their sub- symbolic form, and how we made use of an analysis of body movement from the design process, did indeed produce a design foundation that users could use as a creative tool for emotional expressivity.
Background
eMoto is an ongoing research project at the Interaction lab at SICS (Swedish Institute of Computer Science) and DSV (Department of System Sciences) at Stockholm University/KTH in Kista, Sweden. It is part of the Mobile Services project initiated in October 2002 and financed for three years by Swedish Foundation for Strategic Research. Focus in the project is on future mobile services. The project is conducted in cooperation with partners from the industry, the receiver of the research results from the eMoto-project is Sony Ericsson. This part of the project was conducted by members of the Interaction lab at SICS. The group consisted of one interaction designer/researcher, two
Kiss Communicator
In kiss communicator designed by Heather Martin and Duncan Kerr of IDEO a kiss is blown into an oval shaped product, which in turn creates a red, blinking light in the other person’s device.
The White Stone
Concern for the other is shown by holding one of the stones in your hand, which in turn makes the other person’s stone warm, thereby communicating affect.
ExMS
Is an avatar-abased messaging system where users create short pieces of animated film to send to each other on their mobile phones.
AffectiveColor
AffectiveColor is not for emotional communication, but for structuring existing text from an emotional perspective, using color.
Kinedit System
In this system a combination of shape and animation of text in a message is used to convey emotions.
Human Computer Interaction researchers with programming skills.
Challenge
The specific focus in this project is on mobile messaging, and thus limited by the restrictions offered by mobile devices in terms of size and communication bandwidth, so-called baby interfaces.
Solution
In this section we categorize existing systems and artefacts for emotional communication, we describe the design requirements for designing for emotional expressivity, the eMoto system, and the design process of how we arrived at the expressions using the sub- symbolic form of colour, shape and animation.
Process
Categorizing emotional communication artefact
Finding novel ways of expressing emotional communication through interactive media has been tackled in different ways by designers before us. Besides the smilies and variants of smilies, such as emoticons, we can distinguish between three classes of solutions: communicating through specifically designed artifacts, communicating through composing multimedia messages (MMS, Instant messaging with photos and emoticons, etc), and communication of affect through adding sub-symbolic expressions, such as colors or animations, to textual media of different kinds. The first set of solutions, the designed communicating artifacts, typically involves equipping two people who have a close relationship with some kind of artifacts that can communicate wirelessly (e.g. [24], [20], [1], [11]). These solutions can often communicate presence or indicate that the other party is showing some kind of concern. Examples of systems using this kind of communication is the Kiss Communicator [2] (see sidebar) and the White Stone [23] (see sidebar). These
3 solutions have obvious constraints when it comes to
communicating different and complex emotions. They are also dependent on specific devices for the communication. But what is interesting about them, is the simplicity of expression and the use of physical interaction means.
Our second category of communicating affect is through various multimedia possibilities. Many communication media allows for emoticons, photographs, avatar expressions or other ways of communicating affect. Here the emotional expression is entirely within the hands of the user. They choose which smiley, emoticon or avatar expression to pick, and it is subsequently shown to the other party.
An example is ExMS [16] (see sidebar). Visualization of affect, through sub-symbolic expressions, has been used in several systems. One example is AffectiveColor [14] (see side bar). A combination of shape and animation of text in a message to convey emotional content is used in the Kinedit system [9], (see sidebar). Apart from adding color and animations, some have also attempted to add haptics to the multimedia communication channels, see e g the system ContactIM [17]. Many of these systems tend to emphasize the positive emotions that we communicate to others, but in our relationships with one-another, dealing with negative emotions are oftentimes equally, or even more important.
Design requirements
First, in interaction between people, emotions are seldom labeled and explicitly expressed. Emotions are complex states conveyed dynamically through combinations of signals in body language and what is spoken. In this sense, they are mostly sub-symbolic expressions, not phrased in words or other symbols. They are understood from the specific context in which they are said. Emotions may be suppressed or emphasized depending on what the sender wants to achieve e.g. expressing pride to support a kid or anger
Figure 1. The p900 phone with the extended stylus.
to balance an intimate relationship. Any medium designed to express emotions must similarly avoid explicit labeling of emotions, but instead strive to have less sharp edges. We may feel slightly angry, as in close to giving up rather than furious, or we may feel happy as in excited or happy as in satisfied. Emotions tend to blend into one another without any defined borders. This needs to be mirrored in the way affective content is represented.
At the same time, simply providing a richer channel, such as videoconferencing or other attempts to mirror real life-meetings, does not harmonize with the media- specific qualities of text messaging interaction channels such as SMS, IM and email. In text messaging the communication unfolds in different ways than face-to- face meetings. We can choose to hide exactly where we are, we can leave messages for others to pick up at a later stage, and we can convey somewhat different emotional content than what we are transmitting unconsciously for real. Thus, any solution that tries to create a richer, more expressive media, as part of existing text messaging needs to take the media- specific qualities seriously so that its advantages are not lost.
In addition, a medium for expressing emotions using sub-symbolic expressions needs to strike a balance between making the expressions too abstract or too depictive. Too abstract expressions will lead to random interpretations and make it hard to find suitable expressions. Too depictive and symbolic expressions on the other hand, makes its hard for the sender to express herself freely and does not allow enough leeway for interpretation.
Finally, we understand each other from a dialogue and knowledge about each other. Raising an eyebrow to your mother might have a different meaning than doing so to your boyfriend. It is about your knowledge of the other party, the specific dialogue you are in, and the social setting of where you are meeting, any representation of this need to allow for you and the
4 other party to interpret and agree upon the meaning of
the message.
eMoto
We have explored how to design for emotional expressivity in the mobile application eMoto. eMoto is designed for a specific targeted user group, a persona [3] was set up. In short, the persona is named Sandra, she is 29 years old, Sandra does not care much of how things work technically, but she likes new cool features. Let us provide a short description of the system. eMoto is built in Personal java and runs on Sony Ericsson’s Symbian phones, P800 and P900, both have touch sensitive screens that the user interacts with through a stylus. In eMoto the interaction is done through an extended stylus with an accelerometer and a pressure sensor added (see sidebar, Figure 1).
Imagine that Sandra is waiting for the bus that is delayed as usual. Sandra is on her way to the city, she has planned to meet her friend Mia later. She starts to write a message in eMoto to Mia. Sandra and her friend have recently discussed that the bus from where Sandra lives is always late. So Sandra writes: I am enjoying another pleasant bus trip to the city centre! (See sidebar, Figure 2)
The navigation in eMoto is done by gestures, consisting of a combination of pressing and shaking the extended stylus to navigate a background with colours, shapes and animations (see Figure 3).
The navigation starts out from the centre of the background. By different strength in pressure on the stylus the user navigates horizontally, more pressure lead to the dark and purple expressions and less pressure to the yellow areas. By more or less vigorous shaking the user navigates vertically, more shaking lead to the red areas and less shaking to the blue areas. By a combination of pressing and shaking at the
Sandra is waiting at the bus stop
Sandra composes a message
Mia receives Sandra’s message
Figure 2. Scenario of use. Laban
Was a famous choreographer who invented a notation for describing the shape and experience, or effort, of different body movements. Laban’s theory oftentimes referred to as LMA (Laban’s Movement Analysis).
same time the user can navigate the whole background.
Figure 3. The background with colors, shapes and animations. To express this irritation over the delayed bus, Sandra
starts pressing the stylus quite hard and shakes it just a little bit. The background of her message starts changing, moving from the white starting point and taking lighter blue shades with slow animations. Sandra puts more pressure on the pen and keeps the amount of shaking and the background gets even darker. Now she is pleased. This background together with the text will make Mia understand. (See sidebar, Figure 2).
The expressions in the background are built up by a combination of colours, shapes and animations. Vertically in the circle, different strength in arousal is expressed, low arousal in the blue areas with few and slow animations and high arousal in the red areas with many and fast animations. Horizontally, negative expressions are on the blue-purple-red side with
5 sharper objects and positive expressions on the green-
yellow-orange side with softer, rounder shapes.
Mia receives Sandra’s message with the bored dark- purple colours and slow animations. She immediately understands the irony in the message. (See sidebar, Figure 2).
In eMoto, it is important that the gestures and the response from the system form a coherent whole that empowers the users to express themselves, not something that hinders them and has to be consciously reflected upon.
In eMoto we want to address the physical, almost bodily aspects of emotional experiences, and thus we decided to analyze emotional body language as a starting point for our design. This analysis is presented elsewhere [8], but in short summary, we analyzed the body language of an actor expressing different emotional processes through using a so-called Laban- analysis [6] (see sidebar). Four examples of the resulting analysis are presented here (see Table 1).
Emotion
Excitement
Anger
Surprise-Afraid
Shape
Extremely spreading, rising and advancing Somewhat spreading, rising and advancing Enclosing, somewhat descending and retiring Enclosing, somewhat rising and retiring
Sulkiness Table1. Examples from Laban-analysis
Together with a dimensional model of emotion, using the two dimensions of valence (from positive to negative emotions) and arousal (from high to low arousal), we could create a description of a large set of emotions, blending into one another. In a sense, we could also describe how these emotions are experienced. The dimensional model we picked is named a circumplex model of affect and was created by psychologist Russell who in turn had based it on how a
Figure 6. Transformation of Itten’s circle
large amount of subjects placed different emotions in a circle created by the two dimensions [18] (see Figure 4).
Figure 4. The circumplex model of affect
First, we turned these findings into a set of gestures that are described elsewhere [8], [21]. Basically, the gestures are designed to pick up on the two dimensions arousal and valence, where arousal is expressed through the more or less vigorous shaking of the stylus with inbuilt sensors, and valence is expressed through more or less pressure on the same stylus.
Creating expressions from color, shape and animations We used the characteristics expressed in the variables shape, arousal and valence of the emotional body movements, described above, as a basis for the design. Different colors, shape of objects and animations can be used as a medium to express the characteristics of each emotional expression and thereby position them in Russell’s circumplex model of affect. By allowing objects of different shapes to move at different speeds, in different directions and for different durations, it
6 should be possible to mirror the shape and arousal of
emotional body movement. Thereby let the gestures and affective content form a coherent whole. As a starting point to find suitable expressions both in color, shape and animation an image board was put together as inspiration (see Figure 5, illustrations in the end). Where different photos, representing the expressions we wanted to achieve, were placed at different locations in the circular model. For example, pictures showing high energy, i.e. high in arousal, such as a picture of lightening, were placed in the top right- end corner. Those pictures in turn inspired the design in that area to include smaller, sharp-edged objects, animated with faster movements and for shorter durations. Pictures showing less energy, calmer emotional expressions, such as pictures of water, were placed in the bottom right-end corner. They inspired the design of objects to be bigger, more connected shapes, and the animations to be slower and wavelike.
Color, the theories of color psychology attempts to generalize people’s initial reaction to different colors, it is for example claimed that the color red raises blood pressure and therefore should be a good color to represent anger. There is a debate to what extent these color theories are generally applicable and how much is cultural dependant. The difference is that these theories deal with the initial reaction to the color, something that then should precede the learnt cultural interpretation. We decided to use the color theory by Ryberg [19] that does not take cultural interpretation into account, but only talks about the energy of different colors and then test this against our user group (in our user study described below). Thus, we would not claim that our choice of colors is culturally independent or even relevant to all users groups within a culture. According to Ryberg’s theory, red represents the most powerful and strong emotions, moving along a color scale ending with blue, representing less energy. This concurs nicely with how colors are distributed in different theories
Figure 7. Sketches, (for bigger picture see illustrations in the end).
Figure 9. All of the objects.
[10], [13] where colors are placed in a circular model, complementary to each other (see Figure 6). Colors are often associated with specific qualities like warm–cold, light–heavy, passive–active and so on. Goethe writes about positive and negative colors, where the negative colors are blue, red-blue and blue- red, and the positive colors are yellow, red-yellow (orange) and yellow-red.
Our starting point became Itten’s color system that was adjusted to fit Russell’s circumplex model of affect. This meant making the colors gets weaker towards the middle of the circle where arousal and valence are on neutral. The colors in Itten’s circle were also reflected and turned so that the negative colors, described above, were placed on the negative valence side of the circle and the positive on the positive valence side. We ended up with a color circle as in Figure 6. The colors that are most powerful, like red, are placed at the top of the arousal/energy scale, while blue, that is considered to be more peaceful, ends up at the bottom- part of the circle. The colors were then smoothed into one-another in the circle in order to allow the corresponding affective expressions blend into one another without any sharp boundaries (see Figure 6). Shapes, in product semantics within the field of industrial design, products’ and objects’ physical shape and its relationship to how they are interpreted are discussed. Round shapes with soft curves are interpreted as friendly and positive while angular, harder shapes are interpreted as more negative.
The shape of an object is a sign to the user; the more depictive the object is, the less meaning users can read into it. For example, a rectangular volume shape with random proportions is more ambiguous and can be open to many more different interpretations than can an emoticon portraying a smiling face. These insights were used when designing objects to add to the colored background. [15].
We used the shape-descriptions from the Laban- analysis, e.g. designing the objects to allow for the
7 more energetic emotions, so when animated they could
be inspired by the upwards, rising movements found in the analysis. The shapes of the objects, that are supposed to be spread all around the circle, were first sketched out using pen and paper (see Figure 7), exploring the shapes and trying to find the right expression that later on could be animated. Based on product semantics and the image board, objects placed on the positive valence side in Russell’s circumplex model of affect were designed with some roundness in shapes and lines. Objects placed on the negative valence side in the circle were made more angular and sharp in their expression (see Figure 8).
Figure 8. The objects, on the positive side, negative side.
Excitement and sadness are the two extremes on the arousal scale, in-between those extremes the objects’ shapes are modified to go smoothly towards less arousal or more arousal and to be able to keep their characteristics in shape when animated (see Figure 9). The objects’ shapes also entail the different strengths in each emotional expression, like the background colors, the strength of the objects’ color get weaker towards the middle of the circle, the objects are also smaller in shape and there are fewer animations taking place in the middle.
Figure 10. The whole circle with color, shapes and animations. The circle with animations can be downloaded from: Hhttp://emoto.sics.se/animationsH For bigger picture see illustrations in the end.
Animations. Using familiar patterns of movements that we can recognize from our daily life, like body language or movements in nature like a lightening or a calm sea is one way of conveying affect in animations. Harmony and disharmony of animation as conveyed through objects’ movements can also be used. Disharmonic movements that e.g. make it seem as if objects are close to colliding can portray negative expressions, while harmonic, soft movements may be interpreted as more positive. Affect in animation can also be created in the way the objects are laid out, by using depth in the picture, creating overlays. [22].
These insights were complemented with the Laban- analysis of the characteristics of both shape and arousal found for each emotion. For example, to create an animation expressing sadness, which according to the Laban-analysis is enclosing, descending and retiring in its shape, the object has to entail these qualities in its visual shape. That means that the object has to be big and connected in its shape, since a set of small, divided objects would not easily portray an enclosing, descending and retiring movement. On the other end of the circumplex model, it would be difficult to create movements for the emotion excited with one big connected shape, since this emotion is extremely spreading, rising and advancing in its shape, and thus is more easily expressed through a set of small, divided objects.
Adding animations allows for more emotional expressive powers since not only the arousal-parameter but also the characteristics of the shape parameter from the Laban-analysis can really come forward here. In order to entail the arousal-parameter, emotions with higher arousal are moving faster, and an even higher impression of hastiness is created through letting objects appear and disappear, enabling a blinking sensation. With higher arousal the numbers of animated objects are higher. With lower arousal the animations are slower and have more wavelike
8 movements and the numbers of animated objects are
fewer. To portray the difference in positive and negative valence within the animations, the animated objects on the positive side, make much more harmonic, smoother, rounder movements, while on the negative side the movements entail more disharmony and are harder and jerkier in their appearance. When the user has chosen an affective expression that suits the written message and stopped navigating around in the circular area in eMoto, the entire picture that fits within the baby interface space, starts moving slightly. This creates a depth in the picture, making the background less static. Thus, in summary the whole circle with color, shapes and animations combined, forms the affective feedback with a basis in shape, arousal and valence (see Figure 10).
Solution details
To see if the designed expressions could work as a foundation for emotional expressivity the expressions were tested in a user study, which we describe in detail in this section, the tasks and procedure and the results, which in turn lead to a redesign of the expressions are described. To sum up, a discussion concerning the results of the study and the redesign is held.
User Study
In order to get feedback on the designed expressions, we set up a user study. Our aim was to show that the way we realized the sub-symbolic form, the ambiguity and subtleness in the design of the expressions and how we made use of the Laban-analysis, did indeed produce a design foundation that users could use as creative tool of emotional expressivity. We wanted to know whether:
•	the design captured the ambiguity and subtleness that characterizes emotional communication
•	the sub-symbolic form (color, shapes and animations) in the expressions was interpreted and could be used as a common foundation for creating emotional expressions
•	our design inspiration from body movements (the Laban-analysis) was reflected as intended in the colors, shapes and animations
Important to remember is that there is not really any right or wrong here. We can only compare to what we as designers intended and whether this came through in the design or not. We are looking for clues that will help us to redesign the expressions to better convey our design intentions. To find these we are looking for those instances where the designer had intended for one kind of expression and it was altogether misunderstood by the subjects.
Subjects
We recruited the subjects through putting up notes around Kista, a working area outside Stockholm, asking for female subjects between 25 and 35 who were frequent mobile phone and SMS users. 12 subjects signed up for the user study.
Tasks and procedure
The subjects worked in pairs during the study [17] and the pairs knew each other from before. The idea was that if they were acquainted it would make it easier for them to talk aloud about what they were seeing and experiencing, especially as they would be talking about emotional expressions. Both of the subjects were placed in front of the computer, with a camera filming them from the front. In situations where only one of the subjects was supposed to see the computer screen, the other subject moved to a second chair opposite to the screen. One of the tasks in the study was projected onto a wall, for the subjects to see the whole designed circle in full scale. The subjects then moved and stood
9 next to the projection (see Figure 10). The study was
divided into four tasks. Cut-out-task. In the first task, the cut-out task, subjects were shown cut-outs of four emotional expressions each. The cut-outs from the circle had the size of the screen of a P900 mobile phone (208x256px). They were asked to take turns in describing the expressions to each other using adjectives that fitted with what they saw. The second subject would listen, sitting in a second chair where they could not see the computer screen. Before starting this task, they were given an example, where a cut-out of an expression was shown to them which was described as: leafy, green, alga-like, soothing, peaceful, billowing, relaxing, calm and soft. In total, eight expressions were described this way by each subject pair (see Figure 9). 8 expressions. Second in the 8 expressions–task, pairs sat together in front of the computer, viewing all eight expressions they had previously described to one another. They were now asked to agree upon one emotional word suitable for each expression. Since they had to agree upon one word, they had to discuss and motivate to each other what and why they experienced the expression as they did. (See Figure 11)
Figure 11. The eight cut-outs, representing different parts of the circle, shown to the subjects.
Scenarios:
Countryside visit: You want to go the countryside together with your boyfriend to relax after a stressful week. You send him an SMS about your wish.
Housecleaning: You and your boyfriend had agreed upon him vacuum cleaning, when you come home he has not done it yet and your parents are coming over for dinner tonight. You send him an SMS.
Racist: You and one of your closest friends where not allowed at the pub where the rest of your girlfriends were hanging. The guy at the door suggested that people like your friend did not fit at the pub. Your friend happens to be colored. You decide to write an SMS to tell the others.
SPA-weekend: You are at a SPA- weekend, having a nice relaxing time. You reply to a message from a friend asking what you are up to during the weekend.
Dinner: You went to a nice dinner at a friends place yester, nothing special but nice. Now you want to thank her for the dinner through sending an SMS.
Job: You’ve just finished your education and have applied for a position that is absolutely perfect for you and that you really want. Now you’ve just been told that you got the job amongst more than a thousand applications. You write to tell your boyfriend about this.
Post-its. During the third task, the post-its task, the whole circle with colors, shapes and animations was projected, in full size, onto the wall. The subjects were asked to place post-it notes with different emotions written on them where they thought that emotion was expressed in the circle. If they could not find a suitable place, they were to place it outside the projection. The emotions written on the post-its were: excited, happy, frustrated, angry, depressed, bored, relaxed and pleased. The reason for choosing these were to get different parts of the circle tested.
When the subjects had agreed upon a place the experimental leader asked them what in the expression chosen that resembled the emotion. If no expression had been found, they were asked to describe what they were missing. Scenario. The fourth and last task, the scenario task, was to navigate in the circle to find an affective expression fitting a pre-written text message and a scenario read by the experimental leader. The subjects navigated using buttons in the interface as input for moving up, down, left and right, seeing only the cut-out representing the screen of the P900 mobile phone. Eight scenarios were tested (see side bar). After each scenario and chosen expression the subjects were asked which emotion they wanted to convey and what it was in the expression chosen that expressed that emotion (since these scenarios are not necessarily associated with one single emotion). If they could not find anything suitable, they were asked to describe what they were missing. In all of the four tasks, except for the cut-out task, the subjects were allowed to make one choice each, if they could not agree upon an expression between themselves.
Results
The results are divided into the three main questions asked before the study:
10 •	the design captured the ambiguity and subtleness
that characterizes emotional communication •	the sub-symbolic form (color, shapes and
animations) in the expressions was interpreted and could be used as a common foundation for creating emotional expressions
•	our design inspiration from body movements (the Laban-analysis) was reflected as intended in the colors, shapes and animations
Ambiguity and subtleness
One of the design intentions was to try to capture the ambiguity and subtleness that characterizes emotional communication that emotions blend into one another and that users could make use of the expressions as a creative tool.
Blending and ambiguity
As mentioned above, emotions blend into each other and are ambiguous. For example, satisfied can be more towards satisfied-relaxed or towards satisfied-happy. In the post-it task when placing the notes with the emotion satisfied one of the groups saw satisfied as close to happy and therefore placed there note high up towards the intended area of happy (see Figure 10).
Satisfied – I would like to have it here actually. It is warmer than happy. Yes, I am more towards that end of the scale. A sunny warm day...
Another group interpreted satisfied as being of a more relaxed character and therefore placed their note further down closer to the intended area of relaxed (see Figure 12).
Down towards this blue here, it feels kind of satisfied sort of, that kind of relaxed content.
Work-out: You’ve decided with a friend that you are going to go to the gym after work, but you discover that you do not have time enough and want to send a message to express that you’re sorry but there is just no time.
Break-up: You’ve been together with your boyfriend for three years and you think it has been pretty good, but now he has told you that he wants to break up. You have no energy to phone so instead you send a message to tell your friend about the break-up.
11 might be that those areas of expression are important
as they provide a counterbalance.
Communicating through the expressions
It is important that users can use the expressions and feel that they can communicate through them, mirror their specific way of expressing emotions, also when having a specific receiver in mind. In some tasks it became evident that the subjects made a different choice of affective expression when they started to reflect over who the receiver of the message was. They had the receiver’s personality and their relationship in mind. Following is from the scenario-task when picking out an expression to the house-cleaning scenario:
Subject 1: I would be pretty angry. Subject 2: OK. Subject 3: But I would not express it that way, then he would just be angry in turn and then we would start fighting and then there would certainly not be any vacuuming done.
The sub-symbolic form
The tasks 8 expressions and post-its both tried to discern whether the affective expressions with their sub symbolic form of color, shape and animation could be used as common foundation without any previous knowledge or shared context.
In the same ballpark but individual The illustration of the result in post-its-task (see figure 13, illustrations in the end) shows where the six groups placed the eight post-it notes with different emotion labels. This result shows that for each emotion almost all post-its were placed generally within the intended expression-area for that emotion. This means that the affective expressions work as a common foundation for emotional expressivity. With the exception of about one note each in depressed, relaxed and happy. Important to note here is that we would and should not, expect all
Figure 12. Placement of post it satisfied by the two groups.
When letting the subjects choose different affective expressions to the emotion they want to convey, it is shown that the affective expressions must allow for blending and ambiguity. As can be seen in Figure 13 (see Figure 13, illustrations in the end), different groups have placed their notes in different parts of the circle. This shows that the circle allows for ambiguity and works as a foundation for creating expressions, since they remain the area intended. In both the scenario task and the post-it-task, the difference in placement show that the expressions allow for ambiguity within the emotions.
Studying the results from the scenario-task it became evident that a larger area of affective expressions gets covered when expressing some emotions – some emotions require a larger difference in strength, blending and ambiguity than others. This might be an indication that the whole expression area should not have the shape of a perfect round circle with equal slices for each emotion area. On the other hand, it
notes to be placed at the exactly same location. The interpretation of an emotion varies, and the circle is designed for ambiguity and for emotions to gradually blend into one-another and allow for users to pick different expressions in order to express their own personality and experience.
An extreme example of this occurred for the post-it with the word depressed. All subjects but one placed the notes in the area intended. The subject who picked a different location, decided to place her note in the yellow happy area. She claimed that sunsets made her depressed and the color in that area reminded her of that. A similar situation arose for the post-it relaxed, where one subject placed her note in the happy-area. She explained that she interpreted relaxed as a very positive relaxed emotion. This also shows that our understanding of different emotions can be very individual. For the post-it happy one note was placed in the area intended for frustrated emotions. The subject said that it was placed there because of the colors, although she did not think the shapes were suitable for the emotion happy. The result of the scenario task is indicating the same result as the post-it-task (see Figure 14, illustrations in the end): the circle works as a foundation for the creation of expressions, but allows for letting context, own experience and personality be reflected in the choices. For example, for anger/disappointment one subject picked an expression from the area with calm expression. The subject motivated this from being scared of her dentist who had this color on the walls of his reception:
Here we are with the dentist, at his office. Genuinely 80ties plastic green, dentist chair really. Quite unpleasant. Like, yes, textile wallpaper in a waiting room that someone has intended to be nice, but which really is not nice at all.
12
Mixed emotions An interesting category that appeared was mixed emotion. Subjects sometimes wanted to express a little bit of everything and none of it very strongly. For example, in the countryside-scenario one subject motivated her choice to express a combination of the tired feeling of a long hard week working and the positive feeling expected from being at the countryside:
This tiredness, it is kind of green, these darker colors that come up here, but then you see the light at the horizon there at the top, that it is fun at the country-side, there the sun will be shining. But right now, when you have this dark here mostly, kind of, it is slow on Friday afternoons.
Neutral emotion expressions
Choosing a “neutral” expression, that is picking the middle of the circle, was something that occurred when the subjects did not think the message was really emotional at all or needed any added emotional expression. Picking a neutral expression does not always mean that the subjects did not want a background to the message. It might as well be a way of showing that the message does not have such a strong emotional content. For example, in the work-out scenario, the message was simply neutral emotionally to one of the subjects.
Yes. Because this is yet anyway in the middle, all of it, it is not sad and not like really happy either, neutral. Like: “I am sorry but I cannot, there is no time”
Depictive – non-depictive
When designing for affective expressions it is important to have the right balance between expressions that are too abstract or too depictive. As discussed in our requirements above, users should not become totally confused about the meaning of the expressions, on the
Cut-out 1.
Cut-out 3.
Cut-out 5.
Cut-out 7.
Cut-out 2.
Cut-out 4.
Cut-out 6.
Cut-out 8.
other hand, they need not be so definite in their form so that they get in the way. People often try to read in symbolic meaning into abstract patterns of shapes and try to construct a story around it [12]. But some shapes or pictures seem to carry a stronger symbolism and be bearer of stronger emotional content than others. Certain shapes are also more culturally dependent than others. In our culture roses are associated with courting, an abstract shape reminding of a rose, might make users think of romance. Here the cultural notion overtakes the expression – the shape is too depictive. On the other hand, animations of circles that remind the user of champagne bubbles might trigger interpretations of bubbly feelings, and the link to champagne provides for the right association with happiness and thus will not get in the way for the interpretation. In the cut-out task three out of six groups associated cut-out number 6 in figure 15 (see sidebar) with a rose and thereby with romance rather than the frustrated expression it was supposed to portray. On the other hand four of six groups associated cut-out number 8 in figure 15 (see sidebar) with partying, bubbles and champagne and thereby got the right connotations – an excited expression. In our view, roses have much symbolism associated to them and are a strongly culturally dependent, while bubbles in champagne can be associated with the upwards movement they make and the energy they contain.
Dark ends need more darkness
We transcribed all the words that our subjects used to describe the different expressions. When analyzing the words, the expressions for the areas of relaxed, happy, calm, excited and angry seem to work very well, the expressions for the areas of sad and depressed seem to work quite well, but the expression for the area of frustrated (see sidebar, cut-out 6 in Figure 15) seems not to be interpreted in the way intended at all. Words like love, passion, a flower, artistic and cool were
13 frequently recurring for that cut-out. Like one subject
put it:
Almost something towards James Bond with all those ladies... or I see it a little bit in the background anyway. And then there is... now it is warm colors. Lilac and red. It is kind of stylish, a bit cool kind of.
Our interpretation is that the expressions for frustrated, sad and depressed are a bit contradictory in their appearance; the shapes are sharp, the movements in the animations indicate collisions, but the colors are fairly bright in their appearance. To work as a common foundation for emotional expressivity they need more darkness in order to convey their negative nature.
Design intentions: conveying the Laban-analysis
The design of the affective expressions originates in the analysis (Laban-analysis) of emotional body movement in terms of shape, arousal and valence. Some of the descriptions our subjects gave are easy to tie back to the shape and arousal variables for the different emotion characteristics, while others are harder. For example, the emotion excited has, according to the Laban-analysis, a shape that in its movements is extremely spreading, rising and advancing, its arousal is very high, and it is placed on the positive valence side. The subjects described the expression excitement in terms that can easily be interpreted as spreading, rising and advancing movements: bubbly, carbonized, simmering, lively, sparkling, bouncy, twinkling, and pulsating. The words subjects used that might be interpreted as high level of energy were: party, high energy, movement, full of life, crazy party, colorful, boiling soup, lively, happy, and warm. That the expressions is understood as a positive emotion may be indicated by: the sun, party, warmth, warm colors, happy, harmonic, nice, red, positive emotions, this makes you feel good, amused, and happy.
Figure 15. The cut-outs representing different parts of the circle.
Figure 16. The redesigned circle, For bigger picture see illustrations in the end.
When analyzing all the words that our subjects used to describe the expressions, it is possible to see that some descriptions directly match the circumplex model of affect by Russell, while others need more analysis in order to be interpreted. For example, through using the theories of color, shape and animation, described above, we can make a tie from subjects’ descriptions, via their meaning in terms of color theory, back to shape, arousal and valence. For example, in color psychology, red and yellow contain high energy and are considered to be positive and warm colors. This means that whenever subjects use these color words or words strongly associated to them like the word the sun, they can be viewed as indication of high energy and positive valence.
The same analysis can be used on descriptions of objects that in their shape can be found as soft and round in curvature, and thereby give an indication of belonging to the positive valence side or vice versa. The shape of the emotion angry/frustrated is described as somewhat spreading, rising and advancing movements. It is high in its arousal and has negative valence. When looking at the words describing the cut- out in the area intended for this emotion, it was harder to find words that can by analysis be found to match the description. Some words can be directly matched to the circumplex model of affect, but these are quite few, like aggressive, angry and that it looks frustrated. The descriptions restless movements or messy do create associations with the right shape properties, and arousal can be associated with the descriptions fire, heath and pulsating. The valence can be shown by words like thorny, sharp, angular shapes and stressful. But a predominantly part of the words, like artistic, creativity, cool, love, modern, passion, and a rose cannot be tied back to the characteristics found in the emotion of frustrated.
The words used to describe the cut-outs of the areas relaxed, calm, happy, anger, and excitement contain to a large extent words that this way can be tied back and
14 thereby seem to convey the emotion intended. The cut-outs for depressed and sad contain less of these
characteristic and the cut-out from the area of frustrated contain even less than the previous two. Thus, this helps us to find the problematic areas that may need some redesign in order to convey our design intentions properly. In summary it is, with a few exceptions, possible to tie the subjects’ experiences of the expressions back to our underlying design intentions.
Redesign
The redesign aims for the affective expression to work as a common foundation for emotional expressivity without any previous knowledge or shared context. The intentions of the study were to lead to a redesign, for users to have a foundation of expressions, working as a basis for emotional creativity. The expressions for frustrated, sad and depressed are all placed next to each other and do also blend into one another. Therefore some of the problems with the expressions may be covering the area of all three, more serious in some parts of the area. The words used to describe the area of frustrated with both containing words such as aggressive and thorny, but is by the same subject described as love and passion, indicates that there might be a double message conveyed from the expression. Our interpretation is that the color does not have the right hue to convey the same message as the shapes and their animations. In the areas of sad and depressed, the contradiction is not as evident as in frustrated but it is still there.
In the expression for frustrated, different pictures are also read into the expression, such as roses and women, these pictures in combination with the color leads to even more misinterpretations. The background color and the color of the shapes were therefore redesigned, reducing the brightness, adding much more grey hue. In the area of the expression frustrated some of the shapes were also redesigned to avoid the
depictiveness found earlier (see side bar, Figure 16). This was subsequently tested in a second study [21].
Discussion of results
As the result show, adding channels for emotional expressivity in mobile text messaging, with the limitations that comes with the mobile devices in terms of size and communicative bandwidth, does not have to be a problem as long as the media specific qualities within SMS are taken into account.
Emotional communication is in itself subtle and ambiguous and therefore situated, dependent on context to convey meaning and open up for interpretation. The results of the study show that to be able to capture these characteristics require creative tools for expressivity with a common foundation for understanding, rather than providing users with ready- made expressions chosen by a majority of the subjects. Providing people with an expressive channel in text messaging means helping users to get started in their creation of expressions, but keeping a balance between providing the users with material to take in use and on the other hand giving them too much freedom, leaving them with a blank paper.
The results also show that it works to use the sub- symbolic expressions of color, shape and animation, to help the users get started making use of the expressions. The subjects are all first time users, without any knowledge about the expressions and many of the tasks in the study are without context, but all of the subjects can approximate suitable expressions. By using the sub-symbolic media it is possible to create expressions that are interpreted as emotions and can help getting users started with their messages. The intentions with this study were to get input to a redesign of the affective expressions. Some parts of the affective expressions seem to work better, like the areas expressing relaxed, calm, happy, excited and angry. The areas intended to express frustrated, sad and depressed were not as successful and need to
15 be altered. The expression for frustration seems to
have the wrong level of abstractness, which lead the subjects to interpret objects as pictures being bearer of cultural meaning. The expressions for frustrated, sad and depressed might also be a bit contradictory in their appearance, which caused confusion by the subjects, since the shapes were sharp and negative, the movements on collision course, but the colors were too bright and positive.
The design also shows that the affective expressions allow for ambiguity and blending of emotions. Subjects do place emotional words within the areas intended for a specific emotion, but still their choices were spread out over the whole area, just as emotions tend to blend into one another without having defined borders or sharp delimiting definite meanings.
The subjects seemed to be able to develop their own way of using and interpreting this new creative tool for expressivity, by adopting the expressions and using them in context. In composing messages aimed for other people the expressions opened up for different interpretations depending on whom the expression was aimed for.
Making the expressions too open and ambiguous, might lead too confusion, on the other hand too little ambiguity would not allow for interpretation to happen. The affective expressions, when situated, seem to have the right balance of ambiguity to allow for this to happen, building a foundation for the users to start out from, developing their own ways of using them in their own communicative realities.
References
[1] Brave, S., and Dahley, A. inTouch: A Medium for Haptic Interpersonal Communication, Extended Abstracts of CHI'97 (Atlanta, March 1997). ACM, 363-364.
[2] Buchenau, M., Fulton, J. Experience Prototyping. Symposium on Designing Interactive Systems 2000 (Brooklyn, NY, August 2000), ACM, 424-433
[3] Cooper, A. The Inmates are Running the Asylum, Sams Publishing, USA, 1999.
[4] Damasio, A. R. Descartes’ Error: Emotion, Reason and the Human Brain, Grosset/Putnam, New York, 1994.
[5] Davidson, R. J., Scherer, K. R., and Goldsmith, H. H. (eds.) Handbook of Affective Sciences, New York and Oxford: Oxford University Press, 2003.
[6] Davies, E. Beyond Dance, Laban’s Legacy of Movements Analysis, Brechin Books ltd., London, UK, 2001.
[7] Dumas J.S., Redish J.C., Handbook of Usability Testing, John Wiley &amp; Sons Inc, USA, 1994.
[8] Fagerberg, P., Ståhl, A., and Höök, K. Designing gestures for affective input: an analysis of shape, effort and valence, Proceedings of Mobile Ubiquitous and Multimedia, MUM 2003, (Norrköping, Sweden, 2003).
[9] Forlizzi, J., Lee, J., and Hudson, S.E. The Kinedit System: Affective Messages Using Dynamic Texts. Proceedings of CHI 2003, (Ft Lauderdale, April, 2003), ACM, 377-384.
[10] Goethe, J. V. Goethes färglära, Kosmos förlag, Stockholm, Sweden, 1976 (translated by Pehr Sällström from Zur Fahrbenlehre (1810)).
[11] Goodman, E. and Misilim, M. The Sensing Bed. Proceedings of UbiComp 2003, (Seattle, 2003).
[12] Heider, F. and Simmel, M. An Experimental Study of Apparent Behavior, American Journal of Psychology, 57 (April 1944), pp. 243−59.
[13] Itten, J. Kunst der Farbe, Otto Maier Verlag, Ravensburg, 1971.
[14] Liu, H., Selker, T., and Lieberman, H. Visualizing the Affective Structure of a Text Document. Extended Abstracts CHI 2003 (Ft. Lauderdale, April 2003)
[15] Monö, R. (1997) Design for Product Understanding, Liber AB, Stockholm, Sweden.
[16] Persson, Per (2003): Exms: an animated and avatar-based messaging system for expressive peer communication. In: Tremaine, Marilyn, Simone, Carla (ed.): Proceedings of the CSCW
16 2003. (Sanibel Island, Florida, November 2003),
ACM, 31-39.
[17] Rovers, A.F., van Essen, H.A. HIM: A Framework for Haptic Instant Messaging. Proceedings of CHI 2004, (Vienna, April 2004), ACM, 1313-1316.
[18] Russell, J.A. A Circumplex Model of Affect, Journal of Personality and SocialPsychology, Vol. 39, No. 6, 1161-1178, American Psychological Association. (1980)
[19] Ryberg, K. Levande färger, ICA Bokförlag, Västerås, Sweden, 1991.
[20] Strong, R., Gaver, B. Feather, Scent and Shaker: Supporting Simple Intimacy, In:Videos, Demonstrations, and Short Papers of CSCW ’96. Conference on Computer Supported Collaborative Work, (Boston, November 1996), ACM, 29-30.
[21] Sundström, P., Ståhl, A., and Höök, K. eMoto – Affectively Involving both Body and Mind, Extended Abstracts CHI’05, (Portland, Oregon, USA, 2005).
[22] Thomas F., Johnston O., Disney animation: The illusion of life, Abbeville Press, New York, 1981.
[23] Tollmar, K., Junestrand, S., Torgny, O. Virtually Living Together: A design framework for new communication media. Symposium on Designing Interactive Systems (London, June 2000), ACM, 83-91.
[24] van der Hoog, W., Keller, I., Stappers, P. J, Gustbowl: technologysupporting affective communication through routine ritual interactions. CHI Extended Abstracts 2004 (Vienna, April 2004), ACM 775-776.
[25] Voida, A., Mynatt, E.D. Six Themes of the Communicative Appropriation Photographic Images. Proceedings of CHI 2005 (Portland, April 2005), ACM, 171-180.
Acknowledgement
We like to thank the anonymous subjects who took part in the user study. This work is partly funded by SSF through the “Mobile Services”-project.
Further Illustrations
Figure 5. The image board, used as inspiration.
Figure 7. Sketches of the objects.
Figure 13. Result of the post-its-task.
Figure 14. Result of the scenario-task, where the squares indicate the chosen background to the emotion the subjects wanted to convey.
Figure 10. The whole circle with color, shapes and animations. The circle can be downloaded from: http://emoto.sics.se/animations
Figure 16. The redesigned circle.</Text>
        </Document>
        <Document ID="298">
            <Title>Presentations</Title>
        </Document>
        <Document ID="187">
            <Title>Domains of Knowledges</Title>
            <Text>Domains of Knowledge


Performance Studies

</Text>
        </Document>
        <Document ID="24">
            <Title>dobson_whitman_ellis_machinevoices</Title>
            <Text>2005 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics	October 16-19, 2005, New Paltz, NY
LEARNING AUDITORY MODELS OF MACHINE VOICES
Kelly Dobson and Brian Whitman
MIT Media Lab Massachusetts Institute of Technology Cambridge MA 02139 USA monster, bwhitman@media.mit.edu
ABSTRACT
Vocal imitation is often found useful in Machine Therapy ses- sions as it creates an emphatic relational bridge between human and machine. The feedback of the machine directly responding to the person’s imitation can strengthen the trust of this connection. However, vocal imitation of machines often bear little resemblance to the target due to physiological limitations. In practice, we need a way to detect human vocalization of machine sounds that can generalize to new machines. In this study we learn the relationship between vocal imitation of machine sounds and the target sounds to create a predictive model of vocalization of otherwise humanly impossible sounds. After training on a small set of machines and their imitations, we predict the correct target of a new set of imita- tions with high accuracy. The model outperforms distance metrics between human and machine sounds on the same task and takes into account auditory perception and constraints in vocal expres- sion.
1. INTRODUCTION
Machines and humans can not make the same sounds, and we need a way to detect what machine a human is vocally imitating. We are particularly driven by our work in Machine Therapy in which hu- mans try to vocally imitate machines, but this task also informs problems such as query-by-vocalization for sound effects and mu- sic signals, and affect detection. For example, a birdsong database could be driven by a vocal query front end, but the problem of map- ping from human imitation of animal sounds to the targets remains unsolved.
Our need is for a generalized model that computes symmet- ric similarity between machine and human sounds. Given a new machine, we would like to detect a human imitation without train- ing. And given a human imitation of a machine, we would like to distinguish the specific machine they are imitating without train- ing. In this paper we learn a model that links machine-generated sounds and human imitations and generalizes to new machines and new human vocal imitations.
Our process is described in Figure 1. We could directly com- pute similarity between the features (shown as the faint dotted line.) This approach has worked for us previously as in Blendie, a Machine Therapy apparatus that allowed people to control the motor speed of a kitchen blender by imitating its sound. Blendie used simple similarity through a statistical model based on dis- tance to a ground truth blender machine sound. [1] We want to move from this approach to a more universal approach by design- ing a generalized auditory model inclusive of all machine sounds and their projections into human vocal space and vice versa. In
Daniel P.W. Ellis
LabROSA, Electrical Engineering Columbia University New York NY 10025 USA dpwe@ee.columbia.edu
Figure 1: Our goal: a model that translates machine sounds to a predicted human imitation so that we can correctly predict the triggering of a machine and vary its parameters.
this paper, we learn this model (the black box in Figure 1) by re- gressing the feature dimensions of the machine sound against the feature dimensions of the human imitation. Then, new machine sounds can be projected into ‘human space,’ with better accuracy in the similarity task.
2. BACKGROUND
Machines share some expressive elements with people and harbor meaning and emotional material that is affective but often unrec- ognized by people. Machine Therapy, an alternate to traditional therapy akin to art therapy and music therapy, utilizes the sounds of machines as relational elements for people to vocally connect with. These unconventional vocal expressions can facilitate access to a human’s own personal sounds and linked psychological states. In traditional psychoanalytic situations, non-verbal vocalizations by people are recognized as often very important and meaningful experiences.
Work in general sound analysis and categorization concen- trates on human entered labels or similarity and clustering among databases of sounds [2]. Similarly, work in music instrument de- tection [3] attempts to detect an instrument sound in a mixed or
Machine to imitation model
Machine sound
Human imitation
2005 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
October 16-19, 2005, New Paltz, NY
monophonic set of audio signals. Related work investigates speech vs. music detection [4] using a wide range of auditory-influenced features. In speech analysis, work has been done on laughter de- tection [5] and emotion detection using prosodic features [6].
Our work in acoustic machine analysis is somewhat informed by work in machine condition monitoring, where signal processing systems attempt to detect possible faults in machine processes. In [7], higher order spectra is used to detect degradation of machine health. In [8] a genetic algorithm is used to select vibration fea- tures to inform a neural network trained to detect faults in rotating machinery.
55 00
−5 11
−5
0.5	0.5
00 0 50 100 150 200 250 300 350 400
0	50	100	150
f0 estimation
0	50	100	150
aperiodicity
200	250
200	250
0 50 100 150 200 250 300 350 400
−3 x 10
aperiodicity
00
0 50 100 150 200 250 300 350 400
power
0	50	100
0	50	100
power
150	200	250
150	200	250
00 0 50 100 150 200 250 300 350 400
spectral centroid
spectral centroid
0	50	100	150
modulation spectral centroid
3.
ANALYSIS AND PREDICTION OF MACHINE VOICES
30 20 10
3000 2000 1000
2000 1000
10
5
2
1
0.01 0.005
f0 estimation
00 0 50 100 150 200 250 300 350 400
modulation spectral centroid
200	250
Our problem is to estimate the quality of a particular person’s im- itation of a machine, taking into account the innate limitations of the sound ‘gamut’ that that person, or people in general, can pro- duce, and also the specific acoustic dimensions or attributes that the person is aiming to reproduce. Ideally, we would like to find the appropriate representational space that captures all the signifi- cant aspects of variation in both machine voice and human imita- tion, and to learn the optimal mappings between human and ma- chine sounds – mappings which will likely vary between subjects, but which will also share a common core.
3.1.	Auditory Features
Machines have a wide variety of possible sounds, and as our task is to create a model that generalizes well to new machines, we need to create a representation that captures perceptually dominant characteristics of the sound without relying on the specifics of any particular machine. We observed that the qualities of pitch, rough- ness, energy, and transients were often imitated by the subjects. In this study we first tried to work with modulation cepstra [9], which largely suppresses pitch information, and our predictions did not fare well.
Instead we chose to focus on five auditory features for each short-time window (w samples, usually w = 2048 for a sam- pling rate of 44,100 Hz) of the input audio. We chose a fun- damental frequency (f0) estimation [10] [11], aperiodicity (the residual from the f0 estimation) [12], a power estimation, spec- tral centroid and ‘modulation spectral centroid,’ the centroid of the Fourier transform of the magnitude envelope along time of each frequency channel in our initial short-time Fourier transform (STFT), Fn(∥X(k, n)∥), where X(k, n) is the STFT for frequency band k and time window n, and Fn indicates the Fourier trans- form taken along the n (time) dimension. Our intuition for de- tecting modulation in the spectral bands is that often the machines have high roughness content in the upper modulation bands, while human imitations can only to try to approximate these sounds in lower bands.
An example set of features extracted from a target sound (a blender) and a vocal imitation is seen in Figure 2. None of our fea- tures encapsulates long-term time scale above our window length w; we consider an analysis of this time scale and transients to be part of future work. For the scope of our features, we treat each frame independently of the next, and there is no explicit corre- spondence between the time pattern of the machine sound and im- itation. Other features to be considered in future work include voiced/unvoiced detection, noise detection, and bandwidth.
Figure 2: Features extracted from a blender sound at left, with the human imitation at right. Five perceptual features: f0 estima- tion, residual (aperiodicity), power estimation, spectral centroid and modulation spectral centroid. Time segemnts on the horizon- tal axis, feature output on the vertical axis.
In the experiments section we perform a feature selection task that attempts to discover the best performing individual features from the set of five. Different machine types respond better to different types of auditory modeling, and to make a general model we choose the best performing feature combination.
Before learning or evaluation, all the features (in the train- ing subsets) had their mean removed. All features were scaled to {−1..1} before the regression model was learned. The mean and scaling constants were saved as part of the model to perform the same transform on test data.
3.2. ProjectionLearningTechniques
As we can see in Figure 2, the imitation seems to have some cor- relation to the target sound, but with different scale and dynamics. We need to learn this mapping of machine sound to human imita- tion so that for new machine sounds we can compute the similarity in a projected ‘human space.’
To learn the projection between machine sound and human im- itation, we used a multi-dimensional regression learning technique with support vector machines (SVM) [13]. As the usual configu- ration of a regression problem is to have a set of l d-dimensional frames linked to a set of single-dimension scalar ys, we instead use a multi-class approach. For each input frame of machine sound, we chose a random frame from its human imitation space, and af- ter collecting the entire set of l frames of the machine sound, we train d SVM regression machines, one for each dimension of the human response. Thus, each machine predicts a single dimension of the human imitation space by a regression among all d dimen- sions of the machine sound. We note that this approach considers covariance among variables in machine space but not of the human features. In our model, covariance of machine features can influ- ence a single human feature projection, but features that co-vary in machine space do not directly influence covariance in the human projection. This is a limitation to be addressed in future work.
The SVM regression task expects two parameters: a σ for
2005 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
October 16-19, 2005, New Paltz, NY
−4.5 −5 −5.5 −6 −6.5 −7 −7.5 −8
blender	drill 0.78	0.07 0.03	0.93 0.02	0.02 0.06	0.03 0.03	0.06
vacuum	sewing
blender
Table 1: Confusion of machines’ ground truth. For each row of ground truth, the columns indicate our prediction results for each machine. This sets an upper bound on prediction performance. Mean accuracy of classifiers = 0.77.
on classification. By comparing randomly-selected disjoint sub- sets of each machine, we obtain the natural confusion of the tar- get sounds. In Figure 3 we see the five machines’ audio projected through our features into two dimensions via principal components analysis [14]. While the five machines are clearly separable there is some overlap between similar sounding types of machines.
The results for the ground truth task in Table 1 is obviously high with a mean accuracy of 0.77 (the mean of the diagonal of the confusion matrix.) We note that the coffee machine is the least self-similar while the drill is the most self-similar.
drill −8.5	vacuum sewing
coffee
−9 1.5 2 2.5 3 3.5 4 4.5 5 5.5
Figure 3: Five machine sounds projected into a two dimensional space.
the kernel parameter, which represents data embedded in a kernel space described through the gaussian
− (|x1 −x2 |)2 eσ2 (1)
where x1 and x2 are frames in l, and the regularization parameter C which is often considered a ‘generalization knob,’ affecting the tradeoff between accuracy in classification and generalization to new data.
In our work we trained a single set of regression models (one for each dimension in feature space) instead of separate models for each machine. If we trained one machine’s model at a time, the regression problem could overfit to only predict one type of ma- chine. By forcing the regression to consider all types of machine and their corresponding imitations, we hope to create a model that works well for new types of machines.
4. EXPERIMENTSANDRESULTS
Sounds of five machines and seven subjects were recorded for this study. The machines included an Osterizer Blendor kitchen blender, a Dewalt portable power drill, a Hoover Commercial Wind- tunnel stand up vacuum cleaner, a Acorto 2000s espresso maker and a Singer 20 industrial sewing machine. The subjects included three females and four males with different backgrounds and var- ious primary languages. A PZM microphone was placed between the person imitating a machine and the machine they were working with. The microphone was always within 1 m of the sound source being recorded.
A ‘machine ground truth’ was first recorded for each of the five machines, consisting of the machine sound alone. The human par- ticipants were introduced to the machines and then left alone with the machines in a soundproof studio with the instructions to record their imitation samples when comfortable. Each participant spent a short time alone with all of the machines, three to ten minutes, before recording their imitation of each machine onto the minidisc setup provided. Each of the imitations were between 2 s and 10 s in duration.
4.1. SimilarityusingOnlyGroundTruth
We first compute Euclidean distance between randomly chosen frames of each of the machine sounds in order to set an upper limit
4.2.
Performance Without Model
blender drill vacuum sewing coffee
blender	drill	vacuum	sewing 0.08	0.06	0.04	0.37	0.44 0.02	0.09	0.14	0.07	0.69 0.02	0.08	0.08	0.14	0.68 0.08	0.12	0.03	0.38	0.38 0.06	0.06	0.17	0.26	0.46
blender drill vacuum sewing coffee
Table 2: Confusion of prediction of human imitations (rows) against machine ground truth (columns) without using a learned auditory model. Without the model, all imitations are classified as closest to the “coffee” ground truth machine sound, for a total prediction accuracy of 20%. Mean accuracy of classifers = 0.22.
We then show the result without the auditory model. In this task, we show the accuracy of machine prediction given the entire set of human imitations. To compute the prediction, we arrange the Euclidean distance between the set of imitations × the set of machine sounds, after both are run through our feature space cal- culations. For each frame of human imitation, we find the frame in machine space with the minimum distance and treat it as a vote for that class. The columns in Table 2 indicate probabilities of each machine prediction given the row imitation sound.
We see that all imitations are classified as the coffee machine, for a total accuracy of 20% for a 1-in-5 machine detection task. The overall mean accuracy of the classifiers is 0.22, very close to the random baseline of 0.20. There is a close call for the sewing machine classification, but otherwise most of the frames were in- correctly labeled. We attribute this poor performance to the lack of a perceptual model of human imitation– since the coffee ma- chine was shown to be the least self-similar in Table 1, it follows that it had the widest variety in feature space. Therefore, without a model, much of the imitation sounds have a higher probability of matching with the coffee machine.
coffee 0.05	0.09	0.01 0.01	0.01	0.02 0.81	0.14	0.02 0.13	0.76	0.02 0.18	0.07	0.66
coffee
2005 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics	October 16-19, 2005, New Paltz, NY
4.3. PerformanceWithModel
We then learn the model as described in Section 3.2. To be able to evaluate different machines’ performance through the model, we computed a round-robin evaluation, leaving one machine out each time for a total of five models. After the d regression models were learned for each of the five machines (using a C of 1000 and a σ of 0.5,) we computed our features on the machine audio and put them through its left-out model (i.e. the model trained on the data excluding both that particular machine’s sound and its imitations) to compute a projection in human imitation space for the machine sound. We then computed the similarity classification as above, but instead of computing similarity of human imitation to machine sound, we computed the similarity between human imitation and machine sound projected into human imitation space.
Table 3: Confusion of prediction of human imitations (rows) against machine ground truth (columns) projected through our learned auditory model with the highest probability for each im- itation in bold. This machine prediction task scored 60% overall. Mean accuracy of classifiers = 0.30.
The results for this task are in Table 3. We see that our overall accuracy in the 1-in-5 prediction task is now at 60% over the 20% we achieved without using the model. We also see that our mean accuracy is now 0.30, compared to 0.22 for no model and 0.2 for the baseline. The missed machines include the drill, which had the highest self similarity in Table 1, and the sewing machine. We ex- plain the poor performance of the drill due to poor generalization in our model: since the drill has high self-similarity and low simi- larity to any of the other machines, our model (trained on only the other machines in the round robin) did not account for its unique sound.
4.4. EvaluatingDifferentFeatures
Due to the expressive range of each of the machines, we attempted to determine which of the auditory features were more valuable for each machines’ individual classification task. Just as we computed a leave-one-out evaluation along the machine axis for evaluation in prediction, we here evaluate feature performance by formulating the vector of the (2d) − 1 permutations. For each feature permu- tation, we compute the similarity evaluation as above and search the result space for the best performing overall model and also the best performing classifer for each individual machine.
The best overall feature space for the 1-in-5 task through model
was found to be a combination of all features but the modulation spectral centroid. For the task without the model, the best perform- ing features for the similarity classification were a combination of f0, power, and modulation centroid.
5. CONCLUSIONSANDFUTUREWORK
We show in this paper that it is possible to project a machine sound to a human vocal space applicable for classification. Our results are illuminating but we note there is a large amount of future work to fully understand the problem and increase our accuracy. We hope to integrate long-scale time-aware features as well as a time- aware learning scheme such as hidden Markov models or time ker- nels for SVMs [15]. We also want to perform studies with more machines and more subjects, as well as learn a parameter map- ping to automatically control the functions of the machines (speed, torque, etc.) along with the detection.
6. REFERENCES
[1] K. Dobson, “Blendie.” in Conference on Designing Interac- tive Systems, 2004, p. 309.
[2] M. Slaney, “Semantic-audio retrieval,” in Proc. 2002 IEEE International Conference on Acoustics, Speech and Signal Processing, May 2002.
[3] K. D. Martin, “Sound-source recognition: A theory and com- putational model,” Ph.D. dissertation, MIT Media Lab, 1999.
[4] E. Scheirer and M. Slaney, “Construction and evaluation of a robust multifeature speech/music discriminator,” in Proc. ICASSP ’97, Munich, Germany, 1997, pp. 1331–1334.
[5] L. Kennedy and D. Ellis, “Laughter detection in meetings,” in Proc. NIST Meeting Recognition Workshop, March 2004.
[6] T. Polzin and A. Waibel, “Detecting emotions in speech,” 1998.
[7] N. Arthur and J. Penman, “Induction machine condition monitoring with higher order spectra,” IEEE Transactions on Industrial Electronics, vol. 47, no. 5, October 2000.
[8] L. Jack and A. Nandi, “Genetic algorithms for feature selec- tion in machine condition monitoring with vibration signals,” IEE Proc-Vis Image Signal Processing, vol. 147, no. 3, June 2000.
[9] B. Whitman and D. Ellis, “Automatic record reviews,” in
Proceedings of the 2004 International Symposium on Music Information Retrieval, 2004.
[10] A.deCheveige ́,“Cancellationmodelofpitchperception,”J. Acous. Soc. Am., no. 103, pp. 1261–1271, 1998.
[11] M. Goto, “A predominant-f0 estimation method for cd recordings: Map estimation using em algorithm for adaptive tone models,” in Proc. ICASSP-2001, 2001.
[12] P. R. Cook, “Music, cognition and computerized sound,” pp. 195–208, 1999.
[13] V. N. Vapnik, Statistical Learning Theory.	John Wiley &amp; Sons, 1998.
[14]	C. V. L. G.H. Golub, Matrix Computations.	Johns Hopkins University Press, 1993.
[15] S. Ru ̈ping and K. Morik, “Support vector machines and learning about time.”
blender
drill
vacuum
sewing
coffee
blender
0.44
0.11
0.15
0.25
0.05
drill
0.27
0.03
0.62
0.07
0.01
vacuum
0.22
0.11
0.46
0.13
0.09
sewing
0.24
0.09
0.36
0.21
0.10
coffee
0.18
0.13
0.14
0.17
0.37
machine
best features
performance
blender
aperiodicity
0.79
drill
spectral centroid, modulation centroid
0.24
vacuum
power
0.70
sewing
f0
0.40
coffee
modulation centroid
0.68</Text>
        </Document>
        <Document ID="299">
            <Title>Article Ideas</Title>
        </Document>
        <Document ID="188">
            <Title>List of Key References</Title>
            <Text>[André, Rist, and Baldes (2002)][#Andre:2002p2390]

[Beguin and Rabardel (2000)]

[#Beguin:2000p2404][Bendle (2002)]

[#Bendle:2002p2178][Cameron and Carroll (2009)]

[#Cameron:2009p2420][Hare, Karam, and Lewis (2005)][#Hare:2005p2373]
[Hoogen, Ijsselsteijn, and Kort (2009)][#vandenHoogen:2009p2421]

[Ishii et al. (1998)][#Ishii:1998p2451]

[Karam et al. (2006)][#Karam:2006p2369]

[Karam][#Karam:2010p2319]

[Karam (2005)][#Karam:2005p2186]

[Klemmer, Hartmann, and Takayama (2006)][#Klemmer:2006p2410]

[Klemmer (2004)][#Klemmer:2004p2418]

[Klemmer and Landay (2009)][#Klemmer:2009p2417]

[Levin (2000)][#Levin:2000p2184]

[Norman et al. (2009)][#Norman:2009p2182]

[Orio, Schnell, and Wanderley (2001)][#Orio:2001p2183]

[Paillard][#Paillard:2010p2179]

[Preester and Knockaert (2005)][#dePreester:2005p2180]

[Rabardel and Waern (2003)][#Rabardel:2003vn]

[Verillon and Rabardel (1995)][#Verillon:1995p2408]

[Vérillon (2000)][#Verillon:2000p2375]

[Massumi (2002)][#Massumi:2002uq]


[Massumi (2002)][#Massumi:2002fk]

A clear outline of practice as research, with a very useful schematic / guide for research and writing process.
[Barrett and Bolt (2007)][#barrett07]

From the Video Game Theory Reader 2, there is a lot of ideas and material here: a good treatment of Merleau-Ponty, embodiment, interfaces and more.
[Gregersen and Torben (2009)][#Gregersen:2009ys]

New controllers, new forms of play - good article detailing current takes on play theory, related to interface innovations and user experience.

[Jenson and Castell (2009)][#Jenson:2009uq]


Westecott: A happy inverse of my interest: here the player characters are viewed as performing objects, from the gameplay perspective - rather than the perspective game objects being used to make performance.

[Westecott (2009)][#Westecott:2009fk]



[Belman and Flanagan][#Belman:2010p1887]

I have a hunch this groundbreaker is useful when considering simulation and computational media - I am looking a physics simulation and physics engines.

[Feynman (1982)][#Feynman:1982p1884]

Good old Schechner and all - performance studies comes to the game party.
[Fernández-Vara (2009)][#Fernandez-Vara:2009bh]

[Roque and Castelhano (2009)][#Roque:2009qf]

David Saltz is an academic interested in theatre and digital simulation. A citation from Susan Kozel and an interesting set of articles on his CV site.
/url{http://www.drama.uga.edu/upload/vitae/vitae624.pdf}

[Saltz (2001)][#Saltz:2001vn]
[Saltz (2004)][#Saltz:2004ys]
[Saltz (1997)][#Saltz:1997zr]
[Saltz (2004-2005)][#Saltz:2004ly]

The gaming star of SMARTLab writes on puppetry in cyberspace in the journal Performing Arts Resources.
[Flanagan (2004)][#Flanagan:2004kx]

Fab Marina Warner article/chapter also published in her book Phantasmagoria
[Warner (2007)][#Warner:2007fk]


A useful article on 3D, aesthetics, Brecht (and others) relating computer media experiences to cinematography.
[Manovich (1995)][#Manovich:2007fk]

Similar to the above.
[Manovich (nodate)][#Manovich:2010vn]

A classic text on alternative realities, computer vision and interactive immersive space.
[Krueger (1991)][#Krueger:1991uq]

[Cubitt (1998)][#Cubitt:1998fk]

The French / Belgian (?) company Amoros et Augustin influential exponents of shadow theatre. A position paper outline production history and a statement of the key motivations of the company.
[Amoros (2009)][#Amoros:2007kx]


Postmodern Theory. May provide sources of insights about the relationship between physical objects and their representation in digital media.
[Baudrillard (1994)][#Baudrillard:1994fk]

SenToy: an object driven exploration of expression and affective input.
[Andersson et al. (2002)][#Andersson:2002p1869]

An evaluation of affective interfaces
[Höök (2002)][#Hook:2002p1872]

SAFIRA - affective input and interactions
[Paiva et al.][#Paiva:2010p1870]

[Silva, Vala, and Paiva (2001)][#Silva:2010p1866]

Tri camera siloette business controlling characters.
[Ren et al. (2005)][#Ren:2005:LSF]

Key Reference book on shadow puppetry
[Currell (2007)][#currell07]

Theoretical Text on the historicity of 'new' media
[Lisa gitelman (2003)][#gitelman03]

OpenGL Shadow Puppetry
[Tan kian lam (2008)][#Tan-Kian-Lam:2008uq]

Interaction / typology of new approaches to games (whole body)
[Loviscach (2009)][#Loviscach:2009fk]

Henson Press Release for their patented Digital Puppetry System
[Nelson (2009)][#Nelson:2009p1861]

[Nakahara, Miyazaki, and Sakamoto (2009)][#Nakahara:2009p1530]

[Torpey and Jessop (2009)][#Torpey:2009p1547]

[Wang and Popović (2009)][#Wang:2009p1473]

[Baran et al. (2009)][#Baran:2009p1471]

[Ishigaki et al. (2009)][#Ishigaki:2009p1469]

[Govindaraju et al. (2008)][#Govindaraju:2008p1457]

[Ishiguro and Oshita (2008)][#Ishiguro:2008p1460]

[Candlin and Guins (2008)][#candlin08]

[Merleau-ponty (2002)][#merleau-ponty02]

[Noble (2009)][#noble09]

[Long and Reinhard (2009)][#Long:2009p1447]

[Magnenat-thalmann and Thalmann (2008)][#MagnenatThalmann:2008p1463]

[Thiebaux, Lance, and Marsella (2009)][#Thiebaux:2009p1455]

[Radovan and Pretorius (2006)][#Radovan:2006p1425]

[Bar-lev, Bruckstein, and Elber (2005)][#Bar-Lev:2005ty]

[Vlasic et al. (2008)][#Vlasic:2008p406]

[Hecker et al. (2008)][#Hecker:2008p1290]

[Assa et al. (2008)][#Assa:2008p1201]

[Atkinson (2009)][#Atkinson:2009p1344]

[Baran and Popović (2007)][#Baran:2007p353]

[Barnes et al. (2008)][#Barnes:2008p685]

[Baudisch and Chu (2009)][#Baudisch:2009p584]

[Billinghurst, Kato, and Poupyrev (2008)][#Billinghurst:2008p1006]

[Bragdon et al. (2009)][#Bragdon:2009p621]

[Breazeal (2008)][#Breazeal:2008p1043]

[Brown (2009)][#Brown:2009p702]

[Cabral, Morimoto, and Zuffo (2005)][#Cabral:2005p1198]

[Datcu and Rothkrantz (2007)][#Datcu:2007p231]

[England et al. (2009)][#England:2009p784]

[Fishkin (2004)][#Fishkin:2004p1194]

[Ford (2007)][#Ford:2007p164]

[Freeman (2004)][#Freeman:2004p212]

[Gobbetti, Balaguer, and Thalmann (1993)][#Gobbetti:1993p986]

[Guo, Young, and Sharlin (2009)][#Guo:2009p691]

[Harrison and Hudson (2008)][#Harrison:2008p1047]

[Huang and Fu (2008)][#Huang:2008p993]

[Huang and Lin (2008)][#Huang:2008p273]

[Jones and Deeming (2007)][#Jones:2007p854]

[Kaplin (1999)][#Kaplin:1999p477]

[Knep et al. (1995)][#Knep:1995p869]

[Koleva et al. (2009)][#Koleva:2009p600]

[Ma et al. (2008)][#Ma:2008p1202]

[Magnenat-thalmann (1988)][#MagnenatThalmann:1988p549]

[Marquardt et al. (2009)][#Marquardt:2009p1341]

[Mazalek and Nitsche (2007)][#Mazalek:2007p881]

[Mckenzie (1994)][#McKenzie:1994p569]

[Mcquiggan, Rowe, and Lester (2008)][#McQuiggan:2008p343]

[Meisner et al. (2009)][#Meisner:2009p706]

[Melo and Gratch (2009)][#Melo:2009p1303]

[Mistry, Maes, and Chang (2009)][#Mistry:2009p1004]

[Molyneaux and Gellersen (2009)][#Molyneaux:2009p996]

[Pai and Kry (2006)][#Pai:2006p1264]

[Perry et al. (2009)][#Perry:2009p641]

[Persson (2003)][#Persson:2003p130]

[Rekimoto (2009)][#Rekimoto:2009p827]

[Resnick et al. (1996)][#Resnick:1996p1193]

[Ryman et al. (1984)][#Ryman:1984p578]

[Salminen et al. (2008)][#Salminen:2008p344]

[Salti, Schreer, and Stefano (2008)][#Salti:2008p1000]

[Schmitt and Seitinger (2009)][#Schmitt:2009p687]

[Shi et al. (2007)][#Shi:2007p1276]

[Shimizu et al. (2006)][#Shimizu:2006p1059]

[Shiratori and Hodgins (2008)][#Shiratori:2008p990]

[Shum et al. (2008)][#Shum:2008p1255]

[Spelmezan et al. (2009)][#Spelmezan:2009p661]

[Taylor, Bove, and Jr (2009)][#Taylor:2009p884]

[Thalmann et al. (2004)][#Thalmann:2004p974]

[Vertegaal and Poupyrev (2009)][#Vertegaal:2009p773]

[Weller, Do, and Gross (2008)][#Weller:2008p857]

[Weller, Gross, and Do (2009)][#Weller:2009p681]

[Wu and Balakrishnan (2003)][#Wu:2003p1050]

[Zaman et al. (2009)][#Zaman:2009p694]

[Zeleznik et al. (1991)][#Zeleznik:1991p1028]

[Zielinski (1996.)][#Zielinski:1996ss]

[Zielinski (2008)][#Zielinski:2008ay]

[Zhai, Milgram, and Rastogi (1997)][#Zhai:1997p1195]

[Zhai and Bellotti (2005)][#Zhai:2005p1108]

[Zhai, Milgram, and Drascic (1993)][#Zhai:1993p1138]

[Zhai (1998)][#Zhai:1998p1117]

[Zhai and Milgram (1994)][#Zhai:1994p1137]

[Zhai (2008)][#Zhai:2008p1091]

[Zhan et al. (2006)][#Zhan:2006p274]

[Jansen (2007)][#Jansen:2007up]

[Sadun (2009)][#Sadun:2009p1345]

[Reichardt (1978)][#Reichardt:1978qy]

[Salen and Zimmerman (2004)][#Salen:2004uq]

[ (1971)][#:1971zr]

[Hall (Date Created: nodate. Date Accessed: 01/02/2007)][#Hall:2007fk]

[Boal (1995)][#Boal:1995gf]

[Laurel (1993)][#Laurel:1993fk]

[Conner (2000)][#Conner:2000uq]

[Vygotsky (First Published: 1933. Date Created: 2002 . Date Accessed: 01/02/2007)][#Vygotsky:2007ly]

[Parry (1994)][#Parry:1994vn]

[ (2001)][#:2001gf]

[Brooks (2002)][#Brooks:2002yq]

[Wood (2002)][#Wood:2002fk]

[Strommen (Online PDF of unpublished work. Date Written: 2004. Date Accessed: 01/03/2007)][#Strommen:2007rr]

[Parent (Date Created: 28/11/2004. Date Accessed: 01/02/2007)][#AParent:2004rr]

[Strommen (Online PDF of published work. Date Written: 1998. Date Accessed: 01/03/2007)][#Strommen:2007lq]

[Strommen (Online PDF of published work. Date Written: 1999. Date Accessed: 01/03/2007)][#Strommen:2007ul]

[Strommen (Online PDF of published work. Date Written: 2000. Date Accessed: 01/03/2007)][#Strommen:2007pd]

[Shenk (Date Created: 07/01/1999. Date Accessed: 01/02/2007)][#Shenk:2007vn]

[Newman (Date Modified: 21/05/1999. Date Accessed: 01/03/2007)][#Newman:2007kx]

[Lanier (Last Modified: 08/01/2006. Date Accessed: 01/02/2007)][#Lanier:2006qy]

[Segal (1995)][#Segal:1995kx]

[Tillis (1992)][#Tillis:1992lr]

[Dery (Date Created: 05/1994. Date Accessed: 1/2/2007)][#Dery:2007zr]

[Technologies (Last Modified: 19/02/2002. Date Accessed: 01/03/2007)][#Technologies:2007pd]

[Cassell and Ryokai (Last Modified: 2001. Date Accessed: 01/03/2007)][#Cassell:2007uq]

[Technology (Date Created: 19/02/2002. Date Accessed: 1/2/2007.)][#Technology:2007ys]

[Chilvers (Date Created: nodate. Date Accessed: 01/02/2007)][#Chilvers:2007wd]

[Ninomiya, Miyazaki, and Nakatsu (2008)][#Ninomiya:2008xt]

[Gabor (1971)][#Gabor:1971ly]

[Tillis (2001)][#Tillis:2001ul]

[Rilke (1994)][#Rilke:1994fr]

[Baudelaire (1994)][#Baudelaire:1994ys]

[Kaplin (2001)][#Kaplin:2001ve]

[Reas (1996)][#Reas:1996p1313]

[Reas (2001)][#Reas:2001p1314]

[Adams (2008)][#adams08]

[Bell (2001)][#bell00]

[Blundell (2008)][#blundell08]

[Candy and Edmonds (2002)][#candy02]

[Deng and Neumann (2007)][#deng07]

[Dewey (2005)][#Dewey:2005jy]

[Dixon (2007)][#dixon07]

[Ede (2008)][#ede08]

[Fiala (2008)][#fiala08]

[Grau (2007)][#grau07]

[Grau (2003)][#grau03]

[Hess (2007)][#hess07]

[Junker (2006)][#junker06]

[Mealing (2008)][#mealing08]

[Mullen (2007)][#mullen07]

[Turkle (2005)][#turkle05]

[Turkle and Sherry (2008)][#turkle08]

[Turkle (2007)][#turkle07]

[Friedman (2003)][#Friedman:2003p2]

[Zhai (1995)][#Zhai:2009p701]

[Saxena and Ng][#Saxena:2009p1262]

[Apple (2009)][#Apple:2009at]

[Apple][#apple_working_]

[Apple][#apple_quartz_]

[Apple][#apple_quartzcomposer-dev_]

[Apple][#apple_quartz_-1]

[Apple][#apple_quartz_-2]

[Apple][#apple_technical_]

[Apple][#apple_technical_-1]

[Apple][#apple_technical_-2]

[Drinkwater][#drinkwater_machines_]

[Grant][#grant_daisyrustcom_]

[Industries][#noise_industries_noise_]

[Vidvox][#vidvox_vidvox_]

[Wright][#christopher_kineme_]</Text>
        </Document>
        <Document ID="25">
            <Title>Proposal Requirements</Title>
            <Text>FROM UEL document: Source: http://www.uel.ac.uk/gradschool/resources/doclibrary/documents/TheResearchProposal.doc  

•	A title that indicates the field, scope and subject matter of your projected research.

•	A detailed statement of the topic you intend to research and your research objectives.

		def: research objectives - what are you trying to achieve from the research? 

•	Sufficient information on yourself to explain how your anticipated research fits with your experience and interests or those of your employer, sponsor or organisation.

•	An awareness of relevant academic literature that demonstrates your understanding of the field and your appreciation of debates within it. This literature review is likely to be supported by a bibliography.

•	An identification and defence of how your research will advance, and make an original contribution to, knowledge in the field.

•	A possible methodology, which is to say, an account of the tools or research methods you will employ, for example, experiments, archival research, questionnaires, analytical frameworks, qualitative or quantitative methods etc. 

•	Evidence that you have ascertained the accessibility of such data or resources as will be integral to your research

•	A timescale for your research and an awareness of the need for such planning.
</Text>
        </Document>
        <Document ID="26">
            <Title>Bibliography</Title>
            <Text>Self Authored Reflective Publications on Performance Practice, and Related Techniques

(2008) Grant, Ian. Chapter: Experiments in Digital Puppetry: Video Hybrids in Apple’s Quartz Composer. In Transdisciplinary Digital Art: Sound, Vision and the New Screen, Communications in Computer and Communication Science. Edited by Randy Adams, Steve Gibson, Stefan Muller Arisona. Springer. 

(forthcoming) Grant, Ian. Apple’s Quartz Composer, a pragmatic guide (provisional title). A book for the US publisher ‘The Pragmatic Programmers’. www.pragprog.com 

(February 2007) Grant, Ian. Talking Toys and Digital Puppetry. Society for Artificial Intelligence and the Simulation of Behaviour (AISB) ’07 at Newcastle University, Newcastle Upon Tyne, 2-5 April 2007. Conference: Artificial and Ambient Intelligence. Symposium on Virtual Companions and Digital Pets. "The Reign of Catz &amp; Dogz? The Role of Virtual Pets in a Computerised Society". Paper published in proceedings.

(Jan 2001) Finding the Wooden Voice in Puppetry Into Performance: A Users Guide. London: Theatre Museum, Central School of Speech and Drama and the Puppet Centre Trust. 

Indicative Bibliography [not consistently formatted nor ordered]

C. Baudelaire. The philosophy of toys. In I. Parry, editor, Essays on Dolls. Syrens, London, 1994.

J. Bell, editor. Puppets, Masks, and Performing Objects. MIT Press, Boston, 2001.

R. A. Brooks. Robot: The Future of Flesh and Machine. Penguin, London, 2002.

H. B. Segal. Pinocchio’s Progeny : Puppets, Marionettes, Automatons and Robots in Modernist and Avant-Garde Drama. Johns Hopkins University Press, 
Baltimore, 1995.

J. Cassell and K. Ryokai. Making Space for Voice: Technologies to Support Children’s Fantasy and Storytelling. http://web.media.mit.edu/ kimiko/publications/PersonalTech.pdf, Last Modified: 2001. Date Accessed: 01/03/2007.

P. Chilvers. Creature Labs - Norn Babblings. http://www.gamewaredevelopment.co.uk/creatures_more.php, Date Created: nodate. Date Accessed: 01/02/2007.

S. Conner. Dumbstruck: A Cultural History of Ventriloquism. Oxford University Press, New York, 2000.

M. Dery. Hacking Barbie’s Voice Box: ’Vengeance is Mine! ’. http://www.levity.com/markdery/barbie.html. New Media magazine, "Technoculture" column., Date Created: 05/1994. Date Accessed: 1/2/2007.

D. Gabor. Technological civilisation and man’s future. In J. Reichardt, editor, Cybernetics, art and ideas, pages 18–24. Studio Vista, London, 1971.

V. Hall. Mike (the talking head). http://mambo.ucsc.edu/psl/mike.html, Date Created: nodate. Date Accessed: 01/02/2007.

S. Kaplin. A puppet tree - a model for the field of puppet theatre. In J. Bell, editor, Puppets, Masks, and Performing Objects, pages 18–25. MIT Press, Boston, 2001.

J. Lanier. Jaron’s World: Sing a Song of Evolution - Mind and Brain - DISCOVER Magazine Is language descended from musical mating calls?  http://discovermagazine.com/2006/aug/jaronevol/, Last Modified: 08/01/2006. Date Accessed: 01/02/2007.

B. Laurel. Computers as theatre. Addison-Wesley Pub. Co, Reading, Mass. ; Wokingham, 1993. GBA201776 bnb 2672 Brenda Laurel. ill. (some col.) ; 24 cm. "Now featuring Post-virtual reality"–Cover. Includes bibliographical references (p. 215-222) and index.

M. Newman. Interactive Barney: Good or evil?  Conferees worry about where computerized ’character’ toys are going next. http://www.post-gazette.com/businessnews/19990521barney1.asp, Date Modified: 21/05/1999. Date Accessed: 01/03/2007.

A. Parent. Read Reviews of Hasbro Aloha Stitch Doll 3570 at eOpinions. http://www.epinions.com/content_163285929604? linkin_id=8003929, Date Created: 28/11/2004. Date Accessed: 01/02/2007.

I. Parry. Essays on Dolls. Syrens, London, 1994.

J. Reichardt. Cybernetics, art and ideas. Edited by Jasia Reichardt. London: Studio Vista, 1971. Essays by various authors.

J. Reichardt. Robots : fact, fiction and prediction. Thames and Hudson, London, 1978. (by) Jasia Reichardt. ill(some col), plans, ports ; 28cm (Pbk).

J. Reichardt. Computers and Art.

R. M. Rilke. Dolls: On the wax dolls of lotte pritzel. In I. Parry, editor, Essays on Dolls. Syrens, London, 1994.

K. Salen and E. Zimmerman. Rules of play : game design fundamentals. MIT, London, 2004. Katie Salen and Eric Zimmerman. ill. ; 24 cm.

S. Tillis. Toward an Aesthetics of the Puppet: Puppetry as a theatrical art. Greenwood, New York, 1992. Normal Loan 791.53 TIL.

S. Tillis. The art of puppetry in the age of media production. In J. Bell, editor, Puppets, Masks, and Performing Objects, pages 172–183. MIT Press, Boston, 2001.

G. Wood. Living dolls : a magical history of the quest for mechanical life. Faber, London, 2002.

L. Vygotsky. Play and its role in the Mental Development of the Child. http://www.marxists.org/archive/vygotsky/works/1933/play.htm, First Published: 1933. Date 
Created: 2002 . Date Accessed: 01/02/2007.
 
Wikipedia Community. "Augmented Reality.” Retrieved 10/3/2007, from http://en.wikipedia.org/wiki/Augmented reality (2007). 

Han, Jeff. "Multi-Touch Interaction Research.” Retrieved 1/1/2008, from http://cs.nyu.edu/∼jhan/ftirtouch/index.html. (2006) 

Fuller, M. (2008) Software Studies: A Lexicon (Leonardo Book). MIT Press.

Bimber, R. and R. Ramash. (2005) Spatial Augmented Reality: A Modern Approach to Augmented Reality. Peters.

Augmented Reality: A Practical Guide: The Complete Guide to Understanding and Using Augmented Reality Technology




Barba and Co. Secret Art of the Performer: Dictionary of Theatre Anthropology. Routledge.

D. Shenk. Behold the Toys of Tomorrow (The Atlantic Online - Digital Culture). http://davidshenk.com/webimages/atlantic1.htm, Date Created: 07/01/1999. Date 
Accessed: 01/02/2007.

E. F. Strommen. When the Interface is a Talking Dinosaur: Learning Across Media with ActiMates Barney. http://www.playfulefforts.com/archives/papers/CHI-1998.pdf, Online PDF of published work. Date Written: 1998. Date Accessed: 01/03/2007.

E. F. Strommen. Learning from Television With Interactive Toy Characters As Viewing Companions. http://www.playfulefforts.com/archives/papers/SRCD-1999.pdf, Online PDF of published work. Date Written: 1999. Date Accessed: 01/03/2007.

E. F. Strommen. Interactive Toy Characters as Interfaces For Children. http://www.playfulefforts.com/archives/papers/IA-2000.pdf, Online PDF of published work. Date Written: 2000. Date Accessed: 01/03/2007.

E. F. Strommen. Play?  Learning?  Both...or neither?  http://www.playfulefforts.com/archives/papers/AERA-2004.pdf, Online PDF of unpublished work. Date Written: 2004. Date Accessed: 01/03/2007.

V. S. Technologies. Voice Signal Technologies Announces Breakthrough Speech Interface For Mobile Phones and Handheld Computers. http://www.voicesignal.com/news/press/release_02_19_02.html, Last Modified: 19/02/2002. Date Accessed: 01/03/2007.

V. Technology. VoiceSignals Technology Press Release. http://www.voicesignal.com/news/press/release_02_19_02.html, Date Created: 19/02/2002. Date Accessed: 1/2/2007.</Text>
        </Document>
        <Document ID="189">
            <Title>Snippets with no home</Title>
        </Document>
        <Document ID="27">
            <Title>Curious / Interesting things</Title>
            <Text>Google: "Expressivity and Performance"

Hit 5 (28/4/2008) - http://ieeexplore.ieee.org/Xplore/login.jsp?url=/iel5/11093/35437/01681986.pdf?isnumber=35437&amp;prod=CNF&amp;arnumber=1681986&amp;arSt=2062&amp;ared=2067&amp;arAuthor=Goncalves%2C+M.%3B+Tineo%2C+L is about SQL! </Text>
        </Document>
        <Document ID="28">
            <Title>1346</Title>
            <Text>In J. Sundberg &amp; B. Brunson (Eds.) Proceedings of Music and Music Science, Stockholm, October 28- 30, 2004
A fuzzy analyzer of emotional expression in music performance and body motion
Anders Friberg
KTH – Royal Institute of Technology Dept. of Speech, Music and Hearing Lindstedtsvägen 24 100 44 Stockholm, Sweden Email: andersf@speech.kth.se
Abstract
A real time algorithm is presented for analyzing the emotional expression in music performance and body motion. It is primarily intended to be used as a real time controller in an artistic human‐computer performance system. The analysis is done in three steps consisting of cue analysis, calibration, and fuzzy mapping. The cue analysis extracts tone parameters such as tempo, sound level and articulation from audio input or overall quantity of motion and vertical/horizontal motion from video input. The calibration step adjusts the system to a particular performer’s expression in a semi‐automatic procedure. The fuzzy mapper translates the cue values to three emotion outputs: happiness, sadness, and anger. The mapping to emotions was modeled using qualitative data from previous research, thus no training using control data was necessary. The fuzzy analyzer has been used in several applications including the Expressiball for visualizing music performance and the collaborative game Ghost in the Cave using gestures and voice as the main player input.
Introduction
The development of and experimentation with new musical controllers (e.g. musical instruments) has been an integral part of the musical evolution. With the introduction of electronic sound manipulation these controllers were no longer bound to the acoustic properties of physical objects, thus opening up a wide field for any type of imagined sound processing. Traditional musical instruments have, however, provided natural restrictions in the sound production that we have learned to decode. For example, the physical energy used to excite an instrument can be at least partially decoded by the listener from the resulting timbre1. By removing these
1 This is likely to work well for known instruments – the dynamic level of a piano recording corresponding to the energy used for pushing down the keys is easily decoded even if the listening volume is turned down.
1
restrictions the possible space of all sounds becomes much larger, but on the other hand, presumably a smaller subspace is musically/perceptually relevant.
In creating a new musical controller, a crucial issue is the mapping from the input device parameters to the sound parameters. It has been shown that it is important to make this mapping similar to how traditional musical instruments react in a general sense. More specifically, the mapping should preferably not be one‐to‐ one but rather one‐to‐many or many‐to‐many (Hunt et al. 2004). One simple example in the piano is that increasing key speed increases sound level, high‐frequency content, as well as changes the “bonk” sound obtained when the key is hitting the keybed.
The technology is not restricted to the invention of new musical instruments in the sense that each individual sound is triggered by the performer. A large number of different artistically oriented performance‐computer interactions have been realized in the past (e.g. Wanderley and Battier 2000). One alternative is to have a system for performance control of the overall aspects of the computer‐generated sound similar to a how a conductor controls an orchestra. In order to restrict the control space to perceptually meaningful variations, similar to the instrument control above, such system would benefit from having a more abstract higher‐level expressive musical/gesture description defined as a part of the sound and input processing algorithms. Such high‐level definitions was a goal of the recent European project MEGA2, which resulted in a number of useful tools dealing with expressive features in music and dance aimed for artistic applications (Camurri et al. in press).
We will describe an algorithm that has been used for analyzing the emotional expression either in body motion or in music performance. This system is intended to be used as a real time controller in an artistic setting or in applications such as computer games. Before describing the algorithm, a short overview of emotional expression in music performance and gestures is given.
Emotional expression in music performance and body motion
Most people agree that an important aspect of music is the communication of emotions. Recently, a number of scientific studies have explored this communication (Juslin and Sloboda 2001; Juslin and Laukka 2003). These studies indicate that for basic emotions such as happy, sad or angry, there is a rather simple relationship between the emotional description and the cue values (i.e. measured parameters such as tempo, sound level or articulation). Response data obtained from listeners rating the emotional expression of different music performances can be modeled by multiple regression analysis, thus, suggesting a linear relationship between cues and emotional expression (Juslin 2000). Since we are aiming at real‐time playing applications we will focus here on performance cues such as tempo and dynamics. A complete set of cues should also include compositional features such as tonality, rhythm and instrumentation (Gabrielsson and Lindström 2001).
The emotional expression in body gestures has also been subject to research but to a lesser extent than in music. Camurri et al. (2003) analyzed and modeled the
2 www.megaproject.org
2
emotional expression in dancing. Boone and Cunningham (2001) investigated children’s movement patterns when they listened to music with different emotional expressions. Dahl and Friberg (2004) investigated movement patterns of a musician playing a piece with different emotional expressions. These studies all suggested particular movement cues related to the emotional expression, similar to how we decode the musical expression. We follow the suggestion that musical expression is intimately coupled to expression in body gestures and biological motion in general (Friberg and Sundberg 1999, Juslin et al. 2002). Therefore, we try to apply similar analysis approach to both domains.
Table 1 presents typical results from previous studies in terms of qualitative descriptions of cue values. As seen in the Table, there are several commonalities in terms of cue descriptions between motion and music performance. For example, anger is characterized by both fast gestures and fast tempo. The research regarding emotional expression yielding the qualitative descriptions as given in Table 1 was the starting point for the development of current algorithms.
Emotion
Anger
Sadness
Happiness
Motion cues
Large Fast Uneven jerky
Small Slow Even soft
Large rather fast
Music performance cues
Loud Fast Staccato Sharp timbre
Soft Slow Legato
Loud Fast Staccato Small tempo variability
Table 1. A characterization of different emotional expressions in terms of cue values for body motion and music performance. Data taken from Dahl and Friberg (2004) and Juslin (2001).
Fuzzy analyzer
The analysis of emotional expression in music performance/gestures is realized in three steps, see Figure 1. The first step is the extraction of basic cues. These cues are quite well defined for music input consisting of traditional tone parameters such as sound level, tempo, and articulation (staccato/legato). In the body motion analysis we have been interested in parameters describing general features of the movements rather than individual movements of each limb. A number of such general cues have been identified and algorithms have been developed for automatic extraction from video input (Camurri et al. 2004).
The second step is the semi‐automatic calibration of cues. This is important for practical reasons in order to avoid otherwise lengthy optimization sessions trying to
3
fine‐tune internal parameters adapting to variations in musical content/body gestures or technical setup.
The third step is the mapping from cues to emotion description by the expression mapper in Figure 1. Instead of using the more common data‐driven methods, such as Neural Networks or Hidden Markow Models, we suggest here fuzzy set functions, allowing the direct use of qualitative data obtained from previous studies. The three steps are described in the following.
Audio
Tempo Sound level Articulation ...
Video
Quantity of motion Gesture speed ...
Standardized cues
Cue analysis
Calibration
Expression mapper
Emotion description
Figure 1. The three‐step process of analyzing emotional expression in gestures or singing/playing.
Music cue extraction
The audio cue extraction was designed for monophonic playing or singing. A previous prototype was described in Friberg et al. (2002) and an improved version for non‐real time use is found in Friberg et al. (2005). The first part of the cue extraction segments the audio stream into tones by computing two different sound level envelopes. The RMS sound level is computed on a hanning windowed buffer of 256 samples using a sampling rate of 22050 Hz. The resulting sound level envelope is then low‐pass filtered using either a 40 Hz cut‐off or a 1 Hz cut‐off frequency. The first sound level envelope follows roughly the shape of each tone and the second sound level envelope follows the general shape of the phrase. The crossings of two envelopes define the tone onsets and offsets. For each segmented tone five different cues are computed: sound level (dB), instant tempo (tones/second), articulation (relative pause duration), attack rate (dB/ms), and high‐frequency content (high/low energy).
Body motion cue extraction
Cues for describing body motion are not as clearly defined and are not as easily extracted as in music. Body motion is greatly multidimensional with several degrees of freedom for describing the motion of each body part. Another complication is that segmentation in terms of basic gestures is not as easily defined. However, there has
4
been a considerable body of research for analyzing human motion from video cameras and a number of advanced algorithms have emerged (for an overview, see Moeslund and Granum 2001).
We use the analysis tools for gesture recognition developed at University of Genova included in the software platform EyesWeb (Camurri et al. 2000)3. The current version of the cue analysis uses only a few basic tools within EyesWeb. In order to remove the background, the first step is to compute the difference signal between consecutive video frames. This means that the algorithm just “see” something when there is a movement. This movement detection is improved by also computing the difference signal from the inverted picture. The use of difference signals makes the system quite robust and insensitive to e.g. light variations in the room. Three cues are computed from the difference signal. The total number of visible pixels constitutes the cue Quantity of Motion (QoM). The bounding rectangle defines the area in the picture that contains all non‐zero pixels. The instant width and height of the bounding rectangle are computed and their peak‐to‐peak amplitude variations constitute the cues width‐pp and height‐pp. Figure 2 shows an example of the difference signal with the bounding rectangle.
Figure 2. The difference video signal and the bounding rectangle used for computing the body motion cues. The number of white pixels is a measure of overall quantity of motion (QoM).
Calibration
In the calibration module each cue is standardized, meaning that it is subtracted by its mean value and divided by its standard deviation: CUEstand = (CUEin – m)/SD. This results in cues with a mean of 0 and a standard deviation of 1. This is an important step implying that the following mapping does not need to be adjusted if the input conditions change, such as change of instrument, dancer, or artistic material. In order to obtain realistic values for the mean and standard deviation of each cue a calibration phase is needed before the recognition system is used. In the calibration phase the user is asked to move or play in, for example, a happy, sad and angry way
3 www.eyesweb.org
5
thus defining the space of all possible variations. All cue values are collected and the mean and standard deviation of all the values of each cue are computed and then used in the calibration module.
The emotional expression is not supposed to be changing from note to note or from gesture to gesture. Therefore the cues are averaged over time. A running average of 4‐9 notes was used for the music analyzer. The amount of averaging can be adjusted according to the desired reaction speed versus data output stability.
Expression mapper
As mentioned above, previous mapping between cues and emotional expression in music has successfully been modeled using linear multiple regression (Juslin 2000). This is an example of a data‐driven approach in which a measured data set is used to train the model. A number of other data‐driven methods have been used for analyzing expression in music performance such as Neural Networks (Bresin 1998), Hidden Markow Models (Cirotteau et al. 2004) or Bayesian classification (Mion 2003). Due to lack of large databases the results of any of these methods are restricted and likely to be biased by e.g. coder variability. On the other hand, there is a large amount of qualitative data emerging from over 40 investigations regarding emotional expression in music (such as presented in Table 1). These data has recently been summarized and subjected to a meta‐analysis (Juslin and Laukka 2003). Most often, cues have been characterized mainly in terms of being either high or low in relation to different emotional expressions. One interesting extension in the meta‐ analysis was to classify some cues in terms of three levels. It indicated that an intermediate cue level might be important at least for the sound level in happy expression. This is also in agreement with our informal experience in developing the mapping algorithms. However, it needs to be further examined in controlled experiments.
Cue input (e.g tempo)
Figure 3. Qualitative classification of cues in terms of the three regions high (+), medium (0), and low (‐), each with a separate output.
Fuzzy set
-
-
00	+
11.5 -10+
0.5
0 -1.5 -1 -0.5 0 0.5 1 1.5
6
Following these ideas, we suggest here a method that uses the qualitative cue descriptions divided in three levels in order to predict the intended emotion. The same method is used both for musical and body motion cues. It uses fuzzy set functions to go from continuous cues to qualitative cues (e.g. Niskanen 2004, Zimmerman 1996). Fuzzy logic has been used in a wide variety of applications. Of related interest here can be mentioned the modeling of emotions in robots (Seif El‐ Nasr et al. 2000) and modeling music performance rules (Bresin et al. 1995). Each cue is divided into a three overlapping region functions. Within a region the corresponding output is one and outside the region it is zero with an overlapping linear area at the boundaries. The input is the standardized cues. An example dividing a cue in three regions is given in Figure 3. Thus, if the tempo is higher than 0.75SD from its mean, the “+” output is 1 and all the other outputs are 0.
The final emotion prediction output is computed by taking an average of a selected set of fuzzy set functions. This selection is done according to previous qualitative descriptions. This results in a continuous output for each emotion with a range 0‐1. If the value is 1 the emotion is completely predicted if it is 0 it is not at all predicted. Using an average of a set of region functions makes the output smoothly changing between emotions depending on the number of “right” cues. This averaging method was selected with the final application in mind. For example, if this algorithm is used for controlling the musical expressivity in computer‐played performance, the transitions between different emotions need to be smooth. Other applications might ask for a discrete classification of emotional expression. This could be easily modeled using fuzzy logic.
Audio input
tempo	articulation
Happiness 0-1	Sadness 0-1	Anger 0-1
Figure 4. The complete system for estimating emotional expression in music performance using three cues. An audio input is analyzed in terms of tempo, sound level and articulation and the resulting prediction of emotional expression is output in terms of three functions ranging from 0 to 1.
Cue extraction
ssound level
Calibration
-
-
FFuzzy set
0	+
-
-
FFuzzy set
0	+
-
-
FFuzzy set
0	+
ΣΣ / 3
ΣΣ/3
Σ/3
7
The complete system is shown in Figure 4 using three cues extracted from audio input. The same setup is used for the body motion analysis using the body motion cues. The use of three cues with the mapping configuration indicated by the colored arrows in this example, has the advantage that emotion outputs are mutually exclusive, that is, if one output is 1, the other outputs are 0.
All parts of the fuzzy analyzer except the motion cue analysis have been implemented using the program pd (Puckette 1996). pd is a modular graphic environment for processing primarily audio and control information in real time. The fuzzy analyzer was implemented using preexisting blocks in the release pd‐extended 0.37, complemented with the library SMLib made by Johannes Taelman. The video cue analysis was implemented as a patch in EyesWeb, a similar graphic programming environment primarily for video processing (Camurri et al. 2000). The audio analyzer makes modest claims on processing power and typically uses only a few percent of a Pentium 4 processor running Windows. Due to the cross‐platform compatibility of pd, the audio cue analysis could easily be ported to MacOS or Linux. The video cue analysis is currently restricted to the Windows platform.
Applications
The first prototype that included an early version of the fuzzy analyzer was a system that allowed a dancer to control the music by changing dancing style. It was called The Groove Machine and was presented in a performance at Kulturhuset, Stockholm 2002. Three motion cues were used, QoM, maximum velocity of gestures in the horizontal plane, and the time between gestures in the horizontal plane, thus slightly different from the description above. The emotions analyzed were (as in all applications here) anger, happiness, and sadness. The mixing of three corresponding audio loops was directly controlled by the fuzzy analyzer output. For a more detailed description, see Lindström et al. (in press).
Figure 5. Two different examples of the Expressiball giving visual feedback of musical performance.
X␣Tempo	Color␣Emotion Y␣Sound level	Shape␣Articulation Z␣Attack velocity &amp; Spectrum energy
Slow	Fast
SStaccato Angry Fast attack High energy
SSlow	Fast
LLegato Sad Slow attack Low energy
8
8
Loud
LLoud
SSoft
SSoft
The ExpressiBall, developed by Roberto Bresin, is a way to visualize a music performance in terms of a ball on a computer screen (Friberg et al. 2002). A microphone is connected to the computer and the output of the fuzzy analyzer as well as the basic cue values are used for controlling the appearance of the ball. The position of the ball is controlled by tempo, sound level and a combination of attack velocity and spectral energy, the shape of the ball is controlled by the articulation (rounded‐legato, polygon‐staccato) and the color of the ball is controlled by the emotion analysis (red‐angry, blue‐sad, yellow‐happy), see Figure 5. The choice of color mapping was motivated by recent studies relating color to musical expression (Bresin and Juslin 2005). The ExpressiBall can be used as a pedagogical tool for music students or the general public. It may give an enhanced feedback helping to understand the musical expression. A future display is planned at Tekniska museet, Stockholm.
Figure 6. Picture from the first realization of the game Ghost in the Cave. Motion player to the left (in white) and voice player to the right (in front of the microphones).
The latest application using the fuzzy analyzer has been the collaborative game Ghost in the Cave (Rinman et al. 2004).4 It uses as its main input control either body motion or voice. One of the tasks of the game is to express different emotions either with the body or the voice; thus, both modalities are analyzed using the fuzzy analyzer described above.
The game is played in two teams each with a main player, see Figure 6. The task for each team is to control a fish avatar in an underwater environment and to go to three different caves. In the caves there is a ghost appearing expressing different emotions. Now the main players have to express the same emotion, causing their fish to change accordingly. Points are given for the fastest navigation and the fastest
4 See also http://www.speech.kth.se/music/projects/Ghostgame/ 9
expression of emotions in each subtask. The whole team controls the speed of the fish as well as the music by their motion activity.
The body motion and the voice of the main players are measured with a video camera and a microphone, respectively, connected to two computers running two different fuzzy analyzers described above. The team motion is estimated by small video cameras (webcams) measuring the Quantity of Motion (QoM). QoM for the team motion was categorized in three levels (high, medium, low) using fuzzy set functions as shown in Figure 3. The music consisted of pre‐composed audio sequences, all with the same tempo and key, corresponding to the three motion levels. The sequences were faded in and out directly by control of the fuzzy set functions. One team controlled the drums and one team controlled the accompaniment. The Game has been set up five times since the first realization summer at the Stockholm Music Acoustics Conference 2003, including the Stockholm Art and Science festival, Konserthuset, Stockholm, 2004, and Oslo University, 2004.
Evaluation
The performance and validity of the fuzzy analyzer is not as easily tested as a data‐ driven system in which the outcome of different data sets with known classification can be compared. With the fuzzy mapping we know already the outcome for different cue values corresponding to the selected configuration in terms of connections to the different summation blocks in the bottom of Figure 4. Almost certainly a Neural Network‐based mapper would perform better when using specific datasets for training and evaluation. Such datasets typically consists of performances in which performers are asked to express different emotions and corresponding listener data with emotion ratings of the same performances. These data are far from “ideal” in the sense that they contain several noise sources due to coder (performer) and rater (listener) variability. This will also influence a data‐driven algorithm making the classification less distinct. This means that an evaluation using such datasets might give good test results but still results in poor usability in real situations.
One alternative to evaluate the algorithm would be to generate synthesized performances in which each cue is controlled in a systematic fashion. It is rather simple to synthesize music performances (e.g. Juslin et al. 2002) but more elaborate to synthesize body motion. The expressive content of the examples is then rated in listener experiments. Such a data set would allow a more systematic testing of, for example, the three cue levels used and will be considered in future work.
Another alternative is to make a usability test of the complete application asking the users to give feedback. This was done in terms of questionnaires both for Groove Machine and Ghost in the Cave (Lindström et al in press, Rinman forthcoming). These evaluations were in general very positive but were mostly concerned with overall questions such as market potential or cooperation issues. However, one specific question addressed the players’ controllability. In Ghost in the Cave, the team players believed that the main players were efficiently controlling the avatar giving a mean rating of 6.6 on a scale 0‐10 with a confidence interval above 5. In the Groove Machine the audience strongly believed that the dancer was able to control the music
10
giving a mean rating of 7.6 on the same scale (Lindström et al in press). This is an indication that the algorithm works but needs to be further investigated in future research.
Summary and discussion
We describe a system that can be used for analyzing emotional expression both in music and body motion. The use of fuzzy mapping was a way of directly using previous results summarized in various investigations and turned out to be a robust mapping also in practical testing during the development of the applications. The advantages of the model can be summarized as
Generic ‐ it is the same model for music performance and body motion. Robust ‐ The fuzzy set functions always stay between 0 and 1 implying that the emotion output is always between 0 and 1 as well. A collection of cues lowers the error influence from one cue proportionally. Handle nonlinearities – This is not possible in e.g. a linear regression model. Smooth transitions between emotions – This is achieved by the overlapping fuzzy set functions each with transition range. Flexibility – It is easy to change the mapping using for example more cues since there is no need recalibrate the system.
The use of higher‐level expression descriptions such as emotions has the advantage that it can provide a natural coherence between the controller’s expression (visual or auditive) and the expression of the control device (could be a synthesizer, visual effects etc.) in a public performance. Take an example with a dancer controlling the music performance with a one‐to‐one correspondence between high‐level description in the body motion and music performance. When the movements of the dancer are aggressive ‐ the music also sounds aggressive.
One potential disadvantage is that each cue has the same importance in predicting the emotions. From previous research it is obvious that this is not the case in human recognition of emotional expression. It can easily be modeled using different weights for each fuzzy input in the summation block in Figure 4. However, for simplicity and in order minimize the number of parameters this was not used.
The used cue extraction is rather simple and can easily be improved by, for example, using better tone onset detection for the audio analyser. However, due to the averaging effects (both over cues and over time) in the subsequent fuzzy mapper the resulting improvement will be small.
The KTH music performance rules has recently been complemented with a module called pDM that allows real time control of the rule system (Friberg 2005a). Thus, the expressivity of music performance can be dynamically controlled. pDM contains also mappers from emotional expression to the rule parameters. Since the current system analyzes expression in body motion the two could be easily connected yielding a future application that we call a home conductor system. Such a system would allow also laymen to be actively involved in the music process and “conduct” the music directly by gestures (Friberg 2005b).
11
Acknowledgements
This work was supported by the EU project MEGA, IST‐20410. I would like to acknowledge all people involved in the development of the Groove Machine and Ghost in the Cave, including Ambra Succi, Bounce dance company; Sofia Dahl, Marie‐Louise Rinman, Kasper Marklund, KTH; Antonio Camurri, Gualtiero Volpe, Barbara Mazzarino, University of Genova; Ivar Kjellmo, Bendik Bendiksen, Octaga AS, Oslo; Damien Cirotteau, Hugh McCarthy, University of Padova; Erik Lindström, Uppsala University.
References
Boone, R. T. and Cunningham, J. G. (2001). Children’s expression of emotional meaning in music through expressive body movement. Journal of Nonverbal Behavior, 25(1).
Bresin, R. (1998) Artificial Neural Networks Based Models For Automatic Performance of Musical Scores. Journal of New Music Research, 27(3), 239‐270.
Bresin, R. and Juslin, P. N. (2005). Rating Musical Expression With Colors. manuscript in preparation.
Bresin, R., De Poli, G. and Ghetta, R. (1995). A Fuzzy Formulation of KTH Performance Rule System. In J. Sundberg (ed.), Proceedings of the 2nd International Conference on Acoustics and Musical Research (pp. 15‐36), Stockholm, Sweden: Speech, Music and Hearing, KTH.
Camurri A., Hashimoto S., Ricchetti M., Trocca R., Suzuki K., and Volpe G. (2000). EyesWeb – Toward Gesture and Affect Recognition in Interactive Dance and Music Systems. Computer Music Journal, 24(1), 57‐69.
Camurri A., Lagerlöf I., and Volpe G. (2003). Recognizing Emotion from Dance Movement: Comparison of Spectator Recognition and Automated Techniques. International Journal of Human‐Computer Studies, 59(1‐2), 213‐225.
Camurri A., Mazzarino B, and Volpe G. (2004). Analysis of Expressive Gesture: The EyesWeb Expressive Gesture Processing Library. In A. Camurri, G. Volpe (Eds.), Gesture‐based Communication in Human‐Computer Interaction, LNAI 2915, Springer Verlag.
Camurri, A., De Poli, G., Friberg, A., Leman, L., and Volpe, G. (in press). The MEGA project: analysis and synthesis of multisensory expressive gesture in performing art applications. Journal of New Music Research.
Cirotteau D., De Poli G., Mion L., Vidolin A., and Zanon P. (2004). Recognition of musical gestures in known pieces and in improvisations. In A. Camurri, G. Volpe (eds.) Gesture Based Communication in Human‐Computer Interaction, LNAI 2915 (pp. 497‐508), Berlin, Springer Verlag.
Dahl, S. and Friberg, A. (2004). Expressiveness of musicianʹs body movements in performances on marimba. In A. Camurri, G. Volpe (eds.) Gesture‐based Communication in Human‐Computer Interaction, LNAI 2915 (pp.479‐486), Berlin, Springer Verlag.
Friberg, A. (2005a). pDM: an expressive sequencer with real‐time control of the KTH music performance rules. Manuscript submitted for publication.
Friberg, A. (2005b). Home conducting – control the overall musical expression with gestures. Manuscript submitted for publication.
Friberg, A. and Sundberg, J. (1999). Does music performance allude to locomotion? A model of final ritardandi derived from measurements of stopping runners. Journal of the Acoustical Society of America, 105(3), 1469‐1484
12
Friberg, A., Schoonderwaldt, E., and Juslin, P. N. (2005). CUEX: An algorithm for extracting expressive tone variables from audio recordings. Accepted for publication in Acoustica united with Acta Acoustica.
Friberg, A., Schoonderwaldt, E., Juslin, P.N. and Bresin, R. (2002). Automatic Real‐Time Extraction of Musical Expression. In Proceedings of the International Computer Music Conference 2002 (pp. 365‐367), San Francisco: International Computer Music Association,.
Gabrielsson, A., and Lindström, E. (2001). The influence of musical structure on emotional expression. In P. N. Juslin, &amp; J. A. Sloboda (eds.), Music and emotion: Theory and Research (pp. 223‐248). New York: Oxford University Press.
Hunt, A., Kirk, R. and Neighbour, M. (2004). Multiple Media Interfaces for Music Therapy. IEEE Multimedia, 11(3), 50‐58.
Juslin, P. N. (2000). Cue utilization in communication of emotion in music performance: Relating performance to perception. Journal of Experimental Psychology: Human Perception and Performance, 26, 1797‐1813.
Juslin, P. N. (2001). Communication of emotion in music performance: A review and a theoretical framework. In P. N. Juslin &amp; J. A. Sloboda (eds.), Music and emotion: Theory and research (pp. 309‐337). New York: Oxford University Press.
Juslin, P. N. and Laukka, J. (2003). Communication of Emotions in Vocal Expression and Music Performance: Different Channels, Same Code? Psychological Bulletin, 129(5), 770‐814.
Juslin, P. N. and Sloboda J. A., (Eds.) (2001). Music and Emotion: Theory and Research, Oxford: Oxford University Press.
Juslin, P. N., Friberg, A., and Bresin, R. (2002). Toward a computational model of expression in performance: The GERM model. Musicae Scientiae special issue 2001‐2002, 63‐122.
Lindström, E., Camurri, A., Friberg, A., Volpe G. and Rinman, M.‐L. (in press). Affect, attitude and evaluation of multi‐sensory performances. Journal of New Music Research.
Mion, L. (2003) Application of Bayesian Networks to automatic recognition of expressive content of piano improvisations. In R. Bresin (ed.) Proceedings of the Stockholm Music Acoustics Conference 2003, Vol II (pp. 557‐560).
Moeslund, T. B. and Granum, E. (2001). A Survey of Computer Vision‐Based Human Motion Capture. Computer Vision and Image Understanding, 81, 231‐268.
Niskanen, V. A. (2004). Soft Computing Methods in Human Sciences, Springer, Berlin. Puckette, M. (1996). Pure Data. Proceedings of the 1996 International Computer Music Conference
(pp. 269‐272), San Francisco: International Computer music Association.
Rinman, M.‐L., (forthcoming). Designing game, play and interaction in location‐based game spaces. Doctoral thesis, KTH.
Rinman, M.‐L., Friberg, A., Bendiksen, B., Cirotteau, D., Dahl, S., Kjellmo, I., Mazzarino, B., and Camurri, A. (2004). Ghost in the Cave ‐ an interactive collaborative game using non‐ verbal communication. In A. Camurri, and G. Volpe (eds.), Gesture‐based Communication in Human‐Computer Interaction, LNAI 2915 (pp. 549–556), Berlin: Springer Verlag.
Seif El‐Nasr, M., Yen, J. and Iorger, T. R. (2000). FLAME – Fuzzy logic adaptive mode of emotions. Autonomous Agents and Multi‐Agent Systems, 3, 219‐257.
Wanderley, M. and Battier, M. (eds.) (2000). Trends in Gestural Control of Music. IRCAM ‐ Centre Pompidou.
Zimmerman, H.‐J. (1996). Fuzzy set theory – and its applications, 3rd ed., Kluwer, Boston.
13</Text>
        </Document>
        <Document ID="440">
            <Title>Supervisors - Transfer</Title>
        </Document>
        <Document ID="29">
            <Title>Topic, Scope and Subject Matter</Title>
            <Text>Topic, Scope and Subject Matter

I aim to explore the related contexts of digital puppetry, real-time animation, mimetic and non-mimetic kinetic objects, automata, 'cybernetic sculpture', performance systems and the technological interfaces to such phenomena. 

I aim to create performances that use original software and hardware systems that are designed to explore 'performance expressivity', with reference to relevant historical, art, entertainment and technological precedents.

I wish to theorise and form a taxonomy of 'expressivity' in relationship to digital domains, ideas familiar in contexts that relate to physical performance and, particularly musical performance. By ‘performance expressivity’, I refer to different domains of 'expressivity' including:

- Somatic / whole body
- Face / Mouth / Eyes
- Breath / Voice - prosody and semantics
- Gestural
- Kinetics - qualities and direction of movement
- Sonic / Musical - vibrato, timbre, timing, rhythm and dynamics
- Linguistic / Semantic
- Simplified / Complex Expressive Systems
- Compound Expressive Behaviour  

I will make systematic observations of the interplay between performer and object, audience and object, player/user and virtual object where dynamics surrounding the projection of human qualities [anthropomorphism], emoting and expressive relationships are formed. 'Object' is used to avoid the figurative and historical overtones of the 'puppet'. To date, my focus has been on voice and speaking objects.

Contexts, Concepts and Case Studies

In more straight-forward language - here are some examples, genres and potential case studies, illustrative of concepts and themes that need teasing apart:

I am interested in how the incorporation of the performers body, face, or arms 'inside' the performing object compare and contrast with the automated, remote, telematic, disembodied modes of control and how this relationship affects/effects expressive, improvisational performance. I will explore the animated, articulated, mechanical qualities of digital puppets next to sculptural, still, reified objects. Defining the scope of digital puppetry will be an important part of the work - currently my definition is inclusive and involves a range of genres and examples, from animation, animatronics, toys to avatars.

Cybernetic Sculpture 
Edward Ihnatowicz - SAM (Sound Activated Mobile) in 'Cybernetic Serendipity' and The Senster http://www.senster.com/ ;
The Work of Ken Feingold - http://www.kenfeingold.com/ ;
Ars Electronica - Corebounce's Digital Marionette (http://www.corebounce.org/wiki/Projects/Marionette) ;

Object / Puppet Theatre
Robot Performance and 'robots in art' - e.g. Louis-Philippe Demers, Valère Novarina's "Theatre of the Ears"
Embodiment - the physical and virtual relationship between performer and performing object.
SmartLab's VIP project;

Artificial Creatures
Affective Computing and Mimetic Approaches to Agents, Objects and Robots
	- Virtual Pets.
	- Virtual Companions
	- Talking Toys
	- Maverick machines - analogue computers and automata - autonomous mechanical agents (e.g. Theo Jansen's 'Beach Creatures')

Art and Performance Practice
"Blendie" and the 'Machine Therapy' of Kelly Dobson, MIT;
 Billie Whitelaw as "Mouth" in Samuel Becketts 'Not I';

Animation
Luxo, the angle poise lamp in Pixar's studios "Luxo Jr" (1986);
Animatronics; Special Effects in Entertainment and Film

Games
Expressivity and Avatars / Character Design and Control for Emoting and Performance in Games and Other Virtual 3D Environments;
'Sackboy' - in the forthcoming Media Molecule (Sony PS3) game "Little Big Planet" has a simple, but beautiful, gestural and 'emoting' capability controlled via a standard game pad.

HCI
Performer Flow, Spontaneity and Improvisation;
Gestural Interfaces, facial and other approaches to performance capture;
Interface Expressivity: and the 'New Interfaces for Musical Expression' groups;
Interfaces for Performance Motion Capture / Face Capture;
Collaborative interaction in the control of expressive objects (co-present in a space and networked);
Translations and calibration - e.g. sound to motion, voice to motion, brain-waves to motion and vica versa;
Automated computer control of 'performance expressivity' via motion analysis, gestural interfaces and computer vision;
Human computer control systems - gloves, haptic devices, experimental interfaces e.g. IBVA Brain wave detection - (http://www.ibva.com/);
Animation Software Metaphors - Pixar's Marionette - animation design and control for expressivity.

Some Emerging Questions

What happens to spontaneity and improvisation when real-time performance capture technologies are used?
In what ways do current interfaces shape / enable / inhibit performer 'expressivity'?
What opportunities will the (re)combination of existing techniques and cutting edge emerging interactive technologies provide for the performer, object and audience?
</Text>
        </Document>
    </Documents>
</SearchIndexes>